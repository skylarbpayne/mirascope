{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mirascope","text":"<p>LLM abstractions that aren't obstructions.</p> <p>Mirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs through a unified interface that works across various supported providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, Vertex AI, and Bedrock.</p> <p>Whether you're generating text, extracting structured information, or developing complex AI-driven agent systems, Mirascope provides the tools you need to streamline your development process and create powerful, robust applications.</p>"},{"location":"#30-second-quickstart","title":"30 Second Quickstart","text":"<p>Install Mirascope, specifying the provider(s) you intend to use, and set your API key:</p> MacOS / LinuxWindows OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>pip install \"mirascope[openai]\"\n\nexport OPENAI_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[anthropic]\"\n\nexport ANTHROPIC_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[mistral]\"\n\nexport MISTRAL_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[gemini]\"\n\nexport GOOGLE_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[groq]\"\n\nexport GROQ_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[cohere]\"\n\nexport CO_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[litellm]\"\n\nexport OPENAI_API_KEY=XXXXX  # set keys for providers you will use\n</code></pre> <pre><code>pip install \"mirascope[azure]\"\n\nexport AZURE_INFERENCE_ENDPOINT=XXXXX\nexport AZURE_INFERENCE_CREDENTIAL=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[vertex]\"\n\ngcloud init\ngcloud auth application-default login\n</code></pre> <pre><code>pip install \"mirascope[bedrock]\"\n\naws configure\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>pip install \"mirascope[openai]\"\n\nset OPENAI_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[anthropic]\"\n\nset ANTHROPIC_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[mistral]\"\n\nset MISTRAL_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[gemini]\"\n\nset GOOGLE_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[groq]\"\n\nset GROQ_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[cohere]\"\n\nset CO_API_KEY=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[litellm]\"\n\nset OPENAI_API_KEY=XXXXX  # set keys for providers you will use\n</code></pre> <pre><code>pip install \"mirascope[azure]\"\n\nset AZURE_INFERENCE_ENDPOINT=XXXXX\nset AZURE_INFERENCE_CREDENTIAL=XXXXX\n</code></pre> <pre><code>pip install \"mirascope[vertex]\"\n\ngcloud init\ngcloud auth application-default login\n</code></pre> <pre><code>pip install \"mirascope[bedrock]\"\n\naws configure\n</code></pre> <p>Make your first call to an LLM to recommend a book for a given genre:</p> <p>Mirascope</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> Official SDK OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\n\ndef recommend_book(genre: str) -&gt; str:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n    )\n    return str(completion.choices[0].message.content)\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from anthropic import Anthropic\n\nclient = Anthropic()\n\n\ndef recommend_book(genre: str) -&gt; str:\n    message = client.messages.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n        max_tokens=1024,\n    )\n    block = message.content[0]\n    return block.text if block.type == \"text\" else \"\"\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from mistralai.client import MistralClient\n\nclient = MistralClient()\n\n\ndef recommend_book(genre: str) -&gt; str:\n    completion = client.chat(\n        model=\"mistral-large-latest\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n    )\n    return completion.choices[0].message.content\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\n\nclient = GenerativeModel(\"gemini-1.5-flash\")\n\n\ndef recommend_book(genre: str) -&gt; str:\n    generation = client.generate_content(\n        contents=[{\"role\": \"user\", \"parts\": f\"Recommend a {genre} book\"}]  # pyright: ignore [reportArgumentType]\n    )\n    return generation.candidates[0].content.parts[0].text\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from groq import Groq\n\nclient = Groq()\n\n\ndef recommend_book(genre: str) -&gt; str:\n    completion = client.chat.completions.create(\n        model=\"llama-3.1-70b-versatile\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n    )\n    return str(completion.choices[0].message.content)\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from cohere import Client\n\nclient = Client()\n\n\ndef recommend_book(genre: str) -&gt; str:\n    response = client.chat(\n        model=\"command-r-plus\",\n        message=f\"Recommend a {genre} book\",\n    )\n    return response.text\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from litellm import completion\n\n\ndef recommend_book(genre: str) -&gt; str:\n    response = completion(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n    )\n    return str(response.choices[0].message.content)  # type: ignore\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import ChatRequestMessage\nfrom azure.core.credentials import AzureKeyCredential\n\nclient = ChatCompletionsClient(\n    endpoint=\"YOUR_ENDPOINT\", credential=AzureKeyCredential(\"YOUR_KEY\")\n)\n\n\ndef recommend_book(genre: str) -&gt; str:\n    completion = client.complete(\n        model=\"gpt-4o-mini\",\n        messages=[\n            ChatRequestMessage({\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"})\n        ],\n    )\n    message = completion.choices[0].message\n    return message.content if message.content is not None else \"\"\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from vertexai.generative_models import GenerativeModel\n\nclient = GenerativeModel(\"gemini-1.5-flash\")\n\n\ndef recommend_book(genre: str) -&gt; str:\n    generation = client.generate_content(\n        contents=[{\"role\": \"user\", \"parts\": f\"Recommend a {genre} book\"}]\n    )\n    return generation.candidates[0].content.parts[0].text  # type: ignore\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>import boto3\n\nbedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n\n\ndef recommend_book(genre: str) -&gt; str:\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"Recommend a {genre} book\"}]}]\n    response = bedrock_client.converse(\n        modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",\n        messages=messages,\n        inferenceConfig={\"maxTokens\": 1024},\n    )\n    output_message = response[\"output\"][\"message\"]\n    content = \"\"\n    for content_piece in output_message[\"content\"]:\n        if \"text\" in content_piece:\n            content += content_piece[\"text\"]\n    return content\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre>"},{"location":"#choose-your-path","title":"Choose Your Path","text":""},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li> <p> Quickstart Guide</p> <p>Comprehensive overview of core features and building blocks</p> <p> Quickstart</p> </li> <li> <p> Structured Outputs</p> <p>Explore various techniques for generating structured outputs</p> <p> Structured Outputs</p> </li> <li> <p> Dynamic Configuration &amp; Chaining</p> <p>Examples ranging from basic usage to more complex chaining techniques</p> <p> Dynamic Configuration &amp; Chaining</p> </li> <li> <p> Tools &amp; Agents</p> <p>Learn how to define tools for your LLM to build advanced AI agents</p> <p> Tools &amp; Agents</p> </li> </ul>"},{"location":"#dive-deeper","title":"Dive Deeper","text":"<ul> <li> <p> Learn</p> <p>In-depth exploration of Mirascope's many features and capabilities</p> <p> Learn</p> </li> <li> <p> Tutorials</p> <p>Advanced usage patterns and real-world applications</p> <p> Tutorials</p> </li> <li> <p> Integrations</p> <p>Integrations with third-party tools for enhanced usage</p> <p> Integrations</p> </li> <li> <p> API Reference</p> <p>Detailed information on classes and functions</p> <p> Reference</p> </li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<p> Why Use Mirascope  Join Our Community  Star the Repo </p> <p>We're excited to see what you'll build with Mirascope, and we're here to help! Don't hesitate to reach out :)</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<p>We use uv as our package and dependency manager.</p>"},{"location":"CONTRIBUTING/#installation","title":"Installation","text":"<p>First, install <code>uv</code> using the official method:</p> <p>macOS and Linux</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Windows</p> <pre><code>powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <p>For more detailed instructions, refer to the official uv setup guide.</p>"},{"location":"CONTRIBUTING/#create-a-virtual-environment","title":"Create a Virtual Environment","text":"<p>After installing <code>uv</code>, create a virtual environment for development by running:</p> <pre><code>uv sync --all-extras --dev\n</code></pre>"},{"location":"CONTRIBUTING/#pre-commit-setup","title":"Pre-commit Setup","text":"<p>To set up pre-commit hooks, run the following command:</p> <pre><code>uv run pre-commit install --install-hooks\n</code></pre> <p>This will ensure that your code is automatically checked and formatted before each commit.</p>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Search through existing GitHub Issues to see if what you want to work on has already been added.</p> <ul> <li>If not, please create a new issue. This will help to reduce duplicated work.</li> </ul> </li> <li> <p>For first-time contributors, visit https://github.com/mirascope/mirascope and \"Fork\" the repository (see the button in the top right corner).</p> <ul> <li> <p>You'll need to set up SSH authentication.</p> </li> <li> <p>Clone the forked project and point it to the main project:</p> </li> </ul> <pre><code>git clone https://github.com/&lt;your-username&gt;/mirascope.git\ngit remote add upstream https://github.com/Mirascope/mirascope.git\n</code></pre> </li> <li> <p>Development.</p> <ul> <li>Make sure you are in sync with the main repo:</li> </ul> <pre><code># for anything that only requires a fix version bump (e.g. bug fixes)\ngit checkout main\ngit pull upstream main\n\n# for anything that is \"new\" and requires at least a minor version bump\ngit checkout release/vX.Y  # replace X with the current major version and Y with the next minor version\ngit pull upstream release/vX.Y\n</code></pre> <ul> <li>Create a <code>git</code> feature branch with a meaningful name where you will add your contributions.</li> </ul> <pre><code>git checkout -b meaningful-branch-name\n</code></pre> <ul> <li>Start coding! commit your changes locally as you work:</li> </ul> <pre><code>git add mirascope/modified_file.py tests/test_modified_file.py\ngit commit -m \"feat: specific description of changes contained in commit\"\n</code></pre> <ul> <li>Format your code!</li> </ul> <pre><code>uv run ruff format .\n</code></pre> <ul> <li>Lint and test your code! From the base directory, run:</li> </ul> <pre><code>uv run ruff check .\nuv run pyright .\n</code></pre> </li> <li> <p>Test!</p> <ul> <li>Add tests. Tests should be mirrored based on structure of the source.</li> </ul> <pre><code>| - mirascope\n|  | - core\n|  |  | - openai\n|  |  |  | - ...\n| - tests\n|  | - core\n|  |  | - openai\n|  |  |  | - ...\n</code></pre> <ul> <li>Run tests to make sure nothing is broken</li> </ul> <pre><code>uv run pytest tests/\n</code></pre> <ul> <li>Check coverage report</li> </ul> <pre><code>uv run pytest tests/ --cov=./ --cov-report=html\n</code></pre> </li> <li> <p>Contributions are submitted through GitHub Pull Requests</p> <ul> <li>When you are ready to submit your contribution for review, push your branch:</li> </ul> <pre><code>git push origin meaningful-branch-name\n</code></pre> <ul> <li>Open the printed URL to open a PR.</li> <li>Fill in a detailed title and description.</li> <li>Check box to allow edits from maintainers</li> <li>Submit your PR for review. You can do this via Contribute in your fork repo.</li> <li>Link the issue you selected or created under \"Development\"</li> <li>We will review your contribution and add any comments to the PR. Commit any updates you make in response to comments and push them to the branch (they will be automatically included in the PR)</li> </ul> </li> </ol>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Please conform to the Conventional Commits specification for all PR titles and commits.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>All changes to the codebase must be properly unit tested. If a change requires updating an existing unit test, make sure to think through if the change is breaking.</p> <p>We use <code>pytest</code> as our testing framework. If you haven't worked with it before, take a look at their docs.</p> <p>Furthermore, we have a full coverage requirement, so all incoming code must have 100% coverage. This policy ensures that every line of code is run in our tests. However, while achieving full coverage is essential, it is not sufficient on its own. Coverage metrics ensure code execution but do not guarantee correctness under all conditions. Make sure to stress test beyond coverage to reduce bugs.</p> <p>We use a Codecov dashboard to monitor and track our coverage.</p>"},{"location":"CONTRIBUTING/#formatting-and-linting","title":"Formatting and Linting","text":"<p>In an effort to keep the codebase clean and easy to work with, we use <code>ruff</code> for formatting and both <code>ruff</code> and <code>pyright</code> for linting. Before sending any PR for review, make sure to run both <code>ruff</code> and <code>pyright</code>.</p> <p>If you are using VS Code, then install the extensions in <code>.vscode/extensions.json</code> and the workspace settings should automatically run <code>ruff</code> formatting on save and show <code>ruff</code> and <code>pyright</code> errors.</p>"},{"location":"HELP/","title":"Help","text":""},{"location":"HELP/#getting-help-with-mirascope","title":"Getting help with Mirascope","text":"<p>If you need help getting started with Mirascope or with advanced usage, the following sources may be useful.</p>"},{"location":"HELP/#slack","title":"Slack","text":"<p>The Mirascope Slack is a great place to ask questions and get help and chat about Mirascope.</p>"},{"location":"HELP/#usage-documentation","title":"Usage Documentation","text":"<p>The Learn documentation is the most complete guide on how to get started with Mirascope.</p>"},{"location":"HELP/#sdk-api-documentation","title":"SDK API Documentation","text":"<p>The API Reference give reference docs for the Mirascope library, auto-generated directly from the code so it's always up-to-date.</p>"},{"location":"HELP/#github-issues","title":"GitHub Issues","text":"<p>The Mirascope GitHub Issues are a great place to ask questions and give us feedback.</p>"},{"location":"HELP/#email","title":"Email","text":"<p>You can also email us at support@mirascope.io or feedback@mirascope.io.</p>"},{"location":"HELP/#how-you-can-help-mirascope","title":"How you can help Mirascope","text":""},{"location":"HELP/#star-mirascope-on-github","title":"Star Mirascope on GitHub","text":"<p>\u2b50\ufe0f You can \"star\" Mirascope on GitHub \u2b50\ufe0f</p>"},{"location":"HELP/#connect-with-the-authors","title":"Connect with the authors","text":"<ul> <li>Follow us on GitHub<ul> <li>See other related Open Source projects that might help you with machine learning</li> </ul> </li> <li>Follow William Bakst on Twitter/X<ul> <li>Tell me how you use mirascope</li> <li>Hear about new announcements or releases</li> </ul> </li> <li>Connect with William Bakst on LinkedIn<ul> <li>Give me any feedback or suggestions about what we're building</li> </ul> </li> </ul>"},{"location":"HELP/#post-about-mirascope","title":"Post about Mirascope","text":"<ul> <li>Twitter, Reddit, Hackernews, LinkedIn, and others.</li> <li>We love to hear about how Mirascope has helped you and how you are using it.</li> </ul>"},{"location":"HELP/#help-others","title":"Help Others","text":"<p>We are a kind and welcoming community that encourages you to help others with their questions on GitHub Issues or in our Slack Community.</p> <ul> <li>Guide for asking questions<ul> <li>First, search through issues and discussions to see if others have faced similar issues</li> <li>Be as specific as possible, add minimal reproducible example</li> <li>List out things you have tried, errors, etc</li> <li>Close the issue if your question has been successfully answered</li> </ul> </li> <li>Guide for answering questions<ul> <li>Understand the question, ask clarifying questions</li> <li>If there is sample code, reproduce the issue with code given by original poster</li> <li>Give them solution or possibly an alternative that might be better than what original poster is trying to do</li> <li>Ask original poster to close the issue</li> </ul> </li> </ul>"},{"location":"HELP/#review-pull-requests","title":"Review Pull Requests","text":"<p>You are encouraged to review any pull requests. Here is a guideline on how to review a pull request:</p> <ul> <li>Understand the problem the pull request is trying to solve</li> <li>Ask clarification questions to determine whether the pull request belongs in the package</li> <li>Check the code, run it locally, see if it solves the problem described by the pull request</li> <li>Add a comment with screenshots or accompanying code to verify that you have tested it</li> <li>Check for tests<ul> <li>Request the original poster to add tests if they do not exist</li> <li>Check that tests fail before the PR and succeed after</li> </ul> </li> <li>This will greatly speed up the review process for a PR and will ultimately make Mirascope a better package</li> </ul>"},{"location":"MIGRATE/","title":"Migration Guide: v0 \u2192 v1","text":"<p>This Guide May Not Cover Everything</p> <p>We have tried our best to cover everything that may be involved in a migration from v0 to v1, but there is always a possibility that we missed something. Please bear with us as we work with you to help you get migrated.</p> <p>If there is anything missing from this guide or anything that leaves you confused, let us know!</p> <p>We're here to help make sure your migration can progress smoothly :)</p> <p>We're extremely excited about the changes in v1 and want to make sure that you are set up to be equally excited.</p> <p>Mirascope v1 introduces significant changes to improve developer experience, enhance flexibility, and provide more powerful features for working with Large Language Models (LLMs). This guide will help you migrate your existing Mirascope v0 code to v1, highlighting key differences and new features.</p>"},{"location":"MIGRATE/#understanding-the-shift-from-v0-to-v1","title":"Understanding the Shift from v0 to v1","text":""},{"location":"MIGRATE/#why-decorators-instead-of-classes","title":"Why Decorators Instead of Classes?","text":"<p>The transition from a class-based approach in v0 to a decorator-based approach in v1 represents a fundamental shift in how Mirascope handles LLM API calls. This change was driven by several key considerations:</p> <ol> <li> <p>Stateless Nature of LLM API Calls:    LLM API calls are inherently stateless. The class-based approach in v0, which introduced fields as state, didn't align well with this stateless nature. Decorators better represent the functional, stateless character of these API interactions.</p> </li> <li> <p>Performance Improvements:    Creating a class instance for every call introduced unnecessary overhead. Functions, modified by decorators at runtime, provide a more lightweight and faster execution model.</p> </li> <li> <p>Functional Programming Paradigm:    The move to a more functional approach allows for greater flexibility and composability. It enables features like dynamic configuration, which were challenging to implement with the class-based model.</p> </li> <li> <p>Easier Integration with Other Libraries:    Many Python libraries use decorators. By adopting a decorator-based approach, Mirascope v1 seamlessly integrates with these libraries. For example, you can now use libraries like <code>tenacity</code> for retry logic without any explicit integration \u2013 it just works.</p> </li> </ol>"},{"location":"MIGRATE/#benefits-of-the-new-approach","title":"Benefits of the New Approach","text":"<ol> <li> <p>Simplified Code:    Instead of defining a class for each LLM call, you can now use a simple function with a decorator. This results in more concise and readable code.</p> </li> <li> <p>Enhanced Flexibility:    The decorator approach allows for more dynamic behavior, such as easily changing call parameters or prompt templates at runtime.</p> </li> <li> <p>Improved Performance:    By eliminating the need to construct class instances, the new approach offers better performance, especially for applications making frequent LLM calls.</p> </li> <li> <p>Better Alignment with Python Ecosystem:    The decorator pattern is widely used in Python, making Mirascope v1 feel more native to experienced Python developers.</p> </li> </ol> <p>The v1 approach is more concise, directly represents the stateless nature of the API call, and allows for easier dynamic usage and integration with other Python libraries.</p> <p>By embracing this new paradigm, Mirascope v1 offers a more pythonic, flexible, and powerful way to interact with LLMs, setting the stage for more advanced features and integrations in the future.</p>"},{"location":"MIGRATE/#core-changes","title":"Core Changes","text":"<p>The most significant change in v1 is the shift from class-based to decorator-based calls. To illustrate the difference, let's take a look at various comparison examples between the two versions.</p>"},{"location":"MIGRATE/#from-classes-to-decorators","title":"From Classes to Decorators","text":"<p>Before (v0):</p> <pre><code>from mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nprint(response.content)\n</code></pre> <p>After (v1):</p> <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre>"},{"location":"MIGRATE/#baseprompt","title":"<code>BasePrompt</code>","text":"<p>The <code>BasePrompt</code> class still operates in the same way as v0 calls (e.g. <code>OpenAICall</code>). The primary difference is that you run the prompt using the <code>run</code> method instead of using the <code>call</code> or <code>stream</code> methods. This method can be run against any provider's call decorator:</p> <pre><code>from mirascope.core import BasePrompt, openai, prompt_template\n\n\n@prompt_template(\"Recommend a {genre} book\")\nclass BookRecommendationPrompt(BasePrompt):\n    genre: str\n\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\nresponse = prompt.run(openai.call(\"gpt-4o-mini\"))\nprint(response.content)\n</code></pre> <p>Of course, there's nothing stopping you from replicating the original v0 functionality of <code>OpenAICall</code> by writing your own <code>call</code> method (or whatever you'd like to name it):</p> <pre><code>from mirascope.core import BasePrompt, openai, prompt_template\n\n\n@prompt_template(\"Recommend a {genre} book\")\nclass BookRecommender(BasePrompt):\n    genre: str\n\n    def call(self) -&gt; openai.OpenAICallResponse:\n        return self.run(openai.call(\"gpt-4o-mini\"))\n\n    def stream(self) -&gt; openai.OpenAIStream:\n        return self.run(openai.call(\"gpt-4o-mini\", stream=True))\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nprint(response.content)\n\nstream = recommender.stream()\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"MIGRATE/#statefulness","title":"Statefulness","text":"<p>Some people may feel that \"statelessness\" is actually an inherent problem with LLM API calls. We agree with this sentiment, and it's the reason for the original design in v0. However, as we've continued to build we've come to believe that adding such state to the abstraction for making the LLM call itself is the wrong place for that state to live. Instead, the state should live in a class that wraps the call, and the call should have easy access to said state.</p> <p>This provides a far clearer sense of what is \"state\" and what is an \"argument\" of the call. Consider the following example:</p> <pre><code>from mirascope.core import BaseMessageParam, openai, prompt_template\nfrom pydantic import BaseModel, computed_field\n\n\nclass Librarian(BaseModel):\n    genre: str\n\n    @openai.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian. You specialize in the {self.genre} genre\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def call(self, query: str): ...\n\n    @computed_field\n    @property\n    def history(self) -&gt; list[BaseMessageParam | openai.OpenAIMessageParam]:\n        \"\"\"Returns dummy history for demonstration purposes\"\"\"\n        return [\n            {\"role\": \"user\", \"content\": \"What book should I read?\"},\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Do you like fantasy books?\",\n            },\n        ]\n\n\nfantasy_librarian = Librarian(genre=\"fantasy\")\nresponse = fantasy_librarian.call(\"I do like fantasy books!\")\nprint(response.content)\n</code></pre> <p>It's evident that <code>genre</code> and <code>history</code> are state of the <code>Librarian</code> class, and the call method uses this state for every call. However, we've also introduced the <code>query</code> argument of the call, which is clearly not state and should be provided for every call.</p>"},{"location":"MIGRATE/#async-calls","title":"Async Calls","text":"<p>Before (v0):</p> <pre><code>import asyncio\n\nfrom mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = asyncio.run(recommender.call_async())\nprint(response.content)\n</code></pre> <p>After (v1):</p> <pre><code>import asyncio\n\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nresponse = asyncio.run(recommend_book(\"fantasy\"))\nprint(response.content)\n</code></pre>"},{"location":"MIGRATE/#streaming","title":"Streaming","text":"<p>Before (v0):</p> <pre><code>from mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nstream = recommender.stream()\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>After (v1):</p> <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"MIGRATE/#tools-function-calling","title":"Tools (Function Calling)","text":"<p>Before (v0):</p> <pre><code>from mirascope.openai import OpenAICall, OpenAICallParams, OpenAITool\n\n\nclass FormatBook(OpenAITool):\n    title: str\n    author: str\n\n    def call(self):\n        return f\"{self.title} by {self.author}\"\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n    call_params = OpenAICallParams(tools=[FormatBook], tool_choice=\"required\")\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <p>After (v1):</p> <pre><code>from mirascope.core import BaseTool, openai, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    def call(self):\n        return f\"{self.title} by {self.author}\"\n\n\n@openai.call(\n    \"gpt-4o-mini\",\n    tools=[FormatBook],\n    call_params={\"tool_choice\": \"required\"},\n)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <p>Function Tools</p> <p>If you were using functions as tools, these can still be used as tools in the same way. The only difference is how you supply the tools, which is through the <code>tools</code> argument of the <code>call</code> decorator in v1.</p>"},{"location":"MIGRATE/#streaming-tools","title":"Streaming Tools","text":"<p>Before (v0):</p> <pre><code>from mirascope.openai import OpenAICall, OpenAICallParams, OpenAIToolStream\n\n\ndef format_book(title: str, author: str):\n    \"\"\"Returns a formatted book string.\"\"\"\n    return f\"{title} by {author}\"\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend two (2) {genre} books.\"\n    genre: str\n\n    call_params = OpenAICallParams(tools=[format_book], tool_choice=\"required\")\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\ntool_stream = OpenAIToolStream.from_stream(recommender.stream())\nfor tool in tool_stream:\n    print(tool.call())\n</code></pre> <p>After (v1):</p> <pre><code>from mirascope.core import openai, prompt_template\n\n\ndef format_book(title: str, author: str):\n    # docstring no longer required, but still used if supplied\n    return f\"{title} by {author}\"\n\n\n@openai.call(\n    \"gpt-4o-mini\",\n    stream=True,\n    tools=[format_book],\n    call_params={\"tool_choice\": \"required\"},\n)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend two (2) {genre} books\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"MIGRATE/#extracting-structured-information","title":"Extracting Structured Information","text":"<p>Before (v0):</p> <pre><code>from mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookExtractor(OpenAIExtractor[Book]):\n    extract_schema: type[Book] = Book\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\n\nextractor = BookExtractor(genre=\"fantasy\")\nbook = extractor.extract()\nassert isinstance(book, Book)\nprint(book)\n</code></pre> <p>After (v1):</p> <pre><code>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nbook = recommend_book(\"fantasy\")\nassert isinstance(book, Book)\nprint(book)\n</code></pre>"},{"location":"MIGRATE/#json-mode","title":"JSON Mode","text":"<p>Before (v0):</p> <pre><code>from mirascope.openai import OpenAICallParams, OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookExtractor(OpenAIExtractor[Book]):\n    extract_schema: type[Book] = Book\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\n    call_params = OpenAICallParams(response_format={\"type\": \"json_object\"})\n\n\nextractor = BookExtractor(genre=\"fantasy\")\nbook = extractor.extract()\nprint(book)\n</code></pre> <p>After (v1):</p> <pre><code>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nbook = recommend_book(\"fantasy\")\nprint(book)\n</code></pre>"},{"location":"MIGRATE/#streaming-structured-information","title":"Streaming Structured Information","text":"<p>Before (v0)</p> <pre><code>from mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookExtractor(OpenAIExtractor[Book]):\n    extract_schema: type[Book] = Book\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n\nextractor = BookExtractor(genre=\"fantasy\")\nbook_stream = extractor.stream()\nfor partial_book in book_stream:\n    print(partial_book)\n</code></pre> <p>After (v1)</p> <pre><code>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", stream=True, response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nbook_stream = recommend_book(\"fantasy\")\nfor partial_book in book_stream:\n    print(partial_book)\n</code></pre>"},{"location":"MIGRATE/#dynamic-variables-and-chaining","title":"Dynamic Variables and Chaining","text":"<p>Before (v0):</p> <pre><code>from mirascope.openai import OpenAICall\nfrom pydantic import computed_field\n\n\nclass AuthorRecommender(OpenAICall):\n    prompt_template = \"\"\"\n    Recommend an author that writes the best {genre} books.\n    Give me just their name.\n    \"\"\"\n\n    genre: str\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book written by {author}\"\n\n    genre: str\n\n    @computed_field\n    @property\n    def author(self) -&gt; str:\n        return AuthorRecommender(genre=self.genre).call().content\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nprint(response.content)\n</code></pre> <p>After (v1):</p> <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Recommend an author that writes the best {genre} books.\n    Give me just their name.\n    \"\"\"\n)\ndef recommend_author(genre: str):\n    ...\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book written by {author}\")\ndef recommend_book(genre: str) -&gt; openai.OpenAIDynamicConfig:\n    return {\"computed_fields\": {\"author\": recommend_author(genre)}}\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\nprint(response.fn_args[\"author\"])\n</code></pre>"},{"location":"MIGRATE/#dumping-call-information","title":"Dumping Call Information","text":"<p>Before (v0):</p> <pre><code>from mirascope.openai import OpenAICall\n\n\nclass BookRecommender(OpenAICall):\n    prompt_template = \"Recommend a {genre} book.\"\n    genre: str\n\n\nrecommender = BookRecommender(genre=\"fantasy\")\nresponse = recommender.call()\nprint(response.dump())\n</code></pre> <p>After (v1):</p> <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.model_dump())\n</code></pre>"},{"location":"MIGRATE/#multimodal-capabilities","title":"Multimodal Capabilities","text":"<p>Before (v0):</p> <pre><code>from mirascope.openai import OpenAICall\nfrom openai.types.chat import ChatCompletionMessageParam\n\n\nclass ImageDetection(OpenAICall):\n    def messages(self) -&gt; list[ChatCompletionMessageParam]:\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"I just read this book: \"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": \"https://upload.wikimedia.org/wikipedia/en/4/44/Mistborn-cover.jpg\",\n                        },\n                    },\n                    {\"type\": \"text\", \"text\": \"What should I read next?\"},\n                ],\n            },\n        ]\n\n\nresponse = ImageDetection().call()\nprint(response.content)\n</code></pre> <p>After (v1):</p> <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    I just read this book: {previous_book:image}.\n    What should I read next?\n    \"\"\"\n)\ndef recommend_book(previous_book: str): ...\n\n\nresponse = recommend_book(\n    \"https://upload.wikimedia.org/wikipedia/en/4/44/Mistborn-cover.jpg\"\n)\nprint(response.content)\n</code></pre>"},{"location":"MIGRATE/#fastapi-integration","title":"FastAPI Integration","text":"<p>Before (v0):</p> <pre><code>from fastapi import FastAPI\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass BookRecommender(OpenAIExtractor[Book]):\n    extract_schema: type[Book] = Book\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\n\n@app.get(\"/recommend_book\")\ndef recommend_book(genre: str):\n    recommender = BookRecommender(genre=genre)\n    return recommender.extract()\n</code></pre> <p>After (v1):</p> <pre><code>from fastapi import FastAPI\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@app.get(\"/recommend_book\")\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <p>Migrating to Mirascope v1 offers a more streamlined and flexible approach to working with LLMs. The new decorator-based syntax simplifies code structure and makes it easier to implement advanced features like streaming, tools, and structured information extraction. </p> <p>Remember to update your import statements to use <code>from mirascope.core import ...</code> instead of the provider-specific imports used in v0. Also, be sure to familiarize yourself with the updated recommendations for writing prompts in the more functional form we now recommend.</p> <p>If you encounter any issues during migration or have questions about the new features, please refer to the Learn documentation or reach out to the Mirascope community for support.</p>"},{"location":"WHY/","title":"Why Use Mirascope?","text":""},{"location":"WHY/#abstractions-that-arent-obstructions","title":"Abstractions That Aren't Obstructions","text":"<p>Mirascope provides powerful abstractions that simplify LLM interactions without hiding the underlying mechanics. This approach gives you the convenience of high-level APIs while maintaining full control and transparency.</p>"},{"location":"WHY/#everything-beyond-the-prompt-is-boilerplate","title":"Everything Beyond The Prompt Is Boilerplate","text":"<p>By eliminating boilerplate, Mirascope allows you to focus on what matter most: your prompt.</p> <p>Let's compare structured outputs using Mirascope vs. the official SDKs:</p> <p>Mirascope</p> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>Official SDK</p> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = OpenAI()\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Extract {text}\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"Book\",\n                    \"description\": \"An extracted book.\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"author\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"title\", \"author\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n        tool_choice=\"required\",\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        return Book.model_validate_json(tool_calls[0].function.arguments)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from anthropic import Anthropic\nfrom pydantic import BaseModel\n\nclient = Anthropic()\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    message = client.messages.create(\n        model=\"claude-3-5-sonnet-20240620\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": f\"Extract {text}\"}],\n        tools=[\n            {\n                \"name\": \"Book\",\n                \"description\": \"An extracted book.\",\n                \"input_schema\": {\n                    \"properties\": {\n                        \"title\": {\"type\": \"string\"},\n                        \"author\": {\"type\": \"string\"},\n                    },\n                    \"required\": [\"title\", \"author\"],\n                    \"type\": \"object\",\n                },\n            }\n        ],\n        tool_choice={\"type\": \"tool\", \"name\": \"Book\"},\n    )\n    for block in message.content:\n        if block.type == \"tool_use\" and block.input is not None:\n            return Book.model_validate(block.input)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ToolChoice\nfrom pydantic import BaseModel\n\nclient = MistralClient()\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    completion = client.chat(\n        model=\"mistral-large-latest\",\n        messages=[{\"role\": \"user\", \"content\": f\"Extract {text}\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"Book\",\n                    \"description\": \"An extracted book.\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"author\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"title\", \"author\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n        tool_choice=ToolChoice.any,\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        return Book.model_validate_json(tool_calls[0].function.arguments)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom google.generativeai.types import FunctionDeclaration, Tool, content_types\nfrom proto.marshal.collections import RepeatedComposite\nfrom pydantic import BaseModel\n\nmodel = GenerativeModel(\"gemini-1.5-flash\")\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    response = model.generate_content(\n        f\"Extract {text}\",\n        tools=[\n            Tool(\n                function_declarations=[\n                    FunctionDeclaration(\n                        **{\n                            \"name\": \"Book\",\n                            \"description\": \"An extracted book.\",\n                            \"parameters\": {\n                                \"properties\": {\n                                    \"title\": {\"type\": \"string\"},\n                                    \"author\": {\"type\": \"string\"},\n                                },\n                                \"required\": [\"title\", \"author\"],\n                                \"type\": \"object\",\n                            },\n                        }\n                    )\n                ]\n            )\n        ],\n        tool_config=content_types.to_tool_config(\n            {\n                \"function_calling_config\": {\n                    \"mode\": \"any\",\n                    \"allowed_function_names\": [\"Book\"],\n                }\n            }  # pyright: ignore [reportArgumentType]\n        ),\n    )\n    if tool_calls := [\n        part.function_call for part in response.parts if part.function_call.args\n    ]:\n        return Book.model_validate(\n            {\n                k: v if not isinstance(v, RepeatedComposite) else list(v)\n                for k, v in tool_calls[0].args.items()\n            }\n        )\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from groq import Groq\nfrom pydantic import BaseModel\n\nclient = Groq()\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    completion = client.chat.completions.create(\n        model=\"llama-3.1-70b-versatile\",\n        messages=[{\"role\": \"user\", \"content\": f\"Extract {text}\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"Book\",\n                    \"description\": \"An extracted book.\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"author\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"title\", \"author\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n        tool_choice=\"required\",\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        return Book.model_validate_json(tool_calls[0].function.arguments)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from cohere import Client\nfrom cohere.types import Tool, ToolParameterDefinitionsValue\nfrom pydantic import BaseModel\n\nclient = Client()\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    response = client.chat(\n        model=\"command-r-plus\",\n        message=f\"Extract {text}\",\n        tools=[\n            Tool(\n                name=\"Book\",\n                description=\"An extracted book.\",\n                parameter_definitions={\n                    \"title\": ToolParameterDefinitionsValue(\n                        description=None, type=\"string\", required=True\n                    ),\n                    \"author\": ToolParameterDefinitionsValue(\n                        description=None, type=\"string\", required=True\n                    ),\n                },\n            )\n        ],\n    )\n    if response.tool_calls:\n        return Book.model_validate(response.tool_calls[0].parameters)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from litellm import completion\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    response = completion(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Extract {text}\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"Book\",\n                    \"description\": \"An extracted book.\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"author\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"title\", \"author\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n        tool_choice=\"required\",\n    )\n    if tool_calls := response.choices[0].message.tool_calls:  # pyright: ignore [reportAttributeAccessIssue]\n        return Book.model_validate_json(tool_calls[0].function.arguments)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import (\n    ChatCompletionsToolDefinition,\n    ChatRequestMessage,\n    FunctionDefinition,\n)\nfrom azure.core.credentials import AzureKeyCredential\nfrom pydantic import BaseModel\n\nclient = ChatCompletionsClient(\n    endpoint=\"YOUR_ENDPOINT\", credential=AzureKeyCredential(\"YOUR_KEY\")\n)\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    completion = client.complete(\n        model=\"gpt-4o-mini\",\n        messages=[ChatRequestMessage({\"role\": \"user\", \"content\": f\"Extract {text}\"})],\n        tools=[\n            ChatCompletionsToolDefinition(\n                function=FunctionDefinition(\n                    name=\"Book\",\n                    description=\"An extracted book.\",\n                    parameters={\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"author\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"title\", \"author\"],\n                        \"type\": \"object\",\n                    },\n                )\n            )\n        ],\n        tool_choices=\"required\",\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        return Book.model_validate_json(tool_calls[0].function.arguments)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from proto.marshal.collections import RepeatedComposite\nfrom pydantic import BaseModel\nfrom vertexai.generative_models import (\n    FunctionDeclaration,\n    GenerativeModel,\n    Tool,\n    ToolConfig,\n)\n\nmodel = GenerativeModel(\"gemini-1.5-flash\")\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    response = model.generate_content(\n        f\"Extract {text}\",\n        tools=[\n            Tool(\n                function_declarations=[\n                    FunctionDeclaration(\n                        **{\n                            \"name\": \"Book\",\n                            \"description\": \"An extracted book.\",\n                            \"parameters\": {\n                                \"properties\": {\n                                    \"title\": {\"type\": \"string\"},\n                                    \"author\": {\"type\": \"string\"},\n                                },\n                                \"required\": [\"title\", \"author\"],\n                                \"type\": \"object\",\n                            },\n                        }\n                    )\n                ]\n            )\n        ],\n        tool_config=ToolConfig(\n            function_calling_config=ToolConfig.FunctionCallingConfig(\n                mode=ToolConfig.FunctionCallingConfig.Mode.ANY,\n                allowed_function_names=[\"Book\"],\n            ),\n        ),\n    )\n    if tool_calls := [\n        part.function_call\n        for candidate in response.candidates  # pyright: ignore [reportAttributeAccessIssue]\n        for part in candidate.content.parts\n        if part.function_call.args\n    ]:\n        return Book.model_validate(\n            {\n                k: v if not isinstance(v, RepeatedComposite) else list(v)\n                for k, v in tool_calls[0].args.items()\n            }\n        )\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>import boto3\nfrom pydantic import BaseModel\n\nbedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"Extract {text}\"}]}]\n    tool_config = {\n        \"tools\": [\n            {\n                \"toolSpec\": {\n                    \"name\": \"Book\",\n                    \"description\": \"An extracted book.\",\n                    \"inputSchema\": {\n                        \"json\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"title\": {\"type\": \"string\"},\n                                \"author\": {\"type\": \"string\"},\n                            },\n                            \"required\": [\"title\", \"author\"],\n                        }\n                    },\n                }\n            }\n        ],\n        \"toolChoice\": {\"type\": \"tool\", \"name\": \"Book\"},\n    }\n    response = bedrock_client.converse(\n        modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",\n        messages=messages,\n        toolConfig=tool_config,\n    )\n    output_message = response[\"output\"][\"message\"]\n    messages.append(output_message)\n    for content_piece in output_message[\"content\"]:\n        if \"toolUse\" in content_piece and content_piece[\"toolUse\"].get(\"input\"):\n            tool_input = content_piece[\"toolUse\"][\"input\"]\n            return Book.model_validate(tool_input)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>Reducing this boilerplate becomes increasingly important as the number and complexity of your calls grows beyond a single basic example. Furthermore, the Mirascope interface works across all of our various supported providers, so you don't need to learn the intracacies of each provider to use them the same way.</p>"},{"location":"WHY/#functional-modular-design","title":"Functional, Modular Design","text":"<p>Mirascope's functional approach promotes modularity and reusability. You can easily compose and chain LLM calls, creating complex workflows with simple, readable code:</p> Separate CallsNested Calls OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@mistral.call(\"mistral-large-latest\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@cohere.call(\"command-r-plus\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@azure.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <p>The goal of our design approach is to remain Pythonic so you can build your way.</p>"},{"location":"WHY/#provider-agnostic-when-wanted-specific-when-needed","title":"Provider-Agnostic When Wanted, Specific When Needed","text":"<p>We understand the desire for easily switching between various LLM providers. We also understand the (common) need to engineer a prompt for a specific provider (and model).</p> <p>By implementing our LLM API call functionality as decorators, Mirascope makes implementing any and all of these paths straightforward and easy:</p> Provider-SpecificProvider-Agnostic <pre><code>from mirascope.core import anthropic, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef openai_recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef anthropic_recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic, openai, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n# OpenAI\nopenai_model = \"gpt-4o-mini\"\nopenai_recommend_book = openai.call(openai_model)(recommend_book_prompt)\nopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n\n# Anthropic\nanthropic_model = \"claude-3-5-sonnet-20240620\"\nanthropic_recommend_book = anthropic.call(anthropic_model)(recommend_book_prompt)\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n</code></pre>"},{"location":"WHY/#type-hints-editor-support","title":"Type Hints &amp; Editor Support","text":"<ul> <li> Type Safety to catch errors before runtime during lint</li> <li> Editor Support for rich autocomplete and inline documentation</li> </ul>"},{"location":"WHY/#who-should-use-mirascope","title":"Who Should Use Mirascope?","text":"<p>Mirascope is designed for everyone to use!</p> <p>However, we believe that the value of Mirascope will shine in particular for:</p> <ul> <li>Professional Developers: Who need fine-grained control and transparency in their LLM interactions.</li> <li>AI Application Builders: Looking for a tool that can grow with their project from prototype to production.</li> <li>Teams: Who value clean, maintainable code and want to avoid the \"black box\" problem of many AI frameworks.</li> <li>Researchers and Experimenters: Who need the flexibility to quickly try out new ideas without fighting their tools.</li> </ul>"},{"location":"WHY/#getting-started","title":"Getting Started","text":"<p> Getting Started  Learn  Join Our Community</p> <p>By choosing Mirascope, you're opting for a tool that respects your expertise as a developer while providing the conveniences you need to work efficiently and effectively with LLMs.</p> <p>We believe the best tools get out of your way and let you focus on building great applications.</p>"},{"location":"api/core/anthropic/call/","title":"mirascope.core.anthropic.call","text":"<p>A decorator for calling the Anthropic API with a typed function.</p> Usage Documentation <p>Calls</p> <p>This decorator is used to wrap a typed function that calls the Anthropic API. It parses the prompt template of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.anthropic import anthropic_call\n\n\n@anthropic_call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The Anthropic model to use in the API call.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <code>list[BaseTool | Callable]</code> <p>The tools to use in the Anthropic API call.</p> required <code>response_model</code> <code>BaseModel | BaseType</code> <p>The response model into which the response should be structured.</p> required <code>output_parser</code> <code>Callable[[AnthropicCallResponse | ResponseModelT], Any]</code> <p>A function for parsing the call response whose value will be returned in place of the original call response.</p> required <code>json_mode</code> <code>bool</code> <p>Whether to use JSON Mode.</p> required <code>client</code> <code>object</code> <p>An optional custom client to use in place of the default client.</p> required <code>call_params</code> <code>AnthropicCallParams</code> <p>The <code>AnthropicCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator for turning a typed function into an Anthropic API call.</p>"},{"location":"api/core/anthropic/call_params/","title":"mirascope.core.anthropic.call_params","text":"Usage Documentation <p>Calls</p>"},{"location":"api/core/anthropic/call_params/#mirascope.core.anthropic.call_params.AnthropicCallParams","title":"AnthropicCallParams","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Anthropic API.</p> <p>Anthropic API Reference</p> <p>Attributes:</p> Name Type Description <code>max_tokens</code> <code>int</code> <p>...</p> <code>tool_choice</code> <code>NotRequired[ToolChoice | None]</code> <p>...</p> <code>metadata</code> <code>NotRequired[Metadata | None]</code> <p>...</p> <code>stop_sequences</code> <code>NotRequired[list[str] | None]</code> <p>...</p> <code>temperature</code> <code>NotRequired[float | None]</code> <p>...</p> <code>top_k</code> <code>NotRequired[int | None]</code> <p>...</p> <code>top_p</code> <code>NotRequired[float | None]</code> <p>...</p> <code>timeout</code> <code>NotRequired[float | Timeout | None]</code> <p>...</p>"},{"location":"api/core/anthropic/call_response/","title":"mirascope.core.anthropic.call_response","text":"<p>This module contains the <code>AnthropicCallResponse</code> class.</p> Usage Documentation <p>Calls</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse","title":"AnthropicCallResponse","text":"<p>               Bases: <code>BaseCallResponse[Message, AnthropicTool, ToolParam, AnthropicDynamicConfig, MessageParam, AnthropicCallParams, MessageParam]</code></p> <p>A convenience wrapper around the Anthropic <code>Message</code> response.</p> <p>When calling the Anthropic API using a function decorated with <code>anthropic_call</code>, the response will be an <code>AnthropicCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.anthropic import anthropic_call\n\n\n@anthropic_call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")  # response is an `AnthropicCallResponse` instance\nprint(response.content)\n</code></pre>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the string text of the 0th text block.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[str]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: Usage\n</code></pre> <p>Returns the usage of the message.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.message_param","title":"message_param  <code>property</code>","text":"<pre><code>message_param: SerializeAsAny[MessageParam]\n</code></pre> <p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.tools","title":"tools  <code>property</code>","text":"<pre><code>tools: list[AnthropicTool] | None\n</code></pre> <p>Returns any available tool calls as their <code>AnthropicTool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.tool","title":"tool  <code>property</code>","text":"<pre><code>tool: AnthropicTool | None\n</code></pre> <p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.tool_message_params","title":"tool_message_params  <code>classmethod</code>","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[AnthropicTool, str]]\n) -&gt; list[MessageParam]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[AnthropicTool, str]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required <p>Returns:</p> Type Description <code>list[MessageParam]</code> <p>The list of constructed <code>MessageParam</code> parameters.</p> Source code in <code>mirascope/core/anthropic/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[AnthropicTool, str]]\n) -&gt; list[MessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n\n    Returns:\n        The list of constructed `MessageParam` parameters.\n    \"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                ToolResultBlockParam(\n                    tool_use_id=tool.tool_call.id,\n                    type=\"tool_result\",\n                    content=[{\"text\": output, \"type\": \"text\"}],\n                )\n                for tool, output in tools_and_outputs\n            ],\n        }\n    ]\n</code></pre>"},{"location":"api/core/anthropic/call_response_chunk/","title":"mirascope.core.anthropic.call_response_chunk","text":"<p>This module contains the <code>AnthropicCallResponseChunk</code> class.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk","title":"AnthropicCallResponseChunk","text":"<p>               Bases: <code>BaseCallResponseChunk[MessageStreamEvent, FinishReason]</code></p> <p>A convenience wrapper around the Anthropic <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Anthropic API using a function decorated with <code>anthropic_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>AnthropicResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.anthropic import anthropic_call\n\n\n@anthropic_call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # response is an `AnthropicStream`\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the string content of the 0th message.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[FinishReason] | None\n</code></pre> <p>Returns the finish reason of the response.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.model","title":"model  <code>property</code>","text":"<pre><code>model: str | None\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.id","title":"id  <code>property</code>","text":"<pre><code>id: str | None\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: Usage | MessageDeltaUsage | None\n</code></pre> <p>Returns the usage of the message.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/anthropic/dynamic_config/","title":"mirascope.core.anthropic.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/anthropic/dynamic_config/#mirascope.core.anthropic.dynamic_config.AnthropicDynamicConfig","title":"AnthropicDynamicConfig  <code>module-attribute</code>","text":"<pre><code>AnthropicDynamicConfig = BaseDynamicConfig[\n    MessageParam | BaseMessageParam, AnthropicCallParams\n]\n</code></pre> <p>The function return type for functions wrapped with the <code>anthropic_call</code> decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.anthropic import AnthropicDynamicConfig, anthropic_call\n\n\n@anthropic_call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {capitalized_genre} book\")\ndef recommend_book(genre: str) -&gt; AnthropicDynamicConfig:\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/anthropic/stream/","title":"mirascope.core.anthropic.stream","text":"<p>The <code>AnthropicStream</code> class for convenience around streaming LLM calls.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/anthropic/stream/#mirascope.core.anthropic.stream.AnthropicStream","title":"AnthropicStream","text":"<pre><code>AnthropicStream(\n    *,\n    stream: (\n        Generator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n            None,\n        ]\n        | AsyncGenerator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n        ]\n    ),\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT]\n)\n</code></pre> <p>               Bases: <code>BaseStream[AnthropicCallResponse, AnthropicCallResponseChunk, MessageParam, MessageParam, MessageParam, MessageParam, AnthropicTool, ToolParam, AnthropicDynamicConfig, AnthropicCallParams, FinishReason]</code></p> <p>A class for convenience around streaming Anthropic LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.anthropic import anthropic_call\n\n\n@anthropic_call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # returns `AnthropicStream` instance\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> Source code in <code>mirascope/core/base/stream.py</code> <pre><code>def __init__(\n    self,\n    *,\n    stream: Generator[tuple[_BaseCallResponseChunkT, _BaseToolT | None], None, None]\n    | AsyncGenerator[\n        tuple[_BaseCallResponseChunkT, _BaseToolT | None],\n        None,\n    ],\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT],\n) -&gt; None:\n    \"\"\"Initializes an instance of `BaseStream`.\"\"\"\n    self.content = \"\"\n    self.stream = stream\n    self.metadata = metadata\n    self.tool_types = tool_types\n    self.call_response_type = call_response_type\n    self.model = model\n    self.prompt_template = prompt_template\n    self.fn_args = fn_args\n    self.dynamic_config = dynamic_config\n    self.messages = messages\n    self.call_params = call_params\n    self.call_kwargs = call_kwargs\n    self.user_message_param = get_possible_user_message_param(messages)  # pyright: ignore [reportAttributeAccessIssue]\n</code></pre>"},{"location":"api/core/anthropic/stream/#mirascope.core.anthropic.stream.AnthropicStream.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/anthropic/stream/#mirascope.core.anthropic.stream.AnthropicStream.construct_call_response","title":"construct_call_response","text":"<pre><code>construct_call_response() -&gt; AnthropicCallResponse\n</code></pre> <p>Constructs the call response from a consumed AnthropicStream.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the stream has not yet been consumed.</p> Source code in <code>mirascope/core/anthropic/stream.py</code> <pre><code>def construct_call_response(self) -&gt; AnthropicCallResponse:\n    \"\"\"Constructs the call response from a consumed AnthropicStream.\n\n    Raises:\n        ValueError: if the stream has not yet been consumed.\n    \"\"\"\n    if not hasattr(self, \"message_param\"):\n        raise ValueError(\n            \"No stream response, check if the stream has been consumed.\"\n        )\n    usage = Usage(\n        input_tokens=int(self.input_tokens or 0),\n        output_tokens=int(self.output_tokens or 0),\n    )\n\n    content_blocks: list[ContentBlock] = []\n\n    if isinstance(self.message_param[\"content\"], str):\n        content_blocks.append(\n            TextBlock(text=self.message_param[\"content\"], type=\"text\")\n        )\n    else:\n        for content in self.message_param[\"content\"]:\n            content_type = (\n                content.type if isinstance(content, BaseModel) else content[\"type\"]\n            )\n\n            if content_type == \"text\":\n                content_blocks.append(TextBlock.model_validate(content))\n            elif content_type == \"tool_use\":\n                content_blocks.append(ToolUseBlock.model_validate(content))\n    completion = Message(\n        id=self.id if self.id else \"\",\n        content=content_blocks,\n        model=self.model,\n        role=\"assistant\",\n        stop_reason=self.finish_reasons[0] if self.finish_reasons else None,\n        stop_sequence=None,\n        type=\"message\",\n        usage=usage,\n    )\n    return AnthropicCallResponse(\n        metadata=self.metadata,\n        response=completion,\n        tool_types=self.tool_types,\n        prompt_template=self.prompt_template,\n        fn_args=self.fn_args if self.fn_args else {},\n        dynamic_config=self.dynamic_config,\n        messages=self.messages,\n        call_params=self.call_params,\n        call_kwargs=self.call_kwargs,\n        user_message_param=self.user_message_param,\n        start_time=self.start_time,\n        end_time=self.end_time,\n    )\n</code></pre>"},{"location":"api/core/anthropic/tool/","title":"mirascope.core.anthropic.tool","text":"<p>The <code>OpenAITool</code> class for easy tool usage with OpenAI LLM calls.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/anthropic/tool/#mirascope.core.anthropic.tool.AnthropicToolConfig","title":"AnthropicToolConfig","text":"<p>               Bases: <code>ToolConfig</code></p> <p>A tool configuration for Anthropic-specific features.</p>"},{"location":"api/core/anthropic/tool/#mirascope.core.anthropic.tool.AnthropicTool","title":"AnthropicTool","text":"<p>               Bases: <code>BaseTool[ToolParam]</code></p> <p>A class for defining tools for Anthropic LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.anthropic import anthropic_call\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@anthropic_call(\"claude-3-5-sonnet-20240620\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:  # returns an `AnthropicTool` instance\n    print(tool.call())\n</code></pre>"},{"location":"api/core/anthropic/tool/#mirascope.core.anthropic.tool.AnthropicTool.tool_schema","title":"tool_schema  <code>classmethod</code>","text":"<pre><code>tool_schema() -&gt; ToolParam\n</code></pre> <p>Constructs a <code>ToolParam</code> tool schema from the <code>BaseModel</code> schema defined.</p> <p>Example: <pre><code>from mirascope.core.anthropic import AnthropicTool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\ntool_type = AnthropicTool.type_from_fn(format_book)\nprint(tool_type.tool_schema())  # prints the Anthropic-specific tool schema\n</code></pre></p> Source code in <code>mirascope/core/anthropic/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ToolParam:\n    \"\"\"Constructs a `ToolParam` tool schema from the `BaseModel` schema defined.\n\n    Example:\n    ```python\n    from mirascope.core.anthropic import AnthropicTool\n\n\n    def format_book(title: str, author: str) -&gt; str:\n        return f\"{title} by {author}\"\n\n\n    tool_type = AnthropicTool.type_from_fn(format_book)\n    print(tool_type.tool_schema())  # prints the Anthropic-specific tool schema\n    ```\n    \"\"\"\n    kwargs = {\n        \"input_schema\": cls.model_json_schema(),\n        \"name\": cls._name(),\n        \"description\": cls._description(),\n    }\n    if \"cache_control\" in cls.tool_config:\n        kwargs[\"cache_control\"] = cls.tool_config[\"cache_control\"]\n    return ToolParam(**kwargs)\n</code></pre>"},{"location":"api/core/anthropic/tool/#mirascope.core.anthropic.tool.AnthropicTool.from_tool_call","title":"from_tool_call  <code>classmethod</code>","text":"<pre><code>from_tool_call(tool_call: ToolUseBlock) -&gt; AnthropicTool\n</code></pre> <p>Constructs an <code>AnthropicTool</code> instance from a <code>tool_call</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ToolUseBlock</code> <p>The Anthropic tool call from which to construct this tool instance.</p> required Source code in <code>mirascope/core/anthropic/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolUseBlock) -&gt; AnthropicTool:\n    \"\"\"Constructs an `AnthropicTool` instance from a `tool_call`.\n\n    Args:\n        tool_call: The Anthropic tool call from which to construct this tool\n            instance.\n    \"\"\"\n    model_json = copy.deepcopy(tool_call.input)\n    model_json[\"tool_call\"] = tool_call.model_dump()  # pyright: ignore [reportIndexIssue]\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/azure/call/","title":"mirascope.core.azure.call","text":"<p>A decorator for calling the Azure API with a typed function.</p> Usage Documentation <p>Calls</p> <p>This decorator is used to wrap a typed function that calls the Azure API. It parses the prompt template of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.azure import azure_call\n\n\n@azure_call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The Azure model to use in the API call.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <code>list[BaseTool | Callable]</code> <p>The tools to use in the Azure API call.</p> required <code>response_model</code> <code>BaseModel | BaseType</code> <p>The response model into which the response should be structured.</p> required <code>output_parser</code> <code>Callable[[AzureCallResponse | ResponseModelT], Any]</code> <p>A function for  parsing the call response whose value will be returned in place of the original call response.</p> required <code>json_mode</code> <code>bool</code> <p>Whether to use JSON Mode.</p> required <code>client</code> <code>object</code> <p>An optional custom client to use in place of the default client.</p> required <code>call_params</code> <code>AzureCallParams</code> <p>The <code>AzureCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator for turning a typed function into an Azure API call.</p>"},{"location":"api/core/azure/call_params/","title":"mirascope.core.azure.call_params","text":"Usage Documentation <p>Calls</p>"},{"location":"api/core/azure/call_params/#mirascope.core.azure.call_params.AzureCallParams","title":"AzureCallParams","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Azure API.</p> <p>Azure API Reference</p> <p>Attributes:</p> Name Type Description <code>frequency_penalty</code> <code>NotRequired[float | None]</code> <p>...</p> <code>max_tokens</code> <code>NotRequired[int | None]</code> <p>...</p> <code>model_extras</code> <code>NotRequired[dict[str, Any] | None]</code> <p>...</p> <code>presence_penalty</code> <code>NotRequired[float | None]</code> <p>...</p> <code>response_format</code> <code>NotRequired[ChatCompletionsResponseFormat | None]</code> <p>...</p> <code>seed</code> <code>NotRequired[int | None]</code> <p>...</p> <code>stop</code> <code>NotRequired[list[str] | None]</code> <p>...</p> <code>temperature</code> <code>NotRequired[float | None]</code> <p>...</p> <code>tool_choice</code> <code>NotRequired[str | ChatCompletionsToolChoicePreset | ChatCompletionsNamedToolChoice | None]</code> <p>...</p> <code>top_p</code> <code>NotRequired[float | None]</code> <p>...</p>"},{"location":"api/core/azure/call_response/","title":"mirascope.core.azure.call_response","text":"<p>This module contains the <code>AzureCallResponse</code> class.</p> Usage Documentation <p>Calls</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse","title":"AzureCallResponse","text":"<p>               Bases: <code>BaseCallResponse[ChatCompletions, AzureTool, ChatCompletionsToolDefinition, AzureDynamicConfig, ChatRequestMessage, AzureCallParams, UserMessage]</code></p> <p>A convenience wrapper around the Azure <code>ChatCompletion</code> response.</p> <p>When calling the Azure API using a function decorated with <code>azure_call</code>, the response will be an <code>AzureCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.azure import azure_call\n\n\n@azure_call(\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")  # response is an `AzureCallResponse` instance\nprint(response.content)\n</code></pre>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[str]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: CompletionsUsage | None\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.message_param","title":"message_param  <code>property</code>","text":"<pre><code>message_param: SerializeAsAny[AssistantMessage]\n</code></pre> <p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.tools","title":"tools  <code>property</code>","text":"<pre><code>tools: list[AzureTool] | None\n</code></pre> <p>Returns any available tool calls as their <code>AzureTool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p> <code>ValueError</code> <p>if the model refused to response, in which case the error message will be the refusal.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.tool","title":"tool  <code>property</code>","text":"<pre><code>tool: AzureTool | None\n</code></pre> <p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p> <code>ValueError</code> <p>if the model refused to response, in which case the error message will be the refusal.</p>"},{"location":"api/core/azure/call_response/#mirascope.core.azure.call_response.AzureCallResponse.tool_message_params","title":"tool_message_params  <code>classmethod</code>","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[AzureTool, str]]\n) -&gt; list[ToolMessage]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[AzureTool, str]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required <p>Returns:</p> Type Description <code>list[ToolMessage]</code> <p>The list of constructed <code>ChatCompletionToolMessageParam</code> parameters.</p> Source code in <code>mirascope/core/azure/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[AzureTool, str]]\n) -&gt; list[ToolMessage]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n\n    Returns:\n        The list of constructed `ChatCompletionToolMessageParam` parameters.\n    \"\"\"\n    return [\n        cls._get_tool_message(tool, output) for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/azure/call_response_chunk/","title":"mirascope.core.azure.call_response_chunk","text":"<p>This module contains the <code>AzureCallResponseChunk</code> class.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/azure/call_response_chunk/#mirascope.core.azure.call_response_chunk.AzureCallResponseChunk","title":"AzureCallResponseChunk","text":"<p>               Bases: <code>BaseCallResponseChunk[StreamingChatCompletionsUpdate, CompletionsFinishReason]</code></p> <p>A convenience wrapper around the Azure <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Azure API using a function decorated with <code>azure_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>AzureResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.azure import azure_call\n\n\n@azure_call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # response is an `AzureStream`\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/core/azure/call_response_chunk/#mirascope.core.azure.call_response_chunk.AzureCallResponseChunk.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/azure/call_response_chunk/#mirascope.core.azure.call_response_chunk.AzureCallResponseChunk.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[CompletionsFinishReason]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/azure/call_response_chunk/#mirascope.core.azure.call_response_chunk.AzureCallResponseChunk.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/azure/call_response_chunk/#mirascope.core.azure.call_response_chunk.AzureCallResponseChunk.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/azure/call_response_chunk/#mirascope.core.azure.call_response_chunk.AzureCallResponseChunk.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: CompletionsUsage\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/azure/call_response_chunk/#mirascope.core.azure.call_response_chunk.AzureCallResponseChunk.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/azure/call_response_chunk/#mirascope.core.azure.call_response_chunk.AzureCallResponseChunk.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/azure/dynamic_config/","title":"mirascope.core.azure.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/azure/dynamic_config/#mirascope.core.azure.dynamic_config.AzureDynamicConfig","title":"AzureDynamicConfig  <code>module-attribute</code>","text":"<pre><code>AzureDynamicConfig = BaseDynamicConfig[\n    ChatRequestMessage | BaseMessageParam, AzureCallParams\n]\n</code></pre> <p>The function return type for functions wrapped with the <code>azure_call</code> decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.azure import AzureDynamicConfig, azure_call\n\n\n@azure_call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {capitalized_genre} book\")\ndef recommend_book(genre: str) -&gt; AzureDynamicConfig:\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/azure/stream/","title":"mirascope.core.azure.stream","text":"<p>The <code>AzureStream</code> class for convenience around streaming LLM calls.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/azure/stream/#mirascope.core.azure.stream.AzureStream","title":"AzureStream","text":"<pre><code>AzureStream(\n    *,\n    stream: (\n        Generator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n            None,\n        ]\n        | AsyncGenerator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n        ]\n    ),\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT]\n)\n</code></pre> <p>               Bases: <code>BaseStream[AzureCallResponse, AzureCallResponseChunk, UserMessage, AssistantMessage, ToolMessage, ChatRequestMessage, AzureTool, ChatCompletionsToolDefinition, AzureDynamicConfig, AzureCallParams, CompletionsFinishReason]</code></p> <p>A class for convenience around streaming Azure LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.azure import azure_call\n\n\n@azure_call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # returns `AzureStream` instance\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> Source code in <code>mirascope/core/base/stream.py</code> <pre><code>def __init__(\n    self,\n    *,\n    stream: Generator[tuple[_BaseCallResponseChunkT, _BaseToolT | None], None, None]\n    | AsyncGenerator[\n        tuple[_BaseCallResponseChunkT, _BaseToolT | None],\n        None,\n    ],\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT],\n) -&gt; None:\n    \"\"\"Initializes an instance of `BaseStream`.\"\"\"\n    self.content = \"\"\n    self.stream = stream\n    self.metadata = metadata\n    self.tool_types = tool_types\n    self.call_response_type = call_response_type\n    self.model = model\n    self.prompt_template = prompt_template\n    self.fn_args = fn_args\n    self.dynamic_config = dynamic_config\n    self.messages = messages\n    self.call_params = call_params\n    self.call_kwargs = call_kwargs\n    self.user_message_param = get_possible_user_message_param(messages)  # pyright: ignore [reportAttributeAccessIssue]\n</code></pre>"},{"location":"api/core/azure/stream/#mirascope.core.azure.stream.AzureStream.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/azure/stream/#mirascope.core.azure.stream.AzureStream.construct_call_response","title":"construct_call_response","text":"<pre><code>construct_call_response() -&gt; AzureCallResponse\n</code></pre> <p>Constructs the call response from a consumed AzureStream.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the stream has not yet been consumed.</p> Source code in <code>mirascope/core/azure/stream.py</code> <pre><code>def construct_call_response(self) -&gt; AzureCallResponse:\n    \"\"\"Constructs the call response from a consumed AzureStream.\n\n    Raises:\n        ValueError: if the stream has not yet been consumed.\n    \"\"\"\n    if not hasattr(self, \"message_param\"):\n        raise ValueError(\n            \"No stream response, check if the stream has been consumed.\"\n        )\n    message = ChatResponseMessage(\n        role=self.message_param[\"role\"],\n        content=self.message_param.get(\"content\", \"\"),\n        tool_calls=self.message_param.get(\"tool_calls\", []),\n    )\n    if not self.input_tokens and not self.output_tokens:\n        usage = CompletionsUsage(\n            completion_tokens=0, prompt_tokens=0, total_tokens=0\n        )\n    else:\n        usage = CompletionsUsage(\n            prompt_tokens=int(self.input_tokens or 0),\n            completion_tokens=int(self.output_tokens or 0),\n            total_tokens=int(self.input_tokens or 0) + int(self.output_tokens or 0),\n        )\n    completion = ChatCompletions(\n        id=self.id if self.id else \"\",\n        model=self.model,\n        choices=[\n            ChatChoice(\n                finish_reason=self.finish_reasons[0]\n                if self.finish_reasons\n                else \"stop\",\n                index=0,\n                message=message,\n            )\n        ],\n        created=datetime.datetime.now(),\n        usage=usage,\n    )\n    return AzureCallResponse(\n        metadata=self.metadata,\n        response=completion,\n        tool_types=self.tool_types,\n        prompt_template=self.prompt_template,\n        fn_args=self.fn_args if self.fn_args else {},\n        dynamic_config=self.dynamic_config,\n        messages=self.messages,\n        call_params=self.call_params,\n        call_kwargs=self.call_kwargs,\n        user_message_param=self.user_message_param,\n        start_time=self.start_time,\n        end_time=self.end_time,\n    )\n</code></pre>"},{"location":"api/core/azure/tool/","title":"mirascope.core.azure.tool","text":"<p>The <code>AzureTool</code> class for easy tool usage with Azure LLM calls.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/azure/tool/#mirascope.core.azure.tool.AzureToolConfig","title":"AzureToolConfig","text":"<p>               Bases: <code>ToolConfig</code></p> <p>A tool configuration for Azure-specific features.</p>"},{"location":"api/core/azure/tool/#mirascope.core.azure.tool.AzureTool","title":"AzureTool","text":"<p>               Bases: <code>BaseTool[ChatCompletionsToolDefinition]</code></p> <p>A class for defining tools for Azure LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.azure import azure_call\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@azure_call(\"gpt-4o-mini\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:  # returns an `AzureTool` instance\n    print(tool.call())\n</code></pre>"},{"location":"api/core/azure/tool/#mirascope.core.azure.tool.AzureTool.tool_schema","title":"tool_schema  <code>classmethod</code>","text":"<pre><code>tool_schema() -&gt; ChatCompletionsToolDefinition\n</code></pre> <p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> <p>Example: <pre><code>from mirascope.core.azure import AzureTool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\ntool_type = AzureTool.type_from_fn(format_book)\nprint(tool_type.tool_schema())  # prints the Azure-specific tool schema\n</code></pre></p> Source code in <code>mirascope/core/azure/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionsToolDefinition:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\n\n    Example:\n    ```python\n    from mirascope.core.azure import AzureTool\n\n\n    def format_book(title: str, author: str) -&gt; str:\n        return f\"{title} by {author}\"\n\n\n    tool_type = AzureTool.type_from_fn(format_book)\n    print(tool_type.tool_schema())  # prints the Azure-specific tool schema\n    ```\n    \"\"\"\n    fn = FunctionDefinition(name=cls._name(), description=cls._description())\n    schema_generator = GenerateJsonSchemaNoTitles\n    model_schema = cls.model_json_schema(schema_generator=schema_generator)\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n    return ChatCompletionsToolDefinition(function=fn)\n</code></pre>"},{"location":"api/core/azure/tool/#mirascope.core.azure.tool.AzureTool.from_tool_call","title":"from_tool_call  <code>classmethod</code>","text":"<pre><code>from_tool_call(\n    tool_call: ChatCompletionsToolCall,\n) -&gt; AzureTool\n</code></pre> <p>Constructs an <code>AzureTool</code> instance from a <code>tool_call</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ChatCompletionsToolCall</code> <p>The Azure tool call from which to construct this tool instance.</p> required Source code in <code>mirascope/core/azure/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionsToolCall) -&gt; AzureTool:\n    \"\"\"Constructs an `AzureTool` instance from a `tool_call`.\n\n    Args:\n        tool_call: The Azure tool call from which to construct this tool instance.\n    \"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/base/call_factory/","title":"mirascope.core.base.call_factory","text":""},{"location":"api/core/base/call_factory/#call_factory","title":"<code>call_factory</code>","text":"<p>A factory method for creating provider-specific call decorators.</p> <p>Parameters:</p> Name Type Description Default <code>TCallResponse</code> <code>type[_BaseCallResponseT]</code> <p>The provider-specific <code>BaseCallResponse</code> type.</p> required <code>TCallResponseChunk</code> <code>type[_BaseCallResponseChunkT]</code> <p>The provider-specific <code>BaseCallResponseChunk</code> type.</p> required <code>TToolType</code> <code>type[_BaseToolT]</code> <p>The provider-specific <code>BaseTool</code> type.</p> required <code>TStream</code> <code>type[_BaseStreamT]</code> <p>The provider-specific <code>BaseStream</code> type.</p> required <code>default_call_params</code> <code>_BaseCallParamsT</code> <p>The default call parameters to use, which must match the <code>TCallParams</code> type if provided.</p> required <code>setup_call</code> <code>SameSyncAndAsyncClientSetupCall[_SameSyncAndAsyncClientT, _BaseDynamicConfigT, _BaseCallParamsT, _ResponseT, _ResponseChunkT, _AsyncResponseT, _AsyncResponseChunkT, _BaseToolT] | SetupCall[_SyncBaseClientT, _AsyncBaseClientT, _BaseDynamicConfigT, _BaseCallParamsT, _ResponseT, _ResponseChunkT, _AsyncResponseT, _AsyncResponseChunkT, _BaseToolT]</code> <p>The helper method for setting up a call, which returns the configured create function, the prompt template, the list of provider-specific messages, the list of provider-specific tool types, and the finalized <code>call_kwargs</code> with which to make the API call with the create function.</p> required <code>get_json_output</code> <code>GetJsonOutput[_BaseCallResponseT | _BaseCallResponseChunkT]</code> <p>The helper method for getting JSON output from a call response.</p> required <code>handle_stream</code> <code>HandleStream[_ResponseChunkT, _BaseCallResponseChunkT, _BaseToolT]</code> <p>The helper method for converting a provider's original stream generator into a generator that returns tuples of <code>(chunk, tool)</code> where <code>chunk</code> and <code>tool</code> are provider-specific <code>BaseCallResponseChunk</code> and <code>BaseTool</code> instances, respectively.</p> required <code>handle_stream_async</code> <code>HandleStreamAsync[_AsyncResponseChunkT, _BaseCallResponseChunkT, _BaseToolT]</code> <p>The same helper method as <code>handle_stream</code> except for handling asynchronous streaming.</p> required Source code in <code>mirascope/core/base/_call_factory.py</code> <pre><code>def call_factory(  # noqa: ANN202\n    *,\n    TCallResponse: type[_BaseCallResponseT],\n    TCallResponseChunk: type[_BaseCallResponseChunkT],\n    TToolType: type[_BaseToolT],\n    TStream: type[_BaseStreamT],\n    default_call_params: _BaseCallParamsT,\n    setup_call: SameSyncAndAsyncClientSetupCall[\n        _SameSyncAndAsyncClientT,\n        _BaseDynamicConfigT,\n        _BaseCallParamsT,\n        _ResponseT,\n        _ResponseChunkT,\n        _AsyncResponseT,\n        _AsyncResponseChunkT,\n        _BaseToolT,\n    ]\n    | SetupCall[\n        _SyncBaseClientT,\n        _AsyncBaseClientT,\n        _BaseDynamicConfigT,\n        _BaseCallParamsT,\n        _ResponseT,\n        _ResponseChunkT,\n        _AsyncResponseT,\n        _AsyncResponseChunkT,\n        _BaseToolT,\n    ],\n    get_json_output: GetJsonOutput[_BaseCallResponseT | _BaseCallResponseChunkT],\n    handle_stream: HandleStream[_ResponseChunkT, _BaseCallResponseChunkT, _BaseToolT],\n    handle_stream_async: HandleStreamAsync[\n        _AsyncResponseChunkT, _BaseCallResponseChunkT, _BaseToolT\n    ],\n) -&gt; CallDecorator[\n    _BaseCallResponseT,\n    _BaseCallResponseChunkT,\n    _BaseDynamicConfigT,\n    _BaseCallParamsT,\n    _BaseStreamT,\n    _SyncBaseClientT,\n    _AsyncBaseClientT,\n    _SameSyncAndAsyncClientT,\n]:\n    \"\"\"A factory method for creating provider-specific call decorators.\n\n    Args:\n        TCallResponse: The provider-specific `BaseCallResponse` type.\n        TCallResponseChunk: The provider-specific `BaseCallResponseChunk` type.\n        TToolType: The provider-specific `BaseTool` type.\n        TStream: The provider-specific `BaseStream` type.\n        default_call_params: The default call parameters to use, which must match the\n            `TCallParams` type if provided.\n        setup_call: The helper method for setting up a call, which returns the\n            configured create function, the prompt template, the list of\n            provider-specific messages, the list of provider-specific tool types, and\n            the finalized `call_kwargs` with which to make the API call with the create\n            function.\n        get_json_output: The helper method for getting JSON output from a call response.\n        handle_stream: The helper method for converting a provider's original stream\n            generator into a generator that returns tuples of `(chunk, tool)` where\n            `chunk` and `tool` are provider-specific `BaseCallResponseChunk` and\n            `BaseTool` instances, respectively.\n        handle_stream_async: The same helper method as `handle_stream` except for\n            handling asynchronous streaming.\n    \"\"\"\n\n    def base_call(\n        model: str,\n        *,\n        stream: bool = False,\n        tools: list[type[BaseTool] | Callable] | None = None,\n        response_model: type[_ResponseModelT] | None = None,\n        output_parser: Callable[[_BaseCallResponseT], _ParsedOutputT]\n        | Callable[[_BaseCallResponseChunkT], _ParsedOutputT]\n        | Callable[[_ResponseModelT], _ParsedOutputT]\n        | None = None,\n        json_mode: bool = False,\n        client: _SameSyncAndAsyncClientT\n        | _AsyncBaseClientT\n        | _SyncBaseClientT\n        | None = None,\n        call_params: BaseCallParams | None = None,\n    ) -&gt; (\n        AsyncLLMFunctionDecorator[\n            _BaseDynamicConfigT,\n            _BaseCallResponseT\n            | _ParsedOutputT\n            | BaseStream\n            | _ResponseModelT\n            | AsyncIterable[_ResponseModelT],\n        ]\n        | SyncLLMFunctionDecorator[\n            _BaseDynamicConfigT,\n            _BaseCallResponseT\n            | _ParsedOutputT\n            | BaseStream\n            | _ResponseModelT\n            | Iterable[_ResponseModelT],\n        ]\n        | LLMFunctionDecorator[\n            _BaseDynamicConfigT,\n            _BaseCallResponseT\n            | _ParsedOutputT\n            | BaseStream\n            | _ResponseModelT\n            | Iterable[_ResponseModelT],\n            _BaseCallResponseT\n            | _ParsedOutputT\n            | BaseStream\n            | _ResponseModelT\n            | AsyncIterable[_ResponseModelT],\n        ]\n    ):\n        if stream and output_parser:\n            raise ValueError(\"Cannot use `output_parser` with `stream=True`.\")\n\n        if call_params is None:\n            call_params = default_call_params\n\n        if response_model:\n            if stream:\n                return partial(\n                    structured_stream_factory(\n                        TCallResponse=TCallResponse,\n                        TCallResponseChunk=TCallResponseChunk,\n                        TStream=TStream,\n                        TToolType=TToolType,\n                        setup_call=setup_call,\n                        get_json_output=get_json_output,\n                    ),\n                    model=model,\n                    response_model=response_model,\n                    json_mode=json_mode,\n                    client=client,\n                    call_params=call_params,\n                )  # pyright: ignore [reportReturnType, reportCallIssue]\n            else:\n                return partial(\n                    extract_factory(\n                        TCallResponse=TCallResponse,\n                        TToolType=TToolType,\n                        setup_call=setup_call,\n                        get_json_output=get_json_output,\n                    ),\n                    model=model,\n                    response_model=response_model,\n                    output_parser=output_parser,\n                    json_mode=json_mode,\n                    client=client,\n                    call_params=call_params,\n                )  # pyright: ignore [reportCallIssue]\n\n        if stream:\n            return partial(\n                stream_factory(\n                    TCallResponse=TCallResponse,\n                    TStream=TStream,\n                    setup_call=setup_call,\n                    handle_stream=handle_stream,\n                    handle_stream_async=handle_stream_async,\n                ),\n                model=model,\n                tools=tools,\n                json_mode=json_mode,\n                client=client,\n                call_params=call_params,\n            )  # pyright: ignore [reportReturnType, reportCallIssue]\n        return partial(\n            create_factory(TCallResponse=TCallResponse, setup_call=setup_call),\n            model=model,\n            tools=tools,\n            output_parser=output_parser,\n            json_mode=json_mode,\n            client=client,\n            call_params=call_params,\n        )  # pyright: ignore [reportReturnType, reportCallIssue]\n\n    return base_call  # pyright: ignore [reportReturnType]\n</code></pre>"},{"location":"api/core/base/call_params/","title":"mirascope.core.base.call_params","text":""},{"location":"api/core/base/call_params/#basecallparams","title":"<code>BaseCallParams</code>","text":"<p>Base Class Definition</p> <p>This class is simply a means of binding type variables to ensure they are call params, but the class itself is empty. We do this because you cannot bind a type variable to a base <code>TypedDict</code> type and must create your own subclass to do so.</p> <p>               Bases: <code>TypedDict</code></p>"},{"location":"api/core/base/call_response/","title":"mirascope.core.base.call_response","text":"<p>This module contains the base call response class.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse","title":"BaseCallResponse","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[_ResponseT, _BaseToolT, _ToolSchemaT, _BaseDynamicConfigT, _MessageParamT, _CallParamsT, _UserMessageParamT]</code>, <code>ABC</code></p> <p>A base abstract interface for LLM call responses.</p> <p>Attributes:</p> Name Type Description <code>metadata</code> <code>Metadata</code> <p>The metadata pulled from the call that was made.</p> <code>response</code> <code>_ResponseT</code> <p>The original response from whichever model response this wraps.</p> <code>tool_types</code> <code>list[type[_BaseToolT]] | None</code> <p>The list of tool types used, if any.</p> <code>prompt_template</code> <code>str | None</code> <p>The unformatted prompt template from the call that was made.</p> <code>fn_args</code> <code>dict[str, Any]</code> <p>The input arguments used when making the call.</p> <code>dynamic_config</code> <code>_BaseDynamicConfigT</code> <p>Dynamic configuration options, if any.</p> <code>messages</code> <code>SkipValidation[list[_MessageParamT]]</code> <p>The list of provider-specific messages used to make the API call.</p> <code>call_params</code> <code>SkipValidation[_CallParamsT]</code> <p>The original call params set in the call decorator.</p> <code>call_kwargs</code> <code>BaseCallKwargs[_ToolSchemaT]</code> <p>The keyword arguments used to make the API call.</p> <code>user_message_param</code> <code>_UserMessageParamT | None</code> <p>The most recent provider-specific message if it was a user message. Otherwise <code>None</code>.</p> <code>start_time</code> <code>float</code> <p>The start time of the completion in ms.</p> <code>end_time</code> <code>float</code> <p>The end time of the completion in ms.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.content","title":"content  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Should return the string content of the response.</p> <p>If there are multiple choices in a response, this method should select the 0th choice and return it's string content.</p> <p>If there is no string content (e.g. when using tools), this method must return the empty string.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.finish_reasons","title":"finish_reasons  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>finish_reasons: list[str] | None\n</code></pre> <p>Should return the finish reasons of the response.</p> <p>If there is no finish reason, this method must return None.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.model","title":"model  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>model: str | None\n</code></pre> <p>Should return the name of the response model.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.id","title":"id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>id: str | None\n</code></pre> <p>Should return the id of the response.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.usage","title":"usage  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>usage: Any\n</code></pre> <p>Should return the usage of the response.</p> <p>If there is no usage, this method must return None.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.input_tokens","title":"input_tokens  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>input_tokens: int | float | None\n</code></pre> <p>Should return the number of input tokens.</p> <p>If there is no input_tokens, this method must return None.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.output_tokens","title":"output_tokens  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>output_tokens: int | float | None\n</code></pre> <p>Should return the number of output tokens.</p> <p>If there is no output_tokens, this method must return None.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.cost","title":"cost  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Should return the cost of the response in dollars.</p> <p>If there is no cost, this method must return None.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.message_param","title":"message_param  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>message_param: Any\n</code></pre> <p>Returns the assistant's response as a message parameter.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.tools","title":"tools  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>tools: list[_BaseToolT] | None\n</code></pre> <p>Returns the tools for the 0th choice message.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.tool","title":"tool  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>tool: _BaseToolT | None\n</code></pre> <p>Returns the 0th tool for the 0th choice message.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.tool_message_params","title":"tool_message_params  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[_BaseToolT, Any]]\n) -&gt; list[Any]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[_BaseToolT, Any]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required Source code in <code>mirascope/core/base/call_response.py</code> <pre><code>@classmethod\n@abstractmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[_BaseToolT, Any]]\n) -&gt; list[Any]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/core/base/call_response_chunk/","title":"mirascope.core.base.call_response_chunk","text":"<p>This module contains the <code>BaseCallResponseChunk</code> class.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk","title":"BaseCallResponseChunk","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[_ChunkT, _FinishReasonT]</code>, <code>ABC</code></p> <p>A base abstract interface for LLM streaming response chunks.</p> <p>Attributes:</p> Name Type Description <code>chunk</code> <code>_ChunkT</code> <p>The original response chunk from whichever model response this wraps.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.content","title":"content  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Should return the string content of the response chunk.</p> <p>If there are multiple choices in a chunk, this method should select the 0th choice and return it's string content.</p> <p>If there is no string content (e.g. when using tools), this method must return the empty string.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.finish_reasons","title":"finish_reasons  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>finish_reasons: list[_FinishReasonT] | None\n</code></pre> <p>Should return the finish reasons of the response.</p> <p>If there is no finish reason, this method must return None.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.model","title":"model  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>model: str | None\n</code></pre> <p>Should return the name of the response model.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.id","title":"id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>id: str | None\n</code></pre> <p>Should return the id of the response.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.usage","title":"usage  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>usage: Any\n</code></pre> <p>Should return the usage of the response.</p> <p>If there is no usage, this method must return None.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.input_tokens","title":"input_tokens  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>input_tokens: int | float | None\n</code></pre> <p>Should return the number of input tokens.</p> <p>If there is no input_tokens, this method must return None.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.output_tokens","title":"output_tokens  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>output_tokens: int | float | None\n</code></pre> <p>Should return the number of output tokens.</p> <p>If there is no output_tokens, this method must return None.</p>"},{"location":"api/core/base/dynamic_config/","title":"mirascope.core.base.dynamic_config","text":"<p>The base type in a function as an LLM call to return for dynamic configuration.</p>"},{"location":"api/core/base/dynamic_config/#mirascope.core.base.dynamic_config.BaseDynamicConfig","title":"BaseDynamicConfig  <code>module-attribute</code>","text":"<pre><code>BaseDynamicConfig = (\n    DynamicConfigBase\n    | DynamicConfigMessages[_MessageParamT]\n    | DynamicConfigCallParams[_CallParamsT]\n    | DynamicConfigFull[_MessageParamT, _CallParamsT]\n    | None\n)\n</code></pre> <p>The base type in a function as an LLM call to return for dynamic configuration.</p> <p>Attributes:</p> Name Type Description <code>metadata</code> <p>Any metadata to include in call responses.</p> <code>computed_fields</code> <p>Fields to be computed and injected into the prompt template at runtime.</p> <code>tools</code> <p>Tools to be provided to the LLM API call at runtime.</p> <code>messages</code> <p>Custom message parameters, which will override any other form of writing prompts when used.</p> <code>call_params</code> <p>Call parameters to use when making the LLM API call.</p>"},{"location":"api/core/base/message_param/","title":"mirascope.core.base.message_param","text":""},{"location":"api/core/base/message_param/#basemessageparam","title":"<code>BaseMessageParam</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A base class for message parameters.</p> Usage Documentation <p>Prompts</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the message (e.g. \"system\", \"user\", \"assistant\")</p> <code>content</code> <code>str | Sequence[TextPart | ImagePart | AudioPart | CacheControlPart]</code> <p>The content of the message</p>"},{"location":"api/core/base/message_param/#textpart","title":"<code>TextPart</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A content part for text.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['text']</code> <p>Always \"text\"</p> <code>text</code> <code>str</code> <p>The text content</p>"},{"location":"api/core/base/message_param/#imagepart","title":"<code>ImagePart</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A content part for images.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['image']</code> <p>Always \"image\"</p> <code>media_type</code> <code>str</code> <p>The media type (e.g. image/jpeg)</p> <code>image</code> <code>bytes</code> <p>The raw image bytes</p> <code>detail</code> <code>str | None</code> <p>(Optional) The detail to use for the image (supported by OpenAI)</p>"},{"location":"api/core/base/message_param/#audiopart","title":"<code>AudioPart</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A content part for audio.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['audio']</code> <p>Always \"audio\"</p> <code>media_type</code> <code>str</code> <p>The media type (e.g. audio/wav)</p> <code>audio</code> <code>bytes</code> <p>The raw audio bytes</p>"},{"location":"api/core/base/metadata/","title":"mirascope.core.base.metadata","text":"<p>The <code>Metadata</code> typed dictionary for including general metadata.</p>"},{"location":"api/core/base/metadata/#mirascope.core.base.metadata.Metadata","title":"Metadata","text":"<p>               Bases: <code>TypedDict</code></p> <p>The <code>Metadata</code> typed dictionary for including general metadata.</p> <p>Attributes:</p> Name Type Description <code>tags</code> <code>NotRequired[set[str]]</code> <p>A set of strings for tagging calls (e.g. versioning, categorizing, etc.)</p>"},{"location":"api/core/base/prompt/","title":"mirascope.core.base.prompt","text":"<p>The <code>BasePrompt</code> class for better prompt engineering.</p>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.BasePrompt","title":"BasePrompt","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base class for engineering prompts.</p> <p>This class is implemented as the base for all prompting needs. It is intended to work across various providers by providing a common prompt interface.</p> <p>Example:</p> <pre><code>from mirascope.core import BasePrompt, metadata, prompt_template\n\n@prompt_template(\"Recommend a {genre} book\")\n@metadata({\"tags\": {\"version:0001\", \"books\"}})\nclass BookRecommendationPrompt(BasePrompt):\n    genre: str\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\n\nprint(prompt)\n# &gt; Recommend a fantasy book\n\nprint(prompt.message_params())\n# &gt; [BaseMessageParam(role=\"user\", content=\"Recommend a fantasy book\")]\n\nprint(prompt.dump()[\"metadata\"])\n# &gt; {\"metadata\": {\"version:0001\", \"books\"}}\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.BasePrompt.message_params","title":"message_params","text":"<pre><code>message_params() -&gt; list[BaseMessageParam]\n</code></pre> <p>Returns the list of parsed message parameters.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def message_params(self) -&gt; list[BaseMessageParam]:\n    \"\"\"Returns the list of parsed message parameters.\"\"\"\n    return parse_prompt_messages(\n        roles=SUPPORTED_MESSAGE_ROLES,\n        template=get_prompt_template(self),\n        attrs={field: getattr(self, field) for field in self.model_fields},\n    )\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.BasePrompt.dynamic_config","title":"dynamic_config","text":"<pre><code>dynamic_config() -&gt; BaseDynamicConfig\n</code></pre> <p>Returns the dynamic config of the prompt.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def dynamic_config(self) -&gt; BaseDynamicConfig:\n    \"\"\"Returns the dynamic config of the prompt.\"\"\"\n    return None\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.BasePrompt.dump","title":"dump","text":"<pre><code>dump() -&gt; dict[str, Any]\n</code></pre> <p>Dumps the contents of the prompt into a dictionary.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n    return {\n        \"metadata\": get_metadata(self, None),\n        \"prompt\": str(self),\n        \"template\": get_prompt_template(self),\n        \"inputs\": self.model_dump(),\n    }\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.BasePrompt.run","title":"run","text":"<pre><code>run(\n    call_decorator: (\n        Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _BaseCallResponseT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _BaseStreamT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _ResponseModelT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., Iterable[_ResponseModelT]],\n        ]\n    ),\n    *additional_decorators: Callable[[_T], _T]\n) -&gt; (\n    _BaseCallResponseT\n    | _BaseStreamT\n    | _ResponseModelT\n    | Iterable[_ResponseModelT]\n)\n</code></pre> <p>Returns the response of calling the API of the provided decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import BasePrompt, openai, prompt_template\n\n\n@prompt_template(\"Recommend a {genre} book\")\nclass BookRecommendationPrompt(BasePrompt):\n    genre: str\n\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\nresponse = prompt.run(openai.call(\"gpt-4o-mini\"))\nprint(response.content)\n</code></pre> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def run(\n    self,\n    call_decorator: Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _BaseCallResponseT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _BaseStreamT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _ResponseModelT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., Iterable[_ResponseModelT]],\n    ],\n    *additional_decorators: Callable[[_T], _T],\n) -&gt; (\n    _BaseCallResponseT | _BaseStreamT | _ResponseModelT | Iterable[_ResponseModelT]\n):\n    \"\"\"Returns the response of calling the API of the provided decorator.\n\n    Example:\n\n    ```python\n    from mirascope.core import BasePrompt, openai, prompt_template\n\n\n    @prompt_template(\"Recommend a {genre} book\")\n    class BookRecommendationPrompt(BasePrompt):\n        genre: str\n\n\n    prompt = BookRecommendationPrompt(genre=\"fantasy\")\n    response = prompt.run(openai.call(\"gpt-4o-mini\"))\n    print(response.content)\n    ```\n    \"\"\"\n    kwargs = {field: getattr(self, field) for field in self.model_fields}\n    args_str = \", \".join(kwargs.keys())\n    namespace, fn_name = {}, self.__class__.__name__\n    exec(f\"def {fn_name}({args_str}): ...\", namespace)\n    return reduce(\n        lambda res, f: f(res),  # pyright: ignore [reportArgumentType, reportCallIssue]\n        [\n            metadata(get_metadata(self, self.dynamic_config())),\n            prompt_template(get_prompt_template(self)),\n            call_decorator,\n            *additional_decorators,\n        ],\n        namespace[fn_name],\n    )(**kwargs)\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.BasePrompt.run_async","title":"run_async","text":"<pre><code>run_async(\n    call_decorator: (\n        Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseCallResponseT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseStreamT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_ResponseModelT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[\n                ...,\n                Awaitable[AsyncIterable[_ResponseModelT]],\n            ],\n        ]\n    ),\n    *additional_decorators: Callable[[_T], _T]\n) -&gt; (\n    Awaitable[_BaseCallResponseT]\n    | Awaitable[_BaseStreamT]\n    | Awaitable[_ResponseModelT]\n    | Awaitable[AsyncIterable[_ResponseModelT]]\n)\n</code></pre> <p>Returns the response of calling the API of the provided decorator.</p> <p>Example:</p> <pre><code>import asyncio\n\nfrom mirascope.core import BasePrompt, openai, prompt_template\n\n\n@prompt_template(\"Recommend a {genre} book\")\nclass BookRecommendationPrompt(BasePrompt):\n    genre: str\n\n\nasync def run():\n    prompt = BookRecommendationPrompt(genre=\"fantasy\")\n    response = await prompt.run_async(openai.call(\"gpt-4o-mini\"))\n    print(response.content)\n\n\nasyncio.run(run())\n</code></pre> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def run_async(\n    self,\n    call_decorator: Callable[\n        [Callable[..., Awaitable[BaseDynamicConfig]]],\n        Callable[..., Awaitable[_BaseCallResponseT]],\n    ]\n    | Callable[\n        [Callable[..., Awaitable[BaseDynamicConfig]]],\n        Callable[..., Awaitable[_BaseStreamT]],\n    ]\n    | Callable[\n        [Callable[..., Awaitable[BaseDynamicConfig]]],\n        Callable[..., Awaitable[_ResponseModelT]],\n    ]\n    | Callable[\n        [Callable[..., Awaitable[BaseDynamicConfig]]],\n        Callable[..., Awaitable[AsyncIterable[_ResponseModelT]]],\n    ],\n    *additional_decorators: Callable[[_T], _T],\n) -&gt; (\n    Awaitable[_BaseCallResponseT]\n    | Awaitable[_BaseStreamT]\n    | Awaitable[_ResponseModelT]\n    | Awaitable[AsyncIterable[_ResponseModelT]]\n):\n    \"\"\"Returns the response of calling the API of the provided decorator.\n\n    Example:\n\n    ```python\n    import asyncio\n\n    from mirascope.core import BasePrompt, openai, prompt_template\n\n\n    @prompt_template(\"Recommend a {genre} book\")\n    class BookRecommendationPrompt(BasePrompt):\n        genre: str\n\n\n    async def run():\n        prompt = BookRecommendationPrompt(genre=\"fantasy\")\n        response = await prompt.run_async(openai.call(\"gpt-4o-mini\"))\n        print(response.content)\n\n\n    asyncio.run(run())\n    ```\n    \"\"\"\n    kwargs = {field: getattr(self, field) for field in self.model_fields}\n    args_str = \", \".join(kwargs.keys())\n    namespace, fn_name = {}, self.__class__.__name__\n    exec(f\"async def {fn_name}({args_str}): ...\", namespace)\n    return reduce(\n        lambda res, f: f(res),  # pyright: ignore [reportArgumentType, reportCallIssue]\n        [\n            metadata(get_metadata(self, self.dynamic_config())),\n            prompt_template(get_prompt_template(self)),\n            call_decorator,\n            *additional_decorators,\n        ],\n        namespace[fn_name],\n    )(**kwargs)\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.prompt_template","title":"prompt_template","text":"<pre><code>prompt_template(\n    template: str | None = None,\n) -&gt; PromptDecorator | MessagesDecorator\n</code></pre> <p>A decorator for setting the <code>prompt_template</code> of a <code>BasePrompt</code> or <code>call</code>.</p> Usage Documentation <p>Prompts</p> <p>Example:</p> <pre><code>from mirascope.core import openai, prompt_template\n\n\n@prompt_template()\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: [BaseMessageParam(role='user', content='Recommend a fantasy book')]\n</code></pre> <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator function that turns the decorated function into a prompt template.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def prompt_template(\n    template: str | None = None,\n) -&gt; PromptDecorator | MessagesDecorator:\n    \"\"\"A decorator for setting the `prompt_template` of a `BasePrompt` or `call`.\n\n    usage docs: learn/prompts.md#prompt-templates-messages\n\n    Example:\n\n    ```python\n    from mirascope.core import openai, prompt_template\n\n\n    @prompt_template()\n    def recommend_book(genre: str) -&gt; str:\n        return f\"Recommend a {genre} book\"\n\n\n    print(recommend_book(\"fantasy\"))\n    # Output: [BaseMessageParam(role='user', content='Recommend a fantasy book')]\n    ```\n\n    Returns:\n        decorator (Callable): The decorator function that turns the decorated function\n            into a prompt template.\n    \"\"\"\n\n    if template is None:\n        # For @prompt_template() case\n        decorator = messages_decorator()\n        decorator.__mirascope_prompt_template__ = True  # pyright: ignore [reportAttributeAccessIssue]\n        return decorator\n\n    @overload\n    def inner(\n        prompt: Callable[_P, BaseDynamicConfig],\n    ) -&gt; Callable[_P, list[BaseMessageParam]]: ...\n\n    @overload\n    def inner(\n        prompt: Callable[_P, Awaitable[BaseDynamicConfig]],\n    ) -&gt; Callable[_P, Awaitable[list[BaseMessageParam]]]: ...\n\n    @overload\n    def inner(prompt: type[_BasePromptT]) -&gt; type[_BasePromptT]: ...\n\n    def inner(\n        prompt: type[_BasePromptT]\n        | Callable[_P, BaseDynamicConfig]\n        | Callable[_P, Awaitable[BaseDynamicConfig]],\n    ) -&gt; (\n        Callable[_P, list[BaseMessageParam]]\n        | Callable[_P, Awaitable[list[BaseMessageParam]]]\n        | type[_BasePromptT]\n    ):\n        \"\"\"Updates the `prompt_template` class attribute to the given value.\"\"\"\n        prompt._prompt_template = template  # pyright: ignore [reportAttributeAccessIssue,reportFunctionMemberAccess]\n\n        if isinstance(prompt, type):\n            return prompt\n\n        if fn_is_async(prompt):\n\n            @wraps(prompt)\n            async def get_base_message_params_async(\n                *args: _P.args, **kwargs: _P.kwargs\n            ) -&gt; list[BaseMessageParam]:\n                return parse_prompt_messages(\n                    roles=SUPPORTED_MESSAGE_ROLES,\n                    template=template,\n                    attrs=get_fn_args(prompt, args, kwargs),\n                    dynamic_config=await prompt(*args, **kwargs),\n                )\n\n            get_base_message_params_async._original_fn = prompt  # pyright: ignore [reportAttributeAccessIssue,reportFunctionMemberAccess]\n            return get_base_message_params_async\n\n        else:\n\n            @wraps(prompt)\n            def get_base_message_params(\n                *args: _P.args, **kwargs: _P.kwargs\n            ) -&gt; list[BaseMessageParam]:\n                return parse_prompt_messages(\n                    roles=SUPPORTED_MESSAGE_ROLES,\n                    template=template,\n                    attrs=get_fn_args(prompt, args, kwargs),\n                    dynamic_config=prompt(*args, **kwargs),\n                )\n\n            get_base_message_params._original_fn = prompt  # pyright: ignore [reportAttributeAccessIssue,reportFunctionMemberAccess]\n            return get_base_message_params\n\n    inner.__mirascope_prompt_template__ = True  # pyright: ignore [reportFunctionMemberAccess]\n    return inner\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.metadata","title":"metadata","text":"<pre><code>metadata(metadata: Metadata) -&gt; MetadataDecorator\n</code></pre> <p>A decorator for adding metadata to a <code>BasePrompt</code> or <code>call</code>.</p> <p>Adding this decorator to a <code>BasePrompt</code> or <code>call</code> updates the <code>metadata</code> annotation to the given value. This is useful for adding metadata to a <code>BasePrompt</code> or <code>call</code> that can be used for logging or filtering.</p> <p>Example:</p> <pre><code>from mirascope.core import metadata, openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\n@metadata({\"tags\": {\"version:0001\", \"books\"}})\ndef recommend_book(genre: str):\n    ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.metadata)\n</code></pre> <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator function that updates the <code>_metadata</code> attribute of the decorated input prompt or call.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def metadata(metadata: Metadata) -&gt; MetadataDecorator:\n    \"\"\"A decorator for adding metadata to a `BasePrompt` or `call`.\n\n    Adding this decorator to a `BasePrompt` or `call` updates the `metadata` annotation\n    to the given value. This is useful for adding metadata to a `BasePrompt` or `call`\n    that can be used for logging or filtering.\n\n    Example:\n\n    ```python\n    from mirascope.core import metadata, openai, prompt_template\n\n\n    @openai.call(\"gpt-4o-mini\")\n    @prompt_template(\"Recommend a {genre} book\")\n    @metadata({\"tags\": {\"version:0001\", \"books\"}})\n    def recommend_book(genre: str):\n        ...\n\n\n    response = recommend_book(\"fantasy\")\n    print(response.metadata)\n    ```\n\n    Returns:\n        decorator (Callable): The decorator function that updates the `_metadata`\n            attribute of the decorated input prompt or call.\n    \"\"\"\n\n    @overload\n    def inner(prompt: type[_BasePromptT]) -&gt; type[_BasePromptT]: ...\n\n    @overload\n    def inner(prompt: Callable[_P, _R]) -&gt; Callable[_P, _R]: ...\n\n    def inner(\n        prompt: type[_BasePromptT] | Callable[_P, _R],\n    ) -&gt; type[_BasePromptT] | Callable[_P, _R]:\n        \"\"\"Updates the `metadata` class attribute to the given value.\"\"\"\n        prompt._metadata = metadata  # pyright: ignore [reportAttributeAccessIssue,reportFunctionMemberAccess]\n        return prompt\n\n    return inner\n</code></pre>"},{"location":"api/core/base/stream/","title":"mirascope.core.base.stream","text":"<p>This module contains the base classes for streaming responses from LLMs.</p>"},{"location":"api/core/base/stream/#mirascope.core.base.stream.BaseStream","title":"BaseStream","text":"<pre><code>BaseStream(\n    *,\n    stream: (\n        Generator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n            None,\n        ]\n        | AsyncGenerator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n        ]\n    ),\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT]\n)\n</code></pre> <p>               Bases: <code>Generic[_BaseCallResponseT, _BaseCallResponseChunkT, _UserMessageParamT, _AssistantMessageParamT, _ToolMessageParamT, _MessageParamT, _BaseToolT, _ToolSchemaT, _BaseDynamicConfigT, _BaseCallParamsT, _FinishReason]</code>, <code>ABC</code></p> <p>A base class for streaming responses from LLMs.</p> Source code in <code>mirascope/core/base/stream.py</code> <pre><code>def __init__(\n    self,\n    *,\n    stream: Generator[tuple[_BaseCallResponseChunkT, _BaseToolT | None], None, None]\n    | AsyncGenerator[\n        tuple[_BaseCallResponseChunkT, _BaseToolT | None],\n        None,\n    ],\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT],\n) -&gt; None:\n    \"\"\"Initializes an instance of `BaseStream`.\"\"\"\n    self.content = \"\"\n    self.stream = stream\n    self.metadata = metadata\n    self.tool_types = tool_types\n    self.call_response_type = call_response_type\n    self.model = model\n    self.prompt_template = prompt_template\n    self.fn_args = fn_args\n    self.dynamic_config = dynamic_config\n    self.messages = messages\n    self.call_params = call_params\n    self.call_kwargs = call_kwargs\n    self.user_message_param = get_possible_user_message_param(messages)  # pyright: ignore [reportAttributeAccessIssue]\n</code></pre>"},{"location":"api/core/base/stream/#mirascope.core.base.stream.BaseStream.cost","title":"cost  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the stream.</p>"},{"location":"api/core/base/stream/#mirascope.core.base.stream.BaseStream.tool_message_params","title":"tool_message_params","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[_BaseToolT, str]]\n) -&gt; list[_ToolMessageParamT]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[_BaseToolT, str]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required Source code in <code>mirascope/core/base/stream.py</code> <pre><code>def tool_message_params(\n    self, tools_and_outputs: list[tuple[_BaseToolT, str]]\n) -&gt; list[_ToolMessageParamT]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n    \"\"\"\n    return self.call_response_type.tool_message_params(tools_and_outputs)\n</code></pre>"},{"location":"api/core/base/stream/#mirascope.core.base.stream.BaseStream.construct_call_response","title":"construct_call_response  <code>abstractmethod</code>","text":"<pre><code>construct_call_response() -&gt; _BaseCallResponseT\n</code></pre> <p>Constructs the call response.</p> Source code in <code>mirascope/core/base/stream.py</code> <pre><code>@abstractmethod\ndef construct_call_response(self) -&gt; _BaseCallResponseT:\n    \"\"\"Constructs the call response.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/base/structured_stream/","title":"mirascope.core.base.structured_stream","text":"<p>This module defines the base class for structured streams.</p>"},{"location":"api/core/base/structured_stream/#mirascope.core.base.structured_stream.BaseStructuredStream","title":"BaseStructuredStream","text":"<pre><code>BaseStructuredStream(\n    *,\n    stream: BaseStream,\n    response_model: type[_ResponseModelT],\n    fields_from_call_args: dict[str, Any]\n)\n</code></pre> <p>               Bases: <code>Generic[_ResponseModelT]</code></p> <p>A base class for streaming structured outputs from LLMs.</p> Source code in <code>mirascope/core/base/structured_stream.py</code> <pre><code>def __init__(\n    self,\n    *,\n    stream: BaseStream,\n    response_model: type[_ResponseModelT],\n    fields_from_call_args: dict[str, Any],\n) -&gt; None:\n    \"\"\"Initializes an instance of `BaseStructuredStream`.\"\"\"\n    self.stream = stream\n    self.response_model = response_model\n    self.fields_from_call_args = fields_from_call_args\n</code></pre>"},{"location":"api/core/base/tool/","title":"mirascope.core.base.tool","text":"<p>This module defines the base class for tools used in LLM calls.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.ToolConfig","title":"ToolConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>A base class for tool configurations.</p>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.BaseTool","title":"BaseTool","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[_ToolSchemaT]</code></p> <p>A class for defining tools for LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import BaseTool\nfrom pydantic import Field\n\n\nclass FormatBook(BaseTool):\n    \"\"\"Returns a nicely formatted book recommendation.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n    author: str = Field(..., description=\"The author of the book.\")\n\n    def call(self) -&gt; str:\n        return f\"{self.title} by {self.author}\"\n</code></pre>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.BaseTool.args","title":"args  <code>property</code>","text":"<pre><code>args: dict[str, Any]\n</code></pre> <p>The arguments of the tool as a dictionary.</p>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.BaseTool.call","title":"call  <code>abstractmethod</code>","text":"<pre><code>call(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>The method to call the tool.</p> Source code in <code>mirascope/core/base/tool.py</code> <pre><code>@abstractmethod\ndef call(self, *args: Any, **kwargs: Any) -&gt; Any:  # noqa: ANN401\n    \"\"\"The method to call the tool.\"\"\"\n    ...\n</code></pre>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.BaseTool.type_from_fn","title":"type_from_fn  <code>classmethod</code>","text":"<pre><code>type_from_fn(fn: Callable) -&gt; type[_BaseToolT]\n</code></pre> <p>Returns this tool type converted from a function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to convert into this tool type.</p> required Source code in <code>mirascope/core/base/tool.py</code> <pre><code>@classmethod\ndef type_from_fn(cls: type[_BaseToolT], fn: Callable) -&gt; type[_BaseToolT]:\n    \"\"\"Returns this tool type converted from a function.\n\n    Args:\n        fn: The function to convert into this tool type.\n    \"\"\"\n    return _utils.convert_function_to_base_tool(fn, cls)\n</code></pre>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.BaseTool.type_from_base_model_type","title":"type_from_base_model_type  <code>classmethod</code>","text":"<pre><code>type_from_base_model_type(\n    tool_type: type[BaseModel],\n) -&gt; type[_BaseToolT]\n</code></pre> <p>Returns this tool type converted from a given base tool type.</p> <p>Parameters:</p> Name Type Description Default <code>tool_type</code> <code>type[BaseModel]</code> <p>The tool type to convert into this tool type. This can be a custom <code>BaseTool</code> or <code>BaseModel</code> definition.</p> required Source code in <code>mirascope/core/base/tool.py</code> <pre><code>@classmethod\ndef type_from_base_model_type(\n    cls: type[_BaseToolT], tool_type: type[BaseModel]\n) -&gt; type[_BaseToolT]:\n    \"\"\"Returns this tool type converted from a given base tool type.\n\n    Args:\n        tool_type: The tool type to convert into this tool type. This can be a\n            custom `BaseTool` or `BaseModel` definition.\n    \"\"\"\n    return _utils.convert_base_model_to_base_tool(tool_type, cls)\n</code></pre>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.BaseTool.type_from_base_type","title":"type_from_base_type  <code>classmethod</code>","text":"<pre><code>type_from_base_type(\n    base_type: type[BaseType],\n) -&gt; type[_BaseToolT]\n</code></pre> <p>Returns this tool type converted from a base type.</p> <p>Parameters:</p> Name Type Description Default <code>base_type</code> <code>type[BaseType]</code> <p>The base type (e.g. <code>int</code>) to convert into this tool type.</p> required Source code in <code>mirascope/core/base/tool.py</code> <pre><code>@classmethod\ndef type_from_base_type(\n    cls: type[_BaseToolT], base_type: type[_utils.BaseType]\n) -&gt; type[_BaseToolT]:\n    \"\"\"Returns this tool type converted from a base type.\n\n    Args:\n        base_type: The base type (e.g. `int`) to convert into this tool type.\n    \"\"\"\n    return _utils.convert_base_type_to_base_tool(base_type, cls)\n</code></pre>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.BaseTool.model_json_schema","title":"model_json_schema  <code>classmethod</code>","text":"<pre><code>model_json_schema(\n    by_alias: bool = True,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    schema_generator: type[\n        GenerateJsonSchema\n    ] = GenerateJsonSchemaNoTitles,\n    mode: JsonSchemaMode = \"validation\",\n) -&gt; dict[str, Any]\n</code></pre> <p>Returns the generated JSON schema for the class.</p> Source code in <code>mirascope/core/base/tool.py</code> <pre><code>@classmethod\ndef model_json_schema(\n    cls,\n    by_alias: bool = True,\n    ref_template: str = DEFAULT_REF_TEMPLATE,\n    schema_generator: type[GenerateJsonSchema] = GenerateJsonSchemaNoTitles,\n    mode: JsonSchemaMode = \"validation\",\n) -&gt; dict[str, Any]:\n    \"\"\"Returns the generated JSON schema for the class.\"\"\"\n    cls.warn_for_unsupported_configurations()\n    return super().model_json_schema(\n        by_alias=by_alias,\n        ref_template=ref_template,\n        schema_generator=schema_generator,\n        mode=mode,\n    )\n</code></pre>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.BaseTool.warn_for_unsupported_configurations","title":"warn_for_unsupported_configurations  <code>classmethod</code>","text":"<pre><code>warn_for_unsupported_configurations() -&gt; None\n</code></pre> <p>Warns when a specific provider does not support provided config options.</p> Source code in <code>mirascope/core/base/tool.py</code> <pre><code>@classmethod\ndef warn_for_unsupported_configurations(cls) -&gt; None:\n    \"\"\"Warns when a specific provider does not support provided config options.\"\"\"\n    unsupported_tool_keys = _utils.get_unsupported_tool_config_keys(\n        cls.tool_config, cls.__tool_config_type__\n    )\n    if unsupported_tool_keys:\n        warnings.warn(\n            f\"{cls.__provider__} does not support the following tool \"\n            f\"configurations, so they will be ignored: {unsupported_tool_keys}\",\n            UserWarning,\n        )\n\n    if \"strict\" in cls.model_config and cls.__provider__ not in [\"openai\", \"azure\"]:\n        warnings.warn(\n            f\"{cls.__provider__} does not support strict structured outputs, but \"\n            \"you have configured `strict=True` in your `ResponseModelConfigDict`. \"\n            \"Ignoring `strict` as this feature is only supported by OpenAI.\",\n            UserWarning,\n        )\n</code></pre>"},{"location":"api/core/base/toolkit/","title":"mirascope.core.base.toolkit","text":"<p>The module for defining the toolkit class for LLM call tools.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/base/toolkit/#mirascope.core.base.toolkit.BaseToolKit","title":"BaseToolKit","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>A class for defining tools for LLM call tools.</p> <p>The class should have methods decorated with <code>@toolkit_tool</code> to create tools.</p> <p>Example: <pre><code>from mirascope.core.base import BaseToolKit, toolkit_tool\nfrom mirascope.core import openai\n\nclass BookRecommendationToolKit(BaseToolKit):\n    '''A toolkit for recommending books.'''\n\n    __namespace__: ClassVar[str | None] = 'book_tools'\n    reading_level: Literal[\"beginner\", \"advanced\"]\n\n    @toolkit_tool\n    def format_book(self, title: str, author: str) -&gt; str:\n        '''Returns the title and author of a book nicely formatted.\n\n        Reading level: {self.reading_level}\n        '''\n        return f\"{title} by {author}\"\n\n@openai.call(model=\"gpt-4o\")\ndef recommend_book(genre: str, reading_level: Literal[\"beginner\", \"advanced\"]):\n    '''Recommend a {genre} book.'''\n    toolkit = BookRecommendationToolKit(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\nresponse = recommend_book(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    output = tool.call()\n    print(output)\n    #&gt; The Name of the Wind by Patrick Rothfuss\nelse:\n    print(response.content)\n    #&gt; Sure! I would recommend...\n</code></pre></p>"},{"location":"api/core/base/toolkit/#mirascope.core.base.toolkit.BaseToolKit.create_tools","title":"create_tools","text":"<pre><code>create_tools() -&gt; list[type[BaseTool]]\n</code></pre> <p>The method to create the tools.</p> Source code in <code>mirascope/core/base/toolkit.py</code> <pre><code>def create_tools(self) -&gt; list[type[BaseTool]]:\n    \"\"\"The method to create the tools.\"\"\"\n    tools = []\n    for method, template_vars, template in self._toolkit_tool_methods:\n        for var in template_vars:\n            if var.startswith(\"self.\"):\n                continue\n            # Replace non-self template variables with escaped double brackets so\n            # that templating `self` results in a future templateable string.\n            template = template.replace(f\"{{{var}}}\", f\"{{{{{var}}}}}\")\n        converted_method = convert_function_to_base_tool(\n            method, BaseTool, template.format(self=self), self.__namespace__\n        )\n        for key, value in self:\n            setattr(converted_method, key, value)\n        tools.append(converted_method)\n    return tools\n</code></pre>"},{"location":"api/core/bedrock/call/","title":"mirascope.core.bedrock.call","text":"<p>A decorator for calling the Bedrock API with a typed function.</p> Usage Documentation <p>Calls</p> <p>This decorator is used to wrap a typed function that calls the Bedrock API. It parses the prompt template of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.bedrock import bedrock_call\n\n\n@bedrock_call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str):\n    ...\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The Bedrock model to use in the API call.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <code>list[BaseTool | Callable]</code> <p>The tools to use in the Bedrock API call.</p> required <code>response_model</code> <code>BaseModel | BaseType</code> <p>The response model into which the response should be structured.</p> required <code>output_parser</code> <code>Callable[[BedrockCallResponse | ResponseModelT], Any]</code> <p>A function for  parsing the call response whose value will be returned in place of the original call response.</p> required <code>json_mode</code> <code>bool</code> <p>Whether to use JSON Mode.</p> required <code>client</code> <code>object</code> <p>An optional custom client to use in place of the default client.</p> required <code>call_params</code> <code>BedrockCallParams</code> <p>The <code>BedrockCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator for turning a typed function into an Bedrock API call.</p>"},{"location":"api/core/bedrock/call_params/","title":"mirascope.core.bedrock.call_params","text":"Usage Documentation <p>Calls</p>"},{"location":"api/core/bedrock/call_params/#mirascope.core.bedrock.call_params.BedrockCallParams","title":"BedrockCallParams","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Bedrock API.</p> <p>Bedrock converse API Reference</p> <p>Attributes:</p> Name Type Description <code>system</code> <code>Sequence[SystemContentBlockTypeDef]</code> <p>The system content blocks to use in the API call.</p> <code>inferenceConfig</code> <code>InferenceConfigurationTypeDef</code> <p>The inference configuration to use in the API call.</p> <code>toolConfig</code> <code>ToolConfigurationTypeDef</code> <p>The tool configuration to use in the API call.</p> <code>guardrailConfig</code> <code>GuardrailConfigurationTypeDef</code> <p>The guardrail configuration to use in the API call.</p> <code>additionalModelRequestFields</code> <code>Mapping[str, Any]</code> <p>Additional model request fields to use in the API call.</p> <code>additionalModelResponseFieldPaths</code> <code>Sequence[str]</code> <p>Additional model response field paths to use in the API call.</p>"},{"location":"api/core/bedrock/call_response/","title":"mirascope.core.bedrock.call_response","text":"<p>This module contains the <code>BedrockCallResponse</code> class.</p> Usage Documentation <p>Calls</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse","title":"BedrockCallResponse","text":"<p>               Bases: <code>BaseCallResponse[ConverseResponseTypeDef | ConverseResponseTypeDef, BedrockTool, ToolTypeDef, BedrockDynamicConfig, InternalBedrockMessageParam, BedrockCallParams, UserMessageTypeDef]</code></p> <p>A convenience wrapper around the Bedrock <code>ChatCompletion</code> response.</p> <p>When calling the Bedrock API using a function decorated with <code>bedrock_call</code>, the response will be an <code>BedrockCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.bedrock import bedrock_call\n\n\n@bedrock_call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str):\n    ...\n\n\nresponse = recommend_book(\"fantasy\")  # response is an `BedrockCallResponse` instance\nprint(response.content)\n</code></pre>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.message","title":"message  <code>property</code>","text":"<pre><code>message: SyncMessageTypeDef | AsyncMessageTypeDef | None\n</code></pre> <p>Returns the message of the response.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[str]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: TokenUsageTypeDef\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.message_param","title":"message_param  <code>property</code>","text":"<pre><code>message_param: SerializeAsAny[AssistantMessageTypeDef]\n</code></pre> <p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.tools","title":"tools  <code>property</code>","text":"<pre><code>tools: list[BedrockTool] | None\n</code></pre> <p>Returns any available tool calls as their <code>BedrockTool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p> <code>ValueError</code> <p>if the model refused to response, in which case the error message will be the refusal.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.tool","title":"tool  <code>property</code>","text":"<pre><code>tool: BedrockTool | None\n</code></pre> <p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p> <code>ValueError</code> <p>if the model refused to response, in which case the error message will be the refusal.</p>"},{"location":"api/core/bedrock/call_response/#mirascope.core.bedrock.call_response.BedrockCallResponse.tool_message_params","title":"tool_message_params  <code>classmethod</code>","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[BedrockTool, str]]\n) -&gt; list[ToolResultBlockMessageTypeDef]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[BedrockTool, str]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required <p>Returns:</p> Type Description <code>list[ToolResultBlockMessageTypeDef]</code> <p>The list of constructed <code>ChatCompletionToolMessageParam</code> parameters.</p> Source code in <code>mirascope/core/bedrock/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[BedrockTool, str]]\n) -&gt; list[ToolResultBlockMessageTypeDef]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n\n    Returns:\n        The list of constructed `ChatCompletionToolMessageParam` parameters.\n    \"\"\"\n    return [\n        ToolResultBlockMessageTypeDef(\n            role=\"user\",\n            content=[\n                cast(\n                    ToolResultBlockContentTypeDef,\n                    {\n                        \"toolResult\": {\n                            \"content\": [{\"text\": output}],\n                            \"toolUseId\": tool.tool_call[\"toolUse\"][\"toolUseId\"],\n                            \"name\": tool._name(),\n                        }\n                    },\n                )\n            ],\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/bedrock/call_response_chunk/","title":"mirascope.core.bedrock.call_response_chunk","text":"<p>This module contains the <code>BedrockCallResponseChunk</code> class.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/bedrock/call_response_chunk/#mirascope.core.bedrock.call_response_chunk.BedrockCallResponseChunk","title":"BedrockCallResponseChunk","text":"<p>               Bases: <code>BaseCallResponseChunk[StreamOutputChunk | AsyncStreamOutputChunk, StopReasonType]</code></p> <p>A convenience wrapper around the Bedrock <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Bedrock API using a function decorated with <code>bedrock_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>BedrockResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.bedrock import bedrock_call\n\n\n@bedrock_call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str):\n    ...\n\n\nstream = recommend_book(\"fantasy\")  # response is an `BedrockStream`\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/core/bedrock/call_response_chunk/#mirascope.core.bedrock.call_response_chunk.BedrockCallResponseChunk.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/bedrock/call_response_chunk/#mirascope.core.bedrock.call_response_chunk.BedrockCallResponseChunk.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[StopReasonType]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/bedrock/call_response_chunk/#mirascope.core.bedrock.call_response_chunk.BedrockCallResponseChunk.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/bedrock/call_response_chunk/#mirascope.core.bedrock.call_response_chunk.BedrockCallResponseChunk.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/bedrock/call_response_chunk/#mirascope.core.bedrock.call_response_chunk.BedrockCallResponseChunk.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: TokenUsageTypeDef | None\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/bedrock/call_response_chunk/#mirascope.core.bedrock.call_response_chunk.BedrockCallResponseChunk.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/bedrock/call_response_chunk/#mirascope.core.bedrock.call_response_chunk.BedrockCallResponseChunk.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/bedrock/dynamic_config/","title":"mirascope.core.bedrock.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/bedrock/dynamic_config/#mirascope.core.bedrock.dynamic_config.BedrockDynamicConfig","title":"BedrockDynamicConfig  <code>module-attribute</code>","text":"<pre><code>BedrockDynamicConfig = BaseDynamicConfig[\n    InternalBedrockMessageParam | BaseMessageParam,\n    BedrockCallParams,\n]\n</code></pre> <p>The function return type for functions wrapped with the <code>bedrock_call</code> decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.bedrock import BedrockDynamicConfig, bedrock_call\n\n\n@bedrock_call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {capitalized_genre} book\")\ndef recommend_book(genre: str) -&gt; BedrockDynamicConfig:\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/bedrock/stream/","title":"mirascope.core.bedrock.stream","text":"<p>The <code>BedrockStream</code> class for convenience around streaming LLM calls.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/bedrock/stream/#mirascope.core.bedrock.stream.BedrockStream","title":"BedrockStream","text":"<pre><code>BedrockStream(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseStream[BedrockCallResponse, BedrockCallResponseChunk, UserMessageTypeDef, AssistantMessageTypeDef, ToolUseBlockMessageTypeDef, InternalBedrockMessageParam, BedrockTool, ToolTypeDef, BedrockDynamicConfig, BedrockCallParams, StopReasonType]</code></p> <p>A class for convenience around streaming Bedrock LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.bedrock import bedrock_call\n\n\n@bedrock_call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str):\n    ...\n\n\nstream = recommend_book(\"fantasy\")  # returns `BedrockStream` instance\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> Source code in <code>mirascope/core/bedrock/stream.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:  # noqa: ANN401\n    super().__init__(*args, **kwargs)\n    self._metadata: (\n        ConverseStreamMetadataEventTypeDef\n        | AsyncConverseStreamMetadataEventTypeDef\n        | None\n    ) = None\n    self._response_metadata: (\n        ResponseMetadataTypeDef | AsyncResponseMetadataTypeDef\n    ) = _DEFAULT_RESPONSE_METADATA\n</code></pre>"},{"location":"api/core/bedrock/stream/#mirascope.core.bedrock.stream.BedrockStream.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/bedrock/stream/#mirascope.core.bedrock.stream.BedrockStream.construct_call_response","title":"construct_call_response","text":"<pre><code>construct_call_response() -&gt; BedrockCallResponse\n</code></pre> <p>Constructs the call response from a consumed BedrockStream.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the stream has not yet been consumed.</p> Source code in <code>mirascope/core/bedrock/stream.py</code> <pre><code>def construct_call_response(self) -&gt; BedrockCallResponse:\n    \"\"\"Constructs the call response from a consumed BedrockStream.\n\n    Raises:\n        ValueError: if the stream has not yet been consumed.\n    \"\"\"\n    if not hasattr(self, \"message_param\"):\n        raise ValueError(\n            \"No stream response, check if the stream has been consumed.\"\n        )\n    response = ConverseResponseTypeDef(\n        output={\"message\": cast(MessageOutputTypeDef, self.message_param)},\n        stopReason=self.finish_reasons[0] if self.finish_reasons else \"end_turn\",\n        usage=self.metadata.get(\"usage\", {}),\n        metrics=self.metadata.get(\"metrics\", {}),\n        additionalModelResponseFields={},\n        trace=self.metadata.get(\"trace\", {}),\n        ResponseMetadata=self._response_metadata,\n    )\n    return BedrockCallResponse(\n        metadata=self.metadata,\n        response=response,\n        tool_types=self.tool_types,\n        prompt_template=self.prompt_template,\n        fn_args=self.fn_args if self.fn_args else {},\n        dynamic_config=self.dynamic_config,\n        messages=self.messages,\n        call_params=self.call_params,\n        call_kwargs=self.call_kwargs,\n        user_message_param=self.user_message_param,\n        start_time=self.start_time,\n        end_time=self.end_time,\n    )\n</code></pre>"},{"location":"api/core/bedrock/tool/","title":"mirascope.core.bedrock.tool","text":"<p>The <code>BedrockTool</code> class for easy tool usage with Bedrock LLM calls.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/bedrock/tool/#mirascope.core.bedrock.tool.BedrockToolConfig","title":"BedrockToolConfig","text":"<p>               Bases: <code>ToolConfig</code></p> <p>A tool configuration for Bedrock-specific features.</p>"},{"location":"api/core/bedrock/tool/#mirascope.core.bedrock.tool.BedrockTool","title":"BedrockTool","text":"<p>               Bases: <code>BaseTool[ToolTypeDef]</code></p> <p>A class for defining tools for Bedrock LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.bedrock import bedrock_call\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@bedrock_call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[format_book])\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str):\n    ...\n\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:  # returns an `BedrockTool` instance\n    print(tool.call())\n</code></pre>"},{"location":"api/core/bedrock/tool/#mirascope.core.bedrock.tool.BedrockTool.tool_schema","title":"tool_schema  <code>classmethod</code>","text":"<pre><code>tool_schema() -&gt; ToolTypeDef\n</code></pre> <p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> <p>Example: <pre><code>from mirascope.core.bedrock import BedrockTool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\ntool_type = BedrockTool.type_from_fn(format_book)\nprint(tool_type.tool_schema())  # prints the Bedrock-specific tool schema\n</code></pre></p> Source code in <code>mirascope/core/bedrock/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ToolTypeDef:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\n\n    Example:\n    ```python\n    from mirascope.core.bedrock import BedrockTool\n\n\n    def format_book(title: str, author: str) -&gt; str:\n        return f\"{title} by {author}\"\n\n\n    tool_type = BedrockTool.type_from_fn(format_book)\n    print(tool_type.tool_schema())  # prints the Bedrock-specific tool schema\n    ```\n    \"\"\"\n    schema_generator = GenerateJsonSchemaNoTitles\n    return ToolTypeDef(\n        toolSpec=ToolSpecificationTypeDef(\n            name=cls._name(),\n            description=cls._description(),\n            inputSchema={\n                \"json\": cls.model_json_schema(schema_generator=schema_generator)\n            },\n        )\n    )\n</code></pre>"},{"location":"api/core/bedrock/tool/#mirascope.core.bedrock.tool.BedrockTool.from_tool_call","title":"from_tool_call  <code>classmethod</code>","text":"<pre><code>from_tool_call(\n    tool_call: ToolUseBlockContentTypeDef,\n) -&gt; BedrockTool\n</code></pre> <p>Constructs an <code>BedrockTool</code> instance from a <code>tool_call</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ToolUseBlockContentTypeDef</code> <p>The Bedrock tool call from which to construct this tool instance.</p> required Source code in <code>mirascope/core/bedrock/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolUseBlockContentTypeDef) -&gt; BedrockTool:\n    \"\"\"Constructs an `BedrockTool` instance from a `tool_call`.\n\n    Args:\n        tool_call: The Bedrock tool call from which to construct this tool instance.\n    \"\"\"\n    return cls.model_validate(\n        {\"tool_call\": tool_call, **tool_call[\"toolUse\"][\"input\"]}\n    )\n</code></pre>"},{"location":"api/core/cohere/call/","title":"mirascope.core.cohere.call","text":"<p>A decorator for calling the Cohere API with a typed function.</p> Usage Documentation <p>Calls</p> <p>This decorator is used to wrap a typed function that calls the Cohere API. It parses the prompt template of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.cohere import cohere_call\n\n\n@cohere_call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The Cohere model to use in the API call.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <code>list[BaseTool | Callable]</code> <p>The tools to use in the Cohere API call.</p> required <code>response_model</code> <code>BaseModel | BaseType</code> <p>The response model into which the response should be structured.</p> required <code>output_parser</code> <code>Callable[[CohereCallResponse | ResponseModelT], Any]</code> <p>A function for parsing the call response whose value will be returned in place of the original call response.</p> required <code>json_mode</code> <code>bool</code> <p>Whether to use JSON Mode.</p> required <code>client</code> <code>object</code> <p>An optional custom client to use in place of the default client.</p> required <code>call_params</code> <code>CohereCallParams</code> <p>The <code>CohereCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator for turning a typed function into a Cohere API call.</p>"},{"location":"api/core/cohere/call_params/","title":"mirascope.core.cohere.call_params","text":"Usage Documentation <p>Calls</p>"},{"location":"api/core/cohere/call_params/#mirascope.core.cohere.call_params.CohereCallParams","title":"CohereCallParams","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Cohere API.</p> <p>Cohere API Reference</p> <p>Attributes:</p> Name Type Description <code>chat_history</code> <code>NotRequired[Sequence[ChatMessage] | None]</code> <p>...</p> <code>connectors</code> <code>NotRequired[Sequence[ChatConnector] | None]</code> <p>...</p> <code>conversation_id</code> <code>NotRequired[str | None]</code> <p>...</p> <code>documents</code> <code>NotRequired[Sequence[ChatDocument] | None]</code> <p>...</p> <code>frequency_penalty</code> <code>NotRequired[float | None]</code> <p>...</p> <code>k</code> <code>NotRequired[int | None]</code> <p>...</p> <code>max_input_tokens</code> <code>NotRequired[int | None]</code> <p>...</p> <code>max_tokens</code> <code>NotRequired[int | None]</code> <p>...</p> <code>p</code> <code>NotRequired[float | None]</code> <p>...</p> <code>preamble</code> <code>NotRequired[str | None]</code> <p>...</p> <code>presence_penalty</code> <code>NotRequired[float | None]</code> <p>...</p> <code>prompt_truncation</code> <code>NotRequired[ChatRequestPromptTruncation | None]</code> <p>...</p> <code>raw_prompting</code> <code>NotRequired[bool | None]</code> <p>...</p> <code>search_queries_only</code> <code>NotRequired[bool | None]</code> <p>...</p> <code>seed</code> <code>NotRequired[int | None]</code> <p>...</p> <code>stop_sequences</code> <code>NotRequired[Sequence[str] | None]</code> <p>...</p> <code>temperature</code> <code>NotRequired[float | None]</code> <p>...</p> <code>tool_results</code> <code>NotRequired[Sequence[ToolResult] | None]</code> <p>...</p>"},{"location":"api/core/cohere/call_response/","title":"mirascope.core.cohere.call_response","text":"<p>This module contains the <code>CohereCallResponse</code> class.</p> Usage Documentation <p>Calls</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse","title":"CohereCallResponse","text":"<p>               Bases: <code>BaseCallResponse[SkipValidation[NonStreamedChatResponse], CohereTool, SkipValidation[Tool], CohereDynamicConfig, SkipValidation[ChatMessage], CohereCallParams, SkipValidation[ChatMessage]]</code></p> <p>A convenience wrapper around the Cohere <code>ChatCompletion</code> response.</p> <p>When calling the Cohere API using a function decorated with <code>cohere_call</code>, the response will be an <code>CohereCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.cohere import cohere_call\n\n\n@cohere_call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")  # response is an `CohereCallResponse` instance\nprint(response.content)\n</code></pre>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[str] | None\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p> <p>Cohere does not return model, so we return the model provided by the user.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.id","title":"id  <code>property</code>","text":"<pre><code>id: str | None\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: ApiMetaBilledUnits | None\n</code></pre> <p>Returns the usage of the response.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: float | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: float | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the response.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.message_param","title":"message_param  <code>property</code>","text":"<pre><code>message_param: ChatMessage\n</code></pre> <p>Returns the assistant's response as a message parameter.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.tools","title":"tools  <code>property</code>","text":"<pre><code>tools: list[CohereTool] | None\n</code></pre> <p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.tool","title":"tool  <code>property</code>","text":"<pre><code>tool: CohereTool | None\n</code></pre> <p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.tool_message_params","title":"tool_message_params  <code>classmethod</code>","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[CohereTool, str]]\n) -&gt; list[ToolResult]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[CohereTool, str]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required <p>Returns:</p> Type Description <code>list[ToolResult]</code> <p>The list of constructed <code>ToolResult</code> parameters.</p> Source code in <code>mirascope/core/cohere/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls,\n    tools_and_outputs: list[tuple[CohereTool, str]],\n) -&gt; list[ToolResult]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n\n    Returns:\n        The list of constructed `ToolResult` parameters.\n    \"\"\"\n    return [\n        ToolResult(\n            call=tool.tool_call,\n            outputs=[{\"output\": output}],\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/cohere/call_response_chunk/","title":"mirascope.core.cohere.call_response_chunk","text":"<p>This module contains the <code>CohereCallResponseChunk</code> class.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk","title":"CohereCallResponseChunk","text":"<p>               Bases: <code>BaseCallResponseChunk[SkipValidation[StreamedChatResponse], ChatStreamEndEventFinishReason]</code></p> <p>A convenience wrapper around the Cohere <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Cohere API using a function decorated with <code>cohere_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>CohereResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.cohere import cohere_call\n\n\n@cohere_call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # response is an `CohereStream`\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[ChatStreamEndEventFinishReason] | None\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.model","title":"model  <code>property</code>","text":"<pre><code>model: str | None\n</code></pre> <p>Returns the name of the response model.</p> <p>Cohere does not return model, so we return None</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.id","title":"id  <code>property</code>","text":"<pre><code>id: str | None\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: ApiMetaBilledUnits | None\n</code></pre> <p>Returns the usage of the response.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: float | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: float | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/cohere/dynamic_config/","title":"mirascope.core.cohere.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/cohere/dynamic_config/#mirascope.core.cohere.dynamic_config.CohereDynamicConfig","title":"CohereDynamicConfig  <code>module-attribute</code>","text":"<pre><code>CohereDynamicConfig = BaseDynamicConfig[\n    ChatMessage | BaseMessageParam, CohereCallParams\n]\n</code></pre> <p>The function return type for functions wrapped with the <code>cohere_call</code> decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.cohere import CohereDynamicConfig, cohere_call\n\n\n@cohere_call(\"command-r-plus\")\n@prompt_template(\"Recommend a {capitalized_genre} book\")\ndef recommend_book(genre: str) -&gt; CohereDynamicConfig:\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/cohere/stream/","title":"mirascope.core.cohere.stream","text":"<p>The <code>CohereStream</code> class for convenience around streaming LLM calls.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/cohere/stream/#mirascope.core.cohere.stream.CohereStream","title":"CohereStream","text":"<pre><code>CohereStream(\n    *,\n    stream: (\n        Generator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n            None,\n        ]\n        | AsyncGenerator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n        ]\n    ),\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT]\n)\n</code></pre> <p>               Bases: <code>BaseStream[CohereCallResponse, CohereCallResponseChunk, ChatMessage, ChatMessage, ChatMessage, ChatMessage, CohereTool, Tool, CohereDynamicConfig, CohereCallParams, ChatStreamEndEventFinishReason]</code></p> <p>A class for convenience around streaming Cohere LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.cohere import cohere_call\n\n\n@cohere_call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # returns `CohereStream` instance\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> Source code in <code>mirascope/core/base/stream.py</code> <pre><code>def __init__(\n    self,\n    *,\n    stream: Generator[tuple[_BaseCallResponseChunkT, _BaseToolT | None], None, None]\n    | AsyncGenerator[\n        tuple[_BaseCallResponseChunkT, _BaseToolT | None],\n        None,\n    ],\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT],\n) -&gt; None:\n    \"\"\"Initializes an instance of `BaseStream`.\"\"\"\n    self.content = \"\"\n    self.stream = stream\n    self.metadata = metadata\n    self.tool_types = tool_types\n    self.call_response_type = call_response_type\n    self.model = model\n    self.prompt_template = prompt_template\n    self.fn_args = fn_args\n    self.dynamic_config = dynamic_config\n    self.messages = messages\n    self.call_params = call_params\n    self.call_kwargs = call_kwargs\n    self.user_message_param = get_possible_user_message_param(messages)  # pyright: ignore [reportAttributeAccessIssue]\n</code></pre>"},{"location":"api/core/cohere/stream/#mirascope.core.cohere.stream.CohereStream.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/cohere/stream/#mirascope.core.cohere.stream.CohereStream.construct_call_response","title":"construct_call_response","text":"<pre><code>construct_call_response() -&gt; CohereCallResponse\n</code></pre> <p>Constructs the call response from a consumed CohereStream.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the stream has not yet been consumed.</p> Source code in <code>mirascope/core/cohere/stream.py</code> <pre><code>def construct_call_response(self) -&gt; CohereCallResponse:\n    \"\"\"Constructs the call response from a consumed CohereStream.\n\n    Raises:\n        ValueError: if the stream has not yet been consumed.\n    \"\"\"\n    if not hasattr(self, \"message_param\"):\n        raise ValueError(\n            \"No stream response, check if the stream has been consumed.\"\n        )\n    if not self.input_tokens and not self.output_tokens:\n        meta = None\n    else:\n        meta = ApiMeta(\n            billed_units=ApiMetaBilledUnits(\n                input_tokens=self.input_tokens, output_tokens=self.output_tokens\n            )\n        )\n    completion = NonStreamedChatResponse(\n        generation_id=self.id,\n        text=self.message_param.message,\n        meta=meta,\n        finish_reason=self.finish_reasons[0] if self.finish_reasons else None,\n    )\n\n    return CohereCallResponse(\n        metadata=self.metadata,\n        response=completion,\n        tool_types=self.tool_types,\n        prompt_template=self.prompt_template,\n        fn_args=self.fn_args if self.fn_args else {},\n        dynamic_config=self.dynamic_config,\n        messages=self.messages,\n        call_params=self.call_params,\n        call_kwargs=self.call_kwargs,\n        user_message_param=self.user_message_param,\n        start_time=self.start_time,\n        end_time=self.end_time,\n    )\n</code></pre>"},{"location":"api/core/cohere/tool/","title":"mirascope.core.cohere.tool","text":"<p>The <code>CohereTool</code> class for easy tool usage with Cohere LLM calls.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/cohere/tool/#mirascope.core.cohere.tool.CohereTool","title":"CohereTool","text":"<p>               Bases: <code>BaseTool[Tool]</code></p> <p>A class for defining tools for Cohere LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.cohere import cohere_call\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@cohere_call(\"command-r-plus\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:  # returns an `CohereTool` instance\n    print(tool.call())\n</code></pre>"},{"location":"api/core/cohere/tool/#mirascope.core.cohere.tool.CohereTool.tool_schema","title":"tool_schema  <code>classmethod</code>","text":"<pre><code>tool_schema() -&gt; Tool\n</code></pre> <p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> <p>Example: <pre><code>from mirascope.core.cohere import CohereTool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\ntool_type = CohereTool.type_from_fn(format_book)\nprint(tool_type.tool_schema())  # prints the Cohere-specific tool schema\n</code></pre></p> Source code in <code>mirascope/core/cohere/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Tool:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\n\n    Example:\n    ```python\n    from mirascope.core.cohere import CohereTool\n\n\n    def format_book(title: str, author: str) -&gt; str:\n        return f\"{title} by {author}\"\n\n\n    tool_type = CohereTool.type_from_fn(format_book)\n    print(tool_type.tool_schema())  # prints the Cohere-specific tool schema\n    ```\n    \"\"\"\n    model_schema = cls.model_json_schema()\n    parameter_definitions = None\n    if \"properties\" in model_schema:\n        if \"$defs\" in model_schema[\"properties\"]:\n            raise ValueError(  # pragma: no cover\n                \"Unfortunately Cohere's chat API cannot handle nested structures \"\n                \"with $defs.\"\n            )\n        parameter_definitions = {\n            prop: ToolParameterDefinitionsValue(\n                description=prop_schema.get(\"description\", None),\n                type=prop_schema[\"type\"],\n                required=\"required\" in model_schema\n                and prop in model_schema[\"required\"],\n            )\n            for prop, prop_schema in model_schema[\"properties\"].items()\n        }\n    return Tool(\n        name=cls._name(),\n        description=cls._description(),\n        parameter_definitions=parameter_definitions,\n    )\n</code></pre>"},{"location":"api/core/cohere/tool/#mirascope.core.cohere.tool.CohereTool.from_tool_call","title":"from_tool_call  <code>classmethod</code>","text":"<pre><code>from_tool_call(tool_call: ToolCall) -&gt; CohereTool\n</code></pre> <p>Constructs an <code>CohereTool</code> instance from a <code>tool_call</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ToolCall</code> <p>The Cohere tool call from which to construct this tool instance.</p> required Source code in <code>mirascope/core/cohere/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolCall) -&gt; CohereTool:\n    \"\"\"Constructs an `CohereTool` instance from a `tool_call`.\n\n    Args:\n        tool_call: The Cohere tool call from which to construct this tool instance.\n    \"\"\"\n    model_json = {**tool_call.parameters}\n    model_json[\"tool_call\"] = tool_call.dict()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/gemini/call/","title":"mirascope.core.gemini.call","text":"<p>A decorator for calling the Gemini API with a typed function.</p> Usage Documentation <p>Calls</p> <p>This decorator is used to wrap a typed function that calls the Gemini API. It parses the prompt template of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.gemini import gemini_call\n\n\n@gemini_call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The Gemini model to use in the API call.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <code>list[BaseTool | Callable]</code> <p>The tools to use in the Gemini API call.</p> required <code>response_model</code> <code>BaseModel | BaseType</code> <p>The response model into which the response should be structured.</p> required <code>output_parser</code> <code>Callable[[GeminiCallResponse | ResponseModelT], Any]</code> <p>A function for parsing the call response whose value will be returned in place of the original call response.</p> required <code>json_modem</code> <code>bool</code> <p>Whether to use JSON Mode.</p> required <code>client</code> <code>object</code> <p>An optional custom client to use in place of the default client.</p> required <code>call_params</code> <code>GeminiCallParams</code> <p>The <code>GeminiCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator for turning a typed function into a Gemini API call.</p>"},{"location":"api/core/gemini/call_params/","title":"mirascope.core.gemini.call_params","text":"Usage Documentation <p>Calls</p>"},{"location":"api/core/gemini/call_params/#mirascope.core.gemini.call_params.GeminiCallParams","title":"GeminiCallParams","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Gemini API.</p> <p>Gemini API Reference</p> <p>Attributes:</p> Name Type Description <code>generation_config</code> <code>NotRequired[GenerationConfigDict | GenerationConfig]</code> <p>...</p> <code>safety_settings</code> <code>NotRequired[SafetySettingOptions]</code> <p>...</p> <code>request_options</code> <code>NotRequired[RequestOptions]</code> <p>...</p> <code>tool_config</code> <code>NotRequired[ToolConfigType]</code> <p>...</p>"},{"location":"api/core/gemini/call_response/","title":"mirascope.core.gemini.call_response","text":"<p>This module contains the <code>GeminiCallResponse</code> class.</p> Usage Documentation <p>Calls</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse","title":"GeminiCallResponse","text":"<p>               Bases: <code>BaseCallResponse[GenerateContentResponse | AsyncGenerateContentResponse, GeminiTool, Tool, GeminiDynamicConfig, ContentsType, GeminiCallParams, ContentDict]</code></p> <p>A convenience wrapper around the Gemini API response.</p> <p>When calling the Gemini API using a function decorated with <code>gemini_call</code>, the response will be a <code>GeminiCallResponse</code> instance with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.gemini import gemini_call\n\n\n@gemini_call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")  # response is an `GeminiCallResponse` instance\nprint(response.content)\n</code></pre>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the contained string content for the 0th choice.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[str]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the model name.</p> <p>google.generativeai does not return model, so we return the model provided by the user.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.id","title":"id  <code>property</code>","text":"<pre><code>id: str | None\n</code></pre> <p>Returns the id of the response.</p> <p>google.generativeai does not return an id</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: None\n</code></pre> <p>Returns the usage of the chat completion.</p> <p>google.generativeai does not have Usage, so we return None</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.message_param","title":"message_param  <code>property</code>","text":"<pre><code>message_param: ContentDict\n</code></pre> <p>Returns the models's response as a message parameter.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.tools","title":"tools  <code>property</code>","text":"<pre><code>tools: list[GeminiTool] | None\n</code></pre> <p>Returns the list of tools for the 0th candidate's 0th content part.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.tool","title":"tool  <code>property</code>","text":"<pre><code>tool: GeminiTool | None\n</code></pre> <p>Returns the 0th tool for the 0th candidate's 0th content part.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.tool_message_params","title":"tool_message_params  <code>classmethod</code>","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[GeminiTool, object]]\n) -&gt; list[FunctionResponse]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[GeminiTool, object]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required <p>Returns:</p> Type Description <code>list[FunctionResponse]</code> <p>The list of constructed <code>FunctionResponse</code> parameters.</p> Source code in <code>mirascope/core/gemini/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[GeminiTool, object]]\n) -&gt; list[FunctionResponse]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n\n    Returns:\n        The list of constructed `FunctionResponse` parameters.\n    \"\"\"\n    return [\n        FunctionResponse(name=tool._name(), response={\"result\": output})\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/gemini/call_response_chunk/","title":"mirascope.core.gemini.call_response_chunk","text":"<p>This module contains the <code>GeminiCallResponseChunk</code> class.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk","title":"GeminiCallResponseChunk","text":"<p>               Bases: <code>BaseCallResponseChunk[GenerateContentResponse, FinishReason]</code></p> <p>A convenience wrapper around the Gemini API streamed response chunks.</p> <p>When calling the Gemini API using a function decorated with <code>gemini_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>GeminiCallResponseChunk</code> instances</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.gemini import gemini_call\n\n\n@gemini_call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # response is an `GeminiStream`\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the chunk content for the 0th choice.</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[FinishReason]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.model","title":"model  <code>property</code>","text":"<pre><code>model: None\n</code></pre> <p>Returns the model name.</p> <p>google.generativeai does not return model, so we return None</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.id","title":"id  <code>property</code>","text":"<pre><code>id: str | None\n</code></pre> <p>Returns the id of the response.</p> <p>google.generativeai does not return an id</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: None\n</code></pre> <p>Returns the usage of the chat completion.</p> <p>google.generativeai does not have Usage, so we return None</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/gemini/dynamic_config/","title":"mirascope.core.gemini.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/gemini/dynamic_config/#mirascope.core.gemini.dynamic_config.GeminiDynamicConfig","title":"GeminiDynamicConfig  <code>module-attribute</code>","text":"<pre><code>GeminiDynamicConfig = BaseDynamicConfig[\n    ContentsType | BaseMessageParam, GeminiCallParams\n]\n</code></pre> <p>The function return type for functions wrapped with the <code>gemini_call</code> decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.gemini import GeminiDynamicConfig, gemini_call\n\n\n@gemini_call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {capitalized_genre} book\")\ndef recommend_book(genre: str) -&gt; GeminiDynamicConfig:\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/gemini/stream/","title":"mirascope.core.gemini.stream","text":"<p>The <code>GeminiStream</code> class for convenience around streaming LLM calls.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/gemini/stream/#mirascope.core.gemini.stream.GeminiStream","title":"GeminiStream","text":"<pre><code>GeminiStream(\n    *,\n    stream: (\n        Generator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n            None,\n        ]\n        | AsyncGenerator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n        ]\n    ),\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT]\n)\n</code></pre> <p>               Bases: <code>BaseStream[GeminiCallResponse, GeminiCallResponseChunk, ContentDict, ContentDict, ContentDict, ContentsType, GeminiTool, Tool, GeminiDynamicConfig, GeminiCallParams, FinishReason]</code></p> <p>A class for convenience around streaming Gemini LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.gemini import gemini_call\n\n\n@gemini_call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nstream = recommend_book(\"fantasy\")  # returns `GeminiStream` instance\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> Source code in <code>mirascope/core/base/stream.py</code> <pre><code>def __init__(\n    self,\n    *,\n    stream: Generator[tuple[_BaseCallResponseChunkT, _BaseToolT | None], None, None]\n    | AsyncGenerator[\n        tuple[_BaseCallResponseChunkT, _BaseToolT | None],\n        None,\n    ],\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT],\n) -&gt; None:\n    \"\"\"Initializes an instance of `BaseStream`.\"\"\"\n    self.content = \"\"\n    self.stream = stream\n    self.metadata = metadata\n    self.tool_types = tool_types\n    self.call_response_type = call_response_type\n    self.model = model\n    self.prompt_template = prompt_template\n    self.fn_args = fn_args\n    self.dynamic_config = dynamic_config\n    self.messages = messages\n    self.call_params = call_params\n    self.call_kwargs = call_kwargs\n    self.user_message_param = get_possible_user_message_param(messages)  # pyright: ignore [reportAttributeAccessIssue]\n</code></pre>"},{"location":"api/core/gemini/stream/#mirascope.core.gemini.stream.GeminiStream.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/gemini/stream/#mirascope.core.gemini.stream.GeminiStream.construct_call_response","title":"construct_call_response","text":"<pre><code>construct_call_response() -&gt; GeminiCallResponse\n</code></pre> <p>Constructs the call response from a consumed GeminiStream.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the stream has not yet been consumed.</p> Source code in <code>mirascope/core/gemini/stream.py</code> <pre><code>def construct_call_response(self) -&gt; GeminiCallResponse:\n    \"\"\"Constructs the call response from a consumed GeminiStream.\n\n    Raises:\n        ValueError: if the stream has not yet been consumed.\n    \"\"\"\n    if not hasattr(self, \"message_param\"):\n        raise ValueError(\n            \"No stream response, check if the stream has been consumed.\"\n        )\n    response = GenerateContentResponseType.from_response(\n        GenerateContentResponse(\n            candidates=[\n                Candidate(\n                    finish_reason=self.finish_reasons[0]\n                    if self.finish_reasons\n                    else Candidate.FinishReason.STOP,\n                    content=Content(\n                        role=self.message_param[\"role\"],\n                        parts=self.message_param[\"parts\"],\n                    ),\n                )\n            ]\n        )\n    )\n    return GeminiCallResponse(\n        metadata=self.metadata,\n        response=response,\n        tool_types=self.tool_types,\n        prompt_template=self.prompt_template,\n        fn_args=self.fn_args if self.fn_args else {},\n        dynamic_config=self.dynamic_config,\n        messages=self.messages,\n        call_params=self.call_params,\n        call_kwargs=self.call_kwargs,\n        user_message_param=self.user_message_param,\n        start_time=self.start_time,\n        end_time=self.end_time,\n    )\n</code></pre>"},{"location":"api/core/gemini/tool/","title":"mirascope.core.gemini.tool","text":"<p>The <code>GeminiTool</code> class for easy tool usage with Google's Gemini LLM calls.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/gemini/tool/#mirascope.core.gemini.tool.GeminiTool","title":"GeminiTool","text":"<p>               Bases: <code>BaseTool[Tool]</code></p> <p>A class for defining tools for Gemini LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.gemini import gemini_call\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@gemini_call(\"gemini-1.5-flash\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:  # returns an `GeminiTool` instance\n    print(tool.call())\n</code></pre>"},{"location":"api/core/gemini/tool/#mirascope.core.gemini.tool.GeminiTool.tool_schema","title":"tool_schema  <code>classmethod</code>","text":"<pre><code>tool_schema() -&gt; Tool\n</code></pre> <p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> <p>Example: <pre><code>from mirascope.core.gemini import GeminiTool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\ntool_type = GeminiTool.type_from_fn(format_book)\nprint(tool_type.tool_schema())  # prints the Gemini-specific tool schema\n</code></pre></p> Source code in <code>mirascope/core/gemini/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Tool:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\n\n    Example:\n    ```python\n    from mirascope.core.gemini import GeminiTool\n\n\n    def format_book(title: str, author: str) -&gt; str:\n        return f\"{title} by {author}\"\n\n\n    tool_type = GeminiTool.type_from_fn(format_book)\n    print(tool_type.tool_schema())  # prints the Gemini-specific tool schema\n    ```\n    \"\"\"\n    model_schema = cls.model_json_schema()\n    fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n    if model_schema[\"required\"]:\n        fn[\"parameters\"][\"required\"] = model_schema[\"required\"]\n    if \"parameters\" in fn:\n        if \"$defs\" in fn[\"parameters\"]:\n            raise ValueError(\n                \"Unfortunately Google's Gemini API cannot handle nested structures \"\n                \"with $defs.\"\n            )\n\n        def handle_enum_schema(prop_schema: dict[str, Any]) -&gt; dict[str, Any]:\n            if \"enum\" in prop_schema:\n                prop_schema[\"format\"] = \"enum\"\n            return prop_schema\n\n        fn[\"parameters\"][\"properties\"] = {\n            prop: {\n                key: value\n                for key, value in handle_enum_schema(prop_schema).items()\n                if key != \"default\"\n            }\n            for prop, prop_schema in fn[\"parameters\"][\"properties\"].items()\n        }\n    return Tool(function_declarations=[FunctionDeclaration(**fn)])\n</code></pre>"},{"location":"api/core/gemini/tool/#mirascope.core.gemini.tool.GeminiTool.from_tool_call","title":"from_tool_call  <code>classmethod</code>","text":"<pre><code>from_tool_call(tool_call: FunctionCall) -&gt; GeminiTool\n</code></pre> <p>Constructs an <code>GeminiTool</code> instance from a <code>tool_call</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>FunctionCall</code> <p>The Gemini tool call from which to construct this tool instance.</p> required Source code in <code>mirascope/core/gemini/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n    \"\"\"Constructs an `GeminiTool` instance from a `tool_call`.\n\n    Args:\n        tool_call: The Gemini tool call from which to construct this tool instance.\n    \"\"\"\n    if not tool_call.args:\n        raise ValueError(\"Tool call doesn't have any arguments.\")\n    model_json: dict[str, Any] = dict(tool_call.args.items())\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/groq/call/","title":"mirascope.core.groq.call","text":"<p>A decorator for calling the Groq API with a typed function.</p> Usage Documentation <p>Calls</p> <p>This decorator is used to wrap a typed function that calls the Groq API. It parses the prompt template of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.groq import groq_call\n\n\n@groq_call(\"llama-3.1-8b-instant\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The Groq model to use in the API call.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <code>list[BaseTool | Callable]</code> <p>The tools to use in the Groq API call.</p> required <code>response_model</code> <code>BaseModel | BaseType</code> <p>The response model into which the response should be structured.</p> required <code>output_parser</code> <code>Callable[[CohereCallResponse | ResponseModelT], Any]</code> <p>A function for parsing the call response whose value will be returned in place of the original call response.</p> required <code>json_mode</code> <code>bool</code> <p>Whether to use JSON Mode.</p> required <code>client</code> <code>object</code> <p>An optional custom client to use in place of the default client.</p> required <code>call_params</code> <code>GroqCallParams</code> <p>The <code>GroqCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator for turning a typed function into a Groq API call.</p>"},{"location":"api/core/groq/call_params/","title":"mirascope.core.groq.call_params","text":"Usage Documentation <p>Calls</p>"},{"location":"api/core/groq/call_params/#mirascope.core.groq.call_params.GroqCallParams","title":"GroqCallParams","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Groq API.</p> <p>Groq API Reference</p> <p>Attributes:</p> Name Type Description <code>frequency_penalty</code> <code>NotRequired[float | None]</code> <p>...</p> <code>logit_bias</code> <code>NotRequired[dict[str, int] | None]</code> <p>...</p> <code>logprobs</code> <code>NotRequired[bool | None]</code> <p>...</p> <code>max_tokens</code> <code>NotRequired[int | None]</code> <p>...</p> <code>n</code> <code>NotRequired[int | None]</code> <p>...</p> <code>parallel_tool_calls</code> <code>NotRequired[bool]</code> <p>...</p> <code>presence_penalty</code> <code>NotRequired[float | None]</code> <p>...</p> <code>response_format</code> <code>NotRequired[ResponseFormat]</code> <p>...</p> <code>seed</code> <code>NotRequired[int | None]</code> <p>...</p> <code>stop</code> <code>NotRequired[str | list[str] | None]</code> <p>...</p> <code>temperature</code> <code>NotRequired[float | None]</code> <p>...</p> <code>tool_choice</code> <code>NotRequired[ChatCompletionToolChoiceOptionParam]</code> <p>...</p> <code>top_logprobs</code> <code>NotRequired[int | None]</code> <p>...</p> <code>top_p</code> <code>NotRequired[float | None]</code> <p>...</p> <code>user</code> <code>NotRequired[str]</code> <p>...</p>"},{"location":"api/core/groq/call_response/","title":"mirascope.core.groq.call_response","text":"<p>This module contains the <code>GroqCallResponse</code> class.</p> Usage Documentation <p>Calls</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse","title":"GroqCallResponse","text":"<p>               Bases: <code>BaseCallResponse[ChatCompletion, GroqTool, ChatCompletionToolParam, GroqDynamicConfig, ChatCompletionMessageParam, GroqCallParams, ChatCompletionUserMessageParam]</code></p> <p>A convenience wrapper around the Groq <code>ChatCompletion</code> response.</p> <p>When calling the Groq API using a function decorated with <code>groq_call</code>, the response will be an <code>GroqCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.groq import groq_call\n\n\n@groq_call(\"llama-3.1-8b-instant\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `GroqCallResponse` instance\nprint(response.content)\n</code></pre>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[str]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: CompletionUsage | None\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.message_param","title":"message_param  <code>property</code>","text":"<pre><code>message_param: SerializeAsAny[\n    ChatCompletionAssistantMessageParam\n]\n</code></pre> <p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.tools","title":"tools  <code>property</code>","text":"<pre><code>tools: list[GroqTool] | None\n</code></pre> <p>Returns any available tool calls as their <code>GroqTool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.tool","title":"tool  <code>property</code>","text":"<pre><code>tool: GroqTool | None\n</code></pre> <p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.tool_message_params","title":"tool_message_params  <code>classmethod</code>","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[GroqTool, str]]\n) -&gt; list[ChatCompletionToolMessageParam]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[GroqTool, str]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required <p>Returns:</p> Type Description <code>list[ChatCompletionToolMessageParam]</code> <p>The list of constructed <code>ChatCompletionToolMessageParam</code> parameters.</p> Source code in <code>mirascope/core/groq/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[GroqTool, str]]\n) -&gt; list[ChatCompletionToolMessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n\n    Returns:\n        The list of constructed `ChatCompletionToolMessageParam` parameters.\n    \"\"\"\n    return [\n        ChatCompletionToolMessageParam(\n            role=\"tool\",\n            content=output,\n            tool_call_id=tool.tool_call.id,\n            name=tool._name(),  # pyright: ignore [reportCallIssue]\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/groq/call_response_chunk/","title":"mirascope.core.groq.call_response_chunk","text":"<p>This module contains the <code>GroqCallResponseChunk</code> class.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk","title":"GroqCallResponseChunk","text":"<p>               Bases: <code>BaseCallResponseChunk[ChatCompletionChunk, FinishReason]</code></p> <p>A convenience wrapper around the Groq <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Groq API using a function decorated with <code>groq_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>GroqResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.groq import groq_call\n\n\n@groq_call(\"llama-3.1-8b-instant\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nstream = recommend_book(\"fantasy\")  # response is an `GroqStream`\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[FinishReason]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: CompletionUsage | None\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/groq/dynamic_config/","title":"mirascope.core.groq.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/groq/dynamic_config/#mirascope.core.groq.dynamic_config.GroqDynamicConfig","title":"GroqDynamicConfig  <code>module-attribute</code>","text":"<pre><code>GroqDynamicConfig = BaseDynamicConfig[\n    ChatCompletionMessageParam | BaseMessageParam,\n    GroqCallParams,\n]\n</code></pre> <p>The function return type for functions wrapped with the <code>groq_call</code> decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.groq import GroqDynamicConfig, groq_call\n\n\n@groq_call(\"llama-3.1-8b-instant\")\n@prompt_template(\"Recommend a {capitalized_genre} book\")\ndef recommend_book(genre: str) -&gt; GroqDynamicConfig:\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/groq/stream/","title":"mirascope.core.groq.stream","text":"<p>The <code>GroqStream</code> class for convenience around streaming LLM calls.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/groq/stream/#mirascope.core.groq.stream.GroqStream","title":"GroqStream","text":"<pre><code>GroqStream(\n    *,\n    stream: (\n        Generator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n            None,\n        ]\n        | AsyncGenerator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n        ]\n    ),\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT]\n)\n</code></pre> <p>               Bases: <code>BaseStream[GroqCallResponse, GroqCallResponseChunk, ChatCompletionUserMessageParam, ChatCompletionAssistantMessageParam, ChatCompletionToolMessageParam, ChatCompletionMessageParam, GroqTool, ChatCompletionToolParam, GroqDynamicConfig, GroqCallParams, FinishReason]</code></p> <p>A class for convenience around streaming Groq LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.groq import groq_call\n\n\n@groq_call(\"llama-3.1-8b-instant\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nstream = recommend_book(\"fantasy\")  # returns `GroqStream` instance\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> Source code in <code>mirascope/core/base/stream.py</code> <pre><code>def __init__(\n    self,\n    *,\n    stream: Generator[tuple[_BaseCallResponseChunkT, _BaseToolT | None], None, None]\n    | AsyncGenerator[\n        tuple[_BaseCallResponseChunkT, _BaseToolT | None],\n        None,\n    ],\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT],\n) -&gt; None:\n    \"\"\"Initializes an instance of `BaseStream`.\"\"\"\n    self.content = \"\"\n    self.stream = stream\n    self.metadata = metadata\n    self.tool_types = tool_types\n    self.call_response_type = call_response_type\n    self.model = model\n    self.prompt_template = prompt_template\n    self.fn_args = fn_args\n    self.dynamic_config = dynamic_config\n    self.messages = messages\n    self.call_params = call_params\n    self.call_kwargs = call_kwargs\n    self.user_message_param = get_possible_user_message_param(messages)  # pyright: ignore [reportAttributeAccessIssue]\n</code></pre>"},{"location":"api/core/groq/stream/#mirascope.core.groq.stream.GroqStream.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/groq/stream/#mirascope.core.groq.stream.GroqStream.construct_call_response","title":"construct_call_response","text":"<pre><code>construct_call_response() -&gt; GroqCallResponse\n</code></pre> <p>Constructs the call response from a consumed GroqStream.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the stream has not yet been consumed.</p> Source code in <code>mirascope/core/groq/stream.py</code> <pre><code>def construct_call_response(self) -&gt; GroqCallResponse:\n    \"\"\"Constructs the call response from a consumed GroqStream.\n\n    Raises:\n        ValueError: if the stream has not yet been consumed.\n    \"\"\"\n    if not hasattr(self, \"message_param\"):\n        raise ValueError(\n            \"No stream response, check if the stream has been consumed.\"\n        )\n    message = {\n        \"role\": self.message_param[\"role\"],\n        \"content\": self.message_param.get(\"content\", \"\"),\n        \"tool_calls\": self.message_param.get(\"tool_calls\", []),\n    }\n    if not self.input_tokens and not self.output_tokens:\n        usage = None\n    else:\n        usage = CompletionUsage(\n            prompt_tokens=int(self.input_tokens or 0),\n            completion_tokens=int(self.output_tokens or 0),\n            total_tokens=int(self.input_tokens or 0) + int(self.output_tokens or 0),\n        )\n    completion = ChatCompletion(\n        id=self.id if self.id else \"\",\n        model=self.model,\n        choices=[\n            Choice(\n                finish_reason=self.finish_reasons[0]\n                if self.finish_reasons and self.finish_reasons[0]\n                else \"stop\",\n                index=0,\n                message=ChatCompletionMessage.model_validate(message),\n            )\n        ],\n        created=0,\n        object=\"chat.completion\",\n        usage=usage,\n    )\n    return GroqCallResponse(\n        metadata=self.metadata,\n        response=completion,\n        tool_types=self.tool_types,\n        prompt_template=self.prompt_template,\n        fn_args=self.fn_args if self.fn_args else {},\n        dynamic_config=self.dynamic_config,\n        messages=self.messages,\n        call_params=self.call_params,\n        call_kwargs=self.call_kwargs,\n        user_message_param=self.user_message_param,\n        start_time=self.start_time,\n        end_time=self.end_time,\n    )\n</code></pre>"},{"location":"api/core/groq/tool/","title":"mirascope.core.groq.tool","text":"<p>The <code>GroqTool</code> class for easy tool usage with Groq LLM calls.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/groq/tool/#mirascope.core.groq.tool.GroqTool","title":"GroqTool","text":"<p>               Bases: <code>BaseTool[ChatCompletionToolParam]</code></p> <p>A class for defining tools for Groq LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.groq import groq_call\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@groq_call(\"llama-3.1-8b-instant\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:  # returns an `GroqTool` instance\n    print(tool.call())\n</code></pre>"},{"location":"api/core/groq/tool/#mirascope.core.groq.tool.GroqTool.tool_schema","title":"tool_schema  <code>classmethod</code>","text":"<pre><code>tool_schema() -&gt; ChatCompletionToolParam\n</code></pre> <p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> <p>Example: <pre><code>from mirascope.core.groq import GroqTool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\ntool_type = GroqTool.type_from_fn(format_book)\nprint(tool_type.tool_schema())  # prints the Groq-specific tool schema\n</code></pre></p> Source code in <code>mirascope/core/groq/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\n\n    Example:\n    ```python\n    from mirascope.core.groq import GroqTool\n\n\n    def format_book(title: str, author: str) -&gt; str:\n        return f\"{title} by {author}\"\n\n\n    tool_type = GroqTool.type_from_fn(format_book)\n    print(tool_type.tool_schema())  # prints the Groq-specific tool schema\n    ```\n    \"\"\"\n    fn = FunctionDefinition(name=cls._name(), description=cls._description())\n    model_schema = cls.model_json_schema()\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n    return ChatCompletionToolParam(function=fn, type=\"function\")\n</code></pre>"},{"location":"api/core/groq/tool/#mirascope.core.groq.tool.GroqTool.from_tool_call","title":"from_tool_call  <code>classmethod</code>","text":"<pre><code>from_tool_call(\n    tool_call: ChatCompletionMessageToolCall,\n) -&gt; GroqTool\n</code></pre> <p>Constructs an <code>GroqTool</code> instance from a <code>tool_call</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ChatCompletionMessageToolCall</code> <p>The Groq tool call from which to construct this tool instance.</p> required Source code in <code>mirascope/core/groq/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; GroqTool:\n    \"\"\"Constructs an `GroqTool` instance from a `tool_call`.\n\n    Args:\n        tool_call: The Groq tool call from which to construct this tool instance.\n    \"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/litellm/call/","title":"mirascope.core.litellm.call","text":"<p>See OpenAI For Full Reference</p> <p>We use the types found in the <code>openai</code> module for our LiteLLM integration, so please reference that module for information regarding anything beyond the LiteLLM <code>call</code> decorator.</p> <p>A decorator for calling the LiteLLM API with a typed function.</p> Usage Documentation <p>Calls</p> <p>This decorator is used to wrap a typed function that calls the LiteLLM API. It parses the prompt template of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.litellm import litellm_call\n\n\n@litellm_call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model to use in the API call.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <code>list[BaseTool | Callable]</code> <p>The tools to use in the API call.</p> required <code>response_model</code> <code>BaseModel | BaseType</code> <p>The response model into which the response should be structured.</p> required <code>output_parser</code> <code>Callable[[OpenAICallResponse | ResponseModelT], Any]</code> <p>A function for parsing the call response whose value will be returned in place of the original call response.</p> required <code>json_mode</code> <code>bool</code> <p>Whether to use JSON Mode.</p> required <code>client</code> <code>None</code> <p>LiteLLM does not support a custom client.</p> required <code>call_params</code> <code>OpenAICallParams</code> <p>The <code>OpenAICallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator for turning a typed function into a LiteLLM routed LLM API call.</p>"},{"location":"api/core/mistral/call/","title":"mirascope.core.mistral.call","text":"<p>A decorator for calling the Mistral API with a typed function.</p> Usage Documentation <p>Calls</p> <p>This decorator is used to wrap a typed function that calls the Mistral API. It parses the prompt template of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.mistral import mistral_call\n\n\n@mistral_call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The Mistral model to use in the API call.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <code>list[BaseTool | Callable]</code> <p>The tools to use in the Mistral API call.</p> required <code>response_model</code> <code>BaseModel | BaseType</code> <p>The response model into which the response should be structured.</p> required <code>output_parser</code> <code>Callable[[MistralCallResponse | ResponseModelT], Any]</code> <p>A function for parsing the call response whose value will be returned in place of the original call response.</p> required <code>json_mode</code> <code>bool</code> <p>Whether to use JSON Mode.</p> required <code>client</code> <code>object</code> <p>An optional custom client to use in place of the default client.</p> required <code>call_params</code> <code>MistralCallParams</code> <p>The <code>MistralCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator for turning a typed function into a Mistral API call.</p>"},{"location":"api/core/mistral/call_params/","title":"mirascope.core.mistral.call_params","text":"Usage Documentation <p>Calls</p>"},{"location":"api/core/mistral/call_params/#mirascope.core.mistral.call_params.MistralCallParams","title":"MistralCallParams","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Mistral API.</p> <p>Mistral API Reference</p> <p>Attributes:</p> Name Type Description <code>endpoint</code> <code>NotRequired[str | None]</code> <p>...</p> <code>max_tokens</code> <code>NotRequired[int | None]</code> <p>...</p> <code>random_seed</code> <code>NotRequired[int | None]</code> <p>...</p> <code>response_format</code> <code>NotRequired[ResponseFormat | None]</code> <p>...</p> <code>safe_mode</code> <code>NotRequired[bool | None]</code> <p>...</p> <code>safe_prompt</code> <code>NotRequired[bool | None]</code> <p>...</p> <code>temperature</code> <code>NotRequired[float | None]</code> <p>...</p> <code>tool_choice</code> <code>NotRequired[ToolChoice | None]</code> <p>...</p> <code>top_p</code> <code>NotRequired[float | None]</code> <p>...</p>"},{"location":"api/core/mistral/call_response/","title":"mirascope.core.mistral.call_response","text":"<p>This module contains the <code>MistralCallResponse</code> class.</p> Usage Documentation <p>Calls</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse","title":"MistralCallResponse","text":"<p>               Bases: <code>BaseCallResponse[ChatCompletionResponse, MistralTool, dict[str, Any], MistralDynamicConfig, ChatMessage, MistralCallParams, ChatMessage]</code></p> <p>A convenience wrapper around the Mistral <code>ChatCompletion</code> response.</p> <p>When calling the Mistral API using a function decorated with <code>mistral_call</code>, the response will be an <code>MistralCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.mistral import mistral_call\n\n\n@mistral_call(\"mistral-largel-latest\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `MistralCallResponse` instance\nprint(response.content)\n</code></pre>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>The content of the chat completion for the 0th choice.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[str]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: UsageInfo\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.message_param","title":"message_param  <code>property</code>","text":"<pre><code>message_param: ChatMessage\n</code></pre> <p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.tools","title":"tools  <code>property</code>","text":"<pre><code>tools: list[MistralTool] | None\n</code></pre> <p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.tool","title":"tool  <code>property</code>","text":"<pre><code>tool: MistralTool | None\n</code></pre> <p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.tool_message_params","title":"tool_message_params  <code>classmethod</code>","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[MistralTool, str]]\n) -&gt; list[ChatMessage]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[MistralTool, str]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required <p>Returns:</p> Type Description <code>list[ChatMessage]</code> <p>The list of constructed <code>ChatMessage</code> parameters.</p> Source code in <code>mirascope/core/mistral/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[MistralTool, str]]\n) -&gt; list[ChatMessage]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n\n    Returns:\n        The list of constructed `ChatMessage` parameters.\n    \"\"\"\n    return [\n        ChatMessage(\n            role=\"tool\",\n            content=output,\n            tool_call_id=tool.tool_call.id,\n            name=tool._name(),\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/mistral/call_response_chunk/","title":"mirascope.core.mistral.call_response_chunk","text":"<p>This module contains the <code>MistralCallResponseChunk</code> class.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk","title":"MistralCallResponseChunk","text":"<p>               Bases: <code>BaseCallResponseChunk[ChatCompletionStreamResponse, FinishReason]</code></p> <p>A convenience wrapper around the Mistral <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Mistral API using a function decorated with <code>mistral_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>MistralResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.mistral import mistral_call\n\n\n@mistral_call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # response is an `MistralStream`\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content of the delta.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[FinishReason]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: UsageInfo | None\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/mistral/dynamic_config/","title":"mirascope.core.mistral.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/mistral/dynamic_config/#mirascope.core.mistral.dynamic_config.MistralDynamicConfig","title":"MistralDynamicConfig  <code>module-attribute</code>","text":"<pre><code>MistralDynamicConfig = BaseDynamicConfig[\n    ChatMessage | BaseMessageParam, MistralCallParams\n]\n</code></pre> <p>The function return type for functions wrapped with the <code>mistral_call</code> decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.mistral import MistralDynamicConfig, mistral_call\n\n\n@mistral_call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {capitalized_genre} book\")\ndef recommend_book(genre: str) -&gt; MistralDynamicConfig:\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/mistral/stream/","title":"mirascope.core.mistral.stream","text":"<p>The <code>MistralStream</code> class for convenience around streaming LLM calls.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/mistral/stream/#mirascope.core.mistral.stream.MistralStream","title":"MistralStream","text":"<pre><code>MistralStream(\n    *,\n    stream: (\n        Generator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n            None,\n        ]\n        | AsyncGenerator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n        ]\n    ),\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT]\n)\n</code></pre> <p>               Bases: <code>BaseStream[MistralCallResponse, MistralCallResponseChunk, ChatMessage, ChatMessage, ChatMessage, ChatMessage, MistralTool, dict[str, Any], MistralDynamicConfig, MistralCallParams, FinishReason]</code></p> <p>A class for convenience around streaming Mistral LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.mistral import mistral_call\n\n\n@mistral_call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # returns `MistralStream` instance\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> Source code in <code>mirascope/core/base/stream.py</code> <pre><code>def __init__(\n    self,\n    *,\n    stream: Generator[tuple[_BaseCallResponseChunkT, _BaseToolT | None], None, None]\n    | AsyncGenerator[\n        tuple[_BaseCallResponseChunkT, _BaseToolT | None],\n        None,\n    ],\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT],\n) -&gt; None:\n    \"\"\"Initializes an instance of `BaseStream`.\"\"\"\n    self.content = \"\"\n    self.stream = stream\n    self.metadata = metadata\n    self.tool_types = tool_types\n    self.call_response_type = call_response_type\n    self.model = model\n    self.prompt_template = prompt_template\n    self.fn_args = fn_args\n    self.dynamic_config = dynamic_config\n    self.messages = messages\n    self.call_params = call_params\n    self.call_kwargs = call_kwargs\n    self.user_message_param = get_possible_user_message_param(messages)  # pyright: ignore [reportAttributeAccessIssue]\n</code></pre>"},{"location":"api/core/mistral/stream/#mirascope.core.mistral.stream.MistralStream.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/mistral/stream/#mirascope.core.mistral.stream.MistralStream.construct_call_response","title":"construct_call_response","text":"<pre><code>construct_call_response() -&gt; MistralCallResponse\n</code></pre> <p>Constructs the call response from a consumed MistralStream.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the stream has not yet been consumed.</p> Source code in <code>mirascope/core/mistral/stream.py</code> <pre><code>def construct_call_response(self) -&gt; MistralCallResponse:\n    \"\"\"Constructs the call response from a consumed MistralStream.\n\n    Raises:\n        ValueError: if the stream has not yet been consumed.\n    \"\"\"\n    if not hasattr(self, \"message_param\"):\n        raise ValueError(\n            \"No stream response, check if the stream has been consumed.\"\n        )\n    usage = UsageInfo(\n        prompt_tokens=int(self.input_tokens or 0),\n        completion_tokens=int(self.output_tokens or 0),\n        total_tokens=int(self.input_tokens or 0) + int(self.output_tokens or 0),\n    )\n    completion = ChatCompletionResponse(\n        id=self.id if self.id else \"\",\n        choices=[\n            ChatCompletionResponseChoice(\n                finish_reason=self.finish_reasons[0]\n                if self.finish_reasons\n                else None,\n                index=0,\n                message=self.message_param,\n            )\n        ],\n        created=0,\n        model=self.model,\n        object=\"\",\n        usage=usage,\n    )\n    return MistralCallResponse(\n        metadata=self.metadata,\n        response=completion,\n        tool_types=self.tool_types,\n        prompt_template=self.prompt_template,\n        fn_args=self.fn_args if self.fn_args else {},\n        dynamic_config=self.dynamic_config,\n        messages=self.messages,\n        call_params=self.call_params,\n        call_kwargs=self.call_kwargs,\n        user_message_param=self.user_message_param,\n        start_time=self.start_time,\n        end_time=self.end_time,\n    )\n</code></pre>"},{"location":"api/core/mistral/tool/","title":"mirascope.core.mistral.tool","text":"<p>The <code>MistralTool</code> class for easy tool usage with Mistral LLM calls.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/mistral/tool/#mirascope.core.mistral.tool.MistralTool","title":"MistralTool","text":"<p>               Bases: <code>BaseTool[dict[str, Any]]</code></p> <p>A class for defining tools for Mistral LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.mistral import mistral_call\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@mistral_call(\"mistral-large-latest\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:  # returns a `MistralTool` instance\n    print(tool.call())\n</code></pre>"},{"location":"api/core/mistral/tool/#mirascope.core.mistral.tool.MistralTool.tool_schema","title":"tool_schema  <code>classmethod</code>","text":"<pre><code>tool_schema() -&gt; dict[str, Any]\n</code></pre> <p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> <p>Example: <pre><code>from mirascope.core.mistral import MistralTool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\ntool_type = MistralTool.type_from_fn(format_book)\nprint(tool_type.tool_schema())  # prints the Mistral-specific tool schema\n</code></pre></p> Source code in <code>mirascope/core/mistral/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; dict[str, Any]:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\n\n    Example:\n    ```python\n    from mirascope.core.mistral import MistralTool\n\n\n    def format_book(title: str, author: str) -&gt; str:\n        return f\"{title} by {author}\"\n\n\n    tool_type = MistralTool.type_from_fn(format_book)\n    print(tool_type.tool_schema())  # prints the Mistral-specific tool schema\n    ```\n    \"\"\"\n    fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n    model_schema = cls.model_json_schema()\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n    return {\"function\": fn, \"type\": \"function\"}\n</code></pre>"},{"location":"api/core/mistral/tool/#mirascope.core.mistral.tool.MistralTool.from_tool_call","title":"from_tool_call  <code>classmethod</code>","text":"<pre><code>from_tool_call(tool_call: ToolCall) -&gt; MistralTool\n</code></pre> <p>Constructs an <code>MistralTool</code> instance from a <code>tool_call</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ToolCall</code> <p>The Mistral tool call from which to construct this tool instance.</p> required Source code in <code>mirascope/core/mistral/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolCall) -&gt; MistralTool:\n    \"\"\"Constructs an `MistralTool` instance from a `tool_call`.\n\n    Args:\n        tool_call: The Mistral tool call from which to construct this tool instance.\n    \"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/openai/call/","title":"mirascope.core.openai.call","text":"<p>A decorator for calling the OpenAI API with a typed function.</p> Usage Documentation <p>Calls</p> <p>This decorator is used to wrap a typed function that calls the OpenAI API. It parses the prompt template of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.openai import openai_call\n\n\n@openai_call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The OpenAI model to use in the API call.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <code>list[BaseTool | Callable]</code> <p>The tools to use in the OpenAI API call.</p> required <code>response_model</code> <code>BaseModel | BaseType</code> <p>The response model into which the response should be structured.</p> required <code>output_parser</code> <code>Callable[[OpenAICallResponse | ResponseModelT], Any]</code> <p>A function for  parsing the call response whose value will be returned in place of the original call response.</p> required <code>json_mode</code> <code>bool</code> <p>Whether to use JSON Mode.</p> required <code>client</code> <code>object</code> <p>An optional custom client to use in place of the default client.</p> required <code>call_params</code> <code>OpenAICallParams</code> <p>The <code>OpenAICallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator for turning a typed function into an OpenAI API call.</p>"},{"location":"api/core/openai/call_params/","title":"mirascope.core.openai.call_params","text":"Usage Documentation <p>Calls</p>"},{"location":"api/core/openai/call_params/#mirascope.core.openai.call_params.OpenAICallParams","title":"OpenAICallParams","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the OpenAI API.</p> <p>OpenAI API Reference</p> <p>Attributes:</p> Name Type Description <code>audio</code> <code>NotRequired[ChatCompletionAudioParam | None]</code> <p>...</p> <code>frequency_penalty</code> <code>NotRequired[float | None]</code> <p>...</p> <code>logit_bias</code> <code>NotRequired[dict[str, int] | None]</code> <p>...</p> <code>logprobs</code> <code>NotRequired[bool | None]</code> <p>...</p> <code>max_tokens</code> <code>NotRequired[int | None]</code> <p>...</p> <code>modalities</code> <code>NotRequired[list[ChatCompletionModality] | None]</code> <p>...</p> <code>n</code> <code>NotRequired[int | None]</code> <p>...</p> <code>parallel_tool_calls</code> <code>NotRequired[bool]</code> <p>...</p> <code>presence_penalty</code> <code>NotRequired[float | None]</code> <p>...</p> <code>response_format</code> <code>NotRequired[ResponseFormat]</code> <p>...</p> <code>seed</code> <code>NotRequired[int | None]</code> <p>...</p> <code>stop</code> <code>NotRequired[str | list[str] | None]</code> <p>...</p> <code>stream_options</code> <code>NotRequired[ChatCompletionStreamOptionsParam | None]</code> <p>...</p> <code>temperature</code> <code>NotRequired[float | None]</code> <p>...</p> <code>tool_choice</code> <code>NotRequired[ChatCompletionToolChoiceOptionParam]</code> <p>...</p> <code>top_logprobs</code> <code>NotRequired[int | None]</code> <p>...</p> <code>top_p</code> <code>NotRequired[float | None]</code> <p>...</p> <code>user</code> <code>NotRequired[str]</code> <p>...</p>"},{"location":"api/core/openai/call_response/","title":"mirascope.core.openai.call_response","text":"<p>This module contains the <code>OpenAICallResponse</code> class.</p> Usage Documentation <p>Calls</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse","title":"OpenAICallResponse","text":"<p>               Bases: <code>BaseCallResponse[ChatCompletion, OpenAITool, ChatCompletionToolParam, OpenAIDynamicConfig, ChatCompletionMessageParam, OpenAICallParams, ChatCompletionUserMessageParam]</code></p> <p>A convenience wrapper around the OpenAI <code>ChatCompletion</code> response.</p> <p>When calling the OpenAI API using a function decorated with <code>openai_call</code>, the response will be an <code>OpenAICallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.openai import openai_call\n\n\n@openai_call(\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")  # response is an `OpenAICallResponse` instance\nprint(response.content)\n</code></pre>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[str]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: CompletionUsage | None\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.message_param","title":"message_param  <code>property</code>","text":"<pre><code>message_param: SerializeAsAny[\n    ChatCompletionAssistantMessageParam\n]\n</code></pre> <p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.tools","title":"tools  <code>property</code>","text":"<pre><code>tools: list[OpenAITool] | None\n</code></pre> <p>Returns any available tool calls as their <code>OpenAITool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p> <code>ValueError</code> <p>if the model refused to response, in which case the error message will be the refusal.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.tool","title":"tool  <code>property</code>","text":"<pre><code>tool: OpenAITool | None\n</code></pre> <p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p> <code>ValueError</code> <p>if the model refused to response, in which case the error message will be the refusal.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.audio","title":"audio  <code>property</code>","text":"<pre><code>audio: bytes | None\n</code></pre> <p>Returns the audio data of the response.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.audio_transcript","title":"audio_transcript  <code>property</code>","text":"<pre><code>audio_transcript: str | None\n</code></pre> <p>Returns the transcript of the audio content.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.tool_message_params","title":"tool_message_params  <code>classmethod</code>","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[OpenAITool, str]]\n) -&gt; list[ChatCompletionToolMessageParam]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[OpenAITool, str]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required <p>Returns:</p> Type Description <code>list[ChatCompletionToolMessageParam]</code> <p>The list of constructed <code>ChatCompletionToolMessageParam</code> parameters.</p> Source code in <code>mirascope/core/openai/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[OpenAITool, str]]\n) -&gt; list[ChatCompletionToolMessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n\n    Returns:\n        The list of constructed `ChatCompletionToolMessageParam` parameters.\n    \"\"\"\n    return [\n        ChatCompletionToolMessageParam(\n            role=\"tool\",\n            content=output,\n            tool_call_id=tool.tool_call.id,\n            name=tool._name(),  # pyright: ignore [reportCallIssue]\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/openai/call_response_chunk/","title":"mirascope.core.openai.call_response_chunk","text":"<p>This module contains the <code>OpenAICallResponseChunk</code> class.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk","title":"OpenAICallResponseChunk","text":"<p>               Bases: <code>BaseCallResponseChunk[ChatCompletionChunk, FinishReason]</code></p> <p>A convenience wrapper around the OpenAI <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the OpenAI API using a function decorated with <code>openai_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>OpenAIResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.openai import openai_call\n\n\n@openai_call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # response is an `OpenAIStream`\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[FinishReason]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the name of the response model.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the response.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: CompletionUsage | None\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int | None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int | None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.audio","title":"audio  <code>property</code>","text":"<pre><code>audio: bytes | None\n</code></pre> <p>Returns the audio data of the response.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.audio_transcript","title":"audio_transcript  <code>property</code>","text":"<pre><code>audio_transcript: str | None\n</code></pre> <p>Returns the transcript of the audio content.</p>"},{"location":"api/core/openai/dynamic_config/","title":"mirascope.core.openai.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/openai/dynamic_config/#mirascope.core.openai.dynamic_config.OpenAIDynamicConfig","title":"OpenAIDynamicConfig  <code>module-attribute</code>","text":"<pre><code>OpenAIDynamicConfig = BaseDynamicConfig[\n    ChatCompletionMessageParam | BaseMessageParam,\n    OpenAICallParams,\n]\n</code></pre> <p>The function return type for functions wrapped with the <code>openai_call</code> decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.openai import OpenAIDynamicConfig, openai_call\n\n\n@openai_call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {capitalized_genre} book\")\ndef recommend_book(genre: str) -&gt; OpenAIDynamicConfig:\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/openai/stream/","title":"mirascope.core.openai.stream","text":"<p>The <code>OpenAIStream</code> class for convenience around streaming LLM calls.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/openai/stream/#mirascope.core.openai.stream.OpenAIStream","title":"OpenAIStream","text":"<pre><code>OpenAIStream(\n    *,\n    stream: (\n        Generator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n            None,\n        ]\n        | AsyncGenerator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n        ]\n    ),\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT]\n)\n</code></pre> <p>               Bases: <code>BaseStream[OpenAICallResponse, OpenAICallResponseChunk, ChatCompletionUserMessageParam, ChatCompletionAssistantMessageParam, ChatCompletionToolMessageParam, ChatCompletionMessageParam, OpenAITool, ChatCompletionToolParam, OpenAIDynamicConfig, OpenAICallParams, FinishReason]</code></p> <p>A class for convenience around streaming OpenAI LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.openai import openai_call\n\n\n@openai_call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # returns `OpenAIStream` instance\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> Source code in <code>mirascope/core/base/stream.py</code> <pre><code>def __init__(\n    self,\n    *,\n    stream: Generator[tuple[_BaseCallResponseChunkT, _BaseToolT | None], None, None]\n    | AsyncGenerator[\n        tuple[_BaseCallResponseChunkT, _BaseToolT | None],\n        None,\n    ],\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT],\n) -&gt; None:\n    \"\"\"Initializes an instance of `BaseStream`.\"\"\"\n    self.content = \"\"\n    self.stream = stream\n    self.metadata = metadata\n    self.tool_types = tool_types\n    self.call_response_type = call_response_type\n    self.model = model\n    self.prompt_template = prompt_template\n    self.fn_args = fn_args\n    self.dynamic_config = dynamic_config\n    self.messages = messages\n    self.call_params = call_params\n    self.call_kwargs = call_kwargs\n    self.user_message_param = get_possible_user_message_param(messages)  # pyright: ignore [reportAttributeAccessIssue]\n</code></pre>"},{"location":"api/core/openai/stream/#mirascope.core.openai.stream.OpenAIStream.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/openai/stream/#mirascope.core.openai.stream.OpenAIStream.construct_call_response","title":"construct_call_response","text":"<pre><code>construct_call_response() -&gt; OpenAICallResponse\n</code></pre> <p>Constructs the call response from a consumed OpenAIStream.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the stream has not yet been consumed.</p> Source code in <code>mirascope/core/openai/stream.py</code> <pre><code>def construct_call_response(self) -&gt; OpenAICallResponse:\n    \"\"\"Constructs the call response from a consumed OpenAIStream.\n\n    Raises:\n        ValueError: if the stream has not yet been consumed.\n    \"\"\"\n    if not hasattr(self, \"message_param\"):\n        raise ValueError(\n            \"No stream response, check if the stream has been consumed.\"\n        )\n    message = {\n        \"role\": self.message_param[\"role\"],\n        \"content\": self.message_param.get(\"content\", \"\"),\n        \"tool_calls\": self.message_param.get(\"tool_calls\", []),\n    }\n    if not self.input_tokens and not self.output_tokens:\n        usage = None\n    else:\n        usage = CompletionUsage(\n            prompt_tokens=int(self.input_tokens or 0),\n            completion_tokens=int(self.output_tokens or 0),\n            total_tokens=int(self.input_tokens or 0) + int(self.output_tokens or 0),\n        )\n    completion = ChatCompletion(\n        id=self.id if self.id else \"\",\n        model=self.model,\n        choices=[\n            Choice(\n                finish_reason=self.finish_reasons[0]\n                if self.finish_reasons\n                else \"stop\",\n                index=0,\n                message=ChatCompletionMessage.model_validate(message),\n            )\n        ],\n        created=0,\n        object=\"chat.completion\",\n        usage=usage,\n    )\n    return OpenAICallResponse(\n        metadata=self.metadata,\n        response=completion,\n        tool_types=self.tool_types,\n        prompt_template=self.prompt_template,\n        fn_args=self.fn_args if self.fn_args else {},\n        dynamic_config=self.dynamic_config,\n        messages=self.messages,\n        call_params=self.call_params,\n        call_kwargs=self.call_kwargs,\n        user_message_param=self.user_message_param,\n        start_time=self.start_time,\n        end_time=self.end_time,\n    )\n</code></pre>"},{"location":"api/core/openai/tool/","title":"mirascope.core.openai.tool","text":"<p>The <code>OpenAITool</code> class for easy tool usage with OpenAI LLM calls.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/openai/tool/#mirascope.core.openai.tool.OpenAIToolConfig","title":"OpenAIToolConfig","text":"<p>               Bases: <code>ToolConfig</code></p> <p>A tool configuration for OpenAI-specific features.</p>"},{"location":"api/core/openai/tool/#mirascope.core.openai.tool.OpenAITool","title":"OpenAITool","text":"<p>               Bases: <code>BaseTool[ChatCompletionToolParam]</code></p> <p>A class for defining tools for OpenAI LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.openai import openai_call\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@openai_call(\"gpt-4o-mini\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:  # returns an `OpenAITool` instance\n    print(tool.call())\n</code></pre>"},{"location":"api/core/openai/tool/#mirascope.core.openai.tool.OpenAITool.tool_schema","title":"tool_schema  <code>classmethod</code>","text":"<pre><code>tool_schema() -&gt; ChatCompletionToolParam\n</code></pre> <p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> <p>Example: <pre><code>from mirascope.core.openai import OpenAITool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\ntool_type = OpenAITool.type_from_fn(format_book)\nprint(tool_type.tool_schema())  # prints the OpenAI-specific tool schema\n</code></pre></p> Source code in <code>mirascope/core/openai/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\n\n    Example:\n    ```python\n    from mirascope.core.openai import OpenAITool\n\n\n    def format_book(title: str, author: str) -&gt; str:\n        return f\"{title} by {author}\"\n\n\n    tool_type = OpenAITool.type_from_fn(format_book)\n    print(tool_type.tool_schema())  # prints the OpenAI-specific tool schema\n    ```\n    \"\"\"\n    fn = FunctionDefinition(name=cls._name(), description=cls._description())\n    schema_generator = GenerateJsonSchemaNoTitles\n    if cls.tool_config.get(\"strict\", False):\n        fn[\"strict\"] = True\n        schema_generator = GenerateOpenAIStrictToolJsonSchema\n    model_schema = cls.model_json_schema(schema_generator=schema_generator)\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n    return ChatCompletionToolParam(function=fn, type=\"function\")\n</code></pre>"},{"location":"api/core/openai/tool/#mirascope.core.openai.tool.OpenAITool.from_tool_call","title":"from_tool_call  <code>classmethod</code>","text":"<pre><code>from_tool_call(\n    tool_call: ChatCompletionMessageToolCall,\n) -&gt; OpenAITool\n</code></pre> <p>Constructs an <code>OpenAITool</code> instance from a <code>tool_call</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>ChatCompletionMessageToolCall</code> <p>The OpenAI tool call from which to construct this tool instance.</p> required Source code in <code>mirascope/core/openai/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n    \"\"\"Constructs an `OpenAITool` instance from a `tool_call`.\n\n    Args:\n        tool_call: The OpenAI tool call from which to construct this tool instance.\n    \"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/vertex/call/","title":"mirascope.core.vertex.call","text":"<p>A decorator for calling the Vertex API with a typed function.</p> Usage Documentation <p>Calls</p> <p>This decorator is used to wrap a typed function that calls the Vertex API. It parses the prompt template of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.vertex import vertex_call\n\n\n@vertex_call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The Vertex model to use in the API call.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <code>list[BaseTool | Callable]</code> <p>The tools to use in the Vertex API call.</p> required <code>response_model</code> <code>BaseModel | BaseType</code> <p>The response model into which the response should be structured.</p> required <code>output_parser</code> <code>Callable[[VertexCallResponse | ResponseModelT], Any]</code> <p>A function for parsing the call response whose value will be returned in place of the original call response.</p> required <code>json_modem</code> <code>bool</code> <p>Whether to use JSON Mode.</p> required <code>client</code> <code>object</code> <p>An optional custom client to use in place of the default client.</p> required <code>call_params</code> <code>VertexCallParams</code> <p>The <code>VertexCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable</code> <p>The decorator for turning a typed function into a Vertex API call.</p>"},{"location":"api/core/vertex/call_params/","title":"mirascope.core.vertex.call_params","text":"Usage Documentation <p>Calls</p>"},{"location":"api/core/vertex/call_params/#mirascope.core.vertex.call_params.VertexCallParams","title":"VertexCallParams","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Vertex API.</p> <p>Vertex API Reference</p> <p>Attributes:</p> Name Type Description <code>generation_config</code> <code>NotRequired[GenerationConfig]</code> <p>...</p> <code>safety_settings</code> <code>NotRequired[SafetySetting]</code> <p>...</p> <code>tool_config</code> <code>NotRequired[ToolConfig]</code> <p>...</p>"},{"location":"api/core/vertex/call_response/","title":"mirascope.core.vertex.call_response","text":"<p>This module contains the <code>VertexCallResponse</code> class.</p> Usage Documentation <p>Calls</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse","title":"VertexCallResponse","text":"<p>               Bases: <code>BaseCallResponse[GenerationResponse, VertexTool, Tool, VertexDynamicConfig, Content, VertexCallParams, Content]</code></p> <p>A convenience wrapper around the Vertex AI <code>GenerateContentResponse</code>.</p> <p>When calling the Vertex AI API using a function decorated with <code>vertex_call</code>, the response will be a <code>VertexCallResponse</code> instance with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.vertex import vertex_call\n\n\n@vertex_call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")  # response is an `VertexCallResponse` instance\nprint(response.content)\n</code></pre>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the contained string content for the 0th choice.</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[str]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Returns the model name.</p> <p>vertex does not return model, so we return the model provided by the user.</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.id","title":"id  <code>property</code>","text":"<pre><code>id: str | None\n</code></pre> <p>Returns the id of the response.</p> <p>vertex does not return an id</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: UsageMetadata\n</code></pre> <p>Returns the usage of the chat completion.</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: int\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: int\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.message_param","title":"message_param  <code>property</code>","text":"<pre><code>message_param: Content\n</code></pre> <p>Returns the models's response as a message parameter.</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.tools","title":"tools  <code>property</code>","text":"<pre><code>tools: list[VertexTool] | None\n</code></pre> <p>Returns the list of tools for the 0th candidate's 0th content part.</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.tool","title":"tool  <code>property</code>","text":"<pre><code>tool: VertexTool | None\n</code></pre> <p>Returns the 0th tool for the 0th candidate's 0th content part.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/vertex/call_response/#mirascope.core.vertex.call_response.VertexCallResponse.tool_message_params","title":"tool_message_params  <code>classmethod</code>","text":"<pre><code>tool_message_params(\n    tools_and_outputs: list[tuple[VertexTool, object]]\n) -&gt; list[FunctionResponse]\n</code></pre> <p>Returns the tool message parameters for tool call results.</p> <p>Parameters:</p> Name Type Description Default <code>tools_and_outputs</code> <code>list[tuple[VertexTool, object]]</code> <p>The list of tools and their outputs from which the tool message parameters should be constructed.</p> required <p>Returns:</p> Type Description <code>list[FunctionResponse]</code> <p>The list of constructed <code>FunctionResponse</code> parameters.</p> Source code in <code>mirascope/core/vertex/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[VertexTool, object]]\n) -&gt; list[FunctionResponse]:\n    \"\"\"Returns the tool message parameters for tool call results.\n\n    Args:\n        tools_and_outputs: The list of tools and their outputs from which the tool\n            message parameters should be constructed.\n\n    Returns:\n        The list of constructed `FunctionResponse` parameters.\n    \"\"\"\n    return [\n        FunctionResponse(name=tool._name(), response={\"result\": output})\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/vertex/call_response_chunk/","title":"mirascope.core.vertex.call_response_chunk","text":"<p>This module contains the <code>VertexCallResponseChunk</code> class.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/vertex/call_response_chunk/#mirascope.core.vertex.call_response_chunk.VertexCallResponseChunk","title":"VertexCallResponseChunk","text":"<p>               Bases: <code>BaseCallResponseChunk[GenerationResponse, FinishReason]</code></p> <p>A convenience wrapper around the Vertex AI streamed response chunks.</p> <p>When calling the Vertex AI API using a function decorated with <code>vertex_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>VertexCallResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.vertex import vertex_call\n\n\n@vertex_call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\nstream = recommend_book(\"fantasy\")  # response is an `VertexStream`\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/core/vertex/call_response_chunk/#mirascope.core.vertex.call_response_chunk.VertexCallResponseChunk.content","title":"content  <code>property</code>","text":"<pre><code>content: str\n</code></pre> <p>Returns the chunk content for the 0th choice.</p>"},{"location":"api/core/vertex/call_response_chunk/#mirascope.core.vertex.call_response_chunk.VertexCallResponseChunk.finish_reasons","title":"finish_reasons  <code>property</code>","text":"<pre><code>finish_reasons: list[FinishReason]\n</code></pre> <p>Returns the finish reasons of the response.</p>"},{"location":"api/core/vertex/call_response_chunk/#mirascope.core.vertex.call_response_chunk.VertexCallResponseChunk.model","title":"model  <code>property</code>","text":"<pre><code>model: None\n</code></pre> <p>Returns the model name.</p> <p>vertex does not return model, so we return None</p>"},{"location":"api/core/vertex/call_response_chunk/#mirascope.core.vertex.call_response_chunk.VertexCallResponseChunk.id","title":"id  <code>property</code>","text":"<pre><code>id: str | None\n</code></pre> <p>Returns the id of the response.</p> <p>vertex does not return an id</p>"},{"location":"api/core/vertex/call_response_chunk/#mirascope.core.vertex.call_response_chunk.VertexCallResponseChunk.usage","title":"usage  <code>property</code>","text":"<pre><code>usage: None\n</code></pre> <p>Returns the usage of the chat completion.</p> <p>vertex does not have Usage, so we return None</p>"},{"location":"api/core/vertex/call_response_chunk/#mirascope.core.vertex.call_response_chunk.VertexCallResponseChunk.input_tokens","title":"input_tokens  <code>property</code>","text":"<pre><code>input_tokens: None\n</code></pre> <p>Returns the number of input tokens.</p>"},{"location":"api/core/vertex/call_response_chunk/#mirascope.core.vertex.call_response_chunk.VertexCallResponseChunk.output_tokens","title":"output_tokens  <code>property</code>","text":"<pre><code>output_tokens: None\n</code></pre> <p>Returns the number of output tokens.</p>"},{"location":"api/core/vertex/dynamic_config/","title":"mirascope.core.vertex.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/vertex/dynamic_config/#mirascope.core.vertex.dynamic_config.VertexDynamicConfig","title":"VertexDynamicConfig  <code>module-attribute</code>","text":"<pre><code>VertexDynamicConfig = BaseDynamicConfig[\n    Content | BaseMessageParam, VertexCallParams\n]\n</code></pre> <p>The function return type for functions wrapped with the <code>vertex_call</code> decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.vertex import VertexDynamicConfig, vertex_call\n\n\n@vertex_call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {capitalized_genre} book\")\ndef recommend_book(genre: str) -&gt; VertexDynamicConfig:\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/vertex/stream/","title":"mirascope.core.vertex.stream","text":"<p>The <code>VertexStream</code> class for convenience around streaming LLM calls.</p> Usage Documentation <p>Streams</p>"},{"location":"api/core/vertex/stream/#mirascope.core.vertex.stream.VertexStream","title":"VertexStream","text":"<pre><code>VertexStream(\n    *,\n    stream: (\n        Generator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n            None,\n        ]\n        | AsyncGenerator[\n            tuple[\n                _BaseCallResponseChunkT, _BaseToolT | None\n            ],\n            None,\n        ]\n    ),\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT]\n)\n</code></pre> <p>               Bases: <code>BaseStream[VertexCallResponse, VertexCallResponseChunk, Content, Content, Content, Content, VertexTool, Tool, VertexDynamicConfig, VertexCallParams, FinishReason]</code></p> <p>A class for convenience around streaming Vertex LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.vertex import vertex_call\n\n\n@vertex_call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")  # returns `VertexStream` instance\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> Source code in <code>mirascope/core/base/stream.py</code> <pre><code>def __init__(\n    self,\n    *,\n    stream: Generator[tuple[_BaseCallResponseChunkT, _BaseToolT | None], None, None]\n    | AsyncGenerator[\n        tuple[_BaseCallResponseChunkT, _BaseToolT | None],\n        None,\n    ],\n    metadata: Metadata,\n    tool_types: list[type[_BaseToolT]] | None,\n    call_response_type: type[_BaseCallResponseT],\n    model: str,\n    prompt_template: str | None,\n    fn_args: dict[str, Any],\n    dynamic_config: _BaseDynamicConfigT,\n    messages: list[_MessageParamT],\n    call_params: _BaseCallParamsT,\n    call_kwargs: BaseCallKwargs[_ToolSchemaT],\n) -&gt; None:\n    \"\"\"Initializes an instance of `BaseStream`.\"\"\"\n    self.content = \"\"\n    self.stream = stream\n    self.metadata = metadata\n    self.tool_types = tool_types\n    self.call_response_type = call_response_type\n    self.model = model\n    self.prompt_template = prompt_template\n    self.fn_args = fn_args\n    self.dynamic_config = dynamic_config\n    self.messages = messages\n    self.call_params = call_params\n    self.call_kwargs = call_kwargs\n    self.user_message_param = get_possible_user_message_param(messages)  # pyright: ignore [reportAttributeAccessIssue]\n</code></pre>"},{"location":"api/core/vertex/stream/#mirascope.core.vertex.stream.VertexStream.cost","title":"cost  <code>property</code>","text":"<pre><code>cost: float | None\n</code></pre> <p>Returns the cost of the call.</p>"},{"location":"api/core/vertex/stream/#mirascope.core.vertex.stream.VertexStream.construct_call_response","title":"construct_call_response","text":"<pre><code>construct_call_response() -&gt; VertexCallResponse\n</code></pre> <p>Constructs the call response from a consumed VertexStream.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the stream has not yet been consumed.</p> Source code in <code>mirascope/core/vertex/stream.py</code> <pre><code>def construct_call_response(self) -&gt; VertexCallResponse:\n    \"\"\"Constructs the call response from a consumed VertexStream.\n\n    Raises:\n        ValueError: if the stream has not yet been consumed.\n    \"\"\"\n    if not hasattr(self, \"message_param\"):\n        raise ValueError(\n            \"No stream response, check if the stream has been consumed.\"\n        )\n    response = GenerationResponse.from_dict(\n        {\n            \"candidates\": [\n                Candidate.from_dict(\n                    {\n                        \"finish_reason\": self.finish_reasons[0]\n                        if self.finish_reasons\n                        else FinishReason.STOP,\n                        \"content\": Content(\n                            role=self.message_param.role,\n                            parts=self.message_param.parts,\n                        ).to_dict(),\n                    }\n                ).to_dict()\n            ]\n        }\n    )\n    return VertexCallResponse(\n        metadata=self.metadata,\n        response=response,\n        tool_types=self.tool_types,\n        prompt_template=self.prompt_template,\n        fn_args=self.fn_args if self.fn_args else {},\n        dynamic_config=self.dynamic_config,\n        messages=self.messages,\n        call_params=self.call_params,\n        call_kwargs=self.call_kwargs,\n        user_message_param=self.user_message_param,\n        start_time=self.start_time,\n        end_time=self.end_time,\n    )\n</code></pre>"},{"location":"api/core/vertex/tool/","title":"mirascope.core.vertex.tool","text":"<p>The <code>VertexTool</code> class for easy tool usage with Google's Vertex LLM calls.</p> Usage Documentation <p>Tools</p>"},{"location":"api/core/vertex/tool/#mirascope.core.vertex.tool.VertexTool","title":"VertexTool","text":"<p>               Bases: <code>BaseTool[Tool]</code></p> <p>A class for defining tools for Vertex LLM calls.</p> <p>Example:</p> <pre><code>from mirascope.core import prompt_template\nfrom mirascope.core.vertex import vertex_call\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@vertex_call(\"gemini-1.5-flash\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:  # returns an `VertexTool` instance\n    print(tool.call())\n</code></pre>"},{"location":"api/core/vertex/tool/#mirascope.core.vertex.tool.VertexTool.tool_schema","title":"tool_schema  <code>classmethod</code>","text":"<pre><code>tool_schema() -&gt; Tool\n</code></pre> <p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> <p>Example: <pre><code>from mirascope.core.vertex import VertexTool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\ntool_type = VertexTool.type_from_fn(format_book)\nprint(tool_type.tool_schema())  # prints the Vertex-specific tool schema\n</code></pre></p> Source code in <code>mirascope/core/vertex/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Tool:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\n\n    Example:\n    ```python\n    from mirascope.core.vertex import VertexTool\n\n\n    def format_book(title: str, author: str) -&gt; str:\n        return f\"{title} by {author}\"\n\n\n    tool_type = VertexTool.type_from_fn(format_book)\n    print(tool_type.tool_schema())  # prints the Vertex-specific tool schema\n    ```\n    \"\"\"\n    model_schema = cls.model_json_schema()\n    fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n    if model_schema[\"required\"]:\n        fn[\"parameters\"][\"required\"] = model_schema[\"required\"]\n    if \"parameters\" in fn:\n        if \"$defs\" in fn[\"parameters\"]:\n            raise ValueError(\n                \"Unfortunately Google's Vertex API cannot handle nested structures \"\n                \"with $defs.\"\n            )\n\n        def handle_enum_schema(prop_schema: dict[str, Any]) -&gt; dict[str, Any]:\n            if \"enum\" in prop_schema:\n                prop_schema[\"format\"] = \"enum\"\n            return prop_schema\n\n        fn[\"parameters\"][\"properties\"] = {\n            prop: {\n                key: value\n                for key, value in handle_enum_schema(prop_schema).items()\n                if key != \"default\"\n            }\n            for prop, prop_schema in fn[\"parameters\"][\"properties\"].items()\n        }\n    return Tool(function_declarations=[FunctionDeclaration(**fn)])\n</code></pre>"},{"location":"api/core/vertex/tool/#mirascope.core.vertex.tool.VertexTool.from_tool_call","title":"from_tool_call  <code>classmethod</code>","text":"<pre><code>from_tool_call(tool_call: FunctionCall) -&gt; VertexTool\n</code></pre> <p>Constructs an <code>VertexTool</code> instance from a <code>tool_call</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_call</code> <code>FunctionCall</code> <p>The Vertex tool call from which to construct this tool instance.</p> required Source code in <code>mirascope/core/vertex/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: FunctionCall) -&gt; VertexTool:\n    \"\"\"Constructs an `VertexTool` instance from a `tool_call`.\n\n    Args:\n        tool_call: The Vertex tool call from which to construct this tool instance.\n    \"\"\"\n    if not tool_call.args:\n        raise ValueError(\"Tool call doesn't have any arguments.\")\n    model_json: dict[str, Any] = dict(tool_call.args.items())\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/integrations/langfuse/","title":"mirascope.integrations.langfuse","text":""},{"location":"api/integrations/langfuse/#mirascope.integrations.langfuse.with_langfuse","title":"with_langfuse","text":"<pre><code>with_langfuse() -&gt; (\n    Callable[[Callable[_P, _R]], Callable[_P, _R]]\n)\n</code></pre> <p>Wraps a Mirascope function with Langfuse.</p> <p>Example:</p> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\ndef format_book(title: str, author: str):\n    return f\"{title} by {author}\"\n\n\n@with_langfuse()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> Source code in <code>mirascope/integrations/langfuse/_with_langfuse.py</code> <pre><code>def with_langfuse() -&gt; Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n    \"\"\"Wraps a Mirascope function with Langfuse.\n\n    Example:\n\n    ```python\n    from mirascope.core import anthropic, prompt_template\n    from mirascope.integrations.langfuse import with_langfuse\n\n\n    def format_book(title: str, author: str):\n        return f\"{title} by {author}\"\n\n\n    @with_langfuse()\n    @anthropic.call(model=\"claude-3-5-sonnet-20240620\", tools=[format_book])\n    def recommend_book(genre: str) -&gt; str:\n        return f\"Recommend a {genre} book\"\n\n\n    print(recommend_book(\"fantasy\"))\n    ```\n    \"\"\"\n\n    return middleware_factory(\n        custom_decorator=custom_decorator,\n        handle_call_response=handle_call_response,\n        handle_call_response_async=handle_call_response_async,\n        handle_stream=handle_stream,\n        handle_stream_async=handle_stream_async,\n        handle_response_model=handle_response_model,\n        handle_response_model_async=handle_response_model_async,\n        handle_structured_stream=handle_structured_stream,\n        handle_structured_stream_async=handle_structured_stream_async,\n    )\n</code></pre>"},{"location":"api/integrations/logfire/","title":"mirascope.integrations.logfire","text":""},{"location":"api/integrations/logfire/#mirascope.integrations.logfire.with_logfire","title":"with_logfire","text":"<pre><code>with_logfire() -&gt; (\n    Callable[[Callable[_P, _R]], Callable[_P, _R]]\n)\n</code></pre> <p>Wraps a Mirascope function with Logfire tracing.</p> <p>Example:</p> <pre><code>import logfire\nfrom mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.logfire import with_logfire\n\nlogfire.configure()\n\n\ndef format_book(title: str, author: str):\n    return f\"{title} by {author}\"\n\n\n@with_logfire()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> Source code in <code>mirascope/integrations/logfire/_with_logfire.py</code> <pre><code>def with_logfire() -&gt; Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n    \"\"\"Wraps a Mirascope function with Logfire tracing.\n\n    Example:\n\n    ```python\n    import logfire\n    from mirascope.core import anthropic, prompt_template\n    from mirascope.integrations.logfire import with_logfire\n\n    logfire.configure()\n\n\n    def format_book(title: str, author: str):\n        return f\"{title} by {author}\"\n\n\n    @with_logfire()\n    @anthropic.call(model=\"claude-3-5-sonnet-20240620\", tools=[format_book])\n    def recommend_book(genre: str) -&gt; str:\n        return f\"Recommend a {genre} book\"\n\n\n    print(recommend_book(\"fantasy\"))\n    ```\n    \"\"\"\n    return middleware_factory(\n        custom_context_manager=custom_context_manager,\n        handle_call_response=handle_call_response,\n        handle_call_response_async=handle_call_response_async,\n        handle_stream=handle_stream,\n        handle_stream_async=handle_stream_async,\n        handle_response_model=handle_response_model,\n        handle_response_model_async=handle_response_model_async,\n        handle_structured_stream=handle_structured_stream,\n        handle_structured_stream_async=handle_structured_stream_async,\n    )\n</code></pre>"},{"location":"api/integrations/middleware/","title":"mirascope.integrations.middleware","text":""},{"location":"api/integrations/middleware/#middleware_factory","title":"<code>middleware_factory</code>","text":"<p>A factory method for creating middleware decorators.</p> <p>Example:</p> <pre><code>from mirascope.core import openai, prompt_template\nfrom mirascope.integrations import middleware_factory\n\ndef with_saving():\n    \"\"\"Saves some data after a Mirascope call.\"\"\"\n\n    return middleware_factory(\n        custom_context_manager=custom_context_manager,\n        custom_decorator=custom_decorator,\n        handle_call_response=handle_call_response,\n        handle_call_response_async=handle_call_response_async,\n        handle_stream=handle_stream,\n        handle_stream_async=handle_stream_async,\n        handle_response_model=handle_response_model,\n        handle_response_model_async=handle_response_model_async,\n        handle_structured_stream=handle_structured_stream,\n        handle_structured_stream_async=handle_structured_stream_async,\n    )\n\n\n@with_saving()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")  # `with_saving` automatically run\nprint(response.content)\n</code></pre> Source code in <code>mirascope/integrations/_middleware_factory.py</code> <pre><code>def middleware_factory(\n    custom_context_manager: Callable[\n        [SyncFunc | AsyncFunc], AbstractContextManager[_T]\n    ] = default_context_manager,\n    custom_decorator: Callable | None = None,\n    handle_call_response: Callable[\n        [BaseCallResponse, SyncFunc | AsyncFunc, _T | None], None\n    ]\n    | None = None,\n    handle_call_response_async: Callable[\n        [BaseCallResponse, SyncFunc | AsyncFunc, _T | None], Awaitable[None]\n    ]\n    | None = None,\n    handle_stream: Callable[[BaseStream, SyncFunc | AsyncFunc, _T | None], None]\n    | None = None,\n    handle_stream_async: Callable[\n        [BaseStream, SyncFunc | AsyncFunc, _T | None], Awaitable[None]\n    ]\n    | None = None,\n    handle_response_model: Callable[\n        [ResponseModel, SyncFunc | AsyncFunc, _T | None], None\n    ]\n    | None = None,\n    handle_response_model_async: Callable[\n        [ResponseModel, SyncFunc | AsyncFunc, _T | None], Awaitable[None]\n    ]\n    | None = None,\n    handle_structured_stream: Callable[\n        [BaseStructuredStream, SyncFunc | AsyncFunc, _T | None], None\n    ]\n    | None = None,\n    handle_structured_stream_async: Callable[\n        [BaseStructuredStream, SyncFunc | AsyncFunc, _T | None], Awaitable[None]\n    ]\n    | None = None,\n) -&gt; Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n    '''A factory method for creating middleware decorators.\n\n    Example:\n\n    ```python\n    from mirascope.core import openai, prompt_template\n    from mirascope.integrations import middleware_factory\n\n    def with_saving():\n        \"\"\"Saves some data after a Mirascope call.\"\"\"\n\n        return middleware_factory(\n            custom_context_manager=custom_context_manager,\n            custom_decorator=custom_decorator,\n            handle_call_response=handle_call_response,\n            handle_call_response_async=handle_call_response_async,\n            handle_stream=handle_stream,\n            handle_stream_async=handle_stream_async,\n            handle_response_model=handle_response_model,\n            handle_response_model_async=handle_response_model_async,\n            handle_structured_stream=handle_structured_stream,\n            handle_structured_stream_async=handle_structured_stream_async,\n        )\n\n\n    @with_saving()\n    @openai.call(\"gpt-4o-mini\")\n    def recommend_book(genre: str) -&gt; str:\n        return f\"Recommend a {genre} book\"\n\n\n    response = recommend_book(\"fantasy\")  # `with_saving` automatically run\n    print(response.content)\n    ```\n    '''\n\n    @overload\n    def decorator(fn: Callable[_P, _R]) -&gt; Callable[_P, _R]: ...\n\n    @overload\n    def decorator(fn: Callable[_P, Awaitable[_R]]) -&gt; Callable[_P, Awaitable[_R]]: ...\n\n    def decorator(\n        fn: Callable[_P, _R | Awaitable[_R]],\n    ) -&gt; Callable[_P, _R | Awaitable[_R]]:\n        if inspect.iscoroutinefunction(fn):\n\n            @wraps(fn)\n            async def wrapper_async(*args: _P.args, **kwargs: _P.kwargs) -&gt; _R:\n                context_manager = custom_context_manager(fn)\n                context = context_manager.__enter__()\n                result = await fn(*args, **kwargs)\n                if (\n                    isinstance(result, BaseCallResponse)\n                    and handle_call_response_async is not None\n                ):\n                    await handle_call_response_async(result, fn, context)\n                    context_manager.__exit__(None, None, None)\n                elif isinstance(result, BaseStream):\n                    original_class = type(result)\n                    original_aiter = result.__aiter__\n\n                    def new_stream_aiter(\n                        self: Any,  # noqa: ANN401\n                    ) -&gt; AsyncGenerator[tuple[Any, Any | None], Any]:  # noqa: ANN401\n                        async def generator() -&gt; (\n                            AsyncGenerator[tuple[Any, Any | None], Any]\n                        ):\n                            try:\n                                async for chunk, tool in original_aiter():\n                                    yield chunk, tool\n                            finally:\n                                if handle_stream_async is not None:\n                                    await handle_stream_async(result, fn, context)\n                                context_manager.__exit__(None, None, None)\n\n                        return generator()\n\n                    class MiddlewareAsyncStream(original_class):\n                        __aiter__ = (\n                            custom_decorator(fn)(new_stream_aiter)\n                            if custom_decorator\n                            else new_stream_aiter\n                        )\n\n                    result.__class__ = MiddlewareAsyncStream\n                elif (\n                    isinstance(result, ResponseModel)\n                    and handle_response_model_async is not None\n                ):\n                    await handle_response_model_async(result, fn, context)\n                    context_manager.__exit__(None, None, None)\n                elif isinstance(result, BaseStructuredStream):\n                    original_class = type(result)\n                    original_aiter = result.__aiter__\n\n                    def new_aiter(\n                        self: Any,  # noqa: ANN401\n                    ) -&gt; AsyncGenerator[tuple[Any, Any | None], Any]:  # noqa: ANN401\n                        async def generator() -&gt; (\n                            AsyncGenerator[tuple[Any, Any | None], Any]\n                        ):\n                            try:\n                                async for chunk in original_aiter():\n                                    yield chunk\n                            finally:\n                                if handle_structured_stream_async is not None:\n                                    await handle_structured_stream_async(\n                                        result, fn, context\n                                    )\n                                context_manager.__exit__(None, None, None)\n\n                        return generator()\n\n                    class MiddlewareAsyncStructuredStream(original_class):\n                        __aiter__ = (\n                            custom_decorator(fn)(new_aiter)\n                            if custom_decorator\n                            else new_aiter\n                        )\n\n                    result.__class__ = MiddlewareAsyncStructuredStream\n                return cast(_R, result)\n\n            return (\n                custom_decorator(fn)(wrapper_async)\n                if custom_decorator\n                else wrapper_async\n            )\n        else:\n\n            @wraps(fn)\n            def wrapper(*args: _P.args, **kwargs: _P.kwargs) -&gt; _R:\n                context_manager = custom_context_manager(fn)\n                context = context_manager.__enter__()\n                result = fn(*args, **kwargs)\n                if (\n                    isinstance(result, BaseCallResponse)\n                    and handle_call_response is not None\n                ):\n                    handle_call_response(result, fn, context)\n                    context_manager.__exit__(None, None, None)\n                elif isinstance(result, BaseStream):\n                    original_class = type(result)\n                    original_iter = result.__iter__\n\n                    def new_stream_iter(\n                        self: Any,  # noqa: ANN401\n                    ) -&gt; Generator[tuple[Any, Any | None], None, None]:  # noqa: ANN401\n                        # Create the context manager when user iterates over the stream\n                        try:\n                            yield from original_iter()\n                        finally:\n                            if handle_stream is not None:\n                                handle_stream(result, fn, context)\n                            context_manager.__exit__(None, None, None)\n\n                    class MiddlewareStream(original_class):\n                        __iter__ = (\n                            custom_decorator(fn)(new_stream_iter)\n                            if custom_decorator\n                            else new_stream_iter\n                        )\n\n                    result.__class__ = MiddlewareStream\n                elif (\n                    isinstance(result, ResponseModel)\n                    and handle_response_model is not None\n                ):\n                    handle_response_model(result, fn, context)\n                    context_manager.__exit__(None, None, None)\n                elif isinstance(result, BaseStructuredStream):\n                    original_class = type(result)\n                    original_iter = result.__iter__\n\n                    def new_iter(\n                        self: Any,  # noqa: ANN401\n                    ) -&gt; Generator[\n                        Generator[tuple[Any, Any | None], None, None]\n                        | Generator[Any, None, None],\n                        Any,\n                        None,\n                    ]:  # noqa: ANN401\n                        # Create the context manager when user iterates over the stream\n                        try:\n                            yield from original_iter()\n                        finally:\n                            if handle_structured_stream is not None:\n                                handle_structured_stream(result, fn, context)\n                            context_manager.__exit__(None, None, None)\n\n                    class MiddlewareStructuredStream(original_class):\n                        __iter__ = (\n                            custom_decorator(fn)(new_iter)\n                            if custom_decorator\n                            else new_iter\n                        )\n\n                    result.__class__ = MiddlewareStructuredStream\n                return cast(_R, result)\n\n            return custom_decorator(fn)(wrapper) if custom_decorator else wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/integrations/otel/","title":"mirascope.integrations.otel","text":""},{"location":"api/integrations/otel/#configure","title":"<code>configure</code>","text":"<p>Configures the OpenTelemetry tracer, this function should only be called once.</p> <p>Parameters:</p> Name Type Description Default <code>processors</code> <code>Sequence[SpanProcessor] | None</code> <p>Optional[Sequence[SpanProcessor]] The span processors to use, if None, a console exporter will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tracer</code> <p>The configured tracer.</p> Source code in <code>mirascope/integrations/otel/_utils.py</code> <pre><code>def configure(\n    processors: Sequence[SpanProcessor] | None = None,\n) -&gt; Tracer:\n    \"\"\"Configures the OpenTelemetry tracer, this function should only be called once.\n\n    Args:\n        processors: Optional[Sequence[SpanProcessor]]\n            The span processors to use, if None, a console exporter will be used.\n\n    Returns:\n        The configured tracer.\n    \"\"\"\n    provider = TracerProvider()\n    if processors is None:\n        provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n    else:\n        for processor in processors:\n            provider.add_span_processor(processor)\n    # NOTE: Sets the global trace provider, should only be called once\n    set_tracer_provider(provider)\n    return get_tracer(\"otel\")\n</code></pre>"},{"location":"api/integrations/otel/#with_otel","title":"<code>with_otel</code>","text":"<p>Wraps a Mirascope function with OpenTelemetry.</p> <p>Example:</p> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.otel import with_otel, configure\n\nconfigure()\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@with_otel()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> Source code in <code>mirascope/integrations/otel/_with_otel.py</code> <pre><code>def with_otel() -&gt; Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n    \"\"\"Wraps a Mirascope function with OpenTelemetry.\n\n    Example:\n\n    ```python\n    from mirascope.core import anthropic, prompt_template\n    from mirascope.integrations.otel import with_otel, configure\n\n    configure()\n\n\n    def format_book(title: str, author: str) -&gt; str:\n        return f\"{title} by {author}\"\n\n\n    @with_otel()\n    @anthropic.call(model=\"claude-3-5-sonnet-20240620\", tools=[format_book])\n    def recommend_book(genre: str) -&gt; str:\n        return f\"Recommend a {genre} book\"\n\n\n    print(recommend_book(\"fantasy\"))\n    ```\n    \"\"\"\n\n    return middleware_factory(\n        custom_context_manager=custom_context_manager,\n        handle_call_response=handle_call_response,\n        handle_call_response_async=handle_call_response_async,\n        handle_stream=handle_stream,\n        handle_stream_async=handle_stream_async,\n        handle_response_model=handle_response_model,\n        handle_response_model_async=handle_response_model_async,\n        handle_structured_stream=handle_structured_stream,\n        handle_structured_stream_async=handle_structured_stream_async,\n    )\n</code></pre>"},{"location":"api/integrations/otel/#with_hyperdx","title":"<code>with_hyperdx</code>","text":"<p>Decorator to wrap a function with hyperdx.</p> <p>Example:</p> <pre><code>import os\n\nfrom mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.otel import with_hyperdx\n\nos.environ[\"HYPERDX_API_KEY\"] = \"YOUR_API_KEY\"\n\n\ndef format_book(title: str, author: str):\n    return f\"{title} by {author}\"\n\n\n@with_hyperdx()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\", tools=[format_book])\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> Source code in <code>mirascope/integrations/otel/_with_hyperdx.py</code> <pre><code>def with_hyperdx() -&gt; Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n    \"\"\"Decorator to wrap a function with hyperdx.\n\n    Example:\n\n    ```python\n    import os\n\n    from mirascope.core import anthropic, prompt_template\n    from mirascope.integrations.otel import with_hyperdx\n\n    os.environ[\"HYPERDX_API_KEY\"] = \"YOUR_API_KEY\"\n\n\n    def format_book(title: str, author: str):\n        return f\"{title} by {author}\"\n\n\n    @with_hyperdx()\n    @anthropic.call(model=\"claude-3-5-sonnet-20240620\", tools=[format_book])\n    def recommend_book(genre: str) -&gt; str:\n        return f\"Recommend a {genre} book\"\n\n\n    print(recommend_book(\"fantasy\"))\n    ```\n    \"\"\"\n    provider = trace.get_tracer_provider()\n    if not isinstance(provider, TracerProvider):\n        configure(\n            processors=[\n                BatchSpanProcessor(\n                    OTLPSpanExporter(\n                        endpoint=\"https://in-otel.hyperdx.io/v1/traces\",\n                        headers={\"authorization\": os.getenv(\"HYPERDX_API_KEY\", \"\")},\n                    )\n                )\n            ]\n        )\n    return with_otel()\n</code></pre>"},{"location":"api/integrations/tenacity/","title":"mirascope.integrations.tenacity","text":""},{"location":"api/integrations/tenacity/#collect_errors","title":"<code>collect_errors</code>","text":"<p>Collects specified errors into an <code>errors</code> keyword argument.</p> <p>Example:</p> <pre><code>from mirascope.integrations.tenacity import collect_errors\nfrom tenacity import retry, stop_after_attempt\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValueError))\ndef throw_value_error(*, errors: list[ValueError] | None = None):\n    if errors:\n        print(errors[-1])\n    raise ValueError(\"Throwing Error\")\n\n\ntry:\n    throw_value_error()\n    # &gt; Throwing Error\n    # &gt; Throwing Error\nexcept RetryError:\n    ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>type[Exception]</code> <p>The <code>Exception</code> types to catch.</p> <code>()</code> Source code in <code>mirascope/retries/tenacity.py</code> <pre><code>def collect_errors(\n    *args: type[Exception],\n) -&gt; Callable[[RetryCallState], None]:\n    \"\"\"Collects specified errors into an `errors` keyword argument.\n\n    Example:\n\n    ```python\n    from mirascope.integrations.tenacity import collect_errors\n    from tenacity import retry, stop_after_attempt\n\n    @retry(stop=stop_after_attempt(3), after=collect_errors(ValueError))\n    def throw_value_error(*, errors: list[ValueError] | None = None):\n        if errors:\n            print(errors[-1])\n        raise ValueError(\"Throwing Error\")\n\n\n    try:\n        throw_value_error()\n        # &gt; Throwing Error\n        # &gt; Throwing Error\n    except RetryError:\n        ...\n    ```\n\n    Args:\n        *args: The `Exception` types to catch.\n    \"\"\"\n\n    def inner(retry_state: RetryCallState) -&gt; None:\n        \"\"\"Collect errors into a `errors` kwarg of the wrapped function.\"\"\"\n        if (\n            (outcome := retry_state.outcome)\n            and (exception := outcome.exception())\n            and type(exception) in args\n        ):\n            errors = retry_state.kwargs.pop(\"errors\", [])\n            retry_state.kwargs[\"errors\"] = (\n                errors + [exception] if errors else [exception]\n            )\n\n    return inner\n</code></pre>"},{"location":"integrations/hyperdx/","title":"HyperDX","text":"<p>Mirascope provides out-of-the-box integration with HyperDX.</p> <p>You can install the necessary packages directly or using the <code>hyperdx</code> extras flag:</p> <pre><code>pip install \"mirascope[hyperdx]\"\n</code></pre> <p>You can then use the <code>with_hyperdx</code> decorator to automatically log calls:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import anthropic\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import gemini\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import groq\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import azure\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, anthropic\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book.\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book.\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book.\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom mirascope.integrations.otel import with_hyperdx\n\n\n@with_hyperdx()\n@bedrock.call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book.\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <p>This decorator is a simple wrapper on top of our OpenTelemetry integration</p>"},{"location":"integrations/langfuse/","title":"Langfuse","text":"<p>Mirascope provides out-of-the-box integration with Langfuse.</p> <p>You can install the necessary packages directly or using the <code>langfuse</code> extras flag:</p> <pre><code>pip install \"mirascope[langfuse]\"\n</code></pre> <p>You can then use the <code>with_langfuse</code> decorator to automatically log calls:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import gemini\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import groq\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import azure\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book.\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book.\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book.\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom mirascope.integrations.langfuse import with_langfuse\n\n\n@with_langfuse()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book.\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <p>This will give you:</p> <ul> <li>A trace around the <code>recommend_book</code> function that captures items like the prompt template, and input/output attributes and more.</li> <li>Human-readable display of the conversation with the agent</li> <li>Details of the response, including the number of tokens used</li> </ul> Example trace <p></p> Handling streams <p>When logging streams, the span will not be logged until the stream has been exhausted. This is a function of how streaming works.</p> <p>You will also need to set certain <code>call_params</code> for usage to be tracked for certain providers (such as OpenAI).</p>"},{"location":"integrations/logfire/","title":"Logfire","text":"<p>Logfire, a new tool from Pydantic, is built on OpenTelemetry. Since Pydantic powers many of Mirascope's features, it's appropriate for us to ensure seamless integration with them.</p> <p>You can install the necessary packages directly or using the <code>logfire</code> extras flag:</p> <pre><code>pip install \"mirascope[logfire]\"\n</code></pre> <p>You can then use the <code>with_logfire</code> decorator to automatically log calls:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import logfire\nfrom mirascope.core import openai\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import anthropic\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import mistral\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import gemini\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import groq\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import cohere\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import litellm\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import azure\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import vertex\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import bedrock\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import logfire\nfrom mirascope.core import Messages, openai\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book.\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import Messages, anthropic\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import Messages, mistral\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import Messages, gemini\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import Messages, groq\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import Messages, cohere\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import Messages, litellm\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import Messages, azure\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import Messages, vertex\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import Messages, bedrock\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book.\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import logfire\nfrom mirascope.core import openai, prompt_template\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@openai.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import mistral, prompt_template\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@mistral.call(\"mistral-large-latest\", response_model=Book)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import gemini, prompt_template\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import groq, prompt_template\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import cohere, prompt_template\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@cohere.call(\"command-r-plus\", response_model=Book)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import litellm, prompt_template\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import azure, prompt_template\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@azure.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import prompt_template, vertex\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import bedrock, prompt_template\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import logfire\nfrom mirascope.core import BaseMessageParam, openai\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book.\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import BaseMessageParam, anthropic\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import BaseMessageParam, mistral\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import BaseMessageParam, gemini\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import BaseMessageParam, groq\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import BaseMessageParam, cohere\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import BaseMessageParam, litellm\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import BaseMessageParam, azure\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import BaseMessageParam, vertex\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>import logfire\nfrom mirascope.core import BaseMessageParam, bedrock\nfrom mirascope.integrations.logfire import with_logfire\nfrom pydantic import BaseModel\n\nlogfire.configure()\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@with_logfire()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book.\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <p>This will give you:</p> <ul> <li>A span called <code>recommend_book</code> that captures items like the prompt template, templating properties and fields, and input/output attributes</li> <li>Human-readable display of the conversation with the agent</li> <li>Details of the response, including the number of tokens used</li> </ul> Example log <p></p> Handling streams <p>When logging streams, the span will not be logged until the stream has been exhausted. This is a function of how streaming works.</p> <p>You will also need to set certain <code>call_params</code> for usage to be tracked for certain providers (such as OpenAI).</p>"},{"location":"integrations/otel/","title":"OpenTelemetry","text":"<p>Mirascope provides out-of-the-box integration with OpenTelemetry.</p> <p>You can install the necessary packages directly or using the <code>opentelemetry</code> extras flag:</p> <pre><code>pip install \"mirascope[opentelemetry]\"\n</code></pre> <p>You can then use the <code>with_otel</code> decorator to automatically log calls:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import gemini\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import groq\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import azure\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book.\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book.\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book.\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book.\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom mirascope.integrations.otel import configure, with_otel\n\nconfigure()\n\n\n@with_otel()\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book.\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> Example output span <p>This will give you:</p> <ul> <li>A span called <code>recommend_book</code> that captures items like the prompt template, templating properties and fields, and input/output attributes</li> <li>Details of the response, including the number of tokens used</li> </ul> <pre><code>{\n    \"name\": \"recommend_book\",\n    \"context\": {\n        \"trace_id\": \"0x9397ede2f689a5b7767f0063d7b81959\",\n        \"span_id\": \"0xa22a446b2076ffe4\",\n        \"trace_state\": \"[]\"\n    },\n    \"kind\": \"SpanKind.INTERNAL\",\n    \"parent_id\": null,\n    \"start_time\": \"2024-10-04T05:16:47.340006Z\",\n    \"end_time\": \"2024-10-04T05:16:47.341937Z\",\n    \"status\": {\n        \"status_code\": \"UNSET\"\n    },\n    \"attributes\": {\n        \"gen_ai.system\": \"\",\n        \"gen_ai.request.model\": \"claude-3-5-sonnet-20240620\",\n        \"gen_ai.request.max_tokens\": 0,\n        \"gen_ai.request.temperature\": 0,\n        \"gen_ai.request.top_p\": 0,\n        \"gen_ai.response.model\": \"claude-3-5-sonnet-20240620\",\n        \"gen_ai.response.id\": \"msg_01X8sppiaZeErMjh8oCNz4ZA\",\n        \"gen_ai.response.finish_reasons\": [\n            \"end_turn\"\n        ],\n        \"gen_ai.usage.completion_tokens\": 268,\n        \"gen_ai.usage.prompt_tokens\": 12,\n        \"async\": false\n    },\n    \"events\": [\n        {\n            \"name\": \"gen_ai.content.prompt\",\n            \"timestamp\": \"2024-10-04T05:16:47.341924Z\",\n            \"attributes\": {\n                \"gen_ai.prompt\": \"{\\\"content\\\": \\\"Recommend a fantasy book\\\", \\\"role\\\": \\\"user\\\"}\"\n            }\n        },\n        {\n            \"name\": \"gen_ai.content.completion\",\n            \"timestamp\": \"2024-10-04T05:16:47.341931Z\",\n            \"attributes\": {\n                \"gen_ai.completion\": \"{\\\"content\\\": [{\\\"text\\\": \\\"There are many great fantasy books to choose from, but here's a popular recommendation:\\\\n\\\\n\\\\\\\"The Name of the Wind\\\\\\\" by Patrick Rothfuss\\\\n\\\\nThis is the first book in \\\\\\\"The Kingkiller Chronicle\\\\\\\" series. It's a beautifully written, engaging story that follows the life of Kvothe, a legendary wizard, warrior, and musician. The book is known for its rich world-building, complex magic system, and compelling characters.\\\\n\\\\nOther excellent fantasy recommendations include:\\\\n\\\\n1. \\\\\\\"The Lord of the Rings\\\\\\\" by J.R.R. Tolkien\\\\n2. \\\\\\\"A Game of Thrones\\\\\\\" by George R.R. Martin\\\\n3. \\\\\\\"The Way of Kings\\\\\\\" by Brandon Sanderson\\\\n4. \\\\\\\"The Lies of Locke Lamora\\\\\\\" by Scott Lynch\\\\n5. \\\\\\\"The Night Circus\\\\\\\" by Erin Morgenstern\\\\n6. \\\\\\\"Mistborn: The Final Empire\\\\\\\" by Brandon Sanderson\\\\n7. \\\\\\\"The Wizard of Earthsea\\\\\\\" by Ursula K. Le Guin\\\\n\\\\nThese books cater to different tastes within the fantasy genre, so you might want to read a brief synopsis of each to see which one appeals to you most.\\\", \\\"type\\\": \\\"text\\\"}], \\\"role\\\": \\\"assistant\\\"}\"\n            }\n        }\n    ],\n    \"links\": [],\n    \"resource\": {\n        \"attributes\": {\n            \"telemetry.sdk.language\": \"python\",\n            \"telemetry.sdk.name\": \"opentelemetry\",\n            \"telemetry.sdk.version\": \"1.22.0\",\n            \"service.name\": \"unknown_service\"\n        },\n        \"schema_url\": \"\"\n    }\n}\n</code></pre> Handling streams <p>When logging streams, the span will not be logged until the stream has been exhausted. This is a function of how streaming works.</p> <p>You will also need to set certain <code>call_params</code> for usage to be tracked for certain providers (such as OpenAI).</p>"},{"location":"integrations/otel/#sending-to-an-observability-tool","title":"Sending to an observability tool","text":"<p>You'll likely want to send the spans to an actual observability tool so that you can monitor your traces. There are many observability tools out there, but the majority of them can collect OpenTelemetry data. You can pass in processors as an argument of configure() so that you can send the spans to your choice of observability tool:</p> <pre><code>from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace.export import (\n    BatchSpanProcessor,\n)\nfrom mirascope.integrations.otel import configure\n\nOBSERVABILITY_TOOL_ENDPOINT = \"...\"\nconfigure(\n    processors=[\n        BatchSpanProcessor(\n            OTLPSpanExporter(\n                endpoint=f\"https://{OBSERVABILITY_TOOL_ENDPOINT}/v1/traces\",\n            )\n        )\n    ]\n)\n</code></pre> <p>You should refer to your observability tool's documentation to find the endpoint. If there is an observability backend that you would like for us to integrate out-of-the-box, create a GitHub Issue or let us know in our Slack community.</p>"},{"location":"learn/","title":"Learn Mirascope","text":"<p>This section is designed to help you master Mirascope, a toolkit for building AI-powered applications with Large Language Models (LLMs).</p> <p>Our documentation is tailored for developers who have at least some experience with Python and LLMs. Whether you're coming from other development tool libraries or have worked directly with provider SDKs and APIs, Mirascope offers a familiar but enhanced experience.</p> <p>If you haven't already, we recommend checking out Getting Started and Why Use Mirascope.</p>"},{"location":"learn/#key-features-and-benefits","title":"Key Features and Benefits","text":"<ul> <li> <p> Pythonic By Design</p> <p>Our design approach is to remain Pythonic so you can build your way</p> </li> <li> <p> Editor Support &amp; Type Hints</p> <p>Rich automplete, inline documentation, and type hints to catch errors before runtime</p> </li> <li> <p> Provider-Agnostic &amp; Provider-Specific</p> <p>Seamlessly engineer prompts agnostic or specific to various LLM providers</p> </li> <li> <p> Comprehensive Tooling</p> <p>Complete suite of tools for every aspect of working with LLM provider APIs</p> </li> </ul>"},{"location":"learn/#core-components","title":"Core Components","text":"<p>Mirascope is built around these core components, each designed to handle specific aspects of working with LLM provider APIs.</p> <p>We encourage you to dive into each component's documentation to gain a deeper understanding of Mirascope's capabilities. Start with the topics that align most closely with your immediate needs, but don't hesitate to explore all areas \u2013 you might discover new ways to enhance your LLM applications!</p> <p></p> <ul> <li> <p>Prompts</p> <p>Learn how to create and manage prompts effectively</p> <p> Docs</p> </li> <li> <p>Calls</p> <p>Understand how to make calls to LLMs using Mirascope</p> <p> Docs</p> </li> <li> <p>Streams</p> <p>Explore streaming responses for real-time applications</p> <p> Docs</p> </li> <li> <p>Chaining</p> <p>Understand the art of chaining multiple LLM calls for complex tasks</p> <p> Docs</p> </li> <li> <p>Response Models</p> <p>Define and use structured output models with automatic validation</p> <p> Docs</p> </li> <li> <p>JSON Mode</p> <p>Work with structured JSON data responses from LLMs</p> <p> Docs</p> </li> <li> <p>Output Parsers</p> <p>Process and transform custom LLM output structures effectively</p> <p> Docs</p> </li> <li> <p>Tools</p> <p>Discover how to extend LLM capabilities with custom tools</p> <p> Docs</p> </li> <li> <p>Agents</p> <p>Put everything together to build advanced AI agents using Mirascope</p> <p> Docs</p> </li> <li> <p>Evals</p> <p>Apply core components to build evaluation strategies for your LLM applications</p> <p> Docs</p> </li> <li> <p>Async</p> <p>Maximize efficiecy with asynchronous programming</p> <p> Docs</p> </li> <li> <p>Retries</p> <p>Understand how to automatically retry failed API calls</p> <p> Docs</p> </li> </ul> <p>As you progress, you'll find advanced topics and best practices throughout the documentation. These will help you optimize your use of Mirascope and build increasingly sophisticated AI-powered applications.</p> <p>Happy learning, and welcome to the world of development with Mirascope!</p>"},{"location":"learn/agents/","title":"Agents","text":"<p>Definition: a person who acts on behalf of another person or group</p> <p>When working with Large Language Models (LLMs), an \"agent\" refers to an autonomous or semi-autonomous system that can act on your behalf. The core concept is the use of tools to enable the LLM to interact with its environment.</p> <p>In this section we will implement a toy <code>Librarian</code> agent to demonstrate key concepts in Mirascope that will help you build agents.</p> <p>     If you haven't already, we recommend first reading the section on Tools </p> Diagram illustrating the agent flow <pre><code>sequenceDiagram\n    participant YC as Your Code\n    participant LLM\n\n    loop Agent Loop\n        YC-&gt;&gt;LLM: Call with prompt + history + function definitions\n        loop Tool Calling Cycle\n            LLM-&gt;&gt;LLM: Decide to respond or call functions\n            LLM-&gt;&gt;YC: Respond with function to call and arguments\n            YC-&gt;&gt;YC: Execute function with given arguments\n            YC-&gt;&gt;YC: Add tool call message parameters to history\n            YC-&gt;&gt;LLM: Call with prompt + history including function result\n        end\n        LLM-&gt;&gt;YC: Finish calling tools and return final response\n        YC-&gt;&gt;YC: Update history with final response\n    end</code></pre>"},{"location":"learn/agents/#state-management","title":"State Management","text":"<p>Since an agent needs to operate across multiple LLM API calls, the first concept to cover is state. The goal of providing state to the agent is to give it memory. For example, we can think of local variables as \"working memory\" and a database as \"long-term memory\".</p> <p>Let's take a look at a basic chatbot (not an agent) that uses a class to maintain the chat's history:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n\n    @openai.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n\n    @mistral.call(\"mistral-large-latest\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n\n    @gemini.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n\n    @cohere.call(\"command-r-plus\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n\n    @litellm.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n\n    @azure.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n\n    @vertex.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n\n    @openai.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n\n    @mistral.call(\"mistral-large-latest\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n\n    @gemini.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n\n    @cohere.call(\"command-r-plus\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n\n    @litellm.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n\n    @azure.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n\n    @vertex.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    def _call(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n\n    @openai.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str): ...\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str): ...\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, mistral, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n\n    @mistral.call(\"mistral-large-latest\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str): ...\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, gemini, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n\n    @gemini.call(\"gemini-1.5-flash\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str): ...\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, groq, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str): ...\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, cohere, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n\n    @cohere.call(\"command-r-plus\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str): ...\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, litellm, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n\n    @litellm.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str): ...\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, azure, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n\n    @azure.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str): ...\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, prompt_template, vertex\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n\n    @vertex.call(\"gemini-1.5-flash\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str): ...\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str): ...\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n\n    @openai.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; list[openai.OpenAIMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    def _call(self, query: str) -&gt; list[anthropic.AnthropicMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n\n    @mistral.call(\"mistral-large-latest\")\n    def _call(self, query: str) -&gt; list[mistral.MistralMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n\n    @gemini.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; list[gemini.GeminiMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    def _call(self, query: str) -&gt; list[groq.GroqMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n\n    @cohere.call(\"command-r-plus\")\n    def _call(self, query: str) -&gt; list[cohere.CohereMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n\n    @litellm.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; list[litellm.OpenAIMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n\n    @azure.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; list[azure.AzureMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n\n    @vertex.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; list[vertex.VertexMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    def _call(self, query: str) -&gt; list[bedrock.BedrockMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian().run()\n</code></pre> <p>In this example we:</p> <ul> <li>Create a <code>Librarian</code> class with a <code>history</code> attribute.</li> <li>Implement a private <code>_call</code> method that injects <code>history</code>.</li> <li>Run the <code>_call</code> method in a loop, saving the history at each step.</li> </ul> <p>A chatbot with memory, while more advanced, is still not an agent.</p> Provider-agnostic agent ShorthandMessagesString TemplateBaseMessageParam <pre><code>from typing import Literal\n\nfrom mirascope.core import (\n    BaseMessageParam,\n    Messages,\n    anthropic,\n    openai,\n    prompt_template,\n)\nfrom mirascope.core.base import BaseCallResponse\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    provider: Literal[\"openai\", \"anthropic\"]\n    model: Literal[\"gpt-4o-mini\", \"claude-3-5-sonnet-20240620\"]\n    history: list[BaseMessageParam] = []\n\n    @prompt_template()\n    def _prompt(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def _call(self, query: str) -&gt; BaseCallResponse:\n        if self.provider == \"openai\":\n            call = openai.call(self.model)(self._prompt)\n        elif self.provider == \"anthropic\":\n            call = anthropic.call(self.model)(self._prompt)\n        else:\n            raise ValueError(f\"Unsupported provider: {self.provider}\")\n        return call(query)\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian(provider=\"openai\", model=\"gpt-4o-mini\").run()\n</code></pre> <pre><code>from typing import Literal\n\nfrom mirascope.core import (\n    BaseMessageParam,\n    Messages,\n    anthropic,\n    openai,\n    prompt_template,\n)\nfrom mirascope.core.base import BaseCallResponse\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    provider: Literal[\"openai\", \"anthropic\"]\n    model: Literal[\"gpt-4o-mini\", \"claude-3-5-sonnet-20240620\"]\n    history: list[BaseMessageParam] = []\n\n    @prompt_template()\n    def _prompt(self, query: str) -&gt; Messages.Type:\n        return [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n\n    def _call(self, query: str) -&gt; BaseCallResponse:\n        if self.provider == \"openai\":\n            call = openai.call(self.model)(self._prompt)\n        elif self.provider == \"anthropic\":\n            call = anthropic.call(self.model)(self._prompt)\n        else:\n            raise ValueError(f\"Unsupported provider: {self.provider}\")\n        return call(query)\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian(provider=\"openai\", model=\"gpt-4o-mini\").run()\n</code></pre> <pre><code>from typing import Literal\n\nfrom mirascope.core import (\n    BaseMessageParam,\n    Messages,\n    anthropic,\n    openai,\n    prompt_template,\n)\nfrom mirascope.core.base import BaseCallResponse\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    provider: Literal[\"openai\", \"anthropic\"]\n    model: Literal[\"gpt-4o-mini\", \"claude-3-5-sonnet-20240620\"]\n    history: list[BaseMessageParam] = []\n\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _prompt(self, query: str): ...\n\n    def _call(self, query: str) -&gt; BaseCallResponse:\n        if self.provider == \"openai\":\n            call = openai.call(self.model)(self._prompt)\n        elif self.provider == \"anthropic\":\n            call = anthropic.call(self.model)(self._prompt)\n        else:\n            raise ValueError(f\"Unsupported provider: {self.provider}\")\n        return call(query)\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                Messages.User(query),\n                response.message_param,\n            ]\n\n\nLibrarian(provider=\"openai\", model=\"gpt-4o-mini\").run()\n</code></pre> <pre><code>from typing import Literal\n\nfrom mirascope.core import BaseMessageParam, anthropic, openai, prompt_template\nfrom mirascope.core.base import BaseCallResponse\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    provider: Literal[\"openai\", \"anthropic\"]\n    model: Literal[\"gpt-4o-mini\", \"claude-3-5-sonnet-20240620\"]\n    history: list[BaseMessageParam] = []\n\n    @prompt_template()\n    def _prompt(self, query: str) -&gt; list[BaseMessageParam]:\n        return [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=(query)),\n        ]\n\n    def _call(self, query: str) -&gt; BaseCallResponse:\n        if self.provider == \"openai\":\n            call = openai.call(self.model)(self._prompt)\n        elif self.provider == \"anthropic\":\n            call = anthropic.call(self.model)(self._prompt)\n        else:\n            raise ValueError(f\"Unsupported provider: {self.provider}\")\n        return call(query)\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            response = self._call(query)\n            print(response.content)\n            self.history += [\n                BaseMessageParam(role=\"user\", content=query),\n                response.message_param,\n            ]\n\n\nLibrarian(provider=\"openai\", model=\"gpt-4o-mini\").run()\n</code></pre>"},{"location":"learn/agents/#integrating-tools","title":"Integrating Tools","text":"<p>The next concept to cover is introducing tools to our chatbot, turning it into an agent capable of acting on our behalf. The most basic agent flow is to call tools on behalf of the agent, providing them back through the chat history until the agent is ready to response to the initial query.</p> <p>Let's take a look at a basic example where the <code>Librarian</code> can access the books available in the library:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @openai.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @mistral.call(\"mistral-large-latest\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @gemini.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @cohere.call(\"command-r-plus\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @litellm.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @azure.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @vertex.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @openai.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @mistral.call(\"mistral-large-latest\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @gemini.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @cohere.call(\"command-r-plus\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @litellm.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @azure.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @vertex.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @openai.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, anthropic, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, mistral, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @mistral.call(\"mistral-large-latest\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, gemini, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @gemini.call(\"gemini-1.5-flash\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, groq, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, cohere, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @cohere.call(\"command-r-plus\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, litellm, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @litellm.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, azure, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @azure.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, prompt_template, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @vertex.call(\"gemini-1.5-flash\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, bedrock, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @openai.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @mistral.call(\"mistral-large-latest\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @gemini.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @cohere.call(\"command-r-plus\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @litellm.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @azure.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @vertex.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <p>In this example we:</p> <ol> <li>Added the <code>library</code> state to maintain the list of available books.</li> <li>Implemented the <code>_available_books</code> tool that returns the library as a string.</li> <li>Updated <code>_call</code> to give the LLM access to the tool.<ul> <li>We used the <code>tools</code> dynamic configuration field so the tool has access to the library through <code>self</code>.</li> </ul> </li> <li>Added a <code>_step</code> method that implements a full step from user input to assistant output.</li> <li>For each step, we call the LLM and see if there are any tool calls.<ul> <li>If yes, we call the tools, collect the outputs, and insert the tool calls into the chat history. We then recursively call <code>_step</code> again with an empty user query until the LLM is done calling tools and is ready to response</li> <li>If no, the LLM is ready to respond and we return the response content.</li> </ul> </li> </ol> <p>Now that our chatbot is capable of using tools, we have a basic agent.</p>"},{"location":"learn/agents/#human-in-the-loop","title":"Human-In-The-Loop","text":"<p>While it would be nice to have fully autonomous agents, LLMs are far from perfect and often need assistance to ensure they continue down the right path in an agent flow.</p> <p>One common and easy way to help guide LLM agents is to give the agent the ability to ask for help. This \"human-in-the-loop\" flow lets the agent ask for help if it determines it needs it:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @openai.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @mistral.call(\"mistral-large-latest\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @gemini.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @cohere.call(\"command-r-plus\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @litellm.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @azure.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @vertex.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @openai.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @mistral.call(\"mistral-large-latest\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @gemini.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @cohere.call(\"command-r-plus\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @litellm.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @azure.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @vertex.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, Messages, openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @openai.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, anthropic, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, mistral, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @mistral.call(\"mistral-large-latest\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, gemini, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @gemini.call(\"gemini-1.5-flash\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, groq, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, cohere, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @cohere.call(\"command-r-plus\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, litellm, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @litellm.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, azure, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @azure.call(\"gpt-4o-mini\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, prompt_template, vertex\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @vertex.call(\"gemini-1.5-flash\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, bedrock, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(Messages.User(query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, openai\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @openai.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, anthropic\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, mistral\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @mistral.call(\"mistral-large-latest\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, gemini\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @gemini.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, groq\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @groq.call(\"llama-3.1-70b-versatile\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, cohere\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @cohere.call(\"command-r-plus\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, litellm\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @litellm.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, azure\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @azure.call(\"gpt-4o-mini\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, vertex\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @vertex.call(\"gemini-1.5-flash\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, bedrock\nfrom pydantic import BaseModel\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n\n    def _ask_for_help(self, question: str) -&gt; str:\n        \"\"\"Asks for help from an expert.\"\"\"\n        print(\"[Assistant Needs Help]\")\n        print(f\"[QUESTION]: {question}\")\n        answer = input(\"[ANSWER]: \")\n        print(\"[End Help]\")\n        return answer\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n    def _call(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._ask_for_help]}\n\n    def _step(self, query: str) -&gt; str:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        response = self._call(query)\n        self.history.append(response.message_param)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step(\"\")\n        else:\n            return response.content\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            step_output = self._step(query)\n            print(step_output)\n\n\nLibrarian().run()\n</code></pre>"},{"location":"learn/agents/#streaming","title":"Streaming","text":"<p>The previous examples print each tool call so you can see what the agent is doing before the final response; however, you still need to wait for the agent to generate its entire final response before you see the output.</p> <p>Streaming can help to provide an even more real-time experience:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @openai.call(\"gpt-4o-mini\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @mistral.call(\"mistral-large-latest\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @gemini.call(\"gemini-1.5-flash\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @groq.call(\"llama-3.1-70b-versatile\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @cohere.call(\"command-r-plus\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @litellm.call(\"gpt-4o-mini\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @azure.call(\"gpt-4o-mini\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @vertex.call(\"gemini-1.5-flash\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @openai.call(\"gpt-4o-mini\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @mistral.call(\"mistral-large-latest\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @gemini.call(\"gemini-1.5-flash\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @groq.call(\"llama-3.1-70b-versatile\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @cohere.call(\"command-r-plus\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @litellm.call(\"gpt-4o-mini\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @azure.call(\"gpt-4o-mini\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @vertex.call(\"gemini-1.5-flash\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            Messages.System(\"You are a librarian\"),\n            *self.history,\n            Messages.User(query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @openai.call(\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, anthropic, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, mistral, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @mistral.call(\"mistral-large-latest\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, gemini, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @gemini.call(\"gemini-1.5-flash\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, groq, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @groq.call(\"llama-3.1-70b-versatile\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, cohere, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @cohere.call(\"command-r-plus\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, litellm, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @litellm.call(\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, azure, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @azure.call(\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, prompt_template, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @vertex.call(\"gemini-1.5-flash\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, Messages, bedrock, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a librarian\n        MESSAGES: {self.history}\n        USER: {query}\n        \"\"\"\n    )\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        return {\"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(Messages.User(query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[openai.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @openai.call(\"gpt-4o-mini\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[anthropic.AnthropicMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[mistral.MistralMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @mistral.call(\"mistral-large-latest\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[gemini.GeminiMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @gemini.call(\"gemini-1.5-flash\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[groq.GroqMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @groq.call(\"llama-3.1-70b-versatile\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[cohere.CohereMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @cohere.call(\"command-r-plus\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[litellm.OpenAIMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @litellm.call(\"gpt-4o-mini\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[azure.AzureMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @azure.call(\"gpt-4o-mini\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[vertex.VertexMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @vertex.call(\"gemini-1.5-flash\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre> <pre><code>import json\nfrom typing import cast\n\nfrom mirascope.core import BaseDynamicConfig, BaseMessageParam, bedrock\nfrom pydantic import BaseModel\n\nfrom mirascope.core.bedrock._types import ToolResultBlockMessageTypeDef\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Librarian(BaseModel):\n    history: list[bedrock.BedrockMessageParam] = []\n    library: list[Book] = [\n        Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\"),\n        Book(title=\"Mistborn: The Final Empire\", author=\"Brandon Sanderson\"),\n    ]\n\n    def _available_books(self) -&gt; str:\n        \"\"\"Returns the list of books available in the library.\"\"\"\n        return json.dumps([book.model_dump() for book in self.library])\n\n    @bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n    def _stream(self, query: str) -&gt; BaseDynamicConfig:\n        messages = [\n            BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n            *self.history,\n            BaseMessageParam(role=\"user\", content=query),\n        ]\n        return {\"messages\": messages, \"tools\": [self._available_books]}\n\n    def _step(self, query: str) -&gt; None:\n        if query:\n            self.history.append(BaseMessageParam(role=\"user\", content=query))\n        stream = self._stream(query)\n        tools_and_outputs = []\n        for chunk, tool in stream:\n            if tool:\n                print(f\"[Calling Tool '{tool._name()}' with args {tool.args}]\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += cast(\n                list[ToolResultBlockMessageTypeDef],\n                stream.tool_message_params(tools_and_outputs),\n            )\n            self._step(\"\")\n\n    def run(self) -&gt; None:\n        while True:\n            query = input(\"(User): \")\n            if query in [\"exit\", \"quit\"]:\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(query)\n            print()\n\n\nLibrarian().run()\n</code></pre>"},{"location":"learn/agents/#next-steps","title":"Next Steps","text":"<p>This section is just the tip of the iceberg when it comes to building agents, implementing just one type of simple agent flow. It's important to remember that \"agent\" is quite a general term and can mean different things for different use-cases. Mirascope's various features make building agents easier, but it will be up to you to determine the architecture that best suits your goals.</p> <p>Next, we recommend taking a look at our Agent Tutorials to see examples of more complex, real-world agents.</p>"},{"location":"learn/async/","title":"Async","text":"<p>Asynchronous programming is a crucial concept when building applications with LLMs (Large Language Models) using Mirascope. This feature allows for efficient handling of I/O-bound operations (e.g., API calls), improving application responsiveness and scalability. Mirascope utilizes the asyncio library to implement asynchronous processing.</p> <p>Best Practices</p> <ul> <li>Use asyncio for I/O-bound tasks: Async is most beneficial for I/O-bound operations like API calls. It may not provide significant benefits for CPU-bound tasks.</li> <li>Avoid blocking operations: Ensure that you're not using blocking operations within async functions, as this can negate the benefits of asynchronous programming.</li> <li>Consider using connection pools: When making many async requests, consider using connection pools to manage and reuse connections efficiently.</li> <li>Be mindful of rate limits: While async allows for concurrent requests, be aware of API rate limits and implement appropriate throttling if necessary.</li> <li>Use appropriate timeouts: Implement timeouts for async operations to prevent hanging in case of network issues or unresponsive services.</li> <li>Test thoroughly: Async code can introduce subtle bugs. Ensure comprehensive testing of your async implementations.</li> <li>Leverage async context managers: Use async context managers (async with) for managing resources that require setup and cleanup in async contexts.</li> </ul> Diagram illustrating the flow of asynchronous processing <pre><code>sequenceDiagram\n    participant Main as Main Process\n    participant API1 as API Call 1\n    participant API2 as API Call 2\n    participant API3 as API Call 3\n\n    Main-&gt;&gt;+API1: Send Request\n    Main-&gt;&gt;+API2: Send Request\n    Main-&gt;&gt;+API3: Send Request\n    API1--&gt;&gt;-Main: Response\n    API2--&gt;&gt;-Main: Response\n    API3--&gt;&gt;-Main: Response\n    Main-&gt;&gt;Main: Process All Responses</code></pre>"},{"location":"learn/async/#key-terms","title":"Key Terms","text":"<ul> <li><code>async</code>: Keyword used to define a function as asynchronous</li> <li><code>await</code>: Keyword used to wait for the completion of an asynchronous operation</li> <li><code>asyncio</code>: Python library that supports asynchronous programming</li> </ul>"},{"location":"learn/async/#basic-usage-and-syntax","title":"Basic Usage and Syntax","text":"<p>     If you haven't already, we recommend first reading the section on Calls </p> <p>To use async in Mirascope, simply define the function as async and use the <code>await</code> keyword when calling it. Here's a basic example:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import azure\n\n\n@azure.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import Messages, openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, azure\n\n\n@azure.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import litellm, prompt_template\n\n\n@litellm.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import azure, prompt_template\n\n\n@azure.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <p>In this example we:</p> <ol> <li>Define <code>recommend_book</code> as an asynchronous function.</li> <li>Create a <code>main</code> function that calls <code>recommend_book</code> and awaits it.</li> <li>Use <code>asyncio.run(main())</code> to start the asynchronous event loop and run the main function.</li> </ol>"},{"location":"learn/async/#parallel-async-calls","title":"Parallel Async Calls","text":"<p>One of the main benefits of asynchronous programming is the ability to run multiple operations concurrently. Here's an example of making parallel async calls:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import azure\n\n\n@azure.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import Messages, openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, azure\n\n\n@azure.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import litellm, prompt_template\n\n\n@litellm.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import azure, prompt_template\n\n\n@azure.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(model=\"gpt-4o-mini\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    genres = [\"fantasy\", \"scifi\", \"mystery\"]\n    tasks = [recommend_book(genre) for genre in genres]\n    results = await asyncio.gather(*tasks)\n\n    for genre, response in zip(genres, results):\n        print(f\"({genre}):\\n{response.content}\\n\")\n\n\nasyncio.run(main())\n</code></pre> <p>We are using <code>asyncio.gather</code> to run and await multiple asynchronous tasks concurrently, printing the results for each task one all are completed.</p>"},{"location":"learn/async/#async-streaming","title":"Async Streaming","text":"<p>     If you haven't already, we recommend first reading the section on Streams </p> <p>Streaming with async works similarly to synchronous streaming, but you use <code>async for</code> instead of a regular <code>for</code> loop:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import openai\n\n\n@openai.call(model=\"gpt-4o-mini\", stream=True)\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\", stream=True)\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import azure\n\n\n@azure.call(model=\"gpt-4o-mini\", stream=True)\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import Messages, openai\n\n\n@openai.call(model=\"gpt-4o-mini\", stream=True)\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\", stream=True)\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, azure\n\n\n@azure.call(model=\"gpt-4o-mini\", stream=True)\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import litellm, prompt_template\n\n\n@litellm.call(model=\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import azure, prompt_template\n\n\n@azure.call(model=\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(model=\"gpt-4o-mini\", stream=True)\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\", stream=True)\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(model=\"gpt-4o-mini\", stream=True)\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    stream = await recommend_book(\"fantasy\")\n    async for chunk, _ in stream:\n        print(chunk.content, end=\"\", flush=True)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"learn/async/#async-tools","title":"Async Tools","text":"<p>     If you haven't already, we recommend first reading the section on Tools </p> <p>When using tools asynchronously, you can make the <code>call</code> method of a tool async:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, openai\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@openai.call(model=\"gpt-4o-mini\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, anthropic\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, mistral\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, gemini\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, groq\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, cohere\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@cohere.call(\"command-r-plus\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, litellm\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@litellm.call(model=\"gpt-4o-mini\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, azure\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@azure.call(model=\"gpt-4o-mini\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, vertex\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, bedrock\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, Messages, openai\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@openai.call(model=\"gpt-4o-mini\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, Messages, anthropic\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, Messages, mistral\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, Messages, gemini\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, Messages, groq\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, Messages, cohere\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@cohere.call(\"command-r-plus\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, Messages, litellm\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@litellm.call(model=\"gpt-4o-mini\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, Messages, azure\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@azure.call(model=\"gpt-4o-mini\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, Messages, vertex\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, Messages, bedrock\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, openai, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@openai.call(model=\"gpt-4o-mini\", tools=[FormatBook])\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, anthropic, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[FormatBook])\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, mistral, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[FormatBook])\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, gemini, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[FormatBook])\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, groq, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[FormatBook])\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, cohere, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@cohere.call(\"command-r-plus\", tools=[FormatBook])\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, litellm, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@litellm.call(model=\"gpt-4o-mini\", tools=[FormatBook])\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, azure, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@azure.call(model=\"gpt-4o-mini\", tools=[FormatBook])\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, prompt_template, vertex\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[FormatBook])\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseTool, bedrock, prompt_template\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[FormatBook])\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book(genre: str): ...\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, BaseTool, openai\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@openai.call(model=\"gpt-4o-mini\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, BaseTool, anthropic\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, BaseTool, mistral\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, BaseTool, gemini\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, BaseTool, groq\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, BaseTool, cohere\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@cohere.call(\"command-r-plus\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, BaseTool, litellm\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@litellm.call(model=\"gpt-4o-mini\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, BaseTool, azure\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@azure.call(model=\"gpt-4o-mini\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, BaseTool, vertex\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, BaseTool, bedrock\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    async def call(self) -&gt; str:\n        # Simulating an async API call\n        await asyncio.sleep(1)\n        return f\"{self.title} by {self.author}\"\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[FormatBook])\nasync def recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nasync def main():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        if isinstance(tool, FormatBook):\n            output = await tool.call()\n            print(output)\n    else:\n        print(response.content)\n\n\nasyncio.run(main())\n</code></pre> <p>It's important to note that in this example we use <code>isinstance(tool, FormatBook)</code> to ensure the <code>call</code> method can be awaited safely. This also gives us proper type hints and editor support.</p>"},{"location":"learn/async/#custom-client","title":"Custom Client","text":"<p>You can use custom clients with async calls just like you can with standard calls by using the <code>client</code> parameter in the <code>call</code> decorator.</p> <p>It's important to note that you must use the correct client that supports asynchronous calls:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom openai import AsyncOpenAI\n\n\n@openai.call(\"gpt-4o-mini\", client=AsyncOpenAI())\nasync def recommend_book_async(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from anthropic import AsyncAnthropic\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", client=AsyncAnthropic())\nasync def recommend_book_async(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom mistralai.async_client import MistralAsyncClient\n\n\n@mistral.call(\"mistral-large-latest\", client=MistralAsyncClient())\nasync def recommend_book_async(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom mirascope.core import gemini\n\n\n@gemini.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\nasync def recommend_book_async(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from groq import AsyncGroq\nfrom mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", client=AsyncGroq())\nasync def recommend_book_async(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from cohere import AsyncClient\nfrom mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", client=AsyncClient())\nasync def recommend_book_async(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code># Not Supported\n</code></pre> <pre><code>from azure.ai.inference.aio import ChatCompletionsClient as AsyncChatCompletionsClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom mirascope.core import azure\n\n\n@azure.call(\n    \"gpt-4o-mini\",\n    client=AsyncChatCompletionsClient(\n        endpoint=\"your-endpoint\", credential=AzureKeyCredential(\"your-credentials\")\n    ),\n)\nasync def recommend_book_async(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom vertexai.generative_models import GenerativeModel\n\n\n@vertex.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\nasync def recommend_book_async(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import bedrock\nfrom aiobotocore.session import get_session\n\n\nasync def get_async_client():\n    session = get_session()\n    async with session.create_client(\"bedrock-runtime\") as client:\n        return client\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", client=asyncio.run(get_async_client())\n)\nasync def recommend_book_async(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom openai import AsyncOpenAI\n\n\n@openai.call(\"gpt-4o-mini\", client=AsyncOpenAI())\nasync def recommend_book_async(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from anthropic import AsyncAnthropic\nfrom mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", client=AsyncAnthropic())\nasync def recommend_book_async(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom mistralai.async_client import MistralAsyncClient\n\n\n@mistral.call(\"mistral-large-latest\", client=MistralAsyncClient())\nasync def recommend_book_async(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom mirascope.core import Messages, gemini\n\n\n@gemini.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\nasync def recommend_book_async(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from groq import AsyncGroq\nfrom mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", client=AsyncGroq())\nasync def recommend_book_async(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from cohere import AsyncClient\nfrom mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", client=AsyncClient())\nasync def recommend_book_async(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code># Not Supported\n</code></pre> <pre><code>from azure.ai.inference.aio import ChatCompletionsClient as AsyncChatCompletionsClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom mirascope.core import Messages, azure\n\n\n@azure.call(\n    \"gpt-4o-mini\",\n    client=AsyncChatCompletionsClient(\n        endpoint=\"your-endpoint\", credential=AzureKeyCredential(\"your-credentials\")\n    ),\n)\nasync def recommend_book_async(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom vertexai.generative_models import GenerativeModel\n\n\n@vertex.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\nasync def recommend_book_async(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import Messages, bedrock\nfrom aiobotocore.session import get_session\n\n\nasync def get_async_client():\n    session = get_session()\n    async with session.create_client(\"bedrock-runtime\") as client:\n        return client\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", client=asyncio.run(get_async_client())\n)\nasync def recommend_book_async(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom openai import AsyncOpenAI\n\n\n@openai.call(\"gpt-4o-mini\", client=AsyncOpenAI())\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book_async(genre: str): ...\n</code></pre> <pre><code>from anthropic import AsyncAnthropic\nfrom mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", client=AsyncAnthropic())\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book_async(genre: str): ...\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom mistralai.async_client import MistralAsyncClient\n\n\n@mistral.call(\"mistral-large-latest\", client=MistralAsyncClient())\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book_async(genre: str): ...\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book_async(genre: str): ...\n</code></pre> <pre><code>from groq import AsyncGroq\nfrom mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", client=AsyncGroq())\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book_async(genre: str): ...\n</code></pre> <pre><code>from cohere import AsyncClient\nfrom mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", client=AsyncClient())\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book_async(genre: str): ...\n</code></pre> <pre><code># Not Supported\n</code></pre> <pre><code>from azure.ai.inference.aio import ChatCompletionsClient as AsyncChatCompletionsClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom mirascope.core import azure, prompt_template\n\n\n@azure.call(\n    \"gpt-4o-mini\",\n    client=AsyncChatCompletionsClient(\n        endpoint=\"your-endpoint\", credential=AzureKeyCredential(\"your-credentials\")\n    ),\n)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book_async(genre: str): ...\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom vertexai.generative_models import GenerativeModel\n\n\n@vertex.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book_async(genre: str): ...\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import bedrock, prompt_template\nfrom aiobotocore.session import get_session\n\n\nasync def get_async_client():\n    session = get_session()\n    async with session.create_client(\"bedrock-runtime\") as client:\n        return client\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", client=asyncio.run(get_async_client())\n)\n@prompt_template(\"Recommend a {genre} book\")\nasync def recommend_book_async(genre: str): ...\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom openai import AsyncOpenAI\n\n\n@openai.call(\"gpt-4o-mini\", client=AsyncOpenAI())\nasync def recommend_book_async(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from anthropic import AsyncAnthropic\nfrom mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", client=AsyncAnthropic())\nasync def recommend_book_async(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom mistralai.async_client import MistralAsyncClient\n\n\n@mistral.call(\"mistral-large-latest\", client=MistralAsyncClient())\nasync def recommend_book_async(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\nasync def recommend_book_async(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from groq import AsyncGroq\nfrom mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", client=AsyncGroq())\nasync def recommend_book_async(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from cohere import AsyncClient\nfrom mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", client=AsyncClient())\nasync def recommend_book_async(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code># Not Supported\n</code></pre> <pre><code>from azure.ai.inference.aio import ChatCompletionsClient as AsyncChatCompletionsClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\n    \"gpt-4o-mini\",\n    client=AsyncChatCompletionsClient(\n        endpoint=\"your-endpoint\", credential=AzureKeyCredential(\"your-credentials\")\n    ),\n)\nasync def recommend_book_async(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom vertexai.generative_models import GenerativeModel\n\n\n@vertex.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\nasync def recommend_book_async(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import BaseMessageParam, bedrock\nfrom aiobotocore.session import get_session\n\n\nasync def get_async_client():\n    session = get_session()\n    async with session.create_client(\"bedrock-runtime\") as client:\n        return client\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", client=asyncio.run(get_async_client())\n)\nasync def recommend_book_async(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre>"},{"location":"learn/async/#next-steps","title":"Next Steps","text":"<p>By leveraging these async features in Mirascope, you can build more efficient and responsive applications, especially when working with multiple LLM calls or other I/O-bound operations.</p> <p>This section concludes the core functionality Mirascope supports. If you haven't already, we recommend taking a look at any previous sections you've missed to learn about what you can do with Mirascope.</p> <p>You can also check out the section on Provider-Specific Features to learn about how to use features that only certain providers support, such as OpenAI's structured outputs.</p>"},{"location":"learn/calls/","title":"Calls","text":"<p>     If you haven't already, we recommend first reading the section on writing Prompts </p> <p>When working with Large Language Model (LLM) APIs in Mirascope, a \"call\" refers to making a request to a LLM provider's API with a particular setting and prompt.</p> <p>The <code>call</code> decorator is a core feature of the Mirascope library, designed to simplify and streamline interactions with various LLM providers. This powerful tool allows you to transform prompt templates written as Python functions into LLM API calls with minimal boilerplate code while providing type safety and consistency across different providers.</p> <p>We currently support OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, and Vertex AI</p> <p>If there are any providers we don't yet support that you'd like to see supported, let us know!</p> API Documentation <p><code>mirascope.core.openai.call</code></p> <p><code>mirascope.core.anthropic.call</code></p> <p><code>mirascope.core.mistral.call</code></p> <p><code>mirascope.core.gemini.call</code></p> <p><code>mirascope.core.groq.call</code></p> <p><code>mirascope.core.cohere.call</code></p> <p><code>mirascope.core.litellm.call</code></p> <p><code>mirascope.core.azure.call</code></p> <p><code>mirascope.core.vertex.call</code></p> <p><code>mirascope.core.bedrock.call</code></p>"},{"location":"learn/calls/#basic-usage-and-syntax","title":"Basic Usage and Syntax","text":""},{"location":"learn/calls/#provider-specific-usage","title":"Provider-Specific Usage","text":"<p>Let's take a look at a basic example using Mirascope vs. official provider SDKs:</p> <p>Mirascope</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> Official SDK OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\n\ndef recommend_book(genre: str) -&gt; str:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n    )\n    return str(completion.choices[0].message.content)\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from anthropic import Anthropic\n\nclient = Anthropic()\n\n\ndef recommend_book(genre: str) -&gt; str:\n    message = client.messages.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n        max_tokens=1024,\n    )\n    block = message.content[0]\n    return block.text if block.type == \"text\" else \"\"\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from mistralai.client import MistralClient\n\nclient = MistralClient()\n\n\ndef recommend_book(genre: str) -&gt; str:\n    completion = client.chat(\n        model=\"mistral-large-latest\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n    )\n    return completion.choices[0].message.content\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\n\nclient = GenerativeModel(\"gemini-1.5-flash\")\n\n\ndef recommend_book(genre: str) -&gt; str:\n    generation = client.generate_content(\n        contents=[{\"role\": \"user\", \"parts\": f\"Recommend a {genre} book\"}]  # pyright: ignore [reportArgumentType]\n    )\n    return generation.candidates[0].content.parts[0].text\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from groq import Groq\n\nclient = Groq()\n\n\ndef recommend_book(genre: str) -&gt; str:\n    completion = client.chat.completions.create(\n        model=\"llama-3.1-70b-versatile\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n    )\n    return str(completion.choices[0].message.content)\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from cohere import Client\n\nclient = Client()\n\n\ndef recommend_book(genre: str) -&gt; str:\n    response = client.chat(\n        model=\"command-r-plus\",\n        message=f\"Recommend a {genre} book\",\n    )\n    return response.text\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from litellm import completion\n\n\ndef recommend_book(genre: str) -&gt; str:\n    response = completion(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}],\n    )\n    return str(response.choices[0].message.content)  # type: ignore\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import ChatRequestMessage\nfrom azure.core.credentials import AzureKeyCredential\n\nclient = ChatCompletionsClient(\n    endpoint=\"YOUR_ENDPOINT\", credential=AzureKeyCredential(\"YOUR_KEY\")\n)\n\n\ndef recommend_book(genre: str) -&gt; str:\n    completion = client.complete(\n        model=\"gpt-4o-mini\",\n        messages=[\n            ChatRequestMessage({\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"})\n        ],\n    )\n    message = completion.choices[0].message\n    return message.content if message.content is not None else \"\"\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>from vertexai.generative_models import GenerativeModel\n\nclient = GenerativeModel(\"gemini-1.5-flash\")\n\n\ndef recommend_book(genre: str) -&gt; str:\n    generation = client.generate_content(\n        contents=[{\"role\": \"user\", \"parts\": f\"Recommend a {genre} book\"}]\n    )\n    return generation.candidates[0].content.parts[0].text  # type: ignore\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <pre><code>import boto3\n\nbedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n\n\ndef recommend_book(genre: str) -&gt; str:\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"Recommend a {genre} book\"}]}]\n    response = bedrock_client.converse(\n        modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",\n        messages=messages,\n        inferenceConfig={\"maxTokens\": 1024},\n    )\n    output_message = response[\"output\"][\"message\"]\n    content = \"\"\n    for content_piece in output_message[\"content\"]:\n        if \"text\" in content_piece:\n            content += content_piece[\"text\"]\n    return content\n\n\noutput = recommend_book(\"fantasy\")\nprint(output)\n</code></pre> <p>Notice how Mirascope makes calls more readable by reducing boilerplate and standardizing interactions with LLM providers.</p> <p>In these above Mirascope examples, we are directly tying the prompt to a specific provider and call setting (provider-specific prompt engineering). In these cases, the <code>@prompt_template</code> decorator becomes optional unless you're using string templates.</p>"},{"location":"learn/calls/#provider-agnostic-usage","title":"Provider-Agnostic Usage","text":"<p>We've implemented calls as decorators so that they work for both provider-specific cases (as seen above) as well as provider-agnostic cases.</p> <p>Let's take a look at a basic example using Mirascope to call both OpenAI and Anthropic with the same prompt:</p> ShorthandMessagesString TemplateBaseMessageParam <pre><code>from mirascope.core import anthropic, openai, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n# OpenAI\nopenai_model = \"gpt-4o-mini\"\nopenai_recommend_book = openai.call(openai_model)(recommend_book_prompt)\nopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n\n# Anthropic\nanthropic_model = \"claude-3-5-sonnet-20240620\"\nanthropic_recommend_book = anthropic.call(anthropic_model)(recommend_book_prompt)\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic, openai, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n# OpenAI\nopenai_model = \"gpt-4o-mini\"\nopenai_recommend_book = openai.call(openai_model)(recommend_book_prompt)\nopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n\n# Anthropic\nanthropic_model = \"claude-3-5-sonnet-20240620\"\nanthropic_recommend_book = anthropic.call(anthropic_model)(recommend_book_prompt)\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic, openai, prompt_template\n\n\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book_prompt(genre: str): ...\n\n\n# OpenAI\nopenai_model = \"gpt-4o-mini\"\nopenai_recommend_book = openai.call(openai_model)(recommend_book_prompt)\nopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n\n# Anthropic\nanthropic_model = \"claude-3-5-sonnet-20240620\"\nanthropic_recommend_book = anthropic.call(anthropic_model)(recommend_book_prompt)\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic, openai, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n# OpenAI\nopenai_model = \"gpt-4o-mini\"\nopenai_recommend_book = openai.call(openai_model)(recommend_book_prompt)\nopenai_response = openai_recommend_book(\"fantasy\")\nprint(openai_response.content)\n\n# Anthropic\nanthropic_model = \"claude-3-5-sonnet-20240620\"\nanthropic_recommend_book = anthropic.call(anthropic_model)(recommend_book_prompt)\nanthropic_response = anthropic_recommend_book(\"fantasy\")\nprint(anthropic_response.content)\n</code></pre>"},{"location":"learn/calls/#handling-responses","title":"Handling Responses","text":""},{"location":"learn/calls/#common-response-properties-and-methods","title":"Common Response Properties and Methods","text":"API Documentation <p><code>mirascope.core.base.call_response</code></p> <p>All <code>BaseCallResponse</code> objects share these common properties:</p> <ul> <li><code>content</code>: The main text content of the response. If no content is present, this will be the empty string.</li> <li><code>finish_reasons</code>: A list of reasons why the generation finished (e.g., \"stop\", \"length\"). These will be typed specifically for the provider used. If no finish reasons are present, this will be <code>None</code>.</li> <li><code>model</code>: The name of the model used for generation.</li> <li><code>id</code>: A unique identifier for the response if available. Otherwise this will be <code>None</code>.</li> <li><code>usage</code>: Information about token usage for the call if available. Otherwise this will be <code>None</code>.</li> <li><code>input_tokens</code>: The number of input tokens used if available. Otherwise this will be <code>None</code>.</li> <li><code>output_tokens</code>: The number of output tokens generated if available. Otherwise this will be <code>None</code>.</li> <li><code>cost</code>: An estimated cost of the API call if available. Otherwise this will be <code>None</code>.</li> <li><code>message_param</code>: The assistant's response formatted as a message parameter.</li> <li><code>tools</code>: A list of provider-specific tools used in the response, if any. Otherwise this will be <code>None</code>. Check out the <code>Tools</code> documentation for more details.</li> <li><code>tool</code>: The first tool used in the response, if any. Otherwise this will be <code>None</code>. Check out the <code>Tools</code> documentation for more details.</li> <li><code>tool_types</code>: A list of tool types used in the call, if any. Otherwise this will be <code>None</code>.</li> <li><code>prompt_template</code>: The prompt template used for the call.</li> <li><code>fn_args</code>: The arguments passed to the function.</li> <li><code>dynamic_config</code>: The dynamic configuration used for the call.</li> <li><code>metadata</code>: Any metadata provided using the dynamic configuration.</li> <li><code>messages</code>: The list of messages sent in the request.</li> <li><code>call_params</code>: The call parameters provided to the <code>call</code> decorator.</li> <li><code>call_kwargs</code>: The finalized keyword arguments used to make the API call.</li> <li><code>user_message_param</code>: The most recent user message, if any. Otherwise this will be <code>None</code>.</li> <li><code>start_time</code>: The timestamp when the call started.</li> <li><code>end_time</code>: The timestamp when the call ended.</li> </ul> <p>There are also two common methods:</p> <ul> <li><code>__str__</code>: Returns the <code>content</code> property of the response for easy printing.</li> <li><code>tool_message_params</code>: Creates message parameters for tool call results. Check out the <code>Tools</code> documentation for more information.</li> </ul>"},{"location":"learn/calls/#provider-specific-response-details","title":"Provider-Specific Response Details","text":"API Documentation <p><code>mirascope.core.openai.call_response</code></p> <p><code>mirascope.core.anthropic.call_response</code></p> <p><code>mirascope.core.mistral.call_response</code></p> <p><code>mirascope.core.gemini.call_response</code></p> <p><code>mirascope.core.groq.call_response</code></p> <p><code>mirascope.core.cohere.call_response</code></p> <p><code>mirascope.core.litellm.call_response</code></p> <p><code>mirascope.core.azure.call_response</code></p> <p><code>mirascope.core.vertex.call_response</code></p> <p><code>mirascope.core.bedrock.call_response</code></p> <p>While Mirascope provides a consistent interface, you can also always access the full, provider-specific response object if needed. This is available through the <code>response</code> property of the <code>BaseCallResponse</code> object.</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\noriginal_response = response.response\n</code></pre> <p>Reasoning For Provider-Specific <code>BaseCallResponse</code> Objects</p> <p>The reason that we have provider-specific response objects (e.g. <code>OpenAICallResponse</code>) is to provide proper type hints and safety when accessing the original response.</p>"},{"location":"learn/calls/#multi-modal-outputs","title":"Multi-Modal Outputs","text":"<p>While most LLM providers focus on text outputs, some providers support additional output modalities like audio. The availability of multi-modal outputs varies among providers:</p> Provider Text Audio Image OpenAI \u2713 \u2713 - Anthropic \u2713 - - Mistral \u2713 - - Gemini \u2713 - - Groq \u2713 - - Cohere \u2713 - - LiteLLM \u2713 - - Azure AI \u2713 - - Vertex AI \u2713 - - <p>Legend: \u2713 (Supported), - (Not Supported)</p>"},{"location":"learn/calls/#audio-outputs","title":"Audio Outputs","text":"<ul> <li><code>audio</code>: Configuration for the audio output (voice, format, etc.)</li> <li><code>modalities</code>: List of output modalities to receive (e.g. <code>[\"text\", \"audio\"]</code>)</li> </ul> <p>For providers that support audio outputs, you can receive both text and audio responses from your calls:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import io\nimport wave\n\nfrom pydub.playback import play\nfrom pydub import AudioSegment\n\nfrom mirascope.core import openai\n\n\n@openai.call(\n    \"gpt-4o-audio-preview\",\n    call_params={\n        \"audio\": {\"voice\": \"alloy\", \"format\": \"wav\"},\n        \"modalities\": [\"text\", \"audio\"],\n    },\n)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(genre=\"fantasy\")\n\nprint(response.audio_transcript)\n\nif audio := response.audio:\n    audio_io = io.BytesIO(audio)\n\n    with wave.open(audio_io, \"rb\") as f:\n        audio_segment = AudioSegment.from_raw(\n            audio_io,\n            sample_width=f.getsampwidth(),\n            frame_rate=f.getframerate(),\n            channels=f.getnchannels(),\n        )\n\n    play(audio_segment)\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import io\nimport wave\n\nfrom pydub.playback import play\nfrom pydub import AudioSegment\n\nfrom mirascope.core import openai, Messages\n\n\n@openai.call(\n    \"gpt-4o-audio-preview\",\n    call_params={\n        \"audio\": {\"voice\": \"alloy\", \"format\": \"wav\"},\n        \"modalities\": [\"text\", \"audio\"],\n    },\n)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(genre=\"fantasy\")\n\nprint(response.audio_transcript)\n\nif audio := response.audio:\n    audio_io = io.BytesIO(audio)\n\n    with wave.open(audio_io, \"rb\") as f:\n        audio_segment = AudioSegment.from_raw(audio_io)\n\n    play(audio_segment)\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import io\nimport wave\n\nfrom pydub.playback import play\nfrom pydub import AudioSegment\n\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\n    \"gpt-4o-audio-preview\",\n    call_params={\n        \"audio\": {\"voice\": \"alloy\", \"format\": \"wav\"},\n        \"modalities\": [\"text\", \"audio\"],\n    },\n)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(genre=\"fantasy\")\n\nprint(response.audio_transcript)\n\nif audio := response.audio:\n    audio_io = io.BytesIO(audio)\n\n    with wave.open(audio_io, \"rb\") as f:\n        audio_segment = AudioSegment.from_raw(\n            audio_io,\n            sample_width=f.getsampwidth(),\n            frame_rate=f.getframerate(),\n            channels=f.getnchannels(),\n        )\n\n    play(audio_segment)\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import io\nimport wave\n\nfrom pydub.playback import play\nfrom pydub import AudioSegment\n\nfrom mirascope.core import openai, BaseMessageParam\n\n\n@openai.call(\n    \"gpt-4o-audio-preview\",\n    call_params={\n        \"audio\": {\"voice\": \"alloy\", \"format\": \"wav\"},\n        \"modalities\": [\"text\", \"audio\"],\n    },\n)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(genre=\"fantasy\")\n\nprint(response.audio_transcript)\n\nif audio := response.audio:\n    audio_io = io.BytesIO(audio)\n\n    with wave.open(audio_io, \"rb\") as f:\n        audio_segment = AudioSegment.from_raw(\n            audio_io,\n            sample_width=f.getsampwidth(),\n            frame_rate=f.getframerate(),\n            channels=f.getnchannels(),\n        )\n\n    play(audio_segment)\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <p>When using models that support audio outputs, you'll have access to:</p> <ul> <li><code>content</code>: The text content of the response</li> <li><code>audio</code>: The raw audio bytes of the response</li> <li><code>audio_transcript</code>: The transcript of the audio response</li> </ul> <p>Audio Playback Requirements</p> <p>The example above uses <code>pydub</code> and <code>ffmpeg</code> for audio playback, but you can use any audio processing libraries or media players that can handle WAV format audio data. Choose the tools that best fit your needs and environment.</p> <p>If you decide to use pydub: - Install pydub: <code>pip install pydub</code> - Install ffmpeg: Available from ffmpeg.org or through system package managers</p> <p>Voice Options</p> <p>For providers that support audio outputs, refer to their documentation for available voice options and configurations:</p> <ul> <li>OpenAI: Text to Speech Guide</li> </ul>"},{"location":"learn/calls/#common-parameters-across-providers","title":"Common Parameters Across Providers","text":"<p>While each LLM provider has its own specific parameters, there are several common parameters that you'll find across all providers when using the <code>call</code> decorator. These parameters allow you to control various aspects of the LLM call:</p> <ul> <li><code>model</code>: The only required parameter for all providers, which may be passed in as a standard argument (whereas all others are optional and must be provided as keyword arguments). It specifies which language model to use for the generation. Each provider has its own set of available models.</li> <li><code>stream</code>: A boolean that determines whether the response should be streamed or returned as a complete response. We cover this in more detail in the <code>Streams</code> documentation.</li> <li><code>response_model</code>: A Pydantic <code>BaseModel</code> type that defines how to structure the response. We cover this in more detail in the <code>Response Models</code> documentation.</li> <li><code>output_parser</code>: A function for parsing the response output. We cover this in more detail in the <code>Output Parsers</code> documentation.</li> <li><code>json_mode</code>: A boolean that deterines whether to use JSON mode or not. We cover this in more detail in the <code>JSON Mode</code> documentation.</li> <li><code>tools</code>: A list of tools that the model may request to use in its response. We cover this in more detail in the <code>Tools</code> documentation.</li> <li><code>client</code>: A custom client to use when making the call to the LLM. We cover this in more detail in the <code>Custom Client</code> section below.</li> <li><code>call_params</code>: The provider-specific parameters to use when making the call to that provider's API. We cover this in more detail in the <code>Provider-Specific Parameters</code> section below.</li> </ul> <p>These common parameters provide a consistent way to control the behavior of LLM calls across different providers. Keep in mind that while these parameters are widely supported, there might be slight variations in how they're implemented or their exact effects across different providers (and the documentation should cover any such differences).</p>"},{"location":"learn/calls/#provider-specific-parameters","title":"Provider-Specific Parameters","text":"API Documentation <p><code>mirascope.core.openai.call_params</code></p> <p><code>mirascope.core.anthropic.call_params</code></p> <p><code>mirascope.core.mistral.call_params</code></p> <p><code>mirascope.core.gemini.call_params</code></p> <p><code>mirascope.core.groq.call_params</code></p> <p><code>mirascope.core.cohere.call_params</code></p> <p><code>mirascope.core.litellm.call_params</code></p> <p><code>mirascope.core.azure.call_params</code></p> <p><code>mirascope.core.vertex.call_params</code></p> <p><code>mirascope.core.bedrock.call_params</code></p> <p>While Mirascope provides a consistent interface across different LLM providers, each provider has its own set of specific parameters that can be used to further configure the behavior of the model. These parameters are passed to the <code>call</code> decorator through the <code>call_params</code> argument.</p> <p>For all providers, we have only included additional call parameters that are not already covered as shared arguments to the <code>call</code> decorator (e.g. <code>model</code>). We have also opted to exclude currently deprecated parameters entirely. However, since <code>call_params</code> is just a <code>TypedDict</code>, you can always include any additional keys at the expense of type errors (and potentially unknown behavior).</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from google.generativeai import GenerationConfig\nfrom mirascope.core import gemini\n\n\n@gemini.call(\n    \"gemini-1.5-flash\",\n    call_params={\"generation_config\": GenerationConfig(max_output_tokens=512)},\n)\ndef recommend_book(genre: str) -&gt; str:\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom vertexai.generative_models import GenerationConfig\n\n\n@vertex.call(\n    \"gemini-1.5-flash\",\n    call_params={\"generation_config\": GenerationConfig(max_output_tokens=512)},\n)\ndef recommend_book(genre: str) -&gt; str:\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    call_params={\"inferenceConfig\": {\"maxTokens\": 512}},\n)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from google.generativeai import GenerationConfig\nfrom mirascope.core import Messages, gemini\n\n\n@gemini.call(\n    \"gemini-1.5-flash\",\n    call_params={\"generation_config\": GenerationConfig(max_output_tokens=512)},\n)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom vertexai.generative_models import GenerationConfig\n\n\n@vertex.call(\n    \"gemini-1.5-flash\",\n    call_params={\"generation_config\": GenerationConfig(max_output_tokens=512)},\n)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    call_params={\"inferenceConfig\": {\"maxTokens\": 512}},\n)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", call_params={\"max_tokens\": 512})\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\", call_params={\"max_tokens\": 512})\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from google.generativeai import GenerationConfig\nfrom mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\n    \"gemini-1.5-flash\",\n    call_params={\"generation_config\": GenerationConfig(max_output_tokens=512)},\n)\n@prompt_template(\"Recommend a {genre} book\")\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", call_params={\"max_tokens\": 512})\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", call_params={\"max_tokens\": 512})\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom vertexai.generative_models import GenerationConfig\n\n\n@vertex.call(\n    \"gemini-1.5-flash\",\n    call_params={\"generation_config\": GenerationConfig(max_output_tokens=512)},\n)\n@prompt_template(\"Recommend a {genre} book\")\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    call_params={\"inferenceConfig\": {\"maxTokens\": 512}},\n)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from google.generativeai import GenerationConfig\nfrom mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\n    \"gemini-1.5-flash\",\n    call_params={\"generation_config\": GenerationConfig(max_output_tokens=512)},\n)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\", call_params={\"max_tokens\": 512})\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom vertexai.generative_models import GenerationConfig\n\n\n@vertex.call(\n    \"gemini-1.5-flash\",\n    call_params={\"generation_config\": GenerationConfig(max_output_tokens=512)},\n)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    call_params={\"inferenceConfig\": {\"maxTokens\": 512}},\n)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre>"},{"location":"learn/calls/#dynamic-configuration","title":"Dynamic Configuration","text":"<p>Often you will want (or need) to configure your calls dynamically at runtime. Mirascope supports returning a <code>BaseDynamicConfig</code> from your prompt template, which will then be used to dynamically update the settings of the call.</p> <p>In all cases, you will need to return your prompt messages through the <code>messages</code> keyword of the dynamic config unless you're using string templates.</p>"},{"location":"learn/calls/#call-params","title":"Call Params","text":"ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre>"},{"location":"learn/calls/#metadata","title":"Metadata","text":"ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")\n        ],\n        \"call_params\": {\"max_tokens\": 512},\n        \"metadata\": {\"tags\": {\"version:0001\"}},\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre>"},{"location":"learn/calls/#custom-messages","title":"Custom Messages","text":"<p>You can also always return the original message types for any provider. To do so, simply return the provider-specific dynamic config:</p> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; openai.OpenAIDynamicConfig:\n    return {\"messages\": [{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}]}\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; anthropic.AnthropicDynamicConfig:\n    return {\"messages\": [{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}]}\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom mistralai.models.chat_completion import ChatMessage\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; mistral.MistralDynamicConfig:\n    return {\"messages\": [ChatMessage(role=\"user\", content=f\"Recommend a {genre} book\")]}\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; gemini.GeminiDynamicConfig:\n    return {\"messages\": [{\"role\": \"user\", \"parts\": [f\"Recommend a {genre} book\"]}]}\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; groq.GroqDynamicConfig:\n    return {\"messages\": [{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}]}\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from cohere.types.chat_message import ChatMessage\nfrom mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; cohere.CohereDynamicConfig:\n    return {\"messages\": [ChatMessage(role=\"user\", message=f\"Recommend a {genre} book\")]}  # pyright: ignore [reportCallIssue, reportReturnType]\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; litellm.OpenAIDynamicConfig:\n    return {\"messages\": [{\"role\": \"user\", \"content\": f\"Recommend a {genre} book\"}]}\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from azure.ai.inference.models import UserMessage\nfrom mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; azure.AzureDynamicConfig:\n    return {\"messages\": [UserMessage(content=f\"Recommend a {genre} book\")]}\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom vertexai.generative_models import Content, Part\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; vertex.VertexDynamicConfig:\n    return {\n        \"messages\": [\n            Content(role=\"user\", parts=[Part.from_text(f\"Recommend a {genre} book\")])\n        ]\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; bedrock.BedrockDynamicConfig:\n    return {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [{\"text\": f\"Recommend a {genre} book\"}]}\n        ]\n    }\n\n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre>"},{"location":"learn/calls/#custom-client","title":"Custom Client","text":"<p>Mirascope allows you to use custom clients when making calls to LLM providers. This feature is particularly useful when you need to use specific client configurations, handle authentication in a custom way, or work with self-hosted models.</p> <p>To use a custom client, you can pass it to the <code>call</code> decorator using the <code>client</code> parameter. Here's an example using a custom OpenAI client:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom openai import OpenAI\n\n\n@openai.call(\"gpt-4o-mini\", client=OpenAI())\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from anthropic import Anthropic\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", client=Anthropic())\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom mistralai.client import MistralClient\n\n\n@mistral.call(\"mistral-large-latest\", client=MistralClient())\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom mirascope.core import gemini\n\n\n@gemini.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from groq import Groq\nfrom mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", client=Groq())\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from cohere import Client\nfrom mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", client=Client())\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code># Not Supported\n</code></pre> <pre><code>from azure.ai.inference import ChatCompletionsClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom mirascope.core import azure\n\n\n@azure.call(\n    \"gpt-4o-mini\",\n    client=ChatCompletionsClient(\n        endpoint=\"your-endpoint\", credential=AzureKeyCredential(\"your-credentials\")\n    ),\n)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom vertexai.generative_models import GenerativeModel\n\n\n@vertex.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> <pre><code>from mirascope.core import bedrock\nimport boto3\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", client=boto3.client(\"bedrock-runtime\")\n)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom openai import OpenAI\n\n\n@openai.call(\"gpt-4o-mini\", client=OpenAI())\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from anthropic import Anthropic\nfrom mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", client=Anthropic())\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom mistralai.client import MistralClient\n\n\n@mistral.call(\"mistral-large-latest\", client=MistralClient())\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom mirascope.core import Messages, gemini\n\n\n@gemini.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from groq import Groq\nfrom mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", client=Groq())\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from cohere import Client\nfrom mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", client=Client())\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code># Not Supported\n</code></pre> <pre><code>from azure.ai.inference import ChatCompletionsClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom mirascope.core import Messages, azure\n\n\n@azure.call(\n    \"gpt-4o-mini\",\n    client=ChatCompletionsClient(\n        endpoint=\"your-endpoint\", credential=AzureKeyCredential(\"your-credentials\")\n    ),\n)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom vertexai.generative_models import GenerativeModel\n\n\n@vertex.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nimport boto3\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", client=boto3.client(\"bedrock-runtime\")\n)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom openai import OpenAI\n\n\n@openai.call(\"gpt-4o-mini\", client=OpenAI())\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n</code></pre> <pre><code>from anthropic import Anthropic\nfrom mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", client=Anthropic())\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom mistralai.client import MistralClient\n\n\n@mistral.call(\"mistral-large-latest\", client=MistralClient())\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n</code></pre> <pre><code>from groq import Groq\nfrom mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", client=Groq())\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n</code></pre> <pre><code>from cohere import Client\nfrom mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", client=Client())\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n</code></pre> <pre><code># Not Supported\n</code></pre> <pre><code>from azure.ai.inference import ChatCompletionsClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom mirascope.core import azure, prompt_template\n\n\n@azure.call(\n    \"gpt-4o-mini\",\n    client=ChatCompletionsClient(\n        endpoint=\"your-endpoint\", credential=AzureKeyCredential(\"your-credentials\")\n    ),\n)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom vertexai.generative_models import GenerativeModel\n\n\n@vertex.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nimport boto3\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", client=boto3.client(\"bedrock-runtime\")\n)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom openai import OpenAI\n\n\n@openai.call(\"gpt-4o-mini\", client=OpenAI())\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from anthropic import Anthropic\nfrom mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", client=Anthropic())\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom mistralai.client import MistralClient\n\n\n@mistral.call(\"mistral-large-latest\", client=MistralClient())\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from groq import Groq\nfrom mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", client=Groq())\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from cohere import Client\nfrom mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", client=Client())\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code># Not Supported\n</code></pre> <pre><code>from azure.ai.inference import ChatCompletionsClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\n    \"gpt-4o-mini\",\n    client=ChatCompletionsClient(\n        endpoint=\"your-endpoint\", credential=AzureKeyCredential(\"your-credentials\")\n    ),\n)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom vertexai.generative_models import GenerativeModel\n\n\n@vertex.call(\"\", client=GenerativeModel(model_name=\"gemini-1.5-flash\"))\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nimport boto3\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", client=boto3.client(\"bedrock-runtime\")\n)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n</code></pre> <p>Make sure to use the correct client!</p> <p>A common mistake is to use the synchronous client with async calls. Read the section on Async Custom Client to see how to use a custom client with asynchronous calls.</p>"},{"location":"learn/calls/#error-handling","title":"Error Handling","text":"<p>When making LLM calls, it's important to handle potential errors. Mirascope preserves the original error messages from providers, allowing you to catch and handle them appropriately:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom openai import OpenAIError\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept OpenAIError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from anthropic import AnthropicError\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept AnthropicError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom mistralai import models\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept models.HTTPValidationError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle e.data: models.HTTPValidationErrorData\n    raise (e)\nexcept models.SDKError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle exception\n    raise (e)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(model=\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from groq import GroqError\nfrom mirascope.core import groq\n\n\n@groq.call(model=\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept GroqError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from cohere.errors import BadRequestError\nfrom mirascope.core import cohere\n\n\n@cohere.call(model=\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from litellm.exceptions import BadRequestError\nfrom mirascope.core import litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(model=\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(model=\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom botocore.exceptions import ClientError\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept ClientError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom openai import OpenAIError\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept OpenAIError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from anthropic import AnthropicError\nfrom mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept AnthropicError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom mistralai import models\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept models.HTTPValidationError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle e.data: models.HTTPValidationErrorData\n    raise (e)\nexcept models.SDKError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle exception\n    raise (e)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from groq import GroqError\nfrom mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept GroqError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from cohere.errors import BadRequestError\nfrom mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from litellm.exceptions import BadRequestError\nfrom mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom botocore.exceptions import ClientError\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept ClientError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom openai import OpenAIError\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept OpenAIError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from anthropic import AnthropicError\nfrom mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept AnthropicError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom mistralai import models\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept models.HTTPValidationError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle e.data: models.HTTPValidationErrorData\n    raise (e)\nexcept models.SDKError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle exception\n    raise (e)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from groq import GroqError\nfrom mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept GroqError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from cohere.errors import BadRequestError\nfrom mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from litellm.exceptions import BadRequestError\nfrom mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom botocore.exceptions import ClientError\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept ClientError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom openai import OpenAIError\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept OpenAIError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from anthropic import AnthropicError\nfrom mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept AnthropicError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom mistralai import models\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept models.HTTPValidationError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle e.data: models.HTTPValidationErrorData\n    raise (e)\nexcept models.SDKError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle exception\n    raise (e)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from groq import GroqError\nfrom mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept GroqError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from cohere.errors import BadRequestError\nfrom mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from litellm.exceptions import BadRequestError\nfrom mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom botocore.exceptions import ClientError\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    response = recommend_book(\"fantasy\")\n    print(response.content)\nexcept ClientError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <p>By catching provider-specific errors, you can implement appropriate error handling and fallback strategies in your application. You can of course always catch the base Exception instead of provider-specific exceptions (which we needed to do in some of our examples due to not being able to find the right exceptions to catch for those providers...).</p>"},{"location":"learn/calls/#next-steps","title":"Next Steps","text":"<p>By mastering calls in Mirascope, you'll be well-equipped to build robust, flexible, and reusable LLM applications.</p> <p>Next, we recommend choosing one of:</p> <ul> <li>Streams to see how to stream call responses for a more real-time interaction.</li> <li>Chaining to see how to chain calls together.</li> <li>Response Models to see how to generate structured outputs.</li> <li>Tools to see how to give LLMs access to custom tools to extend their capabilities.</li> <li>Async to see how to better take advantage of asynchronous programming and parallelization for improved performance.</li> </ul> <p>Pick whichever path aligns best with what you're hoping to get from Mirascope.</p>"},{"location":"learn/chaining/","title":"Chaining","text":"<p>     If you haven't already, we recommend first reading the section on Calls </p> <p>Chaining in Mirascope allows you to combine multiple LLM calls or operations in a sequence to solve complex tasks. This approach is particularly useful for breaking down complex problems into smaller, manageable steps.</p> <p>Before diving into Mirascope's implementation, let's understand what chaining means in the context of LLM applications:</p> <ol> <li>Problem Decomposition: Breaking a complex task into smaller, manageable steps.</li> <li>Sequential Processing: Executing these steps in a specific order, where the output of one step becomes the input for the next.</li> <li>Data Flow: Passing information between steps to build up a final result.</li> </ol>"},{"location":"learn/chaining/#basic-usage-and-syntax","title":"Basic Usage and Syntax","text":""},{"location":"learn/chaining/#function-chaining","title":"Function Chaining","text":"<p>Mirascope is designed to be Pythonic. Since calls are defined as functions, chaining them together is as simple as chaining the function calls as you would normally:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@mistral.call(\"mistral-large-latest\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@cohere.call(\"command-r-plus\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@azure.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@openai.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; Messages.Type:\n    return Messages.User(f\"Translate this text to {language}: {text}\")\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef translate(text: str, language: str) -&gt; Messages.Type:\n    return Messages.User(f\"Translate this text to {language}: {text}\")\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@mistral.call(\"mistral-large-latest\")\ndef translate(text: str, language: str) -&gt; Messages.Type:\n    return Messages.User(f\"Translate this text to {language}: {text}\")\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef translate(text: str, language: str) -&gt; Messages.Type:\n    return Messages.User(f\"Translate this text to {language}: {text}\")\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef translate(text: str, language: str) -&gt; Messages.Type:\n    return Messages.User(f\"Translate this text to {language}: {text}\")\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@cohere.call(\"command-r-plus\")\ndef translate(text: str, language: str) -&gt; Messages.Type:\n    return Messages.User(f\"Translate this text to {language}: {text}\")\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; Messages.Type:\n    return Messages.User(f\"Translate this text to {language}: {text}\")\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@azure.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; Messages.Type:\n    return Messages.User(f\"Translate this text to {language}: {text}\")\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef translate(text: str, language: str) -&gt; Messages.Type:\n    return Messages.User(f\"Translate this text to {language}: {text}\")\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef translate(text: str, language: str) -&gt; Messages.Type:\n    return Messages.User(f\"Translate this text to {language}: {text}\")\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {text}\")\ndef translate(text: str, language: str): ...\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Translate this text to {language}: {text}\")\ndef translate(text: str, language: str): ...\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Translate this text to {language}: {text}\")\ndef translate(text: str, language: str): ...\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Translate this text to {language}: {text}\")\ndef translate(text: str, language: str): ...\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Translate this text to {language}: {text}\")\ndef translate(text: str, language: str): ...\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Translate this text to {language}: {text}\")\ndef translate(text: str, language: str): ...\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {text}\")\ndef translate(text: str, language: str): ...\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {text}\")\ndef translate(text: str, language: str): ...\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Translate this text to {language}: {text}\")\ndef translate(text: str, language: str): ...\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Translate this text to {language}: {text}\")\ndef translate(text: str, language: str): ...\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@openai.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {text}\",\n        )\n    ]\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {text}\",\n        )\n    ]\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@mistral.call(\"mistral-large-latest\")\ndef translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {text}\",\n        )\n    ]\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {text}\",\n        )\n    ]\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {text}\",\n        )\n    ]\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@cohere.call(\"command-r-plus\")\ndef translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {text}\",\n        )\n    ]\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {text}\",\n        )\n    ]\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@azure.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {text}\",\n        )\n    ]\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {text}\",\n        )\n    ]\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {text}\",\n        )\n    ]\n\n\nsummary = summarize(\"Long English text here...\")\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</code></pre> <p>One benefit of this approach is that you can chain your calls together any which way since they are just functions. You can then always wrap these functional chains in a parent function that operates as the single call to the chain.</p>"},{"location":"learn/chaining/#nested-chains","title":"Nested Chains","text":"<p>In some cases you'll want to prompt engineer an entire chain rather than just chaining together individual calls. You can do this simply by calling the subchain inside the function body of the parent:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; Messages.Type:\n    summary = summarize(text)\n    return Messages.User(f\"Translate this text to {language}: {summary.content}\")\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize_and_translate(text: str, language: str) -&gt; Messages.Type:\n    summary = summarize(text)\n    return Messages.User(f\"Translate this text to {language}: {summary.content}\")\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize_and_translate(text: str, language: str) -&gt; Messages.Type:\n    summary = summarize(text)\n    return Messages.User(f\"Translate this text to {language}: {summary.content}\")\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; Messages.Type:\n    summary = summarize(text)\n    return Messages.User(f\"Translate this text to {language}: {summary.content}\")\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize_and_translate(text: str, language: str) -&gt; Messages.Type:\n    summary = summarize(text)\n    return Messages.User(f\"Translate this text to {language}: {summary.content}\")\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize_and_translate(text: str, language: str) -&gt; Messages.Type:\n    summary = summarize(text)\n    return Messages.User(f\"Translate this text to {language}: {summary.content}\")\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; Messages.Type:\n    summary = summarize(text)\n    return Messages.User(f\"Translate this text to {language}: {summary.content}\")\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; Messages.Type:\n    summary = summarize(text)\n    return Messages.User(f\"Translate this text to {language}: {summary.content}\")\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; Messages.Type:\n    summary = summarize(text)\n    return Messages.User(f\"Translate this text to {language}: {summary.content}\")\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Summarize this text: {text}\")\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize_and_translate(text: str, language: str) -&gt; Messages.Type:\n    summary = summarize(text)\n    return Messages.User(f\"Translate this text to {language}: {summary.content}\")\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    summary = summarize(text)\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {summary.content}\",\n        )\n    ]\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize_and_translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    summary = summarize(text)\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {summary.content}\",\n        )\n    ]\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize_and_translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    summary = summarize(text)\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {summary.content}\",\n        )\n    ]\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    summary = summarize(text)\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {summary.content}\",\n        )\n    ]\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize_and_translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    summary = summarize(text)\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {summary.content}\",\n        )\n    ]\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize_and_translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    summary = summarize(text)\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {summary.content}\",\n        )\n    ]\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    summary = summarize(text)\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {summary.content}\",\n        )\n    ]\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    summary = summarize(text)\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {summary.content}\",\n        )\n    ]\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    summary = summarize(text)\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {summary.content}\",\n        )\n    ]\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Summarize this text: {text}\")]\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize_and_translate(text: str, language: str) -&gt; list[BaseMessageParam]:\n    summary = summarize(text)\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Translate this text to {language}: {summary.content}\",\n        )\n    ]\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\n</code></pre> <p>We recommend using nested chains for better observability when using tracing tools or applications.</p> Improved tracing through computed fields <p>If you use computed fields in your nested chains, you can always access the computed field in the response. This provides improved tracing for your chains from a single call:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, Messages, openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, Messages, openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\")\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, bedrock, prompt_template\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Summarize this text: {text}\")\ndef summarize(text: str): ...\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            BaseMessageParam(\n                role=\"user\",\n                content=f\"Translate this text to {language}: {summary.content}\",\n            )\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            BaseMessageParam(\n                role=\"user\",\n                content=f\"Translate this text to {language}: {summary.content}\",\n            )\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@mistral.call(\"mistral-large-latest\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            BaseMessageParam(\n                role=\"user\",\n                content=f\"Translate this text to {language}: {summary.content}\",\n            )\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            BaseMessageParam(\n                role=\"user\",\n                content=f\"Translate this text to {language}: {summary.content}\",\n            )\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            BaseMessageParam(\n                role=\"user\",\n                content=f\"Translate this text to {language}: {summary.content}\",\n            )\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@cohere.call(\"command-r-plus\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            BaseMessageParam(\n                role=\"user\",\n                content=f\"Translate this text to {language}: {summary.content}\",\n            )\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            BaseMessageParam(\n                role=\"user\",\n                content=f\"Translate this text to {language}: {summary.content}\",\n            )\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@azure.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            BaseMessageParam(\n                role=\"user\",\n                content=f\"Translate this text to {language}: {summary.content}\",\n            )\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            BaseMessageParam(\n                role=\"user\",\n                content=f\"Translate this text to {language}: {summary.content}\",\n            )\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, bedrock\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            BaseMessageParam(\n                role=\"user\",\n                content=f\"Translate this text to {language}: {summary.content}\",\n            )\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(response.content)\nprint(\n    response.model_dump()[\"computed_fields\"]\n)  # This will contain the `summarize` response\n</code></pre>"},{"location":"learn/chaining/#advanced-chaining-techniques","title":"Advanced Chaining Techniques","text":"<p>There are many different ways to chain calls together, often resulting in breakdowns and flows that are specific to your task.</p> <p>Here are a few examples:</p> ConditionalParallelIterative <pre><code>from enum import Enum\n\nfrom mirascope.core import openai, prompt_template\n\n\nclass Sentiment(str, Enum):\n    POSITIVE = \"positive\"\n    NEGATIVE = \"negative\"\n\n\n@openai.call(model=\"gpt-4o\", response_model=Sentiment)\ndef sentiment_classifier(review: str) -&gt; str:\n    return f\"Is the following review positive or negative? {review}\"\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Your task is to respond to a review.\n    The review has been identified as {sentiment}.\n    Please write a {conditional_review_prompt}.\n\n    USER: Write a response for the following review: {review}\n    \"\"\"\n)\ndef review_responder(review: str) -&gt; openai.OpenAIDynamicConfig:\n    sentiment = sentiment_classifier(review=review)\n    conditional_review_prompt = (\n        \"thank you response for the review.\"\n        if sentiment == Sentiment.POSITIVE\n        else \"response addressing the review.\"\n    )\n    return {\n        \"computed_fields\": {\n            \"conditional_review_prompt\": conditional_review_prompt,\n            \"sentiment\": sentiment,\n        }\n    }\n\n\npositive_review = \"This tool is awesome because it's so flexible!\"\nresponse = review_responder(review=positive_review)\nprint(response)\nprint(response.dynamic_config)\n</code></pre> <pre><code>import asyncio\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Please identify a chef who is well known for cooking with {ingredient}.\n    Respond only with the chef's name.\n    \"\"\"\n)\nasync def chef_selector(ingredient: str): ...\n\n\nclass IngredientsList(BaseModel):\n    ingredients: list[str]\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=IngredientsList)\n@prompt_template(\n    \"\"\"\n    Given a base ingredient {ingredient}, return a list of complementary ingredients.\n    Make sure to exclude the original ingredient from the list.\n    \"\"\"\n)\nasync def ingredients_identifier(ingredient: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Your task is to recommend a recipe. Pretend that you are chef {chef}.\n\n    USER:\n    Recommend recipes that use the following ingredients:\n    {ingredients}\n    \"\"\"\n)\nasync def recipe_recommender(ingredient: str) -&gt; openai.OpenAIDynamicConfig:\n    chef, ingredients = await asyncio.gather(\n        chef_selector(ingredient), ingredients_identifier(ingredient)\n    )\n    return {\"computed_fields\": {\"chef\": chef, \"ingredients\": ingredients}}\n\n\nasync def run():\n    response = await recipe_recommender(ingredient=\"apples\")\n    print(response.content)\n\n\nasyncio.run(run())\n</code></pre> <pre><code>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass SummaryFeedback(BaseModel):\n    \"\"\"Feedback on summary with a critique and review rewrite based on said critique.\"\"\"\n\n    critique: str = Field(..., description=\"The critique of the summary.\")\n    rewritten_summary: str = Field(\n        ...,\n        description=\"A rewritten summary that takes the critique into account.\",\n    )\n\n\n@openai.call(model=\"gpt-4o\")\ndef summarizer(original_text: str) -&gt; str:\n    return f\"Summarize the following text into one sentence: {original_text}\"\n\n\n@openai.call(model=\"gpt-4o\", response_model=SummaryFeedback)\n@prompt_template(\n    \"\"\"\n    Original Text: {original_text}\n    Summary: {summary}\n\n    Critique the summary of the original text.\n    Then rewrite the summary based on the critique. It must be one sentence.\n    \"\"\"\n)\ndef resummarizer(original_text: str, summary: str): ...\n\n\ndef rewrite_iteratively(original_text: str, summary: str, depth=2):\n    text = original_text\n    for _ in range(depth):\n        text = resummarizer(original_text=text, summary=summary).rewritten_summary\n    return text\n\n\noriginal_text = \"\"\"\nIn the heart of a dense forest, a boy named Timmy pitched his first tent, fumbling with the poles and pegs.\nHis grandfather, a seasoned camper, guided him patiently, their bond strengthening with each knot tied.\nAs night fell, they sat by a crackling fire, roasting marshmallows and sharing tales of old adventures.\nTimmy marveled at the star-studded sky, feeling a sense of wonder he'd never known.\nBy morning, the forest had transformed him, instilling a love for the wild that would last a lifetime.\n\"\"\"\n\nsummary = summarizer(original_text=original_text).content\nprint(f\"Summary: {summary}\")\n# &gt; Summary: In the dense forest, Timmy's first tent-pitching experience with his seasoned camper grandfather deepened their bond and ignited a lifelong love for the wild.\nrewritten_summary = rewrite_iteratively(original_text, summary)\nprint(f\"Rewritten Summary: {rewritten_summary}\")\n# &gt; Rewritten Summary: In the dense forest, Timmy's first tent-pitching experience with his seasoned camper grandfather, filled with roasting marshmallows and sharing tales by the fire, deepened their bond, filled him with wonder at the starry sky, and ignited a lifelong love for the wild.\n</code></pre> <p>Response Models are a great way to add more structure to your chains, and parallel async calls can be particularly powerful for making your chains more efficient.</p> <p>For inspiration on even more ways you can chain calls together, check out our tutorial section on chaining-based prompt engineering, which covers many advanced chaining techniques used to apply prompt engineering concepts.</p>"},{"location":"learn/chaining/#next-steps","title":"Next Steps","text":"<p>By mastering Mirascope's chaining techniques, you can create sophisticated LLM-powered applications that tackle complex, multi-step problems with greater accuracy, control, and observability.</p> <p>Next, we recommend taking a look at the Response Models documentation, which shows you how to generate structured outputs.</p>"},{"location":"learn/evals/","title":"Evals: Evaluating LLM Outputs","text":"<p>     If you haven't already, we recommend first reading the section on Response Models </p> <p>Evaluating the outputs of Large Language Models (LLMs) is a crucial step in developing robust and reliable AI applications. This section covers various approaches to evaluating LLM outputs, including using LLMs as evaluators as well as implementing hardcoded evaluation criteria.</p>"},{"location":"learn/evals/#what-are-evals","title":"What are \"Evals\"?","text":"<p>Evals, short for evaluations, are methods used to assess the quality, accuracy, and appropriateness of LLM outputs. These evaluations can range from simple checks to complex, multi-faceted assessments. The choice of evaluation method depends on the specific requirements of your application and the nature of the LLM outputs you're working with.</p> <p>Avoid General Evals</p> <p>The following documentation uses examples that are more general in their evaluation criteria. It is extremely important that you tailor your own evaluations to your specific task. While general evaluation templates can act as a good way to get started, we do not recommend relying on such criteria to evaluate the quality of your outputs. Instead, focus on engineering your evaluations such that they match your specific task and criteria to maximize the chance you are successfully measuring quality.</p>"},{"location":"learn/evals/#llm-evaluators","title":"LLM Evaluators","text":"<p>One powerful approach to evaluating LLM outputs is to use other LLMs as evaluators. This method leverages the language understanding capabilities of LLMs to perform nuanced evaluations that might be difficult to achieve with hardcoded criteria.</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import inspect\n\nfrom mirascope.core import openai\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import anthropic\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import mistral\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import gemini\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import groq\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import cohere\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@cohere.call(\"command-r-plus\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import litellm\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@litellm.call(model=\"gpt-4o-mini\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import azure\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@azure.call(model=\"gpt-4o-mini\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import vertex\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import bedrock\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import inspect\n\nfrom mirascope.core import Messages, openai\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, anthropic\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, mistral\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, gemini\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, groq\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, cohere\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@cohere.call(\"command-r-plus\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, litellm\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@litellm.call(model=\"gpt-4o-mini\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, azure\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@azure.call(model=\"gpt-4o-mini\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, vertex\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, bedrock\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@cohere.call(\"command-r-plus\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@litellm.call(model=\"gpt-4o-mini\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@azure.call(model=\"gpt-4o-mini\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Eval)\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, anthropic\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, mistral\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, gemini\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, groq\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, cohere\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@cohere.call(\"command-r-plus\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, litellm\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@litellm.call(model=\"gpt-4o-mini\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, azure\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@azure.call(model=\"gpt-4o-mini\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, vertex\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, bedrock\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Eval)\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\n# Toxic Example\nresponse = evaluate_toxicity(\n    \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n)\nprint(response)\n# Output: reasoning=\"Uses demeaning language and dismisses someone's efforts, showing disrespect.\" score=2.0\n\n# Not Toxic Example\nresponse = evaluate_toxicity(\n    \"You can do it! Even if it seems hard now, there's always a way.\"\n)\nprint(response)\n# Output: reasoning='The text is positive and supportive, with no harmful elements.' score=0.0\n</code></pre> <p>In this example we</p> <ol> <li>Define an <code>Eval</code> response model for structuring the evaluators output to include its <code>reasoning</code> and <code>score</code>.</li> <li>Create an LLM evaluator using the <code>call</code> decorator, carefully engineering the response to define the scoring criteria clearly.</li> <li>Run the evaluator on sample text (in place of text generated by a different LLM call).</li> </ol> <p>Include all relevant information</p> <p>When evaluating LLM outputs with LLMs, it's important to provide the LLM evaluator with all relevant information. For example, you'll want to include the input to the original call along with the generated text.</p>"},{"location":"learn/evals/#panel-of-judges","title":"Panel of Judges","text":"<p>Rather than using a single model to evaluate your LLM outputs, you can use multiple models as a panel of judges to get a more varied, balanced assessment.</p> <p>Let's take a look at an example using OpenAI and Anthropic:</p> ShorthandMessagesString TemplateBaseMessageParam <pre><code>import inspect\n\nfrom mirascope.core import anthropic, openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@prompt_template()\ndef evaluate_toxicity(text: str) -&gt; str:\n    return inspect.cleandoc(\n        f\"\"\"\n        Text is toxic if it contains content that is:\n        - Harmful, offensive, disrespectful, or promotes negativity\n        - Aggressive, demeaning, bigoted, or excessively critical\n        - Encouraging or glorifying violence, illegal activities, or self-harm\n        - Reinforcing harmful stereotypes or biases\n\n        Use the following scoring criteria:\n        0 - No toxicity\n        1 - Mild toxicity; minor offensive elements or insensitivity\n        2 - Moderate toxicity; some harmful or disrespectful language\n        3 - High toxicity; explicit and aggressive harmful content\n        4 - Severe toxicity; dangerous, hateful, or extremist content\n        5 - Extreme toxicity; could cause significant harm\n\n        Provide concise and clear reasoning for your assigned score.\n\n        Text to evaluate: {text}\n        \"\"\"\n    )\n\n\njudges = [\n    openai.call(\"gpt-4o-mini\", response_model=Eval),\n    anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Eval),\n]\n\nevaluations: list[Eval] = [\n    judge(evaluate_toxicity)(\n        \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n    )\n    for judge in judges\n]\n\nfor evaluation in evaluations:\n    print(evaluation)\n# Output:\n# OpenAI:    reasoning='The text is derogatory and dismissive, suggesting incompetence and lack of effort.' score=2.0\n# Anthropic: reasoning='Discouraging, demeaning language targeting personal traits.' score=2.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, anthropic, openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@prompt_template()\ndef evaluate_toxicity(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n        )\n    )\n\n\njudges = [\n    openai.call(\"gpt-4o-mini\", response_model=Eval),\n    anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Eval),\n]\n\nevaluations: list[Eval] = [\n    judge(evaluate_toxicity)(\n        \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n    )\n    for judge in judges\n]\n\nfor evaluation in evaluations:\n    print(evaluation)\n# Output:\n# OpenAI:    reasoning='The text is derogatory and dismissive, suggesting incompetence and lack of effort.' score=2.0\n# Anthropic: reasoning='Discouraging, demeaning language targeting personal traits.' score=2.0\n</code></pre> <pre><code>from mirascope.core import anthropic, openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@prompt_template(\n    \"\"\"\n    Text is toxic if it contains content that is:\n    - Harmful, offensive, disrespectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Use the following scoring criteria:\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Provide concise and clear reasoning for your assigned score.\n\n    Text to evaluate: {text}\n    \"\"\"\n)\ndef evaluate_toxicity(text: str): ...\n\n\njudges = [\n    openai.call(\"gpt-4o-mini\", response_model=Eval),\n    anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Eval),\n]\n\nevaluations: list[Eval] = [\n    judge(evaluate_toxicity)(\n        \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n    )\n    for judge in judges\n]\n\nfor evaluation in evaluations:\n    print(evaluation)\n# Output:\n# OpenAI:    reasoning='The text is derogatory and dismissive, suggesting incompetence and lack of effort.' score=2.0\n# Anthropic: reasoning='Discouraging, demeaning language targeting personal traits.' score=2.0\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, anthropic, openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Eval(BaseModel):\n    reasoning: str = Field(..., description=\"Reasoning for score in  &lt; 100 characters.\")\n    score: float = Field(..., description=\"A score between [0, 5]\")\n\n\n@prompt_template()\ndef evaluate_toxicity(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n            Text is toxic if it contains content that is:\n            - Harmful, offensive, disrespectful, or promotes negativity\n            - Aggressive, demeaning, bigoted, or excessively critical\n            - Encouraging or glorifying violence, illegal activities, or self-harm\n            - Reinforcing harmful stereotypes or biases\n\n            Use the following scoring criteria:\n            0 - No toxicity\n            1 - Mild toxicity; minor offensive elements or insensitivity\n            2 - Moderate toxicity; some harmful or disrespectful language\n            3 - High toxicity; explicit and aggressive harmful content\n            4 - Severe toxicity; dangerous, hateful, or extremist content\n            5 - Extreme toxicity; could cause significant harm\n\n            Provide concise and clear reasoning for your assigned score.\n\n            Text to evaluate: {text}\n            \"\"\"\n            ),\n        )\n    ]\n\n\njudges = [\n    openai.call(\"gpt-4o-mini\", response_model=Eval),\n    anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Eval),\n]\n\nevaluations: list[Eval] = [\n    judge(evaluate_toxicity)(\n        \"Why even bother trying? With your laziness and abilities, it's probably not even possible anyway.\"\n    )\n    for judge in judges\n]\n\nfor evaluation in evaluations:\n    print(evaluation)\n# Output:\n# OpenAI:    reasoning='The text is derogatory and dismissive, suggesting incompetence and lack of effort.' score=2.0\n# Anthropic: reasoning='Discouraging, demeaning language targeting personal traits.' score=2.0\n</code></pre> <p>We are taking advantage of provider-agnostic prompts in this example to easily call multiple providers with the same prompt. Of course, you can always engineer each judge specifically for a given provider instead.</p> <p>Async for parallel evaluations</p> <p>We highly recommend using parallel asynchronous calls to run your evaluations more quickly since each call can (and should) be run in parallel.</p>"},{"location":"learn/evals/#hardcoded-evaluation-criteria","title":"Hardcoded Evaluation Criteria","text":"<p>While LLM-based evaluations are powerful, there are cases where simpler, hardcoded criteria can be more appropriate. These methods are particularly useful for evaluating specific, well-defined aspects of LLM outputs.</p> <p>Here are a few examples of such hardcoded evaluations:</p> Exact MatchRecall and PrecisionRegular Expression <pre><code>def exact_match_eval(output: str, expected: list[str]) -&gt; bool:\n    return all(phrase in output for phrase in expected)\n\n\n# Example usage\noutput = \"The capital of France is Paris, and it's known for the Eiffel Tower.\"\nexpected = [\"capital of France\", \"Paris\", \"Eiffel Tower\"]\nresult = exact_match_eval(output, expected)\nprint(result)  # Output: True\n</code></pre> <pre><code>def calculate_recall_precision(output: str, expected: str) -&gt; tuple[float, float]:\n    output_words = set(output.lower().split())\n    expected_words = set(expected.lower().split())\n\n    common_words = output_words.intersection(expected_words)\n\n    recall = len(common_words) / len(expected_words) if expected_words else 0\n    precision = len(common_words) / len(output_words) if output_words else 0\n\n    return recall, precision\n\n\n# Example usage\noutput = \"The Eiffel Tower is a famous landmark in Paris, France.\"\nexpected = (\n    \"The Eiffel Tower, located in Paris, is an iron lattice tower on the Champ de Mars.\"\n)\nrecall, precision = calculate_recall_precision(output, expected)\nprint(f\"Recall: {recall:.2f}, Precision: {precision:.2f}\")\n# Output: Recall: 0.40, Precision: 0.60\n</code></pre> <pre><code>import re\n\n\ndef contains_email(output: str) -&gt; bool:\n    email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n    return bool(re.search(email_pattern, output))\n\n\n# Example usage\noutput = \"My email is john.doe@example.com\"\nprint(contains_email(output))\n# Output: True\n</code></pre>"},{"location":"learn/evals/#next-steps","title":"Next Steps","text":"<p>By leveraging a combination of LLM-based evaluations and hardcoded criteria, you can create robust and nuanced evaluation systems for LLM outputs. Remember to continually refine your approach based on the specific needs of your application and the evolving capabilities of language models.</p> <p>Next, we recommend taking a look at our evaluation tutorials</p>"},{"location":"learn/json_mode/","title":"JSON Mode","text":"<p>     If you haven't already, we recommend first reading the section on Calls </p> <p>JSON Mode is a feature in Mirascope that allows you to request structured JSON output from Large Language Models (LLMs). This mode is particularly useful when you need to extract structured information from the model's responses, making it easier to parse and use the data in your applications.</p> Not all providers have an official JSON Mode <p>For providers with explicit support, Mirascope uses the native JSON Mode feature of the API. For providers without explicit support (Anthropic and Cohere), Mirascope implements a pseudo JSON Mode by instructing the model in the prompt to output JSON.</p> Provider Support Type Implementation OpenAI Explicit Native API feature Anthropic Pseudo Prompt engineering Mistral Explicit Native API feature Gemini Explicit Native API feature Groq Explicit Native API feature Cohere Pseudo Prompt engineering <p>If you'd prefer not to have any internal updates made to your prompt, you can always set JSON mode yourself through <code>call_params</code> rather than using the <code>json_mode</code> argument, which provides provider-agnostic support but is certainly not required to use JSON mode.</p>"},{"location":"learn/json_mode/#basic-usage-and-syntax","title":"Basic Usage and Syntax","text":"<p>Let's take a look at a basic example using JSON Mode:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\nresponse = get_book_info(\"The Name of the Wind\")\nprint(json.loads(response.content))\n# Output: {'author': 'Patrick Rothfuss', 'genre': 'Fantasy'}\n</code></pre> <p>In this example we</p> <ol> <li>Enable JSON Mode with <code>json_mode=True</code> in the <code>call</code> decorator</li> <li>Instruct the model what fields to include in our prompt</li> <li>Load the JSON string response into a Python object and print it</li> </ol>"},{"location":"learn/json_mode/#error-handling-and-validation","title":"Error Handling and Validation","text":"<p>While JSON Mode can signifanctly improve the structure of model outputs, it's important to note that it's far from infallible. LLMs often produce invalid JSON or deviate from the expected structure, so it's crucial to implement proper error handling and validation in your code:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; str:\n    return f\"Provide the author and genre of {book_title}\"\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; Messages.Type:\n    return Messages.User(f\"Provide the author and genre of {book_title}\")\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", json_mode=True)\n@prompt_template(\"Provide the author and genre of {book_title}\")\ndef get_book_info(book_title: str): ...\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", json_mode=True)\ndef get_book_info(book_title: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Provide the author and genre of {book_title}\"\n        )\n    ]\n\n\ntry:\n    response = get_book_info(\"The Name of the Wind\")\n    print(json.loads(response.content))\nexcept json.JSONDecodeError:\n    print(\"The model produced invalid JSON\")\n</code></pre> <p>Beyond JSON Validation</p> <p>While this example catches errors for invalid JSON, there's always a chance that the LLM returns valid JSON that doesn't conform to your expected schema (such as missing fields or incorrect types).</p> <p>For more robust validation, we recommend using Response Models for easier structuring and validation of LLM outputs.</p>"},{"location":"learn/json_mode/#next-steps","title":"Next Steps","text":"<p>By leveraging JSON Mode, you can create more robust and data-driven applications that efficiently process and utilize LLM outputs. This approach allows for easy integration with databases, APIs, or user interfaces, demonstrating the power of JSON Mode in creating robust, data-driven applications.</p> <p>Next, we recommend reading the section on Output Parsers to see how to engineer prompts with specific output structures and parse the outputs automatically on every call.</p>"},{"location":"learn/output_parsers/","title":"Output Parsers","text":"<p>     If you haven't already, we recommend first reading the section on Calls </p> <p>Output Parsers in Mirascope provide a flexible way to process and structure the raw output from Large Language Models (LLMs). They allow you to transform the LLM's response into a more usable format, enabling easier integration with your application logic and improving the overall reliability of your LLM-powered features.</p>"},{"location":"learn/output_parsers/#basic-usage-and-syntax","title":"Basic Usage and Syntax","text":"API Documentation <p><code>mirascope.core.openai.call.output_parser</code></p> <p><code>mirascope.core.anthropic.call.output_parser</code></p> <p><code>mirascope.core.mistral.call.output_parser</code></p> <p><code>mirascope.core.gemini.call.output_parser</code></p> <p><code>mirascope.core.groq.call.output_parser</code></p> <p><code>mirascope.core.cohere.call.output_parser</code></p> <p><code>mirascope.core.litellm.call.output_parser</code></p> <p><code>mirascope.core.azure.call.output_parser</code></p> <p><code>mirascope.core.vertex.call.output_parser</code></p> <p><code>mirascope.core.bedrock.call.output_parser</code></p> <p>Output Parsers are functions that take the call response object as input and return an output of a specified type. When you supply an output parser to a <code>call</code> decorator, it modifies the return type of the decorated function to match the output type of the parser.</p> <p>Let's take a look at a basic example:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\ndef parse_recommendation(response: openai.OpenAICallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@openai.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book. Output only Title by Author\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\ndef parse_recommendation(response: anthropic.AnthropicCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book. Output only Title by Author\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\ndef parse_recommendation(response: mistral.MistralCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@mistral.call(\"mistral-large-latest\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book. Output only Title by Author\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\ndef parse_recommendation(response: gemini.GeminiCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@gemini.call(\"gemini-1.5-flash\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book. Output only Title by Author\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import groq\n\n\ndef parse_recommendation(response: groq.GroqCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@groq.call(\"llama-3.1-70b-versatile\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book. Output only Title by Author\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\ndef parse_recommendation(response: cohere.CohereCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@cohere.call(\"command-r-plus\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book. Output only Title by Author\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\ndef parse_recommendation(response: litellm.OpenAICallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@litellm.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book. Output only Title by Author\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import azure\n\n\ndef parse_recommendation(response: azure.AzureCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@azure.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book. Output only Title by Author\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\ndef parse_recommendation(response: vertex.VertexCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@vertex.call(\"gemini-1.5-flash\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book. Output only Title by Author\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\ndef parse_recommendation(response: bedrock.BedrockCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", output_parser=parse_recommendation\n)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book. Output only Title by Author\"\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\ndef parse_recommendation(response: openai.OpenAICallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@openai.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book. Output only Title by Author\")\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\ndef parse_recommendation(response: anthropic.AnthropicCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book. Output only Title by Author\")\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\ndef parse_recommendation(response: mistral.MistralCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@mistral.call(\"mistral-large-latest\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book. Output only Title by Author\")\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\ndef parse_recommendation(response: gemini.GeminiCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@gemini.call(\"gemini-1.5-flash\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book. Output only Title by Author\")\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\ndef parse_recommendation(response: groq.GroqCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@groq.call(\"llama-3.1-70b-versatile\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book. Output only Title by Author\")\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\ndef parse_recommendation(response: cohere.CohereCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@cohere.call(\"command-r-plus\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book. Output only Title by Author\")\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\ndef parse_recommendation(response: litellm.OpenAICallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@litellm.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book. Output only Title by Author\")\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\ndef parse_recommendation(response: azure.AzureCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@azure.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book. Output only Title by Author\")\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\ndef parse_recommendation(response: vertex.VertexCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@vertex.call(\"gemini-1.5-flash\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book. Output only Title by Author\")\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\ndef parse_recommendation(response: bedrock.BedrockCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", output_parser=parse_recommendation\n)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book. Output only Title by Author\")\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\ndef parse_recommendation(response: openai.OpenAICallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@openai.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\n@prompt_template(\"Recommend a {genre} book. Output only Title by Author\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\ndef parse_recommendation(response: anthropic.AnthropicCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", output_parser=parse_recommendation)\n@prompt_template(\"Recommend a {genre} book. Output only Title by Author\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\ndef parse_recommendation(response: mistral.MistralCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@mistral.call(\"mistral-large-latest\", output_parser=parse_recommendation)\n@prompt_template(\"Recommend a {genre} book. Output only Title by Author\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\ndef parse_recommendation(response: gemini.GeminiCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@gemini.call(\"gemini-1.5-flash\", output_parser=parse_recommendation)\n@prompt_template(\"Recommend a {genre} book. Output only Title by Author\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\ndef parse_recommendation(response: groq.GroqCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@groq.call(\"llama-3.1-70b-versatile\", output_parser=parse_recommendation)\n@prompt_template(\"Recommend a {genre} book. Output only Title by Author\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\ndef parse_recommendation(response: cohere.CohereCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@cohere.call(\"command-r-plus\", output_parser=parse_recommendation)\n@prompt_template(\"Recommend a {genre} book. Output only Title by Author\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\ndef parse_recommendation(response: litellm.OpenAICallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@litellm.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\n@prompt_template(\"Recommend a {genre} book. Output only Title by Author\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\ndef parse_recommendation(response: azure.AzureCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@azure.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\n@prompt_template(\"Recommend a {genre} book. Output only Title by Author\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\ndef parse_recommendation(response: vertex.VertexCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@vertex.call(\"gemini-1.5-flash\", output_parser=parse_recommendation)\n@prompt_template(\"Recommend a {genre} book. Output only Title by Author\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\ndef parse_recommendation(response: bedrock.BedrockCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", output_parser=parse_recommendation\n)\n@prompt_template(\"Recommend a {genre} book. Output only Title by Author\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\ndef parse_recommendation(response: openai.OpenAICallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@openai.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Recommend a {genre} book. Output only Title by Author\",\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\ndef parse_recommendation(response: anthropic.AnthropicCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Recommend a {genre} book. Output only Title by Author\",\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\ndef parse_recommendation(response: mistral.MistralCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@mistral.call(\"mistral-large-latest\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Recommend a {genre} book. Output only Title by Author\",\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\ndef parse_recommendation(response: gemini.GeminiCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@gemini.call(\"gemini-1.5-flash\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Recommend a {genre} book. Output only Title by Author\",\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\ndef parse_recommendation(response: groq.GroqCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@groq.call(\"llama-3.1-70b-versatile\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Recommend a {genre} book. Output only Title by Author\",\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\ndef parse_recommendation(response: cohere.CohereCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@cohere.call(\"command-r-plus\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Recommend a {genre} book. Output only Title by Author\",\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\ndef parse_recommendation(response: litellm.OpenAICallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@litellm.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Recommend a {genre} book. Output only Title by Author\",\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\ndef parse_recommendation(response: azure.AzureCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@azure.call(\"gpt-4o-mini\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Recommend a {genre} book. Output only Title by Author\",\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\ndef parse_recommendation(response: vertex.VertexCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@vertex.call(\"gemini-1.5-flash\", output_parser=parse_recommendation)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Recommend a {genre} book. Output only Title by Author\",\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\ndef parse_recommendation(response: bedrock.BedrockCallResponse) -&gt; tuple[str, str]:\n    title, author = response.content.split(\" by \")\n    return (title, author)\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", output_parser=parse_recommendation\n)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Recommend a {genre} book. Output only Title by Author\",\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\"))\n# Output: ('\"The Name of the Wind\"', 'Patrick Rothfuss')\n</code></pre>"},{"location":"learn/output_parsers/#additional-examples","title":"Additional Examples","text":"<p>There are many different ways to structure and parse LLM outputs, ranging from XML parsing to using regular expressions.</p> <p>Here are a few examples:</p> Regular ExpressionXMLJSON Mode <pre><code>import re\n\nfrom mirascope.core import openai, prompt_template\n\n\ndef parse_cot(response: openai.OpenAICallResponse) -&gt; str:\n    pattern = r\"&lt;thinking&gt;.?*&lt;/thinking&gt;.*?&lt;output&gt;(.*?)&lt;/output&gt;\"\n    match = re.search(pattern, response.content, re.DOTALL)\n    if not match:\n        return response.content\n    return match.group(1).strip()\n\n\n@openai.call(\"gpt-4o-mini\", output_parser=parse_cot)\n@prompt_template(\n    \"\"\"\n    First, output your thought process in &lt;thinking&gt; tags.\n    Then, provide your final output in &lt;output&gt; tags.\n\n    Question: {question}\n    \"\"\"\n)\ndef chain_of_thought(question: str): ...\n\n\nquestion = \"Roger has 5 tennis balls. He buys 2 cans of 3. How many does he have now?\"\noutput = chain_of_thought(question)\nprint(output)\n</code></pre> <pre><code>import xml.etree.ElementTree as ET\n\nfrom mirascope.core import anthropic, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n    year: int\n    summary: str\n\n\ndef parse_book_xml(response: anthropic.AnthropicCallResponse) -&gt; Book | None:\n    try:\n        root = ET.fromstring(response.content)\n        if (node := root.find(\"title\")) is None or not (title := node.text):\n            raise ValueError(\"Missing title\")\n        if (node := root.find(\"author\")) is None or not (author := node.text):\n            raise ValueError(\"Missing author\")\n        if (node := root.find(\"year\")) is None or not (year := node.text):\n            raise ValueError(\"Missing year\")\n        if (node := root.find(\"summary\")) is None or not (summary := node.text):\n            raise ValueError(\"Missing summary\")\n        return Book(title=title, author=author, year=int(year), summary=summary)\n    except (ET.ParseError, ValueError) as e:\n        print(f\"Error parsing XML: {e}\")\n        return None\n\n\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\", output_parser=parse_book_xml)\n@prompt_template(\n    \"\"\"\n    Recommend a {genre} book. Provide the information in the following XML format:\n    &lt;book&gt;\n        &lt;title&gt;Book Title&lt;/title&gt;\n        &lt;author&gt;Author Name&lt;/author&gt;\n        &lt;year&gt;Publication Year&lt;/year&gt;\n        &lt;summary&gt;Brief summary of the book&lt;/summary&gt;\n    &lt;/book&gt;\n\n    Output ONLY the XML and no other text.\n    \"\"\"\n)\ndef recommend_book(genre: str): ...\n\n\nbook = recommend_book(\"science fiction\")\nif book:\n    print(f\"Title: {book.title}\")\n    print(f\"Author: {book.author}\")\n    print(f\"Year: {book.year}\")\n    print(f\"Summary: {book.summary}\")\nelse:\n    print(\"Failed to parse the recommendation.\")\n</code></pre> <pre><code>import json\n\nfrom mirascope.core import anthropic\n\n\ndef only_json(response: anthropic.AnthropicCallResponse) -&gt; str:\n    json_start = response.content.index(\"{\")\n    json_end = response.content.rfind(\"}\")\n    return response.content[json_start : json_end + 1]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True, output_parser=only_json)\ndef json_extraction(text: str, fields: list[str]) -&gt; str:\n    return f\"Extract {fields} from the following text: {text}\"\n\n\njson_response = json_extraction(\n    text=\"The capital of France is Paris\",\n    fields=[\"capital\", \"country\"],\n)\nprint(json.loads(json_response))\n</code></pre>"},{"location":"learn/output_parsers/#next-steps","title":"Next Steps","text":"<p>By leveraging Output Parsers effectively, you can create more robust and reliable LLM-powered applications, ensuring that the raw model outputs are transformed into structured data that's easy to work with in your application logic.</p> <p>Next, we recommend taking a look at the section on Tools to learn how to extend the capabilities of LLMs with custom functions.</p>"},{"location":"learn/prompts/","title":"Prompts","text":"<p>When working with Large Language Model (LLM) APIs, the \"prompt\" is generally a list of messages where each message has a particular role. These prompts are the foundation of effectively working with LLMs, so Mirascope provides powerful tools to help you create, manage, and optimize your prompts for various LLM interactions.</p> <p>Let's look at how we can write prompts using Mirascope in a reusable, modular, and provider-agnostic way.</p> <p>Calls will come later</p> <p>For the following explanations we will be talking only about the messages aspect of prompt engineering and will discuss calling the API later in the Calls documentation.</p> <p>In that section we will show how to use these provider-agnostic prompts to actually call a provider's API as well as how to engineer and tie a prompt to a specific call.</p>"},{"location":"learn/prompts/#prompt-templates-messages","title":"Prompt Templates (Messages)","text":"API Documentation <p><code>mirascope.core.base.message_param</code></p> <p><code>mirascope.core.base.prompt.prompt_template</code></p> <p>First, let's look at a basic example:</p> ShorthandMessagesString TemplateBaseMessageParam <pre><code>from mirascope.core import prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [BaseMessageParam(role='user', content='Recommend a fantasy book')]\n</code></pre> <pre><code>from mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [BaseMessageParam(role='user', content='Recommend a fantasy book')]\n</code></pre> <pre><code>from mirascope.core import prompt_template\n\n\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book_prompt(genre: str): ...\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [BaseMessageParam(role='user', content='Recommend a fantasy book')]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [BaseMessageParam(role='user', content='Recommend a fantasy book')]\n</code></pre> <p>In this example:</p> <ol> <li>The <code>recommend_book_prompt</code> method's signature defines the prompt's template variables.</li> <li>Calling the method with <code>genre=\"fantasy\"</code> returns a list with the corresponding <code>BaseMessageParam</code> instance with role <code>user</code> and content \"Recommend a fantasy book\".</li> </ol> <p>The core concept to understand here is <code>BaseMessageParam</code>. This class operates as the base class for message parameters that Mirascope can handle and use across all supported providers.</p> <p>In Mirascope, we use the <code>@prompt_template</code> decorator to write prompt templates as reusable methods that return the corresponding list of <code>BaseMessageParam</code> instances.</p> <p>There are four methods of writing prompts:</p> <ol> <li>(Shorthand) Returning the <code>str</code> or <code>list</code> content for a single user message.</li> <li>(Messages) Using <code>Messages.{Role}</code> methods, which accept the full or shorthand content and output a <code>BaseMessageParam</code> instance.</li> <li>(String Template) Passing a string template to <code>@prompt_template</code> that gets parsed and then formatted like a normal Python formatted string.</li> <li>(BaseMessageParam) Directly writing <code>BaseMessageParam</code> instances.</li> </ol> <p>Which method you use is mostly up to your preference, so feel free to select which one you prefer in the following sections.</p>"},{"location":"learn/prompts/#message-roles","title":"Message Roles","text":"<p>We can also define additional messages with different roles, such as a system message:</p> ShorthandMessagesString TemplateBaseMessageParam <pre><code>from mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; Messages.Type:\n    return [\n        Messages.System(\"You are a librarian\"),\n        Messages.User(f\"Recommend a {genre} book\"),\n    ]\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [\n#   BaseMessageParam(role='system', content='You are a librarian'),\n#   BaseMessageParam(role='user', content='Recommend a fantasy book'),\n# ]\n</code></pre> <pre><code>from mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; Messages.Type:\n    return [\n        Messages.System(\"You are a librarian\"),\n        Messages.User(f\"Recommend a {genre} book\"),\n    ]\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [\n#   BaseMessageParam(role='system', content='You are a librarian'),\n#   BaseMessageParam(role='user', content='Recommend a fantasy book'),\n# ]\n</code></pre> <pre><code>from mirascope.core import prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM: You are a librarian\n    USER: Recommend a {genre} book\n    \"\"\"\n)\ndef recommend_book_prompt(genre: str): ...\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [\n#   BaseMessageParam(role='system', content='You are a librarian'),\n#   BaseMessageParam(role='user', content='Recommend a fantasy book'),\n# ]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n        BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\"),\n    ]\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [\n#   BaseMessageParam(role='system', content='You are a librarian'),\n#   BaseMessageParam(role='user', content='Recommend a fantasy book'),\n# ]\n</code></pre> <p><code>Messages.Type</code></p> <p>The return type <code>Messages.Type</code> accepts all shorthand methods as well as <code>BaseMessageParam</code> types. Since the message methods (e.g. <code>Messages.User</code>) return <code>BaseMessageParam</code> instances, we generally recommend always typing your prompt templates with the <code>Messages.Type</code> return type since it covers all prompt template writing methods.</p> <p>Supported Roles</p> <p>Mirascope prompt templates currently support the <code>system</code>, <code>user</code>, and <code>assistant</code> roles. When using string templates, the roles are parsed by their corresponding all caps keyword (e.g. SYSTEM).</p> <p>For messages with the <code>tool</code> role, see how Mirascope automatically generates these messages for you in the Tools and Agents sections.</p>"},{"location":"learn/prompts/#multi-line-prompts","title":"Multi-Line Prompts","text":"<p>When writing prompts that span multiple lines, it's important to ensure you don't accidentally include additional, unnecessary tokens (namely <code>\\t</code> tokens):</p> ShorthandMessagesString TemplateBaseMessageParam <pre><code>import inspect\n\nfrom mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; Messages.Type:\n    return inspect.cleandoc(\n        f\"\"\"\n        Recommend a {genre} book.\n        Output in the format Title by Author.\n        \"\"\"\n    )\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [BaseMessageParam(role='system', content='Recommend a fantasy book.\\nOutput in the format Title by Author.')]\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; Messages.Type:\n    return Messages.User(\n        inspect.cleandoc(\n            f\"\"\"\n            Recommend a {genre} book.\n            Output in the format Title by Author.\n            \"\"\"\n        )\n    )\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [BaseMessageParam(role='system', content='Recommend a fantasy book.\\nOutput in the format Title by Author.')]\n</code></pre> <pre><code>from mirascope.core import prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    Recommend a {genre} book.\n    Output in the format Title by Author.\n    \"\"\"\n)\ndef recommend_book_prompt(genre: str): ...\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [BaseMessageParam(role='system', content='Recommend a fantasy book.\\nOutput in the format Title by Author.')]\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                f\"\"\"\n                Recommend a {genre} book.\n                Output in the format Title by Author.\n                \"\"\"\n            ),\n        ),\n    ]\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [BaseMessageParam(role='system', content='Recommend a fantasy book.\\nOutput in the format Title by Author.')]\n</code></pre> <p>In this example, we use <code>inspect.cleandoc</code> to remove unnecessary tokens while maintaining proper formatting in our codebase.</p> Multi-Line String Templates <p>When using string templates, the template is automatically cleaned for you, so there is no need to use <code>inspect.cleandoc</code> in that case. However, it's extremely important to note that you must start messages with the same indentation in order to properly remove the unnecessary tokens. For example:</p> <pre><code># BAD\n@prompt_template(\n    \"\"\"\n    USER: First line\n    Second line\n    \"\"\"\n)\n\n# GOOD\n@prompt_template(\n    \"\"\"\n    USER:\n    First line\n    Second line\n    \"\"\"\n)\n</code></pre>"},{"location":"learn/prompts/#multi-modal-inputs","title":"Multi-Modal Inputs","text":"<p>Recent advancements in Large Language Model architecture has enabled many model providers to support multi-modal inputs (text, images, audio, etc.) for a single endpoint. Mirascope treats these input types as first-class and supports them natively.</p> <p>While Mirascope provides a consistent interface, support varies among providers:</p> Type Anthropic Cohere Gemini Groq Mistral OpenAI text \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 image \u2713 - \u2713 \u2713 \u2713 \u2713 audio - - \u2713 - - \u2713 video - - \u2713 - - - <p>Legend: \u2713 (Supported), - (Not Supported)</p>"},{"location":"learn/prompts/#image-inputs","title":"Image Inputs","text":"ShorthandMessagesString TemplateBaseMessageParam <pre><code>from mirascope.core import Messages, prompt_template\nfrom PIL import Image\n\n\n@prompt_template()\ndef recommend_book_prompt(previous_book: Image.Image) -&gt; Messages.Type:\n    return [\"I just read this book:\", previous_book, \"What should I read next?\"]\n\n\nwith Image.open(\"...\") as image:\n    print(recommend_book_prompt(image))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"I just read this book:\"),\n#             ImagePart(type=\"image\", media_type=\"image/jpeg\", image=b\"...\", detail=None),\n#             TextPart(type=\"text\", text=\"What should I read next?\"),\n#         ],\n#     )\n# ]\n</code></pre> <pre><code>from mirascope.core import Messages, prompt_template\nfrom PIL import Image\n\n\n@prompt_template()\ndef recommend_book_prompt(previous_book: Image.Image) -&gt; Messages.Type:\n    return Messages.User(\n        [\"I just read this book:\", previous_book, \"What should I read next?\"]\n    )\n\n\nwith Image.open(\"...\") as image:\n    print(recommend_book_prompt(image))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"I just read this book:\"),\n#             ImagePart(type=\"image\", media_type=\"image/jpeg\", image=b\"...\", detail=None),\n#             TextPart(type=\"text\", text=\"What should I read next?\"),\n#         ],\n#     )\n# ]\n</code></pre> <pre><code>from mirascope.core import prompt_template\n\n\n@prompt_template(\n    \"I just read this book: {previous_book:image} What should I read next?\"\n)\ndef recommend_book_prompt(previous_book: bytes): ...\n\n\nprint(recommend_book_prompt(b\"...\"))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"I just read this book:\"),\n#             ImagePart(type=\"image\", media_type=\"image/jpeg\", image=b\"...\", detail=None),\n#             TextPart(type=\"text\", text=\"What should I read next?\"),\n#         ],\n#     )\n# ]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam\nfrom mirascope.core.base import ImagePart, TextPart\n\n\ndef recommend_book_prompt(previous_book_jpeg: bytes) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=[\n                TextPart(type=\"text\", text=\"I just read this book:\"),\n                ImagePart(\n                    type=\"image\",\n                    media_type=\"image/jpeg\",\n                    image=previous_book_jpeg,\n                    detail=None,\n                ),\n                TextPart(type=\"text\", text=\"What should I read next?\"),\n            ],\n        )\n    ]\n\n\nprint(recommend_book_prompt(b\"...\"))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"I just read this book:\"),\n#             ImagePart(type=\"image\", media_type=\"image/jpeg\", image=b\"...\", detail=None),\n#             TextPart(type=\"text\", text=\"What should I read next?\"),\n#         ],\n#     )\n# ]\n</code></pre> Additional String Template Image Functionality <p>When using string templates, you can also specify <code>:images</code> to inject multiple image inputs through a single template variable.</p> <p>The <code>:image</code> and <code>:images</code> tags support the <code>bytes | str</code> and <code>list[bytes] | list[str]</code> types, respectively. When passing in a <code>str</code>, the string template assumes it indicates a url or local filepath and will attempt to load the bytes from the source.</p> <p>You can also specify additional options as arguments of the tags, e.g. <code>{url:image(detail=low)}</code></p>"},{"location":"learn/prompts/#audio-inputs","title":"Audio Inputs","text":"pydubwave ShorthandMessagesString TemplateBaseMessageParam <pre><code>from pydub import AudioSegment\n\nfrom mirascope.core import prompt_template, Messages\n\n\n@prompt_template()\ndef identify_book_prompt(audio_wave: AudioSegment) -&gt; Messages.Type:\n    return [\"Here's an audio book snippet:\", audio_wave, \"What book is this?\"]\n\n\nwith open(\"....\", \"rb\") as audio:\n    print(identify_book_prompt(AudioSegment.from_mp3(audio)))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"Here's an audio book snippet:\"),\n#             AudioPart(type='audio', media_type='audio/wav', audio=b'...'),\n#             TextPart(type=\"text\", text=\"What book is this?\"),\n#         ],\n#     )\n# ]\n</code></pre> <pre><code>from pydub import AudioSegment\n\nfrom mirascope.core import prompt_template, Messages\n\n\n@prompt_template()\ndef identify_book_prompt(audio_wave: AudioSegment) -&gt; Messages.Type:\n    return Messages.User(\n        [\"Here's an audio book snippet:\", audio_wave, \"What book is this?\"]\n    )\n\n\nwith open(\"....\", \"rb\") as audio:\n    print(identify_book_prompt(AudioSegment.from_mp3(audio)))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"Here's an audio book snippet:\"),\n#             AudioPart(type='audio', media_type='audio/wav', audio=b'...'),\n#             TextPart(type=\"text\", text=\"What book is this?\"),\n#         ],\n#     )\n# ]\n</code></pre> <pre><code>from mirascope.core import prompt_template\n\n\n@prompt_template(\"Here's an audio book snippet: {audio_wave:audio} What book is this?\")\ndef identify_book_prompt(audio_wave: bytes): ...\n\n\nprint(identify_book_prompt(b\"...\"))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"Here's an audio book snippet:\"),\n#             AudioPart(type='audio', media_type='audio/wav', audio=b'...'),\n#             TextPart(type=\"text\", text=\"What book is this?\"),\n#         ],\n#     )\n# ]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam\nfrom mirascope.core.base import AudioPart, TextPart\n\n\ndef identify_book_prompt(audio_wave: bytes) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=[\n                TextPart(type=\"text\", text=\"Here's an audio book snippet:\"),\n                AudioPart(\n                    type=\"audio\",\n                    media_type=\"audio/wav\",\n                    audio=audio_wave,\n                ),\n                TextPart(type=\"text\", text=\"What book is this?\"),\n            ],\n        )\n    ]\n\n\nprint(identify_book_prompt(b\"...\"))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"Here's an audio book snippet:\"),\n#             AudioPart(type='audio', media_type='audio/mp3', audio=b'...'),\n#             TextPart(type=\"text\", text=\"What book is this?\"),\n#         ],\n#     )\n# ]\n</code></pre> ShorthandMessagesString TemplateBaseMessageParam <pre><code>import wave\n\nfrom mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef identify_book_prompt(audio_wave: wave.Wave_read) -&gt; Messages.Type:\n    return [\"Here's an audio book snippet:\", audio_wave, \"What book is this?\"]\n\n\nwith open(\"....\", \"rb\") as f, wave.open(f, \"rb\") as audio:\n    print(identify_book_prompt(audio))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"Here's an audio book snippet:\"),\n#             AudioPart(type='audio', media_type='audio/wav', audio=b'...'),\n#             TextPart(type=\"text\", text=\"What book is this?\"),\n#         ],\n#     )\n# ]\n</code></pre> <pre><code>import wave\n\nfrom mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef identify_book_prompt(audio_wave: wave.Wave_read) -&gt; Messages.Type:\n    return Messages.User(\n        [\"Here's an audio book snippet:\", audio_wave, \"What book is this?\"]\n    )\n\n\nwith open(\"....\", \"rb\") as f, wave.open(f, \"rb\") as audio:\n    print(identify_book_prompt(audio))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"Here's an audio book snippet:\"),\n#             AudioPart(type='audio', media_type='audio/wav', audio=b'...'),\n#             TextPart(type=\"text\", text=\"What book is this?\"),\n#         ],\n#     )\n# ]\n</code></pre> <pre><code>from mirascope.core import prompt_template\n\n\n@prompt_template(\"Here's an audio book snippet: {audio_wave:audio} What book is this?\")\ndef identify_book_prompt(audio_wave: bytes): ...\n\n\nprint(identify_book_prompt(b\"...\"))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"Here's an audio book snippet:\"),\n#             AudioPart(type='audio', media_type='audio/wav', audio=b'...'),\n#             TextPart(type=\"text\", text=\"What book is this?\"),\n#         ],\n#     )\n# ]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam\nfrom mirascope.core.base import AudioPart, TextPart\n\n\ndef identify_book_prompt(audio_wave: bytes) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=[\n                TextPart(type=\"text\", text=\"Here's an audio book snippet:\"),\n                AudioPart(\n                    type=\"audio\",\n                    media_type=\"audio/wav\",\n                    audio=audio_wave,\n                ),\n                TextPart(type=\"text\", text=\"What book is this?\"),\n            ],\n        )\n    ]\n\n\nprint(identify_book_prompt(b\"...\"))\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=[\n#             TextPart(type=\"text\", text=\"Here's an audio book snippet:\"),\n#             AudioPart(type='audio', media_type='audio/mp3', audio=b'...'),\n#             TextPart(type=\"text\", text=\"What book is this?\"),\n#         ],\n#     )\n# ]\n</code></pre> Additional String Template Audio Functionality <p>When using string templates, you can also specify <code>:audios</code> to inject multiple audio inputs through a single template variable.</p> <p>The <code>:audio</code> and <code>:audios</code> tags support the <code>bytes | str</code> and <code>list[bytes] | list[str]</code> types, respectively. When passing in a <code>str</code>, the string template assumes it indicates a url or local filepath and will attempt to load the bytes from the source.</p>"},{"location":"learn/prompts/#chat-history","title":"Chat History","text":"<p>Often you'll want to inject messages (such as previous chat messages) into the prompt. Generally you can just unroll the messages into the return value of your prompt template. When using string templates, we provide a <code>MESSAGES</code> keyword for this injection, which you can add in whatever position and as many times as you'd like:</p> ShorthandMessagesString TemplateBaseMessageParam <pre><code>from mirascope.core import BaseMessageParam, Messages, prompt_template\n\n\n@prompt_template()\ndef chatbot(query: str, history: list[BaseMessageParam]) -&gt; list[BaseMessageParam]:\n    return [Messages.System(\"You are a librarian\"), *history, Messages.User(query)]\n\n\nhistory = [\n    Messages.User(\"Recommend a book\"),\n    Messages.Assistant(\"What genre do you like?\"),\n]\nprint(chatbot(\"fantasy\", history))\n# Output: [\n#     BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n#     BaseMessageParam(role=\"user\", content=\"Recommend a book\"),\n#     BaseMessageParam(role=\"assistant\", content=\"What genre do you like?\"),\n#     BaseMessageParam(role=\"user\", content=\"fantasy\"),\n# ]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, Messages, prompt_template\n\n\n@prompt_template()\ndef chatbot(query: str, history: list[BaseMessageParam]) -&gt; list[BaseMessageParam]:\n    return [Messages.System(\"You are a librarian\"), *history, Messages.User(query)]\n\n\nhistory = [\n    Messages.User(\"Recommend a book\"),\n    Messages.Assistant(\"What genre do you like?\"),\n]\nprint(chatbot(\"fantasy\", history))\n# Output: [\n#     BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n#     BaseMessageParam(role=\"user\", content=\"Recommend a book\"),\n#     BaseMessageParam(role=\"assistant\", content=\"What genre do you like?\"),\n#     BaseMessageParam(role=\"user\", content=\"fantasy\"),\n# ]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, Messages, prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM: You are a librarian\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef chatbot(query: str, history: list[BaseMessageParam]): ...\n\n\nhistory = [\n    Messages.User(\"Recommend a book\"),\n    Messages.Assistant(\"What genre do you like?\"),\n]\nprint(chatbot(\"fantasy\", history))\n# Output: [\n#     BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n#     BaseMessageParam(role=\"user\", content=\"Recommend a book\"),\n#     BaseMessageParam(role=\"assistant\", content=\"What genre do you like?\"),\n#     BaseMessageParam(role=\"user\", content=\"fantasy\"),\n# ]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, prompt_template\n\n\n@prompt_template()\ndef chatbot(query: str, history: list[BaseMessageParam]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n        *history,\n        BaseMessageParam(role=\"user\", content=query),\n    ]\n\n\nhistory = [\n    BaseMessageParam(role=\"user\", content=\"Recommend a book\"),\n    BaseMessageParam(role=\"assistant\", content=\"What genre do you like?\"),\n]\nprint(chatbot(\"fantasy\", history))\n# Output: [\n#     BaseMessageParam(role=\"system\", content=\"You are a librarian\"),\n#     BaseMessageParam(role=\"user\", content=\"Recommend a book\"),\n#     BaseMessageParam(role=\"assistant\", content=\"What genre do you like?\"),\n#     BaseMessageParam(role=\"user\", content=\"fantasy\"),\n# ]\n</code></pre>"},{"location":"learn/prompts/#object-attribute-access","title":"Object Attribute Access","text":"<p>When using template variables that have attributes, you can easily inject these attributes directly even when using string templates:</p> ShorthandMessagesString TemplateBaseMessageParam <pre><code>from mirascope.core import prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@prompt_template()\ndef recommend_book_prompt(book: Book) -&gt; str:\n    return f\"I read {book.title} by {book.author}. What should I read next?\"\n\n\nbook = Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\")\nprint(recommend_book_prompt(book))\n# Output: [BaseMessageParam(role='user', content='I read The Name of the Wind by Patrick Rothfuss. What should I read next?')]\n</code></pre> <pre><code>from mirascope.core import Messages, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@prompt_template()\ndef recommend_book_prompt(book: Book) -&gt; Messages.Type:\n    return Messages.User(\n        f\"I read {book.title} by {book.author}. What should I read next?\"\n    )\n\n\nbook = Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\")\nprint(recommend_book_prompt(book))\n# Output: [BaseMessageParam(role='user', content='I read The Name of the Wind by Patrick Rothfuss. What should I read next?')]\n</code></pre> <pre><code>from mirascope.core import prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@prompt_template(\"I read {book.title} by {book.author}. What should I read next?\")\ndef recommend_book_prompt(book: Book): ...\n\n\nbook = Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\")\nprint(recommend_book_prompt(book))\n# Output: [BaseMessageParam(role='user', content='I read The Name of the Wind by Patrick Rothfuss. What should I read next?')]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@prompt_template()\ndef recommend_book_prompt(book: Book) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"I read {book.title} by {book.author}. What should I read next?\",\n        )\n    ]\n\n\nbook = Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\")\nprint(recommend_book_prompt(book))\n# Output: [BaseMessageParam(role='user', content='I read The Name of the Wind by Patrick Rothfuss. What should I read next?')]\n</code></pre> <p>It's worth noting that this also works with <code>self</code> when using prompt templates inside of a class, which is particularly important when building Agents.</p>"},{"location":"learn/prompts/#format-specifiers","title":"Format Specifiers","text":"<p>Since Mirascope prompt templates are just formatted strings, standard Python format specifiers work as expected:</p> ShorthandMessagesString TemplateBaseMessageParam <pre><code>from mirascope.core import prompt_template\n\n\n@prompt_template()\ndef recommend_book(genre: str, price: float) -&gt; str:\n    return f\"Recommend a {genre} book under ${price:.2f}\"\n\n\nprint(recommend_book(\"fantasy\", 12.3456))\n# Output: [BaseMessageParam(role='user', content='Recommend a fantasy book under $12.35')]\n</code></pre> <pre><code>from mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef recommend_book(genre: str, price: float) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book under ${price:.2f}\")\n\n\nprint(recommend_book(\"fantasy\", 12.3456))\n# Output: [BaseMessageParam(role='user', content='Recommend a fantasy book under $12.35')]\n</code></pre> <pre><code>from mirascope.core import prompt_template\n\n\n@prompt_template(\"Recommend a {genre} book under ${price:.2f}\")\ndef recommend_book(genre: str, price: float): ...\n\n\nprint(recommend_book(\"fantasy\", 12.3456))\n# Output: [BaseMessageParam(role='user', content='Recommend a fantasy book under $12.35')]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, prompt_template\n\n\n@prompt_template()\ndef recommend_book(genre: str, price: float) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Recommend a {genre} book under ${price:.2f}\"\n        )\n    ]\n\n\nprint(recommend_book(\"fantasy\", 12.3456))\n# Output: [BaseMessageParam(role='user', content='Recommend a fantasy book under $12.35')]\n</code></pre> <p>When writing string templates, we also offer additional <code>list</code> and <code>lists</code> format specifiers for convenience around formatting lists:</p> ShorthandMessagesString TemplateBaseMessageParam <pre><code>import inspect\n\nfrom mirascope.core import prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genres: list[str], examples: list[list[str]]) -&gt; str:\n    formatted_genres = \"\\n\".join(genres)\n    formatted_examples = \"\\n\\n\".join([\"\\n\".join(e) for e in examples])\n    return inspect.cleandoc(\n        \"\"\"\n        Recommend a book from one of the following genres:\n        {genres}\n\n        Examples:\n        {examples}\n        \"\"\"\n    ).format(genres=formatted_genres, examples=formatted_examples)\n\n\nprompt = recommend_book_prompt(\n    genres=[\"fantasy\", \"scifi\", \"mystery\"],\n    examples=[\n        [\"Title: The Name of the Wind\", \"Author: Patrick Rothfuss\"],\n        [\"Title: Mistborn: The Final Empire\", \"Author: Brandon Sanderson\"],\n    ],\n)\nprint(prompt)\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=\"Recommend a book from one of the following genres:\\nfantasy\\nscifi\\nmystery\\n\\nExamples:\\nTitle: The Name of the Wind\\nAuthor: Patrick Rothfuss\\n\\nTitle: Mistborn: The Final Empire\\nAuthor: Brandon Sanderson\",\n#     )\n# ]\n\nprint(prompt[0].content)\n# Output:\n# Recommend a book from one of the following genres:\n# fantasy\n# scifi\n# mystery\n\n# Examples:\n# Title: The Name of the Wind\n# Author: Patrick Rothfuss\n\n# Title: Mistborn: The Final Empire\n# Author: Brandon Sanderson\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import Messages, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(\n    genres: list[str], examples: list[list[str]]\n) -&gt; Messages.Type:\n    formatted_genres = \"\\n\".join(genres)\n    formatted_examples = \"\\n\\n\".join([\"\\n\".join(e) for e in examples])\n    return Messages.User(\n        inspect.cleandoc(\n            \"\"\"\n            Recommend a book from one of the following genres:\n            {genres}\n\n            Examples:\n            {examples}\n            \"\"\"\n        ).format(genres=formatted_genres, examples=formatted_examples)\n    )\n\n\nprompt = recommend_book_prompt(\n    genres=[\"fantasy\", \"scifi\", \"mystery\"],\n    examples=[\n        [\"Title: The Name of the Wind\", \"Author: Patrick Rothfuss\"],\n        [\"Title: Mistborn: The Final Empire\", \"Author: Brandon Sanderson\"],\n    ],\n)\nprint(prompt)\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=\"Recommend a book from one of the following genres:\\nfantasy\\nscifi\\nmystery\\n\\nExamples:\\nTitle: The Name of the Wind\\nAuthor: Patrick Rothfuss\\n\\nTitle: Mistborn: The Final Empire\\nAuthor: Brandon Sanderson\",\n#     )\n# ]\n\nprint(prompt[0].content)\n# Output:\n# Recommend a book from one of the following genres:\n# fantasy\n# scifi\n# mystery\n\n# Examples:\n# Title: The Name of the Wind\n# Author: Patrick Rothfuss\n\n# Title: Mistborn: The Final Empire\n# Author: Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import prompt_template\n\n\n@prompt_template(\n    \"\"\"\n    Recommend a book from one of the following genres:\n    {genres:list}\n\n    Examples:\n    {examples:lists}\n    \"\"\"\n)\ndef recommend_book_prompt(genres: list[str], examples: list[list[str]]): ...\n\n\nprompt = recommend_book_prompt(\n    genres=[\"fantasy\", \"scifi\", \"mystery\"],\n    examples=[\n        [\"Title: The Name of the Wind\", \"Author: Patrick Rothfuss\"],\n        [\"Title: Mistborn: The Final Empire\", \"Author: Brandon Sanderson\"],\n    ],\n)\nprint(prompt)\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=\"Recommend a book from one of the following genres:\\nfantasy\\nscifi\\nmystery\\n\\nExamples:\\nTitle: The Name of the Wind\\nAuthor: Patrick Rothfuss\\n\\nTitle: Mistborn: The Final Empire\\nAuthor: Brandon Sanderson\",\n#     )\n# ]\n\nprint(prompt[0].content)\n# Output:\n# Recommend a book from one of the following genres:\n# fantasy\n# scifi\n# mystery\n\n# Examples:\n# Title: The Name of the Wind\n# Author: Patrick Rothfuss\n\n# Title: Mistborn: The Final Empire\n# Author: Brandon Sanderson\n</code></pre> <pre><code>import inspect\n\nfrom mirascope.core import BaseMessageParam, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(\n    genres: list[str], examples: list[list[str]]\n) -&gt; list[BaseMessageParam]:\n    formatted_genres = \"\\n\".join(genres)\n    formatted_examples = \"\\n\\n\".join([\"\\n\".join(e) for e in examples])\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=inspect.cleandoc(\n                \"\"\"\n                Recommend a book from one of the following genres:\n                {genres}\n\n                Examples:\n                {examples}\n                \"\"\"\n            ).format(genres=formatted_genres, examples=formatted_examples),\n        )\n    ]\n\n\nprompt = recommend_book_prompt(\n    genres=[\"fantasy\", \"scifi\", \"mystery\"],\n    examples=[\n        [\"Title: The Name of the Wind\", \"Author: Patrick Rothfuss\"],\n        [\"Title: Mistborn: The Final Empire\", \"Author: Brandon Sanderson\"],\n    ],\n)\nprint(prompt)\n# Output: [\n#     BaseMessageParam(\n#         role=\"user\",\n#         content=\"Recommend a book from one of the following genres:\\nfantasy\\nscifi\\nmystery\\n\\nExamples:\\nTitle: The Name of the Wind\\nAuthor: Patrick Rothfuss\\n\\nTitle: Mistborn: The Final Empire\\nAuthor: Brandon Sanderson\",\n#     )\n# ]\n\nprint(prompt[0].content)\n# Output:\n# Recommend a book from one of the following genres:\n# fantasy\n# scifi\n# mystery\n\n# Examples:\n# Title: The Name of the Wind\n# Author: Patrick Rothfuss\n\n# Title: Mistborn: The Final Empire\n# Author: Brandon Sanderson\n</code></pre>"},{"location":"learn/prompts/#computed-fields-dynamic-configuration","title":"Computed Fields (Dynamic Configuration)","text":"<p>In Mirascope, we write prompt templates as functions, which enables dynamically configuring our prompts at runtime depending on the values of the template variables. We use the term \"computed fields\" to talk about variables that are computed and formatted at runtime.</p> <p>In the following examples, we demonstrate using computed fields and dynamic configuration across all prompt templating methods. Of course, this is only actually necessary for string templates. For other methods you can simply format the computed fields directly and return <code>Messages.Type</code> as before.</p> <p>However, there is value in always dynamically configuring computed fields for any and all prompt templating methods. While we cover this in more detail in the Calls and Chaining sections, the short of it is that it enables proper tracing even across nested calls and chains.</p> ShorthandMessagesString TemplateBaseMessageParam <pre><code>from mirascope.core import BaseDynamicConfig, Messages, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; BaseDynamicConfig:\n    uppercase_genre = genre.upper()\n    messages = [Messages.User(f\"Recommend a {uppercase_genre} book\")]\n    return {\n        \"messages\": messages,\n        \"computed_fields\": {\"uppercase_genre\": uppercase_genre},\n    }\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: {\n#     \"messages\": [BaseMessageParam(role=\"user\", content=\"Recommend a FANTASY book\")],\n#     \"computed_fields\": {\"uppercase_genre\": \"FANTASY\"},\n# }\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, Messages, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; BaseDynamicConfig:\n    uppercase_genre = genre.upper()\n    messages = [Messages.User(f\"Recommend a {uppercase_genre} book\")]\n    return {\n        \"messages\": messages,\n        \"computed_fields\": {\"uppercase_genre\": uppercase_genre},\n    }\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: {\n#     \"messages\": [BaseMessageParam(role=\"user\", content=\"Recommend a FANTASY book\")],\n#     \"computed_fields\": {\"uppercase_genre\": \"FANTASY\"},\n# }\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, prompt_template\n\n\n@prompt_template(\"Recommend a {uppercase_genre} book\")\ndef recommend_book_prompt(genre: str) -&gt; BaseDynamicConfig:\n    uppercase_genre = genre.upper()\n    return {\n        \"computed_fields\": {\"uppercase_genre\": uppercase_genre},\n    }\n\n\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: [BaseMessageParam(role='user', content='Recommend a FANTASY book')]\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseMessageParam, prompt_template\n\n\n@prompt_template()\ndef recommend_book_prompt(genre: str) -&gt; BaseDynamicConfig:\n    uppercase_genre = genre.upper()\n    messages = [\n        BaseMessageParam(role=\"user\", content=f\"Recommend a {uppercase_genre} book\")\n    ]\n    return {\n        \"messages\": messages,\n        \"computed_fields\": {\"uppercase_genre\": uppercase_genre},\n    }\n\n\nprint(recommend_book_prompt(\"fantasy\"))\nprint(recommend_book_prompt(\"fantasy\"))\n# Output: {\n#     \"messages\": [BaseMessageParam(role=\"user\", content=\"Recommend a FANTASY book\")],\n#     \"computed_fields\": {\"uppercase_genre\": \"FANTASY\"},\n# }\n</code></pre> <p>There are various other parts of an LLM API call that we may want to configure dynamically as well, such as call parameters, tools, and more. We cover such cases in each of their respective sections.</p>"},{"location":"learn/prompts/#next-steps","title":"Next Steps","text":"<p>By mastering prompts in Mirascope, you'll be well-equipped to build robust, flexible, and reusable LLM applications.</p> <p>Next, we recommend taking a look at the Calls documentation, which shows you how to use your prompt templates to actually call LLM APIs and generate a response.</p>"},{"location":"learn/response_models/","title":"Response Models","text":"<p>     If you haven't already, we recommend first reading the section on Calls </p> <p>Response Models in Mirascope provide a powerful way to structure and validate the output from Large Language Models (LLMs). By leveraging Pydantic's <code>BaseModel</code>, Response Models offer type safety, automatic validation, and easier data manipulation for your LLM responses. While we cover some details in this documentation, we highly recommend reading through Pydantic's documentation for a deeper, comprehensive dive into everything you can do with Pydantic's <code>BaseModel</code>.</p>"},{"location":"learn/response_models/#why-use-response-models","title":"Why Use Response Models?","text":"<ol> <li>Structured Output: Define exactly what you expect from the LLM, ensuring consistency in responses.</li> <li>Automatic Validation: Pydantic handles type checking and validation, reducing errors in your application.</li> <li>Improved Type Hinting: Better IDE support and clearer code structure.</li> <li>Easier Data Manipulation: Work with Python objects instead of raw strings or dictionaries.</li> </ol>"},{"location":"learn/response_models/#basic-usage-and-syntax","title":"Basic Usage and Syntax","text":"<p>Let's take a look at a basic example using Mirascope vs. official provider SDKs:</p> <p>Mirascope</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> Official SDK OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = OpenAI()\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Extract {text}\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"Book\",\n                    \"description\": \"An extracted book.\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"author\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"title\", \"author\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n        tool_choice=\"required\",\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        return Book.model_validate_json(tool_calls[0].function.arguments)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from anthropic import Anthropic\nfrom pydantic import BaseModel\n\nclient = Anthropic()\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    message = client.messages.create(\n        model=\"claude-3-5-sonnet-20240620\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": f\"Extract {text}\"}],\n        tools=[\n            {\n                \"name\": \"Book\",\n                \"description\": \"An extracted book.\",\n                \"input_schema\": {\n                    \"properties\": {\n                        \"title\": {\"type\": \"string\"},\n                        \"author\": {\"type\": \"string\"},\n                    },\n                    \"required\": [\"title\", \"author\"],\n                    \"type\": \"object\",\n                },\n            }\n        ],\n        tool_choice={\"type\": \"tool\", \"name\": \"Book\"},\n    )\n    for block in message.content:\n        if block.type == \"tool_use\" and block.input is not None:\n            return Book.model_validate(block.input)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ToolChoice\nfrom pydantic import BaseModel\n\nclient = MistralClient()\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    completion = client.chat(\n        model=\"mistral-large-latest\",\n        messages=[{\"role\": \"user\", \"content\": f\"Extract {text}\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"Book\",\n                    \"description\": \"An extracted book.\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"author\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"title\", \"author\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n        tool_choice=ToolChoice.any,\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        return Book.model_validate_json(tool_calls[0].function.arguments)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom google.generativeai.types import FunctionDeclaration, Tool, content_types\nfrom proto.marshal.collections import RepeatedComposite\nfrom pydantic import BaseModel\n\nmodel = GenerativeModel(\"gemini-1.5-flash\")\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    response = model.generate_content(\n        f\"Extract {text}\",\n        tools=[\n            Tool(\n                function_declarations=[\n                    FunctionDeclaration(\n                        **{\n                            \"name\": \"Book\",\n                            \"description\": \"An extracted book.\",\n                            \"parameters\": {\n                                \"properties\": {\n                                    \"title\": {\"type\": \"string\"},\n                                    \"author\": {\"type\": \"string\"},\n                                },\n                                \"required\": [\"title\", \"author\"],\n                                \"type\": \"object\",\n                            },\n                        }\n                    )\n                ]\n            )\n        ],\n        tool_config=content_types.to_tool_config(\n            {\n                \"function_calling_config\": {\n                    \"mode\": \"any\",\n                    \"allowed_function_names\": [\"Book\"],\n                }\n            }  # pyright: ignore [reportArgumentType]\n        ),\n    )\n    if tool_calls := [\n        part.function_call for part in response.parts if part.function_call.args\n    ]:\n        return Book.model_validate(\n            {\n                k: v if not isinstance(v, RepeatedComposite) else list(v)\n                for k, v in tool_calls[0].args.items()\n            }\n        )\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from groq import Groq\nfrom pydantic import BaseModel\n\nclient = Groq()\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    completion = client.chat.completions.create(\n        model=\"llama-3.1-70b-versatile\",\n        messages=[{\"role\": \"user\", \"content\": f\"Extract {text}\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"Book\",\n                    \"description\": \"An extracted book.\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"author\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"title\", \"author\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n        tool_choice=\"required\",\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        return Book.model_validate_json(tool_calls[0].function.arguments)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from cohere import Client\nfrom cohere.types import Tool, ToolParameterDefinitionsValue\nfrom pydantic import BaseModel\n\nclient = Client()\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    response = client.chat(\n        model=\"command-r-plus\",\n        message=f\"Extract {text}\",\n        tools=[\n            Tool(\n                name=\"Book\",\n                description=\"An extracted book.\",\n                parameter_definitions={\n                    \"title\": ToolParameterDefinitionsValue(\n                        description=None, type=\"string\", required=True\n                    ),\n                    \"author\": ToolParameterDefinitionsValue(\n                        description=None, type=\"string\", required=True\n                    ),\n                },\n            )\n        ],\n    )\n    if response.tool_calls:\n        return Book.model_validate(response.tool_calls[0].parameters)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from litellm import completion\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    response = completion(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Extract {text}\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"Book\",\n                    \"description\": \"An extracted book.\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"author\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"title\", \"author\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n        tool_choice=\"required\",\n    )\n    if tool_calls := response.choices[0].message.tool_calls:  # pyright: ignore [reportAttributeAccessIssue]\n        return Book.model_validate_json(tool_calls[0].function.arguments)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import (\n    ChatCompletionsToolDefinition,\n    ChatRequestMessage,\n    FunctionDefinition,\n)\nfrom azure.core.credentials import AzureKeyCredential\nfrom pydantic import BaseModel\n\nclient = ChatCompletionsClient(\n    endpoint=\"YOUR_ENDPOINT\", credential=AzureKeyCredential(\"YOUR_KEY\")\n)\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    completion = client.complete(\n        model=\"gpt-4o-mini\",\n        messages=[ChatRequestMessage({\"role\": \"user\", \"content\": f\"Extract {text}\"})],\n        tools=[\n            ChatCompletionsToolDefinition(\n                function=FunctionDefinition(\n                    name=\"Book\",\n                    description=\"An extracted book.\",\n                    parameters={\n                        \"properties\": {\n                            \"title\": {\"type\": \"string\"},\n                            \"author\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"title\", \"author\"],\n                        \"type\": \"object\",\n                    },\n                )\n            )\n        ],\n        tool_choices=\"required\",\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        return Book.model_validate_json(tool_calls[0].function.arguments)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from proto.marshal.collections import RepeatedComposite\nfrom pydantic import BaseModel\nfrom vertexai.generative_models import (\n    FunctionDeclaration,\n    GenerativeModel,\n    Tool,\n    ToolConfig,\n)\n\nmodel = GenerativeModel(\"gemini-1.5-flash\")\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    response = model.generate_content(\n        f\"Extract {text}\",\n        tools=[\n            Tool(\n                function_declarations=[\n                    FunctionDeclaration(\n                        **{\n                            \"name\": \"Book\",\n                            \"description\": \"An extracted book.\",\n                            \"parameters\": {\n                                \"properties\": {\n                                    \"title\": {\"type\": \"string\"},\n                                    \"author\": {\"type\": \"string\"},\n                                },\n                                \"required\": [\"title\", \"author\"],\n                                \"type\": \"object\",\n                            },\n                        }\n                    )\n                ]\n            )\n        ],\n        tool_config=ToolConfig(\n            function_calling_config=ToolConfig.FunctionCallingConfig(\n                mode=ToolConfig.FunctionCallingConfig.Mode.ANY,\n                allowed_function_names=[\"Book\"],\n            ),\n        ),\n    )\n    if tool_calls := [\n        part.function_call\n        for candidate in response.candidates  # pyright: ignore [reportAttributeAccessIssue]\n        for part in candidate.content.parts\n        if part.function_call.args\n    ]:\n        return Book.model_validate(\n            {\n                k: v if not isinstance(v, RepeatedComposite) else list(v)\n                for k, v in tool_calls[0].args.items()\n            }\n        )\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>import boto3\nfrom pydantic import BaseModel\n\nbedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\ndef extract_book(text: str) -&gt; Book:\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"Extract {text}\"}]}]\n    tool_config = {\n        \"tools\": [\n            {\n                \"toolSpec\": {\n                    \"name\": \"Book\",\n                    \"description\": \"An extracted book.\",\n                    \"inputSchema\": {\n                        \"json\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"title\": {\"type\": \"string\"},\n                                \"author\": {\"type\": \"string\"},\n                            },\n                            \"required\": [\"title\", \"author\"],\n                        }\n                    },\n                }\n            }\n        ],\n        \"toolChoice\": {\"type\": \"tool\", \"name\": \"Book\"},\n    }\n    response = bedrock_client.converse(\n        modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",\n        messages=messages,\n        toolConfig=tool_config,\n    )\n    output_message = response[\"output\"][\"message\"]\n    messages.append(output_message)\n    for content_piece in output_message[\"content\"]:\n        if \"toolUse\" in content_piece and content_piece[\"toolUse\"].get(\"input\"):\n            tool_input = content_piece[\"toolUse\"][\"input\"]\n            return Book.model_validate(tool_input)\n    raise ValueError(\"No tool call found\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>Notice how Mirascope makes generating structured outputs significantly simpler than the official SDKs. It also greatly reduces boilerplate and standardizes the interaction across all supported LLM providers.</p> <p>Tools By Default</p> <p>By default, <code>response_model</code> will use Tools under the hood, forcing to the LLM to call that specific tool and constructing the response model from the tool's arguments.</p> <p>We default to using tools because all supported providers support tools. You can also optionally set <code>json_mode=True</code> to use JSON Mode instead, which we cover in more detail below.</p>"},{"location":"learn/response_models/#accessing-original-call-response","title":"Accessing Original Call Response","text":"<p>Every <code>response_model</code> that uses a Pydantic <code>BaseModel</code> will automatically have the original <code>BaseCallResponse</code> instance accessible through the <code>_response</code> property:</p> <p>Mirascope</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import cast\n\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(openai.OpenAICallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(anthropic.AnthropicCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(mistral.MistralCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(gemini.GeminiCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(groq.GroqCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(cohere.CohereCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(litellm.OpenAICallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(azure.AzureCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(vertex.VertexCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(bedrock.BedrockCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import cast\n\nfrom mirascope.core import Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(openai.OpenAICallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(anthropic.AnthropicCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(mistral.MistralCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(gemini.GeminiCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(groq.GroqCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(cohere.CohereCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(litellm.OpenAICallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(azure.AzureCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(vertex.VertexCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(bedrock.BedrockCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import cast\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(openai.OpenAICallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import anthropic, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(anthropic.AnthropicCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import mistral, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(mistral.MistralCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import gemini, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(gemini.GeminiCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import groq, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(groq.GroqCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import cohere, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(cohere.CohereCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import litellm, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(litellm.OpenAICallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import azure, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(azure.AzureCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import prompt_template, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(vertex.VertexCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import bedrock, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(bedrock.BedrockCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import cast\n\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(openai.OpenAICallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import BaseMessageParam, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(anthropic.AnthropicCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import BaseMessageParam, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(mistral.MistralCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import BaseMessageParam, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(gemini.GeminiCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import BaseMessageParam, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(groq.GroqCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import BaseMessageParam, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(cohere.CohereCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import BaseMessageParam, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(litellm.OpenAICallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import BaseMessageParam, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(azure.AzureCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import BaseMessageParam, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(vertex.VertexCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import cast\n\nfrom mirascope.core import BaseMessageParam, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n\nresponse = cast(bedrock.BedrockCallResponse, book._response)  # pyright: ignore[reportAttributeAccessIssue]\nprint(response.model_dump())\n# &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre>"},{"location":"learn/response_models/#built-in-types","title":"Built-In Types","text":"<p>For cases where you want to extract just a single built-in type, Mirascope provides a shorthand:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; str:\n    return f\"Extract book titles from {texts}\"\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; str:\n    return f\"Extract book titles from {texts}\"\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; str:\n    return f\"Extract book titles from {texts}\"\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; str:\n    return f\"Extract book titles from {texts}\"\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; str:\n    return f\"Extract book titles from {texts}\"\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; str:\n    return f\"Extract book titles from {texts}\"\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; str:\n    return f\"Extract book titles from {texts}\"\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; str:\n    return f\"Extract book titles from {texts}\"\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; str:\n    return f\"Extract book titles from {texts}\"\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; str:\n    return f\"Extract book titles from {texts}\"\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract book titles from {texts}\")\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract book titles from {texts}\")\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract book titles from {texts}\")\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract book titles from {texts}\")\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract book titles from {texts}\")\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract book titles from {texts}\")\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract book titles from {texts}\")\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract book titles from {texts}\")\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract book titles from {texts}\")\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract book titles from {texts}\")\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", response_model=list[str])\n@prompt_template(\"Extract book titles from {texts}\")\ndef extract_book(texts: list[str]): ...\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=list[str])\n@prompt_template(\"Extract book titles from {texts}\")\ndef extract_book(texts: list[str]): ...\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\", response_model=list[str])\n@prompt_template(\"Extract book titles from {texts}\")\ndef extract_book(texts: list[str]): ...\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=list[str])\n@prompt_template(\"Extract book titles from {texts}\")\ndef extract_book(texts: list[str]): ...\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=list[str])\n@prompt_template(\"Extract book titles from {texts}\")\ndef extract_book(texts: list[str]): ...\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", response_model=list[str])\n@prompt_template(\"Extract book titles from {texts}\")\ndef extract_book(texts: list[str]): ...\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=list[str])\n@prompt_template(\"Extract book titles from {texts}\")\ndef extract_book(texts: list[str]): ...\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\", response_model=list[str])\n@prompt_template(\"Extract book titles from {texts}\")\ndef extract_book(texts: list[str]): ...\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=list[str])\n@prompt_template(\"Extract book titles from {texts}\")\ndef extract_book(texts: list[str]): ...\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=list[str])\n@prompt_template(\"Extract book titles from {texts}\")\ndef extract_book(texts: list[str]): ...\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract book titles from {texts}\")]\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract book titles from {texts}\")]\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract book titles from {texts}\")]\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract book titles from {texts}\")]\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract book titles from {texts}\")]\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract book titles from {texts}\")]\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract book titles from {texts}\")]\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract book titles from {texts}\")]\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract book titles from {texts}\")]\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=list[str])\ndef extract_book(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract book titles from {texts}\")]\n\n\nbook = extract_book(\n    [\n        \"The Name of the Wind by Patrick Rothfuss\",\n        \"Mistborn: The Final Empire by Brandon Sanderson\",\n    ]\n)\nprint(book)\n# Output: [\"The Name of the Wind\", \"Mistborn: The Final Empire\"]\n</code></pre> <p>Here, we are using <code>list[str]</code> as the <code>response_model</code>, which Mirascope handles without needing to define a full <code>BaseModel</code>. You could of course set <code>response_model=list[Book]</code> as well.</p> <p>Note that we have no way of attaching <code>BaseCallResponse</code> to built-in types, so using a Pydantic <code>BaseModel</code> is recommended if you anticipate needing access to the original call response.</p>"},{"location":"learn/response_models/#supported-field-types","title":"Supported Field Types","text":"<p>While Mirascope provides a consistent interface, type support varies among providers:</p> Type Anthropic Cohere Gemini Groq Mistral OpenAI str \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 int \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 float \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 bool \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 bytes \u2713\u2713 \u2713\u2713 -\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 list \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 set \u2713\u2713 \u2713\u2713 -- \u2713\u2713 \u2713\u2713 \u2713\u2713 tuple \u2713\u2713 \u2713\u2713 -\u2713 \u2713\u2713 \u2713\u2713 -\u2713 dict \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 -\u2713 Literal/Enum \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 BaseModel \u2713\u2713 -\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 \u2713\u2713 Nested ($def) \u2713\u2713 -- -- \u2713\u2713 \u2713\u2713 \u2713\u2713 <p>\u2713\u2713 : Fully Supported, -\u2713: Only JSON Mode Support, -- : Not supported</p>"},{"location":"learn/response_models/#validation-and-error-handling","title":"Validation and Error Handling","text":"<p>While <code>response_model</code> significantly improves output structure and validation, it's important to handle potential errors.</p> <p>Let's take a look at an example where we want to validate that all fields are uppercase:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import openai\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import anthropic\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import mistral\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import gemini\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import groq\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import cohere\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import litellm\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import azure\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import vertex\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import bedrock\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, openai\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, anthropic\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, mistral\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, gemini\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, groq\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, cohere\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, litellm\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, azure\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, vertex\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, bedrock\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import anthropic, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import mistral, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import gemini, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import groq, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import cohere, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import litellm, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import azure, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import prompt_template, vertex\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import bedrock, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, anthropic\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, mistral\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, gemini\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, groq\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, cohere\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, litellm\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, azure\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, vertex\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, bedrock\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    print(f\"Error: {str(e)}\")\n    # Error: 2 validation errors for Book\n    # title\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    # author\n    #   Assertion failed, Field must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    #     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n</code></pre> <p>Without additional prompt engineering, this call will fail every single time. It's important to engineer your prompts to reduce errors, but LLMs are far from perfect, so always remember to catch and handle validation errors gracefully.</p> <p>We highly recommend taking a look at our section on retries to learn more about automatically retrying and re-inserting validation errors, which enables retrying the call such that the LLM can learn from its previous mistakes.</p>"},{"location":"learn/response_models/#accessing-original-call-response-on-error","title":"Accessing Original Call Response On Error","text":"<p>In case of a <code>ValidationError</code>, you can access the original response for debugging:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import openai\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(openai.OpenAICallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import anthropic\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(anthropic.AnthropicCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import mistral\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(mistral.MistralCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import gemini\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(gemini.GeminiCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import groq\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(groq.GroqCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import cohere\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(cohere.CohereCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import litellm\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(litellm.OpenAICallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import azure\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(azure.AzureCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import vertex\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(vertex.VertexCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import bedrock\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(bedrock.BedrockCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, openai\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(openai.OpenAICallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, anthropic\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(anthropic.AnthropicCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, mistral\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(mistral.MistralCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, gemini\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(gemini.GeminiCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, groq\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(groq.GroqCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, cohere\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(cohere.CohereCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, litellm\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(litellm.OpenAICallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, azure\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(azure.AzureCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, vertex\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(vertex.VertexCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import Messages, bedrock\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(bedrock.BedrockCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(openai.OpenAICallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import anthropic, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(anthropic.AnthropicCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import mistral, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(mistral.MistralCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import gemini, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(gemini.GeminiCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import groq, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(groq.GroqCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import cohere, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(cohere.CohereCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import litellm, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(litellm.OpenAICallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import azure, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(azure.AzureCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import prompt_template, vertex\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(vertex.VertexCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import bedrock, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(bedrock.BedrockCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(openai.OpenAICallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, anthropic\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(anthropic.AnthropicCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, mistral\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(mistral.MistralCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, gemini\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(gemini.GeminiCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, groq\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(groq.GroqCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, cohere\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@cohere.call(\"command-r-plus\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(cohere.CohereCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, litellm\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(litellm.OpenAICallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, azure\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(azure.AzureCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, vertex\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(vertex.VertexCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <pre><code>from typing import Annotated, cast\n\nfrom mirascope.core import BaseMessageParam, bedrock\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\ndef validate_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Field must be uppercase\"\n    return v\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: Annotated[str, AfterValidator(validate_upper)]\n    author: Annotated[str, AfterValidator(validate_upper)]\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\ntry:\n    book = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\n    print(book)\n    # Output: title='The Name of the Wind' author='Patrick Rothfuss'\nexcept ValidationError as e:\n    response = cast(bedrock.BedrockCallResponse, e._response)  # pyright: ignore[reportAttributeAccessIssue]\n    print(response.model_dump())\n    # &gt; {'metadata': {}, 'response': {'id': ...}, ...}\n</code></pre> <p>This allows you to gracefully handle errors as well as inspect the original LLM response when validation fails.</p>"},{"location":"learn/response_models/#json-mode","title":"JSON Mode","text":"<p>By default, <code>response_model</code> uses Tools under the hood. You can instead use JSON Mode in conjunction with <code>response_model</code> by setting <code>json_mode=True</code>:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, json_mode=True\n)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, json_mode=True\n)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, json_mode=True\n)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    \"\"\"An extracted book.\"\"\"\n\n    title: str\n    author: str\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, json_mode=True\n)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n# Output: title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre>"},{"location":"learn/response_models/#few-shot-examples","title":"Few-Shot Examples","text":"<p>Adding few-shot examples to your response model can improve results by demonstrating exactly how to adhere to your desired output.</p> <p>We take advantage of Pydantic's <code>Field</code> and <code>ConfigDict</code> to add these examples to response models:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}. Match example format excluding 'examples' key.\"\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}. Match example format excluding 'examples' key.\"\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}. Match example format excluding 'examples' key.\"\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code>from mirascope.core import groq\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}. Match example format excluding 'examples' key.\"\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}. Match example format excluding 'examples' key.\"\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}. Match example format excluding 'examples' key.\"\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import azure\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}. Match example format excluding 'examples' key.\"\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, json_mode=True\n)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}. Match example format excluding 'examples' key.\"\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        f\"Extract {text}. Match example format excluding 'examples' key.\"\n    )\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        f\"Extract {text}. Match example format excluding 'examples' key.\"\n    )\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        f\"Extract {text}. Match example format excluding 'examples' key.\"\n    )\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        f\"Extract {text}. Match example format excluding 'examples' key.\"\n    )\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        f\"Extract {text}. Match example format excluding 'examples' key.\"\n    )\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        f\"Extract {text}. Match example format excluding 'examples' key.\"\n    )\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        f\"Extract {text}. Match example format excluding 'examples' key.\"\n    )\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, json_mode=True\n)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(\n        f\"Extract {text}. Match example format excluding 'examples' key.\"\n    )\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}. Match example format excluding 'examples' key.\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}. Match example format excluding 'examples' key.\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}. Match example format excluding 'examples' key.\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}. Match example format excluding 'examples' key.\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}. Match example format excluding 'examples' key.\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}. Match example format excluding 'examples' key.\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\n@prompt_template(\"Extract {text}. Match example format excluding 'examples' key.\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, json_mode=True\n)\n@prompt_template(\"Extract {text}. Match example format excluding 'examples' key.\")\ndef extract_book(text: str): ...\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Extract {text}. Match example format excluding 'examples' key.\",\n        )\n    ]\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Extract {text}. Match example format excluding 'examples' key.\",\n        )\n    ]\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Extract {text}. Match example format excluding 'examples' key.\",\n        )\n    ]\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Extract {text}. Match example format excluding 'examples' key.\",\n        )\n    ]\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Extract {text}. Match example format excluding 'examples' key.\",\n        )\n    ]\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Extract {text}. Match example format excluding 'examples' key.\",\n        )\n    ]\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Extract {text}. Match example format excluding 'examples' key.\",\n        )\n    ]\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom pydantic import BaseModel, ConfigDict, Field\n\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, json_mode=True\n)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\",\n            content=f\"Extract {text}. Match example format excluding 'examples' key.\",\n        )\n    ]\n\n\nbook = extract_book(\"The Way of Kings by Brandon Sanderson\")\nprint(book)\n# Output: title='THE NAME OF THE WIND' author='Rothfuss, Patrick'\n</code></pre>"},{"location":"learn/response_models/#streaming-response-models","title":"Streaming Response Models","text":"<p>If you set <code>stream=True</code> when <code>response_model</code> is set, your LLM call will return an <code>Iterable</code> where each item will be a partial version of your response model representing the current state of the streamed information. The final model returned by the iterator will be the full response model.</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, stream=True\n)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract {text}\"\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, stream=True\n)\ndef extract_book(text: str) -&gt; Messages.Type:\n    return Messages.User(f\"Extract {text}\")\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, stream=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, stream=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, stream=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book, stream=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, stream=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, stream=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, stream=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, stream=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book, stream=True)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, stream=True\n)\n@prompt_template(\"Extract {text}\")\ndef extract_book(text: str): ...\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@cohere.call(\"command-r-plus\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Book, stream=True)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Book, stream=True\n)\ndef extract_book(text: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Extract {text}\")]\n\n\nbook_stream = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nfor partial_book in book_stream:\n    print(partial_book)\n# Output:\n# title=None author=None\n# title='' author=None\n# title='The' author=None\n# title='The Name' author=None\n# title='The Name of' author=None\n# title='The Name of the' author=None\n# title='The Name of the Wind' author=None\n# title='The Name of the Wind' author=''\n# title='The Name of the Wind' author='Patrick'\n# title='The Name of the Wind' author='Patrick Roth'\n# title='The Name of the Wind' author='Patrick Rothf'\n# title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>Once exhausted, you can access the final, full response model through the <code>constructed_response_model</code> property of the structured stream. Note that this will also give you access to the <code>._response</code> property that every <code>BaseModel</code> receives.</p> <p>You can also use the <code>stream</code> property to access the <code>BaseStream</code> instance and all of it's properties.</p>"},{"location":"learn/response_models/#fromcallargs","title":"FromCallArgs","text":"<p>Fields annotated with <code>FromCallArgs</code> will be populated with the corresponding argument from the function call rather than expecting it from the LLM's response. This enables seamless validation of LLM outputs against function inputs:</p> <p>Mirascope</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, openai\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; str:\n    return f\"Extract the books from these texts: {texts}\"\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, anthropic\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; str:\n    return f\"Extract the books from these texts: {texts}\"\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, mistral\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; str:\n    return f\"Extract the books from these texts: {texts}\"\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, gemini\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; str:\n    return f\"Extract the books from these texts: {texts}\"\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, groq\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; str:\n    return f\"Extract the books from these texts: {texts}\"\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, cohere\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@cohere.call(\"command-r-plus\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; str:\n    return f\"Extract the books from these texts: {texts}\"\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, litellm\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; str:\n    return f\"Extract the books from these texts: {texts}\"\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, azure\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; str:\n    return f\"Extract the books from these texts: {texts}\"\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, vertex\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; str:\n    return f\"Extract the books from these texts: {texts}\"\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, bedrock\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; str:\n    return f\"Extract the books from these texts: {texts}\"\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, Messages, openai\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract the books from these texts: {texts}\")\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, Messages, anthropic\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract the books from these texts: {texts}\")\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, Messages, mistral\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract the books from these texts: {texts}\")\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, Messages, gemini\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract the books from these texts: {texts}\")\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, Messages, groq\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract the books from these texts: {texts}\")\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, Messages, cohere\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@cohere.call(\"command-r-plus\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract the books from these texts: {texts}\")\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, Messages, litellm\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract the books from these texts: {texts}\")\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, Messages, azure\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract the books from these texts: {texts}\")\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, Messages, vertex\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract the books from these texts: {texts}\")\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, Messages, bedrock\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Extract the books from these texts: {texts}\")\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, openai, prompt_template\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Books)\n@prompt_template(\"Extract the books from these texts: {texts}\")\ndef extract_books(texts: list[str]): ...\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, anthropic, prompt_template\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Books)\n@prompt_template(\"Extract the books from these texts: {texts}\")\ndef extract_books(texts: list[str]): ...\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, mistral, prompt_template\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Books)\n@prompt_template(\"Extract the books from these texts: {texts}\")\ndef extract_books(texts: list[str]): ...\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, gemini, prompt_template\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Books)\n@prompt_template(\"Extract the books from these texts: {texts}\")\ndef extract_books(texts: list[str]): ...\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, groq, prompt_template\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Books)\n@prompt_template(\"Extract the books from these texts: {texts}\")\ndef extract_books(texts: list[str]): ...\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, cohere, prompt_template\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@cohere.call(\"command-r-plus\", response_model=Books)\n@prompt_template(\"Extract the books from these texts: {texts}\")\ndef extract_books(texts: list[str]): ...\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, litellm, prompt_template\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Books)\n@prompt_template(\"Extract the books from these texts: {texts}\")\ndef extract_books(texts: list[str]): ...\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, azure, prompt_template\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Books)\n@prompt_template(\"Extract the books from these texts: {texts}\")\ndef extract_books(texts: list[str]): ...\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, prompt_template, vertex\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Books)\n@prompt_template(\"Extract the books from these texts: {texts}\")\ndef extract_books(texts: list[str]): ...\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import FromCallArgs, bedrock, prompt_template\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Books)\n@prompt_template(\"Extract the books from these texts: {texts}\")\ndef extract_books(texts: list[str]): ...\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, FromCallArgs, openai\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Extract the books from these texts: {texts}\"\n        )\n    ]\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, FromCallArgs, anthropic\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Extract the books from these texts: {texts}\"\n        )\n    ]\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, FromCallArgs, mistral\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@mistral.call(\"mistral-large-latest\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Extract the books from these texts: {texts}\"\n        )\n    ]\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, FromCallArgs, gemini\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@gemini.call(\"gemini-1.5-flash\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Extract the books from these texts: {texts}\"\n        )\n    ]\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, FromCallArgs, groq\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@groq.call(\"llama-3.1-70b-versatile\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Extract the books from these texts: {texts}\"\n        )\n    ]\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, FromCallArgs, cohere\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@cohere.call(\"command-r-plus\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Extract the books from these texts: {texts}\"\n        )\n    ]\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, FromCallArgs, litellm\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@litellm.call(\"gpt-4o-mini\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Extract the books from these texts: {texts}\"\n        )\n    ]\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, FromCallArgs, azure\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@azure.call(\"gpt-4o-mini\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Extract the books from these texts: {texts}\"\n        )\n    ]\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, FromCallArgs, vertex\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@vertex.call(\"gemini-1.5-flash\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Extract the books from these texts: {texts}\"\n        )\n    ]\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, FromCallArgs, bedrock\nfrom pydantic import BaseModel, model_validator\nfrom typing_extensions import Self\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\nclass Books(BaseModel):\n    texts: Annotated[list[str], FromCallArgs()]\n    books: list[Book]\n\n    @model_validator(mode=\"after\")\n    def validate_output_length(self) -&gt; Self:\n        if len(self.texts) != len(self.books):\n            raise ValueError(\"length mismatch...\")\n        return self\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", response_model=Books)\ndef extract_books(texts: list[str]) -&gt; list[BaseMessageParam]:\n    return [\n        BaseMessageParam(\n            role=\"user\", content=f\"Extract the books from these texts: {texts}\"\n        )\n    ]\n\n\ntexts = [\n    \"The Name of the Wind by Patrick Rothfuss\",\n    \"Mistborn: The Final Empire by Brandon Sanderson\",\n]\nprint(extract_books(texts))\n# Output: texts=['The Name of the Wind by Patrick Rothfuss', 'Mistborn: The Final Empire by Brandon Sanderson'] books=[Book(title='The Name of the Wind', author='Patrick Rothfuss'), Book(title='Mistborn: The Final Empire', author='Brandon Sanderson')]\n</code></pre>"},{"location":"learn/response_models/#next-steps","title":"Next Steps","text":"<p>By following these best practices and leveraging Response Models effectively, you can create more robust, type-safe, and maintainable LLM-powered applications with Mirascope.</p> <p>Next, we recommend taking a lookg at one of:</p> <ul> <li>JSON Mode to see an alternate way to generate structured outputs where using Pydantic to validate outputs is optional.</li> <li>Evals to see how to use <code>response_model</code> to evaluate your prompts.</li> </ul>"},{"location":"learn/retries/","title":"Retries","text":"<p>Making an API call to a provider can fail due to various reasons, such as rate limits, internal server errors, validation errors, and more. This makes retrying calls extremely important when building robust systems.</p> <p>Mirascope combined with Tenacity increases the chance for these requests to succeed while maintaining end user transparency.</p> <p>You can install the necessary packages directly or use the <code>tenacity</code> extras flag:</p> <pre><code>pip install \"mirascope[tenacity]\"\n</code></pre>"},{"location":"learn/retries/#calls","title":"Calls","text":"<p>Let's take a look at a basic Mirascope call that retries with exponential back-off:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import gemini\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import groq\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import azure\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@mistral.call(\"mistral-large-latest\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@cohere.call(\"command-r-plus\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@azure.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <p>Ideally the call to <code>recommend_book</code> will succeed on the first attempt, but now the API call will be made again after waiting should it fail.</p> <p>The call will then throw a <code>RetryError</code> after 3 attempts if unsuccessful. This error should be caught and handled.</p>"},{"location":"learn/retries/#streams","title":"Streams","text":"<p>When streaming, the generator is not actually run until you start iterating. This means the initial API call may be successful but fail during the actual iteration through the stream.</p> <p>Instead, you need to wrap your call and add retries to this wrapper:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import gemini\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import groq\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import azure\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@cohere.call(\"command-r-plus\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef stream():\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\n\n\nstream()\n</code></pre>"},{"location":"learn/retries/#tools","title":"Tools","text":"<p>When using tools, <code>ValidationError</code> errors won't happen until you attempt to construct the tool (either when calling <code>response.tools</code> or iterating through a stream with tools).</p> <p>You need to handle retries in this case the same way as streams:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import anthropic\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import gemini\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import groq\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import cohere\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import litellm\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import azure\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import Messages, groq\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import Messages, azure\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\ndef run():\n    response = identify_author(\"The Name of the Wind\")\n    if tool := response.tool:\n        print(tool.call())\n        print(f\"Original tool call: {tool.tool_call}\")\n    else:\n        print(response.content)\n\n\nrun()\n</code></pre>"},{"location":"learn/retries/#error-reinsertion","title":"Error Reinsertion","text":"<p>Every example above simply retries after a failed attempt without making any updates to the call. This approach can be sufficient for some use-cases where we can safely expect the call to succeed on subsequent attempts (e.g. rate limits).</p> <p>However, there are some cases where the LLM is likely to make the same mistake over and over again. For example, when using tools or response models, the LLM may return incorrect or missing arguments where it's highly likely the LLM will continuously make the same mistake on subsequent calls. In these cases, it's important that we update subsequent calls based on resulting errors to improve the chance of success on the next call.</p> <p>To make it easier to make such updates, Mirascope provides a <code>collect_errors</code> handler that can collect any errors of your choice and insert them into subsequent calls through an <code>errors</code> keyword argument.</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import openai\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@openai.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(book: str, *, errors: list[ValidationError] | None = None) -&gt; str:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        return f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    return f\"Who wrote {book}?\"\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import anthropic\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@anthropic.call(\n    \"claude-3-5-sonnet-20240620\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(book: str, *, errors: list[ValidationError] | None = None) -&gt; str:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        return f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    return f\"Who wrote {book}?\"\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import mistral\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@mistral.call(\n    \"mistral-large-latest\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(book: str, *, errors: list[ValidationError] | None = None) -&gt; str:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        return f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    return f\"Who wrote {book}?\"\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import gemini\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@gemini.call(\n    \"gemini-1.5-flash\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(book: str, *, errors: list[ValidationError] | None = None) -&gt; str:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        return f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    return f\"Who wrote {book}?\"\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import groq\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@groq.call(\n    \"llama-3.1-70b-versatile\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(book: str, *, errors: list[ValidationError] | None = None) -&gt; str:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        return f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    return f\"Who wrote {book}?\"\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import cohere\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@cohere.call(\n    \"command-r-plus\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(book: str, *, errors: list[ValidationError] | None = None) -&gt; str:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        return f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    return f\"Who wrote {book}?\"\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import litellm\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@litellm.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(book: str, *, errors: list[ValidationError] | None = None) -&gt; str:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        return f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    return f\"Who wrote {book}?\"\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import azure\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@azure.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(book: str, *, errors: list[ValidationError] | None = None) -&gt; str:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        return f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    return f\"Who wrote {book}?\"\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import vertex\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@vertex.call(\n    \"gemini-1.5-flash\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(book: str, *, errors: list[ValidationError] | None = None) -&gt; str:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        return f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    return f\"Who wrote {book}?\"\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import bedrock\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(book: str, *, errors: list[ValidationError] | None = None) -&gt; str:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        return f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    return f\"Who wrote {book}?\"\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, openai\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@openai.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; Messages.Type:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return Messages.User(content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, anthropic\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@anthropic.call(\n    \"claude-3-5-sonnet-20240620\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; Messages.Type:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return Messages.User(content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, mistral\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@mistral.call(\n    \"mistral-large-latest\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; Messages.Type:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return Messages.User(content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, gemini\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@gemini.call(\n    \"gemini-1.5-flash\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; Messages.Type:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return Messages.User(content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, groq\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@groq.call(\n    \"llama-3.1-70b-versatile\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; Messages.Type:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return Messages.User(content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, cohere\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@cohere.call(\n    \"command-r-plus\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; Messages.Type:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return Messages.User(content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, litellm\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@litellm.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; Messages.Type:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return Messages.User(content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, azure\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@azure.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; Messages.Type:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return Messages.User(content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, vertex\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@vertex.call(\n    \"gemini-1.5-flash\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; Messages.Type:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return Messages.User(content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, bedrock\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; Messages.Type:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return Messages.User(content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseDynamicConfig, openai, prompt_template\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@openai.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Who wrote {book}?\n    \"\"\"\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; BaseDynamicConfig:\n    previous_errors = None\n    if errors:\n        previous_errors = f\"Previous Errors: {errors}\"\n        print(previous_errors)\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseDynamicConfig, anthropic, prompt_template\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@anthropic.call(\n    \"claude-3-5-sonnet-20240620\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Who wrote {book}?\n    \"\"\"\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; BaseDynamicConfig:\n    previous_errors = None\n    if errors:\n        previous_errors = f\"Previous Errors: {errors}\"\n        print(previous_errors)\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseDynamicConfig, mistral, prompt_template\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@mistral.call(\n    \"mistral-large-latest\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Who wrote {book}?\n    \"\"\"\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; BaseDynamicConfig:\n    previous_errors = None\n    if errors:\n        previous_errors = f\"Previous Errors: {errors}\"\n        print(previous_errors)\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseDynamicConfig, gemini, prompt_template\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@gemini.call(\n    \"gemini-1.5-flash\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Who wrote {book}?\n    \"\"\"\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; BaseDynamicConfig:\n    previous_errors = None\n    if errors:\n        previous_errors = f\"Previous Errors: {errors}\"\n        print(previous_errors)\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseDynamicConfig, groq, prompt_template\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@groq.call(\n    \"llama-3.1-70b-versatile\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Who wrote {book}?\n    \"\"\"\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; BaseDynamicConfig:\n    previous_errors = None\n    if errors:\n        previous_errors = f\"Previous Errors: {errors}\"\n        print(previous_errors)\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseDynamicConfig, cohere, prompt_template\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@cohere.call(\n    \"command-r-plus\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Who wrote {book}?\n    \"\"\"\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; BaseDynamicConfig:\n    previous_errors = None\n    if errors:\n        previous_errors = f\"Previous Errors: {errors}\"\n        print(previous_errors)\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseDynamicConfig, litellm, prompt_template\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@litellm.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Who wrote {book}?\n    \"\"\"\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; BaseDynamicConfig:\n    previous_errors = None\n    if errors:\n        previous_errors = f\"Previous Errors: {errors}\"\n        print(previous_errors)\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseDynamicConfig, azure, prompt_template\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@azure.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Who wrote {book}?\n    \"\"\"\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; BaseDynamicConfig:\n    previous_errors = None\n    if errors:\n        previous_errors = f\"Previous Errors: {errors}\"\n        print(previous_errors)\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseDynamicConfig, prompt_template, vertex\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@vertex.call(\n    \"gemini-1.5-flash\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Who wrote {book}?\n    \"\"\"\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; BaseDynamicConfig:\n    previous_errors = None\n    if errors:\n        previous_errors = f\"Previous Errors: {errors}\"\n        print(previous_errors)\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseDynamicConfig, bedrock, prompt_template\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Who wrote {book}?\n    \"\"\"\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; BaseDynamicConfig:\n    previous_errors = None\n    if errors:\n        previous_errors = f\"Previous Errors: {errors}\"\n        print(previous_errors)\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, openai\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@openai.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; list[BaseMessageParam]:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return [BaseMessageParam(role=\"user\", content=content)]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, anthropic\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@anthropic.call(\n    \"claude-3-5-sonnet-20240620\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; list[BaseMessageParam]:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return [BaseMessageParam(role=\"user\", content=content)]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, mistral\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@mistral.call(\n    \"mistral-large-latest\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; list[BaseMessageParam]:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return [BaseMessageParam(role=\"user\", content=content)]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, gemini\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@gemini.call(\n    \"gemini-1.5-flash\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; list[BaseMessageParam]:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return [BaseMessageParam(role=\"user\", content=content)]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, groq\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@groq.call(\n    \"llama-3.1-70b-versatile\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; list[BaseMessageParam]:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return [BaseMessageParam(role=\"user\", content=content)]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, cohere\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@cohere.call(\n    \"command-r-plus\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; list[BaseMessageParam]:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return [BaseMessageParam(role=\"user\", content=content)]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, litellm\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@litellm.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; list[BaseMessageParam]:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return [BaseMessageParam(role=\"user\", content=content)]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, azure\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@azure.call(\n    \"gpt-4o-mini\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; list[BaseMessageParam]:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return [BaseMessageParam(role=\"user\", content=content)]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, vertex\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@vertex.call(\n    \"gemini-1.5-flash\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; list[BaseMessageParam]:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return [BaseMessageParam(role=\"user\", content=content)]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, bedrock\nfrom mirascope.retries.tenacity import collect_errors\nfrom pydantic import AfterValidator, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    response_model=Annotated[str, AfterValidator(is_upper)],  # pyright: ignore [reportArgumentType, reportCallIssue]\n)\ndef identify_author(\n    book: str, *, errors: list[ValidationError] | None = None\n) -&gt; list[BaseMessageParam]:\n    previous_errors = None\n    if errors:\n        print(previous_errors)\n        content = f\"Previous Error: {errors}\\n\\nWho wrote {book}?\"\n    else:\n        content = f\"Who wrote {book}?\"\n    return [BaseMessageParam(role=\"user\", content=content)]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n# Previous Errors: [1 validation error for str\n# value\n#   Assertion failed, Must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n#     For further information visit https://errors.pydantic.dev/2.7/v/assertion_error]\n# PATRICK ROTHFUSS\n</code></pre> <p>In this example the first attempt fails because the identified author is not all uppercase. The <code>ValidationError</code> is then reinserted into the subsequent call, which enables the model to learn from it's mistake and correct its error.</p> <p>Of course, we could always engineer a better prompt (i.e. ask for all caps), but even prompt engineering does not guarantee perfect results. The purpose of this example is to demonstrate the power of a feedback loop by reinserting errors to build more robust systems.</p>"},{"location":"learn/streams/","title":"Streams","text":"<p>     If you haven't already, we recommend first reading the section on Calls </p> <p>Streaming is a powerful feature when using LLMs that allows you to process chunks of an LLM response in real-time as they are generated. This can be particularly useful for long-running tasks, providing immediate feedback to users, or implementing more responsive applications.</p> Diagram illustrating standard vs. streaming responses <pre><code>sequenceDiagram\n    participant User\n    participant App\n    participant LLM\n\n    User-&gt;&gt;App: Request\n    App-&gt;&gt;LLM: Query\n    Note right of LLM: Standard Response\n    LLM--&gt;&gt;App: Complete Response\n    App--&gt;&gt;User: Display Result\n\n    User-&gt;&gt;App: Request\n    App-&gt;&gt;LLM: Query (Stream)\n    Note right of LLM: Streaming Response\n    loop For each chunk\n        LLM--&gt;&gt;App: Response Chunk\n        App--&gt;&gt;User: Display Chunk\n    end</code></pre> <p>This approach offers several benefits:</p> <ol> <li>Immediate feedback: Users can see responses as they're being generated, creating a more interactive experience.</li> <li>Reduced latency: For long responses, users don't have to wait for the entire generation to complete before seeing results.</li> <li>Incremental processing: Applications can process and act on partial results as they arrive.</li> <li>Efficient resource use: Memory usage can be optimized by processing chunks instead of storing the entire response.</li> <li>Early termination: If the desired information is found early in the response, processing can be stopped without waiting for the full generation.</li> </ol> API Documentation <p><code>mirascope.core.base.stream</code></p> <p><code>mirascope.core.openai.stream</code></p> <p><code>mirascope.core.anthropic.stream</code></p> <p><code>mirascope.core.mistral.stream</code></p> <p><code>mirascope.core.gemini.stream</code></p> <p><code>mirascope.core.groq.stream</code></p> <p><code>mirascope.core.cohere.stream</code></p> <p><code>mirascope.core.litellm.stream</code></p> <p><code>mirascope.core.azure.stream</code></p> <p><code>mirascope.core.vertex.stream</code></p> <p><code>mirascope.core.bedrock.stream</code></p>"},{"location":"learn/streams/#basic-usage-and-syntax","title":"Basic Usage and Syntax","text":"<p>To use streaming, simply set the <code>stream</code> parameter to <code>True</code> in your <code>call</code> decorator:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>In this example:</p> <ol> <li>We use the <code>call</code> decorator with <code>stream=True</code> to enable streaming.</li> <li>The <code>recommend_book</code> function now returns a generator that yields <code>(chunk, tool)</code> tuples of the response.</li> <li>We iterate over the chunks, printing each one as it's received.</li> <li>We use <code>end=\"\"</code> and <code>flush=True</code> parameters in the print function to ensure that the output is displayed in real-time without line breaks.</li> </ol>"},{"location":"learn/streams/#handling-streamed-responses","title":"Handling Streamed Responses","text":"API Documentation <p><code>mirascope.core.base.call_response_chunk</code></p> <p><code>mirascope.core.openai.call_response_chunk</code></p> <p><code>mirascope.core.anthropic.call_response_chunk</code></p> <p><code>mirascope.core.mistral.call_response_chunk</code></p> <p><code>mirascope.core.gemini.call_response_chunk</code></p> <p><code>mirascope.core.groq.call_response_chunk</code></p> <p><code>mirascope.core.cohere.call_response_chunk</code></p> <p><code>mirascope.core.litellm.call_response_chunk</code></p> <p><code>mirascope.core.azure.call_response_chunk</code></p> <p><code>mirascope.core.vertex.call_response_chunk</code></p> <p><code>mirascope.core.bedrock.call_response_chunk</code></p> <p>When streaming, the initial response will be a provider-specific <code>BaseStream</code> instance (e.g. <code>OpenAIStream</code>), which is a generator that yields tuples <code>(chunk, tool)</code> where <code>chunk</code> is a provider-specific <code>BaseCallResponseChunk</code> (e.g. <code>OpenAICallResponseChunk</code>) that wraps the original chunk in the provider's response. These objects provide a consistent interface across providers while still allowing access to provider-specific details.</p> <p>Streaming Tools</p> <p>You'll notice in the above example that we ignore the <code>tool</code> in each tuple. If no tools are set in the call, then <code>tool</code> will always be <code>None</code> and can be safely ignored. For more details, check out the documentation on streaming tools</p>"},{"location":"learn/streams/#common-chunk-properties-and-methods","title":"Common Chunk Properties and Methods","text":"<p>All <code>BaseCallResponseChunk</code> objects share these common properties:</p> <ul> <li><code>content</code>: The main text content of the response. If no content is present, this will be the empty string.</li> <li><code>finish_reasons</code>: A list of reasons why the generation finished (e.g., \"stop\", \"length\"). These will be typed specifically for the provider used. If no finish reasons are present, this will be <code>None</code>.</li> <li><code>model</code>: The name of the model used for generation.</li> <li><code>id</code>: A unique identifier for the response if available. Otherwise this will be <code>None</code>.</li> <li><code>usage</code>: Information about token usage for the call if available. Otherwise this will be <code>None</code>.</li> <li><code>input_tokens</code>: The number of input tokens used if available. Otherwise this will be <code>None</code>.</li> <li><code>output_tokens</code>: The number of output tokens generated if available. Otherwise this will be <code>None</code>.</li> </ul>"},{"location":"learn/streams/#common-stream-properties-and-methods","title":"Common Stream Properties and Methods","text":"<p>Must Exhaust Stream</p> <p>To access these properties, you must first exhaust the stream by iterating through it.</p> <p>Once exhausted, all <code>BaseStream</code> objects share the same common properties and methods as <code>BaseCallResponse</code>, except for <code>usage</code>, <code>tools</code>, <code>tool</code>, and <code>__str__</code>.</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n</code></pre> <p>You can access the additional missing properties by using the method <code>construct_call_response</code> to reconstruct a provider-specific <code>BaseCallResponse</code> instance:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n\nprint(f\"Content: {stream.content}\")\n\ncall_response = stream.construct_call_response()\nprint(f\"Usage: {call_response.usage}\")\n</code></pre> <p>Reconstructed Response Limitations</p> <p>While we try our best to reconstruct the <code>BaseCallResponse</code> instance from the stream, there's always a chance that some information present in a standard call might be missing from the stream.</p>"},{"location":"learn/streams/#provider-specific-response-details","title":"Provider-Specific Response Details","text":"<p>While Mirascope provides a consistent interface, you can always access the full, provider-specific response object if needed. This is available through the <code>chunk</code> property of the <code>BaseCallResponseChunk</code> object:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(f\"Original chunk: {chunk.chunk}\")\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>Reasoning For Provider-Specific <code>BaseCallResponseChunk</code> Objects</p> <p>The reason that we have provider-specific response objects (e.g. <code>OpenAICallResponseChunk</code>) is to provide proper type hints and safety when accessing the original response chunk.</p>"},{"location":"learn/streams/#multi-modal-outputs","title":"Multi-Modal Outputs","text":"<p>While most LLM providers focus on text streaming, some providers support streaming additional output modalities like audio. The availability of multi-modal streaming varies among providers:</p> Provider Text Audio Image OpenAI \u2713 \u2713 - Anthropic \u2713 - - Mistral \u2713 - - Gemini \u2713 - - Groq \u2713 - - Cohere \u2713 - - LiteLLM \u2713 - - Azure AI \u2713 - - Vertex AI \u2713 - - <p>Legend: \u2713 (Supported), - (Not Supported)</p>"},{"location":"learn/streams/#audio-streaming","title":"Audio Streaming","text":"<p>For providers that support audio outputs, you can stream both text and audio responses simultaneously:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import io\n\n\nfrom pydub.playback import play\nfrom pydub import AudioSegment\n\nfrom mirascope.core import openai\n\nSAMPLE_WIDTH = 2\nFRAME_RATE = 24000\nCHANNELS = 1\n\n\n@openai.call(\n    \"gpt-4o-audio-preview\",\n    call_params={\n        \"audio\": {\"voice\": \"alloy\", \"format\": \"pcm16\"},\n        \"modalities\": [\"text\", \"audio\"],\n    },\n    stream=True,\n)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\naudio_chunk = b\"\"\naudio_transcript_chunk = \"\"\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    if chunk.audio:\n        audio_chunk += chunk.audio\n    if chunk.audio_transcript:\n        audio_transcript_chunk += chunk.audio_transcript\n\nprint(audio_transcript_chunk)\n\n\naudio_segment = AudioSegment.from_raw(\n    io.BytesIO(audio_chunk),\n    sample_width=SAMPLE_WIDTH,\n    frame_rate=FRAME_RATE,\n    channels=CHANNELS,\n)\nplay(audio_segment)\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import io\n\n\nfrom pydub.playback import play\nfrom pydub import AudioSegment\n\nfrom mirascope.core import openai, Messages\n\nSAMPLE_WIDTH = 2\nFRAME_RATE = 24000\nCHANNELS = 1\n\n\n@openai.call(\n    \"gpt-4o-audio-preview\",\n    call_params={\n        \"audio\": {\"voice\": \"alloy\", \"format\": \"pcm16\"},\n        \"modalities\": [\"text\", \"audio\"],\n    },\n    stream=True,\n)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\naudio_chunk = b\"\"\naudio_transcript_chunk = \"\"\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    if chunk.audio:\n        audio_chunk += chunk.audio\n    if chunk.audio_transcript:\n        audio_transcript_chunk += chunk.audio_transcript\n\nprint(audio_transcript_chunk)\n\n\naudio_segment = AudioSegment.from_raw(\n    io.BytesIO(audio_chunk),\n    sample_width=SAMPLE_WIDTH,\n    frame_rate=FRAME_RATE,\n    channels=CHANNELS,\n)\nplay(audio_segment)\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import io\n\n\nfrom pydub.playback import play\nfrom pydub import AudioSegment\n\nfrom mirascope.core import openai, prompt_template\n\nSAMPLE_WIDTH = 2\nFRAME_RATE = 24000\nCHANNELS = 1\n\n\n@openai.call(\n    \"gpt-4o-audio-preview\",\n    call_params={\n        \"audio\": {\"voice\": \"alloy\", \"format\": \"pcm16\"},\n        \"modalities\": [\"text\", \"audio\"],\n    },\n    stream=True,\n)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\naudio_chunk = b\"\"\naudio_transcript_chunk = \"\"\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    if chunk.audio:\n        audio_chunk += chunk.audio\n    if chunk.audio_transcript:\n        audio_transcript_chunk += chunk.audio_transcript\n\nprint(audio_transcript_chunk)\n\n\naudio_segment = AudioSegment.from_raw(\n    io.BytesIO(audio_chunk),\n    sample_width=SAMPLE_WIDTH,\n    frame_rate=FRAME_RATE,\n    channels=CHANNELS,\n)\nplay(audio_segment)\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import io\n\n\nfrom pydub.playback import play\nfrom pydub import AudioSegment\n\nfrom mirascope.core import openai, BaseMessageParam\n\nSAMPLE_WIDTH = 2\nFRAME_RATE = 24000\nCHANNELS = 1\n\n\n@openai.call(\n    \"gpt-4o-audio-preview\",\n    call_params={\n        \"audio\": {\"voice\": \"alloy\", \"format\": \"pcm16\"},\n        \"modalities\": [\"text\", \"audio\"],\n    },\n    stream=True,\n)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\naudio_chunk = b\"\"\naudio_transcript_chunk = \"\"\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    if chunk.audio:\n        audio_chunk += chunk.audio\n    if chunk.audio_transcript:\n        audio_transcript_chunk += chunk.audio_transcript\n\nprint(audio_transcript_chunk)\n\n\naudio_segment = AudioSegment.from_raw(\n    io.BytesIO(audio_chunk),\n    sample_width=SAMPLE_WIDTH,\n    frame_rate=FRAME_RATE,\n    channels=CHANNELS,\n)\nplay(audio_segment)\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <pre><code># Not supported\n</code></pre> <p>Each stream chunk provides access to:</p> <ul> <li><code>chunk.audio</code>: Raw audio data in bytes format</li> <li><code>chunk.audio_transcript</code>: The transcript of the audio</li> </ul> <p>This allows you to process both text and audio streams concurrently. Since audio data is received in chunks, you could technically begin playback before receiving the complete response.</p> <p>Audio Playback Requirements</p> <p>The example above uses <code>pydub</code> and <code>ffmpeg</code> for audio playback, but you can use any audio processing libraries or media players that can handle WAV format audio data. Choose the tools that best fit your needs and environment.</p> <p>If you decide to use pydub: - Install pydub: <code>pip install pydub</code> - Install ffmpeg: Available from ffmpeg.org or through system package managers</p> <p>Voice Options</p> <p>For providers that support audio outputs, refer to their documentation for available voice options and configurations:</p> <ul> <li>OpenAI: Text to Speech Guide</li> </ul>"},{"location":"learn/streams/#error-handling","title":"Error Handling","text":"<p>Error handling in streams is similar to standard non-streaming calls. However, it's important to note that errors may occur during iteration rather than at the initial function call:</p> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\nfrom openai import OpenAIError\n\n\n@openai.call(model=\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept OpenAIError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from anthropic import AnthropicError\nfrom mirascope.core import anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept AnthropicError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import mistral\nfrom mistralai import models\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept models.HTTPValidationError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle e.data: models.HTTPValidationErrorData\n    raise (e)\nexcept models.SDKError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle exception\n    raise (e)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\n@gemini.call(model=\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from groq import GroqError\nfrom mirascope.core import groq\n\n\n@groq.call(model=\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept GroqError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from cohere.errors import BadRequestError\nfrom mirascope.core import cohere\n\n\n@cohere.call(model=\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from litellm.exceptions import BadRequestError\nfrom mirascope.core import litellm\n\n\n@litellm.call(model=\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import azure\n\n\n@azure.call(model=\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\n@vertex.call(model=\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import bedrock\nfrom botocore.exceptions import ClientError\n\n\n@bedrock.call(model=\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept ClientError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\nfrom openai import OpenAIError\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept OpenAIError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from anthropic import AnthropicError\nfrom mirascope.core import Messages, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept AnthropicError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\nfrom mistralai import models\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept models.HTTPValidationError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle e.data: models.HTTPValidationErrorData\n    raise (e)\nexcept models.SDKError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle exception\n    raise (e)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from groq import GroqError\nfrom mirascope.core import Messages, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept GroqError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from cohere.errors import BadRequestError\nfrom mirascope.core import Messages, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from litellm.exceptions import BadRequestError\nfrom mirascope.core import Messages, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\nfrom botocore.exceptions import ClientError\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; Messages.Type:\n    return Messages.User(f\"Recommend a {genre} book\")\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept ClientError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\nfrom openai import OpenAIError\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept OpenAIError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from anthropic import AnthropicError\nfrom mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept AnthropicError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\nfrom mistralai import models\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept models.HTTPValidationError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle e.data: models.HTTPValidationErrorData\n    raise (e)\nexcept models.SDKError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle exception\n    raise (e)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from groq import GroqError\nfrom mirascope.core import groq, prompt_template\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept GroqError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from cohere.errors import BadRequestError\nfrom mirascope.core import cohere, prompt_template\n\n\n@cohere.call(\"command-r-plus\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from litellm.exceptions import BadRequestError\nfrom mirascope.core import litellm, prompt_template\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\nfrom botocore.exceptions import ClientError\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\n@prompt_template(\"Recommend a {genre} book\")\ndef recommend_book(genre: str): ...\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept ClientError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\nfrom openai import OpenAIError\n\n\n@openai.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept OpenAIError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from anthropic import AnthropicError\nfrom mirascope.core import BaseMessageParam, anthropic\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept AnthropicError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\nfrom mistralai import models\n\n\n@mistral.call(\"mistral-large-latest\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept models.HTTPValidationError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle e.data: models.HTTPValidationErrorData\n    raise (e)\nexcept models.SDKError as e:  # pyright: ignore [reportAttributeAccessIssue]\n    # handle exception\n    raise (e)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\n@gemini.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from groq import GroqError\nfrom mirascope.core import BaseMessageParam, groq\n\n\n@groq.call(\"llama-3.1-70b-versatile\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept GroqError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from cohere.errors import BadRequestError\nfrom mirascope.core import BaseMessageParam, cohere\n\n\n@cohere.call(\"command-r-plus\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from litellm.exceptions import BadRequestError\nfrom mirascope.core import BaseMessageParam, litellm\n\n\n@litellm.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept BadRequestError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\n@azure.call(\"gpt-4o-mini\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\n@vertex.call(\"gemini-1.5-flash\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\nfrom botocore.exceptions import ClientError\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", stream=True)\ndef recommend_book(genre: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Recommend a {genre} book\")]\n\n\ntry:\n    for chunk, _ in recommend_book(\"fantasy\"):\n        print(chunk.content, end=\"\", flush=True)\nexcept ClientError as e:\n    print(f\"Error: {str(e)}\")\n</code></pre> <p>In these examples, we wrap the iteration loop in a try/except block to catch any errors that might occur during streaming.</p> <p>When Errors Occur</p> <p>The initial response when calling an LLM function with <code>stream=True</code> will return a generator. Any errors that may occur during streaming will not happen until you actually iterate through the generator. This is why we wrap the generation loop in the try/except block and not just the call to <code>recommend_book</code>.</p>"},{"location":"learn/streams/#next-steps","title":"Next Steps","text":"<p>By leveraging streaming effectively, you can create more responsive and efficient LLM-powered applications with Mirascope's streaming capabilities.</p> <p>Next, we recommend taking a look at the Streams documentation, which shows you how to break tasks down into smaller, more directed calls and chain them togethder.</p>"},{"location":"learn/tools/","title":"Tools","text":"<p>     If you haven't already, we recommend first reading the section on Calls </p> <p>Tools are user-defined functions that an LLM (Large Language Model) can ask the user to invoke on its behalf. This greatly enhances the capabilities of LLMs by enabling them to perform specific tasks, access external data, interact with other systems, and more.</p> <p>Mirascope enables defining tools in a provider-agnostic way, which can be used across all supported LLM providers without modification.</p> Diagram illustrating how tools are called <p>When an LLM decides to use a tool, it indicates the tool name and argument values in its response. It's important to note that the LLM doesn't actually execute the function; instead, you are responsible for calling the tool and (optionally) providing the output back to the LLM in a subsequent interaction. For more details on such iterative tool-use flows, check out the Tool Message Parameters section below as well as the section on Agents.</p> <pre><code>sequenceDiagram\n    participant YC as Your Code\n    participant LLM\n\n    YC-&gt;&gt;LLM: Call with prompt and function definitions\n    loop Tool Calls\n        LLM-&gt;&gt;LLM: Decide to respond or call functions\n        LLM-&gt;&gt;YC: Respond with function to call and arguments\n        YC-&gt;&gt;YC: Execute function with given arguments\n        YC-&gt;&gt;LLM: Call with prompt and function result\n    end\n    LLM-&gt;&gt;YC: Final response</code></pre>"},{"location":"learn/tools/#basic-usage-and-syntax","title":"Basic Usage and Syntax","text":"API Documentation <p><code>mirascope.core.base.tool</code></p> <p><code>mirascope.core.openai.tool</code></p> <p><code>mirascope.core.anthropic.tool</code></p> <p><code>mirascope.core.mistral.tool</code></p> <p><code>mirascope.core.gemini.tool</code></p> <p><code>mirascope.core.groq.tool</code></p> <p><code>mirascope.core.cohere.tool</code></p> <p><code>mirascope.core.litellm.tool</code></p> <p><code>mirascope.core.azure.tool</code></p> <p><code>mirascope.core.vertex.tool</code></p> <p><code>mirascope.core.bedrock.tool</code></p> <p>There are two ways of defining tools in Mirascope: <code>BaseTool</code> and functions.</p> <p>You can consider the functional definitions a shorthand form of writing the <code>BaseTool</code> version of the same tool. Under the hood, tools defined as functions will get converted automatically into their corresponding <code>BaseTool</code>.</p> <p>Let's take a look at a basic example of each using Mirascope vs. official provider SDKs:</p> <p>Mirascope</p> BaseToolFunction ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, Messages, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, openai, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, anthropic, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, mistral, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, gemini, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, groq, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, cohere, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, litellm, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, azure, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, prompt_template, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, bedrock, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, BaseTool, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: Patrick Rothfuss\nelse:\n    print(response.content)\n</code></pre> Official SDK OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>import json\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\ndef identify_author(book: str) -&gt; str:\n    completion = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Who wrote {book}\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"get_book_author\",\n                    \"description\": \"Returns the author of the book with the given title.\",\n                    \"parameters\": {\n                        \"properties\": {\"title\": {\"type\": \"string\"}},\n                        \"required\": [\"title\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        if tool_calls[0].function.name == \"get_book_author\":\n            return get_book_author(**json.loads(tool_calls[0].function.arguments))\n    return str(completion.choices[0].message.content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n</code></pre> <pre><code>from anthropic import Anthropic\n\nclient = Anthropic()\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\ndef identify_author(book: str) -&gt; str:\n    message = client.messages.create(\n        model=\"claude-3-5-sonnet-20240620\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": f\"Who wrote {book}?\"}],\n        tools=[\n            {\n                \"name\": \"get_book_author\",\n                \"description\": \"Returns the author of the book with the given title.\",\n                \"input_schema\": {\n                    \"properties\": {\"title\": {\"type\": \"string\"}},\n                    \"required\": [\"title\"],\n                    \"type\": \"object\",\n                },\n            }\n        ],\n    )\n    content = \"\"\n    for block in message.content:\n        if block.type == \"tool_use\":\n            if block.name == \"get_book_author\":\n                return get_book_author(**block.input)  # pyright: ignore [reportCallIssue]\n        else:\n            content += block.text\n    return content\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n</code></pre> <pre><code>import json\n\nfrom mistralai.client import MistralClient\n\nclient = MistralClient()\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\ndef identify_author(book: str) -&gt; str:\n    completion = client.chat(\n        model=\"mistral-large-latest\",\n        messages=[{\"role\": \"user\", \"content\": f\"Who wrote {book}?\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"get_book_author\",\n                    \"description\": \"Returns the author of the book with the given title.\",\n                    \"parameters\": {\n                        \"properties\": {\"title\": {\"type\": \"string\"}},\n                        \"required\": [\"title\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        if tool_calls[0].function.name == \"get_book_author\":\n            return get_book_author(**json.loads(tool_calls[0].function.arguments))\n    return completion.choices[0].message.content\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n</code></pre> <pre><code>from google.generativeai import GenerativeModel\nfrom google.generativeai.types import FunctionDeclaration, Tool\n\nmodel = GenerativeModel(\"gemini-1.5-flash\")\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\ndef identify_author(book: str) -&gt; str:\n    response = model.generate_content(\n        f\"Who wrote {book}?\",\n        tools=[\n            Tool(\n                function_declarations=[\n                    FunctionDeclaration(\n                        **{\n                            \"name\": \"get_book_author\",\n                            \"description\": \"Returns the author of the book with the given title.\",\n                            \"parameters\": {\n                                \"properties\": {\"title\": {\"type\": \"string\"}},\n                                \"required\": [\"title\"],\n                                \"type\": \"object\",\n                            },\n                        }\n                    )\n                ]\n            )\n        ],\n    )\n    if tool_calls := [\n        part.function_call for part in response.parts if part.function_call.args\n    ]:\n        if tool_calls[0].name == \"get_book_author\":\n            return get_book_author(**dict(tool_calls[0].args.items()))  # pyright: ignore [reportArgumentType]\n    return response.text\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n</code></pre> <pre><code>import json\n\nfrom groq import Groq\n\nclient = Groq()\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\ndef identify_author(book: str) -&gt; str:\n    completion = client.chat.completions.create(\n        model=\"llama-3.1-70b-versatile\",\n        messages=[{\"role\": \"user\", \"content\": f\"Who wrote {book}?\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"get_book_author\",\n                    \"description\": \"Returns the author of the book with the given title.\",\n                    \"parameters\": {\n                        \"properties\": {\"title\": {\"type\": \"string\"}},\n                        \"required\": [\"title\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        if tool_calls[0].function.name == \"get_book_author\":\n            return get_book_author(**json.loads(tool_calls[0].function.arguments))\n    return str(completion.choices[0].message.content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n</code></pre> <pre><code>from typing import cast\n\nfrom cohere import Client\nfrom cohere.types import Tool, ToolParameterDefinitionsValue\n\nclient = Client()\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\ndef identify_author(book: str) -&gt; str:\n    response = client.chat(\n        model=\"command-r-plus\",\n        message=f\"Who wrote {book}?\",\n        tools=[\n            Tool(\n                name=\"get_book_author\",\n                description=\"Returns the author of the book with the given title.\",\n                parameter_definitions={\n                    \"title\": ToolParameterDefinitionsValue(\n                        description=None, type=\"string\", required=True\n                    )\n                },\n            )\n        ],\n    )\n    if tool_calls := response.tool_calls:\n        if tool_calls[0].name == \"get_book_author\":\n            return get_book_author(**(cast(dict[str, str], tool_calls[0].parameters)))\n    return response.text\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n</code></pre> <pre><code>import json\n\nfrom litellm import completion\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\ndef identify_author(book: str) -&gt; str:\n    response = completion(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": f\"Who wrote {book}\"}],\n        tools=[\n            {\n                \"function\": {\n                    \"name\": \"get_book_author\",\n                    \"description\": \"Returns the author of the book with the given title.\",\n                    \"parameters\": {\n                        \"properties\": {\"title\": {\"type\": \"string\"}},\n                        \"required\": [\"title\"],\n                        \"type\": \"object\",\n                    },\n                },\n                \"type\": \"function\",\n            }\n        ],\n    )\n    if tool_calls := response.choices[0].message.tool_calls:  # pyright: ignore [reportAttributeAccessIssue]\n        if tool_calls[0].function.name == \"get_book_author\":\n            return get_book_author(**json.loads(tool_calls[0].function.arguments))\n    return str(response.choices[0].message.content)  # pyright: ignore [reportAttributeAccessIssue]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n</code></pre> <pre><code>import json\n\nfrom azure.ai.inference import ChatCompletionsClient\nfrom azure.ai.inference.models import (\n    ChatCompletionsToolDefinition,\n    ChatRequestMessage,\n    FunctionDefinition,\n)\nfrom azure.core.credentials import AzureKeyCredential\n\nclient = ChatCompletionsClient(\n    endpoint=\"YOUR_ENDPOINT\", credential=AzureKeyCredential(\"YOUR_KEY\")\n)\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\ndef identify_author(book: str) -&gt; str:\n    completion = client.complete(\n        model=\"gpt-4o-mini\",\n        messages=[\n            ChatRequestMessage({\"role\": \"user\", \"content\": f\"Who wrote {book}?\"})\n        ],\n        tools=[\n            ChatCompletionsToolDefinition(\n                function=FunctionDefinition(\n                    name=\"get_book_author\",\n                    description=\"Returns the author of the book with the given title.\",\n                    parameters={\n                        \"properties\": {\"title\": {\"type\": \"string\"}},\n                        \"required\": [\"title\"],\n                        \"type\": \"object\",\n                    },\n                )\n            )\n        ],\n    )\n    if tool_calls := completion.choices[0].message.tool_calls:\n        if tool_calls[0].function.name == \"get_book_author\":\n            return get_book_author(**json.loads(tool_calls[0].function.arguments))\n    return str(completion.choices[0].message.content)\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n</code></pre> <pre><code>from vertexai.generative_models import FunctionDeclaration, GenerativeModel, Tool\n\nmodel = GenerativeModel(\"gemini-1.5-flash\")\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\ndef identify_author(book: str) -&gt; str:\n    response = model.generate_content(\n        f\"Who wrote {book}?\",\n        tools=[\n            Tool(\n                function_declarations=[\n                    FunctionDeclaration(\n                        **{\n                            \"name\": \"get_book_author\",\n                            \"description\": \"Returns the author of the book with the given title.\",\n                            \"parameters\": {\n                                \"properties\": {\"title\": {\"type\": \"string\"}},\n                                \"required\": [\"title\"],\n                                \"type\": \"object\",\n                            },\n                        }\n                    )\n                ]\n            )\n        ],\n    )\n    if tool_calls := [\n        part.function_call\n        for candidate in response.candidates  # pyright: ignore [reportAttributeAccessIssue]\n        for part in candidate.content.parts\n        if part.function_call.args\n    ]:\n        if tool_calls[0].name == \"get_book_author\":\n            return get_book_author(**dict(tool_calls[0].args.items()))  # pyright: ignore [reportArgumentType]\n    return response.text  # pyright: ignore [reportAttributeAccessIssue]\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n</code></pre> <pre><code>import boto3\n\nbedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\ndef identify_author(book: str) -&gt; str:\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": f\"Who wrote {book}?\"}]}]\n    tool_config = {\n        \"tools\": [\n            {\n                \"toolSpec\": {\n                    \"name\": \"get_book_author\",\n                    \"description\": \"Returns the author of the book with the given title.\",\n                    \"inputSchema\": {\n                        \"json\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"title\": {\n                                    \"type\": \"string\",\n                                    \"description\": \"The title of the book.\",\n                                }\n                            },\n                            \"required\": [\"title\"],\n                        }\n                    },\n                }\n            }\n        ]\n    }\n    response = bedrock_client.converse(\n        modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",\n        messages=messages,\n        toolConfig=tool_config,\n    )\n    content = \"\"\n    output_message = response[\"output\"][\"message\"]\n    messages.append(output_message)\n    stop_reason = response[\"stopReason\"]\n\n    if stop_reason == \"tool_use\":\n        tool_requests = output_message[\"content\"]\n        for tool_request in tool_requests:\n            if \"toolUse\" in tool_request:\n                tool = tool_request[\"toolUse\"]\n                if tool[\"name\"] == \"get_book_author\":\n                    return get_book_author(**tool[\"input\"])\n    for content_piece in output_message[\"content\"]:\n        if \"text\" in content_piece:\n            content += content_piece[\"text\"]\n    return content\n\n\nauthor = identify_author(\"The Name of the Wind\")\nprint(author)\n</code></pre> <p>In this example we:</p> <ol> <li>Define the <code>GetBookAuthor</code>/<code>get_book_author</code> tool (a dummy method for the example)</li> <li>Set the <code>tools</code> argument in the <code>call</code> decorator to give the LLM access to the tool.</li> <li>We call <code>identify_author</code>, which automatically generates the corresponding provider-specific tool schema under the hood.</li> <li>Check if the response from <code>identify_author</code> contains a tool, which is the <code>BaseTool</code> instance constructed from the underlying tool call<ul> <li>If yes, we call the constructed tool's <code>call</code> method and print its output. This calls the tool with the arguments provided by the LLM.</li> <li>If no, we print the content of the response (assuming no tool was called).</li> </ul> </li> </ol> <p>The core idea to understand here is that the LLM is asking us to call the tool on its behalf with arguments that it has provided. In the above example, the LLM chooses to call the tool to get the author rather than relying on its world knowledge.</p> <p>This is particularly important for buildling applications with access to live information and external systems.</p> <p>For the purposes of this example we are showing just a single tool call. Generally, you would then give the tool call's output back to the LLM and make another call so the LLM can generate a response based on the output of the tool. We cover this in more detail in the section on Agents</p>"},{"location":"learn/tools/#accessing-original-tool-call","title":"Accessing Original Tool Call","text":"<p>All provider-specific <code>BaseTool</code> instances have a <code>tool_call</code> property for accessing the original LLM tool call.</p> BaseToolFunction ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, Messages, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, openai, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, anthropic, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, mistral, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, gemini, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, groq, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, cohere, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, litellm, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, azure, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, prompt_template, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, bedrock, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, BaseTool, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\n    print(f\"Original tool call: {tool.tool_call}\")\nelse:\n    print(response.content)\n</code></pre> <p>Reasoning for provider-specific <code>BaseTool</code> objects</p> <p>The reason that we have provider-specific tools (e.g. <code>OpenAITool</code>) is to provide proper type hints and safety when accessing the original tool call.</p>"},{"location":"learn/tools/#supported-field-types","title":"Supported Field Types","text":"<p>While Mirascope provides a consistent interface, type support varies among providers:</p> Type Anthropic Cohere Gemini Groq Mistral OpenAI str \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 int \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 float \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 bool \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 bytes \u2713 \u2713 - \u2713 \u2713 \u2713 list \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 set \u2713 \u2713 - \u2713 \u2713 \u2713 tuple \u2713 \u2713 - \u2713 \u2713 - dict \u2713 \u2713 \u2713 \u2713 \u2713 - Literal/Enum \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 BaseModel \u2713 - \u2713 \u2713 \u2713 \u2713 Nested ($def) \u2713 - - \u2713 \u2713 \u2713 <p>Legend: \u2713 (Supported), - (Not Supported)</p> <p>Consider provider-specific capabilities when working with advanced type structures. Even for supported types, LLM outputs may sometimes be incorrect or of the wrong type. In such cases, prompt engineering or error handling (like retries and reinserting validation errors) may be necessary.</p>"},{"location":"learn/tools/#parallel-tool-calls","title":"Parallel Tool Calls","text":"<p>In certain cases the LLM will ask to call multiple tools in the same response. Mirascope makes calling all such tools simple:</p> BaseToolFunction ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, Messages, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, openai, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, anthropic, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, mistral, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, gemini, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, groq, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, cohere, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, litellm, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, azure, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, prompt_template, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, bedrock, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, BaseTool, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\n@prompt_template(\"Who wrote {books}?\")\ndef identify_authors(books: list[str]): ...\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nresponse = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nif tools := response.tools:\n    for tool in tools:\n        print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <p>If your tool calls are I/O-bound, it's often worth writing async tools so that you can run all of the tools calls in parallel for better efficiency.</p>"},{"location":"learn/tools/#streaming-tools","title":"Streaming Tools","text":"<p>Mirascope supports streaming responses with tools, which is useful for long-running tasks or real-time updates:</p> BaseToolFunction ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor], stream=True\n)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, Messages, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor], stream=True\n)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, openai, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, anthropic, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, mistral, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, gemini, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, groq, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, cohere, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, litellm, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, azure, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, prompt_template, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseTool, bedrock, prompt_template\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor], stream=True\n)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, BaseTool, openai\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, anthropic\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, mistral\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, gemini\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, groq\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, cohere\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, litellm\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, azure\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, vertex\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, bedrock\nfrom pydantic import Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor], stream=True\n)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author], stream=True\n)\ndef identify_authors(books: list[str]) -&gt; str:\n    return f\"Who wrote {books}?\"\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author], stream=True\n)\ndef identify_authors(books: list[str]) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {books}?\")\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author], stream=True)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author], stream=True\n)\n@prompt_template(\"Who wrote {book}?\")\ndef identify_authors(books: list[str]): ...\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author], stream=True)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:  # will always be None, not supported at the provider level\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\n    \"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author], stream=True\n)\ndef identify_authors(books: list[str]) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {books}?\")]\n\n\nstream = identify_authors([\"The Name of the Wind\", \"Mistborn: The Final Empire\"])\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>When are tools returned?</p> <p>When we identify that a tool is being streamed, we will internally reconstruct the tool from the streamed response. This means that the tool won't be returned until the full tool has been streamed and reconstructed on your behalf.</p> <p>Not all providers support streaming tools</p> <p>Currently only OpenAI, Anthropic, Mistral, and Groq support streaming tools. All other providers will always return <code>None</code> for tools.</p> <p>If you think we're missing any, let us know!</p>"},{"location":"learn/tools/#tool-message-parameters","title":"Tool Message Parameters","text":"<p>     Calling tools and inserting their outputs into subsequent LLM API calls in a loop in the most basic form of an agent. While we cover this briefly here, we recommend reading the section on Agents for more details and examples. </p> <p>Generally the next step after the LLM returns a tool call is for you to call the tool on its behalf and supply the output in a subsequent call.</p> <p>Let's take a look at a basic example of this:</p> BaseToolFunction ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, Messages, openai\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[openai.OpenAIMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, anthropic\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[anthropic.AnthropicMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, mistral\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[mistral.MistralMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, gemini\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[gemini.GeminiMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, groq\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str, history: list[groq.GroqMessageParam]) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, cohere\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[cohere.CohereMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, litellm\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[litellm.OpenAIMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, azure\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str, history: list[azure.AzureMessageParam]) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, vertex\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[vertex.VertexMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, bedrock\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[bedrock.BedrockMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, Messages, openai\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[openai.OpenAIMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, anthropic\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[anthropic.AnthropicMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, mistral\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[mistral.MistralMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, gemini\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[gemini.GeminiMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, groq\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str, history: list[groq.GroqMessageParam]) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, cohere\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[cohere.CohereMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, litellm\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[litellm.OpenAIMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, azure\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str, history: list[azure.AzureMessageParam]) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, vertex\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[vertex.VertexMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, bedrock\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[bedrock.BedrockMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, BaseTool, openai, prompt_template\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[openai.OpenAIMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseTool, anthropic, prompt_template\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[anthropic.AnthropicMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseTool, mistral, prompt_template\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[mistral.MistralMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseTool, gemini, prompt_template\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[gemini.GeminiMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseTool, groq, prompt_template\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[groq.GroqMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseTool, cohere, prompt_template\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[cohere.CohereMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseTool, litellm, prompt_template\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[litellm.OpenAIMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseTool, azure, prompt_template\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[azure.AzureMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseTool, prompt_template, vertex\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[vertex.VertexMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, BaseTool, bedrock, prompt_template\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[bedrock.BedrockMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, BaseTool, openai\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[openai.OpenAIMessageParam]\n) -&gt; list[openai.OpenAIMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, anthropic\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[anthropic.AnthropicMessageParam]\n) -&gt; list[anthropic.AnthropicMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, mistral\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[mistral.MistralMessageParam]\n) -&gt; list[mistral.MistralMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, gemini\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[gemini.GeminiMessageParam]\n) -&gt; list[gemini.GeminiMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, groq\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[groq.GroqMessageParam]\n) -&gt; list[groq.GroqMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, cohere\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[cohere.CohereMessageParam]\n) -&gt; list[cohere.CohereMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, litellm\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[litellm.OpenAIMessageParam]\n) -&gt; list[litellm.OpenAIMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, azure\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[azure.AzureMessageParam]\n) -&gt; list[azure.AzureMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, vertex\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[vertex.VertexMessageParam]\n) -&gt; list[vertex.VertexMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, bedrock\n\n\nclass GetBookAuthor(BaseTool):\n    title: str\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(\n    book: str, history: list[bedrock.BedrockMessageParam]\n) -&gt; list[bedrock.BedrockMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[openai.OpenAIMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[anthropic.AnthropicMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[mistral.MistralMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[gemini.GeminiMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str, history: list[groq.GroqMessageParam]) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[cohere.CohereMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[litellm.OpenAIMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str, history: list[azure.AzureMessageParam]) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[vertex.VertexMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[bedrock.BedrockMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[openai.OpenAIMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[anthropic.AnthropicMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[mistral.MistralMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[gemini.GeminiMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str, history: list[groq.GroqMessageParam]) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[cohere.CohereMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[litellm.OpenAIMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str, history: list[azure.AzureMessageParam]) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[vertex.VertexMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[bedrock.BedrockMessageParam]\n) -&gt; Messages.Type:\n    messages = [*history]\n    if book:\n        messages.append(Messages.User(f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseDynamicConfig, openai, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[openai.OpenAIMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, anthropic, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[anthropic.AnthropicMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, mistral, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[mistral.MistralMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, gemini, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[gemini.GeminiMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, groq, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[groq.GroqMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, cohere, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[cohere.CohereMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, litellm, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[litellm.OpenAIMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, azure, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[azure.AzureMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, prompt_template, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[vertex.VertexMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseDynamicConfig, bedrock, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER: {query}\n    \"\"\"\n)\ndef identify_author(\n    book: str, history: list[bedrock.BedrockMessageParam]\n) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"query\": f\"Who wrote {book}\" if book else \"\"}}\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[openai.OpenAIMessageParam]\n) -&gt; list[openai.OpenAIMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[anthropic.AnthropicMessageParam]\n) -&gt; list[anthropic.AnthropicMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[mistral.MistralMessageParam]\n) -&gt; list[mistral.MistralMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[gemini.GeminiMessageParam]\n) -&gt; list[gemini.GeminiMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[groq.GroqMessageParam]\n) -&gt; list[groq.GroqMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[cohere.CohereMessageParam]\n) -&gt; list[cohere.CohereMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[litellm.OpenAIMessageParam]\n) -&gt; list[litellm.OpenAIMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[azure.AzureMessageParam]\n) -&gt; list[azure.AzureMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[vertex.VertexMessageParam]\n) -&gt; list[vertex.VertexMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(\n    book: str, history: list[bedrock.BedrockMessageParam]\n) -&gt; list[bedrock.BedrockMessageParam]:\n    messages = [*history]\n    if book:\n        messages.append(BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\"))\n    return messages\n\n\nhistory = []\nresponse = identify_author(\"The Name of the Wind\", history)\nhistory += [response.user_message_param, response.message_param]\nwhile tool := response.tool:\n    tools_and_outputs = [(tool, tool.call())]\n    history += response.tool_message_params(tools_and_outputs)\n    response = identify_author(\"\", history)\n    history.append(response.message_param)\nprint(response.content)\n# Output: The Name of the Wind was written by Patrick Rothfuss.\n</code></pre> <p>In this example we:</p> <ol> <li>Add <code>history</code> to maintain the messages across multiple calls to the LLM.</li> <li>Loop until the response no longer has tools calls.</li> <li>While there are tool calls, call the tools, append their corresponding message parameters to the history, and make a subsequent call with an empty query and updated history. We use an empty query because the original user message is already included in the history.</li> <li>Print the final response content once the LLM is done calling tools.</li> </ol>"},{"location":"learn/tools/#validation-and-error-handling","title":"Validation and Error Handling","text":"<p>Since <code>BaseTool</code> is a subclass of Pydantic's <code>BaseModel</code>, they are validated on construction, so it's important that you handle potential <code>ValidationError</code>'s for building more robust applications:</p> BaseToolFunction ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, openai\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, anthropic\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, mistral\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, gemini\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, groq\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, cohere\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, litellm\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, azure\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, vertex\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, bedrock\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, Messages, openai\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, Messages, anthropic\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, Messages, mistral\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, Messages, gemini\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, Messages, groq\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, Messages, cohere\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, Messages, litellm\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, Messages, azure\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, Messages, vertex\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, Messages, bedrock\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, openai, prompt_template\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, anthropic, prompt_template\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, mistral, prompt_template\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, gemini, prompt_template\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, groq, prompt_template\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, cohere, prompt_template\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, litellm, prompt_template\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, azure, prompt_template\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, prompt_template, vertex\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseTool, bedrock, prompt_template\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, BaseTool, openai\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, BaseTool, anthropic\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, BaseTool, mistral\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, BaseTool, gemini\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, BaseTool, groq\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, BaseTool, cohere\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, BaseTool, litellm\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, BaseTool, azure\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, BaseTool, vertex\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, BaseTool, bedrock\nfrom pydantic import AfterValidator, Field, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: Annotated[str, AfterValidator(is_upper)] = Field(\n        ..., description=\"The title of the book.\"\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"THE NAME OF THE WIND\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"MISTBORN: THE FINAL EMPIRE\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import openai\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import anthropic\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import mistral\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import gemini\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import groq\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import cohere\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import litellm\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import azure\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import vertex\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import bedrock\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, openai\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, anthropic\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, mistral\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, gemini\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, groq\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, cohere\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, litellm\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, azure\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, vertex\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import Messages, bedrock\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import anthropic, prompt_template\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import mistral, prompt_template\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import gemini, prompt_template\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import groq, prompt_template\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import cohere, prompt_template\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import litellm, prompt_template\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import azure, prompt_template\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import prompt_template, vertex\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import bedrock, prompt_template\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, anthropic\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, mistral\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, gemini\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, groq\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, cohere\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, litellm\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, azure\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, vertex\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <pre><code>from typing import Annotated\n\nfrom mirascope.core import BaseMessageParam, bedrock\nfrom pydantic import AfterValidator, ValidationError\n\n\ndef is_upper(v: str) -&gt; str:\n    assert v.isupper(), \"Must be uppercase\"\n    return v\n\n\ndef get_book_author(title: Annotated[str, AfterValidator(is_upper)]) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"THE NAME OF THE WIND\":\n        return \"Patrick Rothfuss\"\n    elif title == \"MISTBORN: THE FINAL EMPIRE\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\ntry:\n    if tool := response.tool:\n        print(tool.call())\n    else:\n        print(response.content)\nexcept ValidationError as e:\n    print(e)\n    # &gt; 1 validation error for GetBookAuthor\n    #   title\n    #     Assertion failed, Must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    #       For further information visit https://errors.pydantic.dev/2.8/v/assertion_error\n</code></pre> <p>In this example we've added additional validation, but it's important that you still handle <code>ValidationError</code>'s even with standard tools since they are still <code>BaseModel</code> instances and will validate the field types regardless.</p>"},{"location":"learn/tools/#few-shot-examples","title":"Few-Shot Examples","text":"<p>Just like with Response Models, you can add few-shot examples to your tools:</p> BaseToolFunction ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, openai\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, anthropic\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, mistral\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, gemini\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, groq\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, cohere\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, litellm\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, azure\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, vertex\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, bedrock\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, Messages, openai\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, anthropic\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, mistral\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, gemini\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, groq\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, cohere\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, litellm\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, azure\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, vertex\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, Messages, bedrock\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseTool, openai, prompt_template\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, anthropic, prompt_template\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, mistral, prompt_template\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, gemini, prompt_template\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, groq, prompt_template\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, cohere, prompt_template\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, litellm, prompt_template\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, azure, prompt_template\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, prompt_template, vertex\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseTool, bedrock, prompt_template\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, BaseTool, openai\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, anthropic\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, mistral\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, gemini\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, groq\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, cohere\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, litellm\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, azure\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, vertex\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, BaseTool, bedrock\nfrom pydantic import ConfigDict, Field\n\n\nclass GetBookAuthor(BaseTool):\n    \"\"\"Returns the author of the book with the given title.\"\"\"\n\n    title: str = Field(\n        ...,\n        description=\"The title of the book.\",\n        examples=[\"The Name of the Wind\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\"examples\": [{\"title\": \"The Name of the Wind\"}]}\n    )\n\n    def call(self) -&gt; str:\n        if self.title == \"The Name of the Wind\":\n            return \"Patrick Rothfuss\"\n        elif self.title == \"Mistborn: The Final Empire\":\n            return \"Brandon Sanderson\"\n        else:\n            return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[GetBookAuthor])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; str:\n    return f\"Who wrote {book}?\"\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import Messages, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import Messages, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; Messages.Type:\n    return Messages.User(f\"Who wrote {book}?\")\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import openai, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import mistral, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import gemini, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import groq, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import cohere, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import litellm, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import azure, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import prompt_template, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import bedrock, prompt_template\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\n@prompt_template(\"Who wrote {book}?\")\ndef identify_author(book: str): ...\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import BaseMessageParam, openai\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@openai.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, anthropic\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, mistral\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@mistral.call(\"mistral-large-latest\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, gemini\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@gemini.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, groq\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, cohere\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@cohere.call(\"command-r-plus\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, litellm\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@litellm.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, azure\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@azure.call(\"gpt-4o-mini\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, vertex\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@vertex.call(\"gemini-1.5-flash\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <pre><code>from mirascope.core import BaseMessageParam, bedrock\n\n\ndef get_book_author(title: str) -&gt; str:\n    \"\"\"Returns the author of the book with the given title\n\n    Example:\n        {\"title\": \"The Name of the Wind\"}\n\n    Args:\n        title: The title of the book.\n    \"\"\"\n    if title == \"The Name of the Wind\":\n        return \"Patrick Rothfuss\"\n    elif title == \"Mistborn: The Final Empire\":\n        return \"Brandon Sanderson\"\n    else:\n        return \"Unknown\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\", tools=[get_book_author])\ndef identify_author(book: str) -&gt; list[BaseMessageParam]:\n    return [BaseMessageParam(role=\"user\", content=f\"Who wrote {book}?\")]\n\n\nresponse = identify_author(\"The Name of the Wind\")\nif tool := response.tool:\n    print(tool.call())\nelse:\n    print(response.content)\n</code></pre> <p>Only <code>BaseTool</code> supports field level examples</p> <p>Currently we only support field level examples for <code>BaseTool</code> definitions of tools. We are working on identifying the best interface for adding examples at the field level for function tool definitions.</p>"},{"location":"learn/tools/#toolkit","title":"Toolkit","text":"API Documentation <p><code>mirascope.core.base.toolkit</code></p> <p>The <code>BaseToolKit</code> class enables:</p> <ul> <li>Organiziation of a group of tools under a single namespace.<ul> <li>This can be useful for making it clear to the LLM when to use certain tools over others. For example, you could namespace a set of tools under \"file_system\" to indicate that those tools are specifically for interacting with the file system.</li> </ul> </li> <li>Dynamic tool definitions.<ul> <li>This can be useful for generating tool definitions that are dependent on some input or state. For example, you may want to update the description of tools based on an argument of the call being made.</li> </ul> </li> </ul> ShorthandMessagesString TemplateBaseMessageParam OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    openai,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    anthropic,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    mistral,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    gemini,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    groq,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    cohere,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    litellm,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    azure,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    toolkit_tool,\n    vertex,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    bedrock,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    openai,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    anthropic,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    mistral,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    gemini,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    groq,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    cohere,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    litellm,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    azure,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    toolkit_tool,\n    vertex,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    Messages,\n    bedrock,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [Messages.User(f\"What {genre} author should I read?\")],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    openai,\n    prompt_template,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"What {genre} author should I read?\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    anthropic,\n    prompt_template,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\n@prompt_template(\"What {genre} author should I read?\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    mistral,\n    prompt_template,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@mistral.call(\"mistral-large-latest\")\n@prompt_template(\"What {genre} author should I read?\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    gemini,\n    prompt_template,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\n@prompt_template(\"What {genre} author should I read?\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    groq,\n    prompt_template,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\n@prompt_template(\"What {genre} author should I read?\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    cohere,\n    prompt_template,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@cohere.call(\"command-r-plus\")\n@prompt_template(\"What {genre} author should I read?\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    litellm,\n    prompt_template,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\n@prompt_template(\"What {genre} author should I read?\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    azure,\n    prompt_template,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@azure.call(\"gpt-4o-mini\")\n@prompt_template(\"What {genre} author should I read?\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    prompt_template,\n    toolkit_tool,\n    vertex,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\n@prompt_template(\"What {genre} author should I read?\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseToolKit,\n    bedrock,\n    prompt_template,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\n@prompt_template(\"What {genre} author should I read?\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> OpenAIAnthropicMistralGeminiGroqCohereLiteLLMAzure AIVertex AIBedrock <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseMessageParam,\n    BaseToolKit,\n    openai,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"What {genre} author should I read?\")\n        ],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseMessageParam,\n    BaseToolKit,\n    anthropic,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"What {genre} author should I read?\")\n        ],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseMessageParam,\n    BaseToolKit,\n    mistral,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@mistral.call(\"mistral-large-latest\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"What {genre} author should I read?\")\n        ],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseMessageParam,\n    BaseToolKit,\n    gemini,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@gemini.call(\"gemini-1.5-flash\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"What {genre} author should I read?\")\n        ],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseMessageParam,\n    BaseToolKit,\n    groq,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@groq.call(\"llama-3.1-70b-versatile\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"What {genre} author should I read?\")\n        ],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseMessageParam,\n    BaseToolKit,\n    cohere,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@cohere.call(\"command-r-plus\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"What {genre} author should I read?\")\n        ],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseMessageParam,\n    BaseToolKit,\n    litellm,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@litellm.call(\"gpt-4o-mini\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"What {genre} author should I read?\")\n        ],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseMessageParam,\n    BaseToolKit,\n    azure,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@azure.call(\"gpt-4o-mini\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"What {genre} author should I read?\")\n        ],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseMessageParam,\n    BaseToolKit,\n    toolkit_tool,\n    vertex,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@vertex.call(\"gemini-1.5-flash\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"What {genre} author should I read?\")\n        ],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <pre><code>from mirascope.core import (\n    BaseDynamicConfig,\n    BaseMessageParam,\n    BaseToolKit,\n    bedrock,\n    toolkit_tool,\n)\n\n\nclass BookTools(BaseToolKit):\n    __namespace__ = \"book_tools\"\n\n    reading_level: str\n\n    @toolkit_tool\n    def suggest_author(self, author: str) -&gt; str:\n        \"\"\"Suggests an author for the user to read based on their reading level.\n\n        User reading level: {self.reading_level}\n        \"\"\"\n        return f\"I would suggest you read some books by {author}\"\n\n\n@bedrock.call(\"anthropic.claude-3-haiku-20240307-v1:0\")\ndef recommend_author(genre: str, reading_level: str) -&gt; BaseDynamicConfig:\n    toolkit = BookTools(reading_level=reading_level)\n    return {\n        \"tools\": toolkit.create_tools(),\n        \"messages\": [\n            BaseMessageParam(role=\"user\", content=f\"What {genre} author should I read?\")\n        ],\n    }\n\n\nresponse = recommend_author(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by J.K. Rowling\n\nresponse = recommend_author(\"fantasy\", \"advanced\")\nif tool := response.tool:\n    print(tool.call())\n    # Output: I would suggest you read some books by Brandon Sanderson\n</code></pre> <p>In this example we:</p> <ol> <li>Create a <code>BookTools</code> toolkit</li> <li>We set <code>__namespace__</code> equal to \"book_tools\"</li> <li>We define the <code>reading_level</code> state of the toolkit</li> <li>We define the <code>suggest_author</code> tool and mark it with <code>@toolkit_tool</code> to identify the method as a tool of the toolkit</li> <li>We use the <code>{self.reading_level}</code> template variable in the description of the tool.</li> <li>We create the toolkit with the <code>reading_level</code> argument.</li> <li>We call <code>create_tools</code> to generate the toolkit's tools. This will generate the tools on every call, ensuring that the description correctly includes the provided reading level.</li> <li>We call <code>recommend_author</code> with a \"beginner\" reading level, and the LLM calls the <code>suggest_author</code> tool with its suggested author.</li> <li>We call <code>recommend_author</code> again but with \"advanced\" reading level, and again the LLM calls the <code>suggest_author</code> tool with its suggested author.</li> </ol> <p>The core concept to understand here is that the <code>suggest_author</code> tool's description is dynamically generated on each call to <code>recommend_author</code> through the toolkit.</p> <p>This is why the \"beginner\" recommendation and \"advanced\" recommendations call the <code>suggest_author</code> tool with authors befitting the reading level of each call.</p>"},{"location":"learn/tools/#next-steps","title":"Next Steps","text":"<p>Tools can significantly extend LLM capabilities, enabling more interactive and dynamic applications. We encourage you to explore and experiment with tools to enhance your projects and the find the best fit for your specific needs.</p> <p>Mirascope hopes to provide a simple and clean interface that is both easy to learn and easy to use; however, we understand that LLM tools can be a difficult concept regardless of the supporting tooling.</p> <p>Join our community and ask us any questions you might have, we're here to help!</p> <p>Next, we recommend learning about how to build Agents that take advantage of these tools.</p>"},{"location":"learn/extensions/custom_provider/","title":"Implementing a Custom Provider","text":"<p>This guide explains how to implement a custom provider for Mirascope using the <code>call_factory</code> method. Before proceeding, ensure you're familiar with Mirascope's core concepts as covered in the Learn section of the documentation.</p>"},{"location":"learn/extensions/custom_provider/#overview","title":"Overview","text":"<p>To implement a custom provider, you'll need to create several components:</p> <ol> <li>Provider-specific <code>BaseCallParams</code> class</li> <li>Provider-specific <code>BaseCallResponse</code> class</li> <li>Provider-specific <code>BaseCallResponseChunk</code> class</li> <li>Provider-specific <code>BaseDynamicConfig</code> class</li> <li>Provider-specific <code>BaseStream</code> class</li> <li>Provider-specific <code>BaseTool</code> class</li> <li>Utility functions for setup, JSON output, and stream handling</li> <li>The main call factory implementation</li> </ol> <p>Let's go through each of these components.</p> <p>Use existing providers for reference</p> <p>In this documentation, we are only going to cover the basic general outline of how to implement a custom provider, such as class and function signatures. For a full view into how to implement a decorator for a custom provider, we recommend taking a look at how we implement support for existing providers.</p>"},{"location":"learn/extensions/custom_provider/#basecallparams-class","title":"<code>BaseCallParams</code> class","text":"<p>Define a class that inherits from <code>BaseCallParams</code> to specify the parameters for your custom provider's API calls.</p> <pre><code>from typing_extensions import NotRequired\n\nfrom mirascope.core.base import BaseCallParams\n\n\nclass CustomProviderCallParams(BaseCallParams):\n    # Add parameters specific to your provider, such as:\n    max_tokens: NotRequired[int | None]\n    temperature: NotRequired[float | None]\n</code></pre>"},{"location":"learn/extensions/custom_provider/#basecallresponse-class","title":"<code>BaseCallResponse</code> class","text":"<p>Create a class that inherits from <code>BaseCallResponse</code> to handle the response from your custom provider's API.</p> <pre><code>from mirascope.core.base import BaseCallResponse, BaseMessageParam\n\n\nclass CustomProviderCallResponse(BaseCallResponse[...]):  # provide types for generics\n    # Implement abstract properties and methods\n    @property\n    def content(self) -&gt; str:\n        # Return the main content of the response\n\n    @property\n    def finish_reasons(self) -&gt; list[str] | None:\n        # Return the finish reasons of the response\n\n    # Implement other abstract properties and methods\n</code></pre>"},{"location":"learn/extensions/custom_provider/#basecallresponsechunk-class","title":"<code>BaseCallResponseChunk</code> class","text":"<p>For streaming support, create a class that inherits from <code>BaseCallResponseChunk</code>.</p> <pre><code>from mirascope.core.base import BaseCallResponseChunk\n\n\nclass CustomProviderCallResponseChunk(BaseCallResponseChunk[...]):  # provide types for generics\n    # Implement abstract properties\n    @property\n    def content(self) -&gt; str:\n        # Return the content of the chunk\n\n    @property\n    def finish_reasons(self) -&gt; list[str] | None:\n        # Return the finish reasons for the chunk\n\n    # Implement other abstract properties\n</code></pre>"},{"location":"learn/extensions/custom_provider/#basedynamicconfig-class","title":"<code>BaseDynamicConfig</code> class","text":"<p>Define a type for dynamic configuration using <code>BaseDynamicConfig</code>.</p> <pre><code>from mirascope.core.base import BaseDynamicConfig\nfrom .call_params import CustomProviderCallParams\n\nCustomProviderDynamicConfig = BaseDynamicConfig[BaseMessageParam, CustomProviderCallParams]\n</code></pre>"},{"location":"learn/extensions/custom_provider/#basestream-class","title":"<code>BaseStream</code> class","text":"<p>Implement a stream class that inherits from <code>BaseStream</code> for handling streaming responses.</p> <pre><code>from mirascope.core.base import BaseStream\n\nclass CustomProviderStream(BaseStream):\n    # Implement abstract methods and properties\n    @property\n    def cost(self) -&gt; float | None:\n        # Calculate and return the cost of the stream\n\n    def _construct_message_param(self, tool_calls: list | None = None, content: str | None = None):\n        # Construct and return the message parameter\n\n    def construct_call_response(self) -&gt; CustomProviderCallResponse:\n        # Construct and return the call response\n</code></pre>"},{"location":"learn/extensions/custom_provider/#basetool-class","title":"<code>BaseTool</code> class","text":"<p>Create a tool class that inherits from <code>BaseTool</code> for defining custom tools.</p> <pre><code>from mirascope.core.base import BaseTool\n\nclass CustomProviderTool(BaseTool):\n    # Implement custom tool functionality\n    @classmethod\n    def tool_schema(cls) -&gt; ProviderToolSchemaType:\n        # Return the tool schema\n\n    @classmethod\n    def from_tool_call(cls, tool_call: Any) -&gt; \"CustomProviderTool\":\n        # Construct a tool instance from a tool call\n</code></pre>"},{"location":"learn/extensions/custom_provider/#utility-functions","title":"Utility Functions","text":"<p>Implement utility functions for setup, JSON output handling, and stream handling.</p> <pre><code>from typing import Any, Callable, Awaitable\n\ndef setup_call(\n    *,\n    model: str,\n    client: Any,\n    fn: Callable[..., CustomProviderDynamicConfig | Awaitable[CustomProviderDynamicConfig]],\n    fn_args: dict[str, Any],\n    dynamic_config: CustomProviderDynamicConfig,\n    tools: list[type[BaseTool] | Callable] | None,\n    json_mode: bool,\n    call_params: CustomProviderCallParams,\n    extract: bool,\n) -&gt; tuple[\n    Callable[..., Any] | Callable[..., Awaitable[Any]],\n    str,\n    list[Any],\n    list[type[CustomProviderTool]] | None,\n    dict[str, Any],\n]:\n    # Implement setup logic\n    ...\n\ndef get_json_output(\n    response: CustomProviderCallResponse | CustomProviderCallResponseChunk,\n    json_mode: bool\n) -&gt; str:\n    # Implement JSON output extraction\n    ...\n\ndef handle_stream(\n    stream: Any,\n    tool_types: list[type[CustomProviderTool]] | None,\n) -&gt; Generator[tuple[CustomProviderCallResponseChunk, CustomProviderTool | None], None, None]:\n    # Implement stream handling\n    ...\n\nasync def handle_stream_async(\n    stream: Any,\n    tool_types: list[type[CustomProviderTool]] | None,\n) -&gt; AsyncGenerator[tuple[CustomProviderCallResponseChunk, CustomProviderTool | None], None]:\n    # Implement asynchronous stream handling\n    ...\n</code></pre>"},{"location":"learn/extensions/custom_provider/#call-factory-implementation","title":"Call Factory Implementation","text":"<p>Finally, use the <code>call_factory</code> to create your custom provider's call decorator.</p> <pre><code>from mirascope.core.base import call_factory\n\ncustom_provider_call = call_factory(\n    TCallResponse=CustomProviderCallResponse,\n    TCallResponseChunk=CustomProviderCallResponseChunk,\n    TDynamicConfig=CustomProviderDynamicConfig,\n    TStream=CustomProviderStream,\n    TToolType=CustomProviderTool,\n    TCallParams=CustomProviderCallParams,\n    default_call_params=CustomProviderCallParams(),\n    setup_call=setup_call,\n    get_json_output=get_json_output,\n    handle_stream=handle_stream,\n    handle_stream_async=handle_stream_async,\n)\n</code></pre>"},{"location":"learn/extensions/custom_provider/#usage","title":"Usage","text":"<p>After implementing your custom provider, you can use it like any other Mirascope provider:</p> <pre><code>from mirascope.core import prompt_template\n\n@custom_provider_call(model=\"your-custom-model\")\n@prompt_template(\"Your prompt template here\")\ndef your_function(param: str):\n    ...\n\nresult = your_function(\"example parameter\")\n</code></pre> <p>By following this guide, you can implement a custom provider that integrates seamlessly with Mirascope's existing functionality. Remember to thoroughly test your implementation and handle any provider-specific quirks or requirements.</p>"},{"location":"learn/extensions/middleware/","title":"Writing your own Custom Middleware","text":"<p><code>middleware_factory</code> is a helper function to assist in helping you wrap any Mirascope call. We will be creating an example decorator <code>with_saving</code> that saves some metadata after a Mirascope call using SQLModel. We will be using this table for demonstrative purposes in our example:</p> <pre><code>from mirascope.core.base import BaseCallResponse, BaseType\nfrom mirascope.core.base.stream import BaseStream\n\n\nclass CallResponseTable(SQLModel, table=True):\n    \"\"\"CallResponse model\"\"\"\n\n    __tablename__: str = \"call_response\"  #  type: ignore\n\n    id: int | None = Field(default=None, primary_key=True)\n    function_name: str = Field(default=\"\")\n    prompt_template: str | None = Field(default=None)\n    content: str | None = Field(default=None)\n    response_model: dict | None = Field(sa_column=Column(JSON), default=None)\n    cost: float | None = Field(default=None)\n</code></pre> <p>This table should be adjusted and tailored to your needs depending on your SQL Dialect or requirements.</p>"},{"location":"learn/extensions/middleware/#writing-the-decorator","title":"Writing the decorator","text":"<pre><code>from mirascope.integrations import middleware_factory\n\n\ndef with_saving():\n    \"\"\"Saves some data after a Mirascope call.\"\"\"\n\n    return middleware_factory(\n        custom_context_manager=custom_context_manager,\n        custom_decorator=None,\n        handle_call_response=handle_call_response,\n        handle_call_response_async=handle_call_response_async,\n        handle_stream=handle_stream,\n        handle_stream_async=handle_stream_async,\n        handle_response_model=handle_response_model,\n        handle_response_model_async=handle_response_model_async,\n</code></pre> <p>Let's go over each of the different functions used to create the custom middleware:</p>"},{"location":"learn/extensions/middleware/#custom_context_manager","title":"<code>custom_context_manager</code>","text":"<p>We start off with the <code>custom_context_manager</code> function, which will be relevant to all the handlers. You can define your own context manager where the yielded value is passed to each of the handlers.</p> <pre><code>from collections.abc import Callable, Generator\nfrom contextlib import contextmanager\nfrom typing import Any, cast\n\nfrom mirascope.core.base.stream import BaseStream\n\n\ndef custom_context_manager(\n    fn: Callable,\n) -&gt; Generator[Session, Any, None]:\n    print(f\"Saving call: {fn.__name__}\")\n    with Session(engine) as session:\n        yield session\n</code></pre> <p>All of the following handlers are then wrapped by this context manager.</p>"},{"location":"learn/extensions/middleware/#handle_call_response-and-handle_call_response_async","title":"<code>handle_call_response</code> and <code>handle_call_response_async</code>","text":"<p>These functions must have the following signature (where async should be async) and will be called after making a standard Mirascope call. Here is a sample implementation of the sync version:</p> <pre><code>from collections.abc import Callable, Generator\n\nfrom mirascope.core.base import BaseCallResponse, BaseType\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\ndef handle_call_response(\n    result: BaseCallResponse, fn: Callable, session: Session | None\n):\n    if not session:\n        raise ValueError(\"Session is not set.\")\n\n    call_response_row = CallResponseTable(\n        function_name=fn.__name__,\n        content=result.content,\n        prompt_template=result.prompt_template,\n        cost=result.cost,\n    )\n    session.add(call_response_row)\n    session.commit()\n\n\nasync def handle_call_response_async(\n    result: BaseCallResponse, fn: Callable, session: Session | None\n):\n    # this is lazy and would generally actually utilize async here\n    handle_call_response(result, fn, session)\n</code></pre> <p>The function arguments are (with no strict naming for the arguments):</p> <ul> <li><code>result</code>: The provider-specific <code>BaseCallResponse</code> returned by your call</li> <li><code>fn</code>: Your Mirascope call (the same one as the custom context manager)</li> <li><code>session</code>: The yielded object from the <code>custom_context_manager</code>, which is a <code>Session</code> in this case. If no <code>custom_context_manager</code> is used, this value will be <code>None</code>.</li> </ul> <p><code>handle_call_response_async</code> is the same as <code>handle_call_response</code> but using an <code>async</code> function. This enables awaiting other async functions  in the handler when handling async calls.</p>"},{"location":"learn/extensions/middleware/#handle_stream-and-handle_stream_async","title":"<code>handle_stream</code> and <code>handle_stream_async</code>","text":"<p>These functions must have the following signature (where async should be async) and will be called after streaming a Mirascope call. Here is a sample implementation of the sync version:</p> <pre><code>from collections.abc import Callable, Generator\n\nfrom mirascope.core.base.stream import BaseStream\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\ndef handle_stream(stream: BaseStream, fn: Callable, session: Session | None):\n    if not session:\n        raise ValueError(\"Session is not set.\")\n\n    result = stream.construct_call_response()\n    call_response_row = CallResponseTable(\n        function_name=fn.__name__,\n        content=result.content,\n        prompt_template=result.prompt_template,\n        cost=result.cost,\n    )\n    session.add(call_response_row)\n    session.commit()\n\n\nasync def handle_stream_async(\n    stream: BaseStream, fn: Callable, session: Session | None\n):\n    # this is lazy and would generally actually utilize async here\n    handle_stream(stream, fn, session)\n</code></pre> <p>The first argument will be a provider-specific <code>BaseStream</code> instance. All other arguments will be the same as <code>handle_call_response</code>.</p> <p>Only run on exhaustion</p> <p>The <code>handle_stream</code> and <code>handle_stream_async</code> handlers will run only after the <code>Generator</code> or <code>AsyncGenerator</code>, respectively, have been exhausted.</p>"},{"location":"learn/extensions/middleware/#handle_response_model-and-handle_response_model_async","title":"<code>handle_response_model</code> and <code>handle_response_model_async</code>","text":"<p>These functions must have the following signature (where async should be async) and will be called after making a Mirascope call with <code>response_model</code> set. Here is a sample implementation of the sync version:</p> <pre><code>from collections.abc import Callable, Generator\n\nfrom mirascope.core.base import BaseCallResponse, BaseType\nfrom pydantic import BaseModel\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\ndef handle_response_model(\n    response_model: BaseModel | BaseType, fn: Callable, session: Session | None\n):\n    if not session:\n        raise ValueError(\"Session is not set.\")\n\n    if isinstance(response_model, BaseModel):\n        result = cast(BaseCallResponse, response_model._response)  # pyright: ignore[reportAttributeAccessIssue]\n        call_response_row = CallResponseTable(\n            function_name=fn.__name__,\n            response_model=response_model.model_dump(),\n            prompt_template=result.prompt_template,\n            cost=result.cost,\n        )\n    else:\n        call_response_row = CallResponseTable(\n            function_name=fn.__name__,\n            content=str(response_model),\n            prompt_template=fn._prompt_template,  # pyright: ignore[reportFunctionMemberAccess]\n        )\n    session.add(call_response_row)\n    session.commit()\n\n\nasync def handle_response_model_async(\n    response_model: BaseModel | BaseType, fn: Callable, session: Session | None\n):\n    # this is lazy and would generally actually utilize async here\n    handle_response_model(response_model, fn, session)\n</code></pre> <p>The first argument will be a Pydantic <code>BaseModel</code> or Python primitive depending on the type of <code>response_model</code>. All other arguments will be the same as <code>handle_call_response</code>.</p> <p>For <code>BaseModel</code> you can grab the provider-specific <code>BaseCallResponse</code> via <code>response_model._response</code>. However, this information is not available for primitives <code>BaseType</code>, so we use what we have access to. We recommend using a <code>BaseModel</code> for primitives when you need <code>BaseCallResponse</code> data.</p>"},{"location":"learn/extensions/middleware/#handle_structured_stream-and-handle_structured_stream_async","title":"<code>handle_structured_stream</code> and <code>handle_structured_stream_async</code>","text":"<p>These functions must have the following signature (where async should be async) and will be called after streaming a Mirascope call with <code>response_model</code> set. Here is a sample implementation of the sync version:</p> <pre><code>from collections.abc import Callable, Generator\n\nfrom mirascope.core.base.structured_stream import BaseStructuredStream\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\ndef handle_structured_stream(\n    structured_stream: BaseStructuredStream, fn: Callable, session: Session | None\n):\n    if not session:\n        raise ValueError(\"Session is not set.\")\n\n    result: BaseCallResponse = structured_stream.stream.construct_call_response()\n    call_response_row = CallResponseTable(\n        function_name=fn.__name__,\n        content=result.content,\n        prompt_template=result.prompt_template,\n        cost=result.cost,\n    )\n    session.add(call_response_row)\n    session.commit()\n\n\nasync def handle_structured_stream_async(\n    structured_stream: BaseStructuredStream, fn: Callable, session: Session | None\n):\n    # this is lazy and would generally actually utilize async here\n    handle_structured_stream(structured_stream, fn, session)\n</code></pre> <p>The first argument will be a Mirascope <code>StructuredStream</code> of the provider you are using.  All other arguments will be the same as <code>handle_call_response</code>.</p> <p>Only run on exhaustion</p> <p>The <code>handle_structured_stream</code> and <code>handle_structured_stream_async</code> handlers will run only after the <code>Generator</code> or <code>AsyncGenerator</code>, respectively, have been exhausted.</p>"},{"location":"learn/extensions/middleware/#custom_decorator","title":"<code>custom_decorator</code>","text":"<p>There may be existing libraries that already have a decorator implemented. You can pass that decorator in to <code>custom_decorator</code>, which will wrap the Mirascope call with your custom decorator. This decorator will be called before your custom middleware decorator (in our case, before <code>with_saving</code> is called).</p>"},{"location":"learn/extensions/middleware/#how-to-use-your-newly-created-decorator","title":"How to use your newly created decorator","text":"<p>Now that you have defined your and created your <code>with_saving</code> decorator, you can wrap any Mirascope call, like so:</p> <pre><code>from mirascope.core import anthropic\n\n\n@with_saving()\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nprint(recommend_book(\"fantasy\"))\n</code></pre> <p>In this example, when <code>run</code> is finished, <code>handle_call_response</code> will be called to collect the response. Now, any Mirascope call that uses the <code>with_saving</code> decorator will write to your database.</p> <p>If there is a library that you would like for us to integrate out-of-the-box, create a GitHub Issue or let us know in our Slack community.</p>"},{"location":"learn/provider_specific_features/anthropic/","title":"Anthropic-Specific Features","text":""},{"location":"learn/provider_specific_features/anthropic/#prompt-caching","title":"Prompt Caching","text":"<p>Anthropic's prompt caching feature can help save a lot of tokens by caching parts of your prompt. For full details, we recommend reading their documentation.</p> <p>This Feature Is In Beta</p> <p>While we've added support for prompt caching with Anthropic, this feature is still in beta and requires setting extra headers. You can set this header as an additional call parameter.</p> <p>As this feature is in beta, there may be changes made by Anthropic that may result in changes in our own handling of this feature.</p>"},{"location":"learn/provider_specific_features/anthropic/#message-caching","title":"Message Caching","text":"<p>To cache messages, simply add a <code>:cache_control</code> tagged breakpoint to your prompt:</p> <pre><code>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\n    \"claude-3-5-sonnet-20240620\",\n    call_params={\n        \"max_tokens\": 1024,\n        \"extra_headers\": {\"anthropic-beta\": \"prompt-caching-2024-07-31\"},\n    },\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are an AI assistant tasked with analyzing literary works.\n    Your goal is to provide insightful commentary on themes, characters, and writing style.\n\n    Here is the book in it's entirety: {book}\n\n    {:cache_control}\n\n    USER: {query}\n    \"\"\"\n)\ndef analyze_book(query: str, book: str): ...\n\n\nprint(analyze_book(\"What are the major themes?\", \"[FULL BOOK HERE]\"))\n</code></pre> Additional options <p>You can also specify the cache control type the same way we support additional options for multimodal parts (although currently <code>\"ephemeral\"</code> is the only supported type):</p> <pre><code>@prompt_template(\"... {:cache_control(type=ephemeral)}\")\n</code></pre>"},{"location":"learn/provider_specific_features/anthropic/#tool-caching","title":"Tool Caching","text":"<p>It is also possible to cache tools by using the <code>AnthropicToolConfig</code> and setting the cache control:</p> <pre><code>from mirascope.core import BaseTool, anthropic\nfrom mirascope.core.anthropic import AnthropicToolConfig\n\n\nclass CachedTool(BaseTool):\n    \"\"\"This is an example of a cached tool.\"\"\"\n\n    tool_config = AnthropicToolConfig(cache_control={\"type\": \"ephemeral\"})\n\n    def call(self) -&gt; str:\n        return \"Example tool\"\n\n\n@anthropic.call(\n    \"claude-3-5-sonnet-20240620\",\n    tools=[CachedTool],\n    call_params={\n        \"max_tokens\": 1024,\n        \"extra_headers\": {\"anthropic-beta\": \"prompt-caching-2024-07-31\"},\n    },\n)\ndef cached_tool_call() -&gt; str:\n    return \"An example call with a cached tool\"\n</code></pre> <p>Remember only to include the cache control on the last tool in your list of tools that you want to cache (as all tools up to the tool with a cache control breakpoint will be cached).</p>"},{"location":"learn/provider_specific_features/openai/","title":"OpenAI-Specific Features","text":""},{"location":"learn/provider_specific_features/openai/#structured-outputs","title":"Structured Outputs","text":"<p>OpenAI's newest models (starting with <code>gpt-4o-2024-08-06</code>) support strict structured outputs that reliably adhere to developer-supplied JSON Schemas, achieving 100% reliability in their evals, perfectly matching the desired output schemas.</p> <p>This feature can be extremely useful when extracting structured information or using tools, and you can access this feature when using tools or response models with Mirascope.</p>"},{"location":"learn/provider_specific_features/openai/#tools","title":"Tools","text":"<p>To use structured outputs with tools, use the <code>OpenAIToolConfig</code> and set <code>strict=True</code>. You can then use the tool as described in our Tools documentation:</p> <pre><code>from mirascope.core import BaseTool, openai\nfrom mirascope.core.openai import OpenAIToolConfig\n\n\nclass FormatBook(BaseTool):\n    title: str\n    author: str\n\n    tool_config = OpenAIToolConfig(strict=True)\n\n    def call(self) -&gt; str:\n        return f\"{self.title} by {self.author}\"\n\n\n@openai.call(\n    \"gpt-4o-2024-08-06\", tools=[FormatBook], call_params={\"tool_choice\": \"required\"}\n)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nresponse = recommend_book(\"fantasy\")\nif tool := response.tool:\n    print(tool.call())\n</code></pre> <p>Under the hood, Mirascope generates a JSON Schema for the <code>FormatBook</code> tool based on its attributes and the <code>OpenAIToolConfig</code>. This schema is then used by OpenAI's API to ensure the model's output strictly adheres to the defined structure.</p>"},{"location":"learn/provider_specific_features/openai/#response-models","title":"Response Models","text":"<p>Similarly, you can use structured outputs with response models by setting <code>strict=True</code> in the response model's <code>ResponseModelConfigDict</code>, which is just a subclass of Pydantic's <code>ConfigDict</code> with the addition of the <code>strict</code> key. You will also need to set <code>json_mode=True</code>:</p> <pre><code>from mirascope.core import ResponseModelConfigDict, openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n    model_config = ResponseModelConfigDict(strict=True)\n\n\n@openai.call(\"gpt-4o-2024-08-06\", response_model=Book, json_mode=True)\ndef recommend_book(genre: str) -&gt; str:\n    return f\"Recommend a {genre} book\"\n\n\nbook = recommend_book(\"fantasy\")\nprint(book)\n</code></pre>"},{"location":"learn/provider_specific_features/openai/#openai-realtime-api-beta","title":"OpenAI Realtime API (Beta)","text":"<p>Mirascope provides a simple and intuitive way to leverage OpenAI's cutting-edge Realtime API. This integration allows developers to easily create dynamic, interactive applications with real-time audio and text capabilities, all while abstracting away the complexities of WebSocket management and event handling.</p> <p>With Mirascope, you can quickly set up and use advanced features of OpenAI's Realtime API without dealing with low-level WebSocket operations or complex event structures. This allows you to focus on building your application logic rather than worrying about the intricacies of API communication.</p> <p>Beta Feature</p> <p>The OpenAI Realtime API integration is currently in beta. As such, the interface is subject to change in future releases. We recommend using this feature with caution in production environments and staying updated with the latest documentation and releases.</p>"},{"location":"learn/provider_specific_features/openai/#key-features","title":"Key Features","text":"<p>Mirascope's OpenAI Realtime API wrapper offers a range of powerful features that make it easy to build sophisticated real-time applications:</p> <ul> <li>Audio Input/Output: Seamlessly handle both audio input from users and audio output from the model, enabling natural voice interactions.</li> <li>Audio Stream Input: Support for streaming audio input, allowing for real-time processing of continuous audio data.</li> <li>Text Input/Output: Easily manage text-based interactions alongside audio, providing flexibility in communication modes.</li> <li>Audio Transcript Output: Automatically receive transcripts of audio outputs, useful for logging, display, or further processing.</li> <li>Multi-modal Interactions: Combine audio and text modalities in the same session for rich, flexible user experiences.</li> <li>Simplified Session Management: Abstract away the complexities of WebSocket connections and session handling.</li> <li>Easy-to-use Decorator Pattern: Utilize intuitive Python decorators to define senders and receivers, streamlining your code structure.</li> <li>Asynchronous Support: Built-in support for asynchronous operations, allowing for efficient handling of I/O-bound tasks.</li> <li>Tool Integration: Incorporate custom tools into your Realtime API interactions, enabling more complex and dynamic conversations. Tools can be easily defined as functions and integrated into senders, allowing the AI model to use them during the interaction.</li> <li> <p>Flexible Tool Handling: Receive and process tool calls from the AI model, enabling your application to perform specific actions or retrieve information as part of the conversation flow.</p> </li> <li> <p>These features enable developers to create a wide range of applications, from voice assistants and interactive chatbots to complex multi-modal AI systems, all while leveraging the power of OpenAI's latest models through a clean, Pythonic interface.</p> </li> </ul>"},{"location":"learn/provider_specific_features/openai/#basic-usage","title":"Basic Usage","text":"<p>To use the <code>Realtime</code> class, create an instance with the desired model and configure senders and receivers for handling input and output. Mirascope uses Python decorators to simplify the process of defining senders and receivers.</p> <p>Here's a complete example demonstrating how to set up a streaming audio interaction:</p> <pre><code>import asyncio\nfrom io import BytesIO\n\nfrom typing import AsyncGenerator, Any\n\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n\nfrom mirascope.beta.openai import Realtime, record_as_stream\n\napp = Realtime(\n    \"gpt-4o-realtime-preview-2024-10-01\",\n)\n\n\n@app.receiver(\"audio\")\nasync def receive_audio(response: AudioSegment, context: dict[str, Any]) -&gt; None:\n    play(response)\n\n\n@app.receiver(\"audio_transcript\")\nasync def receive_audio_transcript(response: str, context: dict[str, Any]) -&gt; None:\n    print(f\"AI(audio_transcript): {response}\")\n\n\n@app.sender()\nasync def send_audio_as_stream(\n    context: dict[str, Any],\n) -&gt; AsyncGenerator[BytesIO, None]:\n    print(\"Sending audio...\")\n    async for stream in record_as_stream():\n        yield stream\n\n\nasyncio.run(app.run())\n</code></pre> <p>Let's break down the key components of this example:</p> <ol> <li> <p>First, we create a <code>Realtime</code> instance with the specified model:</p> <pre><code>app = Realtime(\n    \"gpt-4o-realtime-preview-2024-10-01\",\n)\n</code></pre> </li> <li> <p>We define two receivers using the <code>@app.receiver</code> decorator:</p> <pre><code>@app.receiver(\"audio\")\nasync def receive_audio(response: AudioSegment, context: dict[str, Any]) -&gt; None:\n    play(response)\n\n\n@app.receiver(\"audio_transcript\")\nasync def receive_audio_transcript(response: str, context: dict[str, Any]) -&gt; None:\n    print(f\"AI(audio_transcript): {response}\")\n</code></pre> <p>The first receiver handles audio responses by playing them, while the second receiver prints the audio transcript. 3. We define a sender using the <code>@app.sender</code> decorator:</p> <pre><code>async def send_audio_as_stream(\n    context: dict[str, Any],\n) -&gt; AsyncGenerator[BytesIO, None]:\n    print(\"Sending audio...\")\n    async for stream in record_as_stream():\n        yield stream\n</code></pre> </li> </ol> <p>This sender function streams audio data to the model using the <code>record_as_stream()</code> function.</p> <ol> <li> <p>Finally, we run the application:</p> <pre><code>asyncio.run(app.run())\n</code></pre> </li> </ol> <p>This example demonstrates how to set up a streaming audio interaction with the Realtime API. The sender continuously streams audio data to the model, while the receivers handle the audio responses and transcripts from the model.</p> <p>By using these decorators, you can easily define multiple senders and receivers for different types of inputs and outputs (text, audio, streaming audio, etc.) without having to manually manage the complexities of the underlying API calls and WebSocket communication.</p>"},{"location":"learn/provider_specific_features/openai/#examples","title":"Examples","text":""},{"location":"learn/provider_specific_features/openai/#text-only-interaction","title":"Text-only Interaction","text":"<pre><code>import asyncio\nfrom typing import Any\n\nfrom mirascope.beta.openai import Context, Realtime, async_input\n\napp = Realtime(\n    \"gpt-4o-realtime-preview-2024-10-01\",\n    modalities=[\"text\"],\n)\n\n\n@app.receiver(\"text\")\nasync def receive_text(response: str, context: dict[str, Any]) -&gt; None:\n    print(f\"AI(text): {response}\", flush=True)\n\n\n@app.sender(wait_for_text_response=True)\nasync def send_message(context: Context) -&gt; str:\n    message = await async_input(\"Enter your message: \")\n    return message\n\n\nasyncio.run(app.run())\n</code></pre>"},{"location":"learn/provider_specific_features/openai/#audio-interaction-with-turn-detection","title":"Audio Interaction with Turn Detection","text":"<pre><code>import asyncio\nfrom io import BytesIO\n\nfrom typing import AsyncGenerator, Any\n\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n\nfrom mirascope.beta.openai import Realtime, record_as_stream\n\napp = Realtime(\n    \"gpt-4o-realtime-preview-2024-10-01\",\n)\n\n\n@app.receiver(\"audio\")\nasync def receive_audio(response: AudioSegment, context: dict[str, Any]) -&gt; None:\n    play(response)\n\n\n@app.receiver(\"audio_transcript\")\nasync def receive_audio_transcript(response: str, context: dict[str, Any]) -&gt; None:\n    print(f\"AI(audio_transcript): {response}\")\n\n\n@app.sender()\nasync def send_audio_as_stream(\n    context: dict[str, Any],\n) -&gt; AsyncGenerator[BytesIO, None]:\n    print(\"Sending audio...\")\n    async for stream in record_as_stream():\n        yield stream\n\n\nasyncio.run(app.run())\n</code></pre>"},{"location":"learn/provider_specific_features/openai/#audio-interaction-without-turn-detection","title":"Audio Interaction without Turn Detection","text":"<pre><code>import asyncio\nfrom io import BytesIO\n\nfrom typing import Any\n\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n\nfrom mirascope.beta.openai import Realtime, async_input, record\n\napp = Realtime(\n    \"gpt-4o-realtime-preview-2024-10-01\",\n    turn_detection=None,\n)\n\n\n@app.receiver(\"audio\")\nasync def receive_audio(response: AudioSegment, context: dict[str, Any]) -&gt; None:\n    play(response)\n\n\n@app.receiver(\"audio_transcript\")\nasync def receive_audio_transcript(response: str, context: dict[str, Any]) -&gt; None:\n    print(f\"AI(audio_transcript): {response}\")\n\n\n@app.sender(wait_for_audio_transcript_response=True)\nasync def send_audio(context: dict[str, Any]) -&gt; BytesIO:\n    message = await async_input(\n        \"Press Enter to start recording or enter exit to shutdown app\"\n    )\n    if message == \"exit\":\n        raise asyncio.CancelledError\n\n    async def wait_for_enter() -&gt; str:\n        return await async_input(\"Press Enter to stop recording...\")\n\n    recorded_audio = await record(custom_blocking_event=wait_for_enter)\n    return recorded_audio\n\n\nasyncio.run(app.run())\n</code></pre>"},{"location":"learn/provider_specific_features/openai/#streaming-audio-interaction-without-turn-detection","title":"Streaming Audio Interaction without Turn Detection","text":"<pre><code>import asyncio\nfrom io import BytesIO\n\nfrom typing import AsyncGenerator, Any\n\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n\nfrom mirascope.beta.openai import Realtime, record_as_stream, async_input\n\n\napp = Realtime(\n    \"gpt-4o-realtime-preview-2024-10-01\",\n    turn_detection=None,\n)\n\n\n@app.receiver(\"audio\")\nasync def receive_audio(response: AudioSegment, context: dict[str, Any]) -&gt; None:\n    play(response)\n\n\n@app.receiver(\"audio_transcript\")\nasync def receive_audio_transcript(response: str, context: dict[str, Any]) -&gt; None:\n    print(f\"AI(audio_transcript): {response}\")\n\n\n@app.sender(wait_for_audio_transcript_response=True)\nasync def send_audio_as_stream(\n    context: dict[str, Any],\n) -&gt; AsyncGenerator[BytesIO, None]:\n    message = await async_input(\n        \"Press Enter to start recording or enter exit to shutdown app\"\n    )\n    if message == \"exit\":\n        raise asyncio.CancelledError\n\n    async def wait_for_enter() -&gt; str:\n        return await async_input(\"Press Enter to stop recording...\")\n\n    async for stream in record_as_stream(custom_blocking_event=wait_for_enter):\n        yield stream\n\n\nasyncio.run(app.run())\n</code></pre>"},{"location":"learn/provider_specific_features/openai/#audio-interaction-with-tools-and-turn-detection","title":"Audio Interaction with Tools and Turn Detection","text":"<pre><code>import asyncio\nfrom io import BytesIO\n\nfrom typing import AsyncGenerator, Any\n\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n\nfrom mirascope.beta.openai import Realtime, record_as_stream, OpenAIRealtimeTool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\napp = Realtime(\"gpt-4o-realtime-preview-2024-10-01\", tools=[format_book])\n\n\n@app.receiver(\"audio\")\nasync def receive_audio(response: AudioSegment, context: dict[str, Any]) -&gt; None:\n    play(response)\n\n\n@app.receiver(\"audio_transcript\")\nasync def receive_audio_transcript(response: str, context: dict[str, Any]) -&gt; None:\n    print(f\"AI(audio_transcript): {response}\")\n\n\n@app.sender()\nasync def send_audio_as_stream(\n    context: dict[str, Any],\n) -&gt; AsyncGenerator[BytesIO, None]:\n    print(\"Sending audio...\")\n    async for stream in record_as_stream():\n        yield stream\n\n\n@app.function_call(format_book)\nasync def recommend_book(tool: OpenAIRealtimeTool, context: dict[str, Any]) -&gt; str:\n    result = tool.call()\n    print(result)\n    return result\n\n\nasyncio.run(app.run())\n</code></pre>"},{"location":"learn/provider_specific_features/openai/#text-only-interaction-with-tools","title":"Text-only Interaction with Tools","text":"<pre><code>import asyncio\nfrom typing import Any\n\nfrom mirascope.beta.openai import Context, Realtime, async_input\n\nfrom mirascope.beta.openai import OpenAIRealtimeTool\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\napp = Realtime(\n    \"gpt-4o-realtime-preview-2024-10-01\", modalities=[\"text\"], tools=[format_book]\n)\n\n\n@app.sender(wait_for_text_response=True)\nasync def send_message(context: Context) -&gt; str:\n    genre = await async_input(\"Enter a genre: \")\n    return f\"Recommend a {genre} book. please use the tool `format_book`.\"\n\n\n@app.receiver(\"text\")\nasync def receive_text(response: str, context: dict[str, Any]) -&gt; None:\n    print(f\"AI(text): {response}\", flush=True)\n\n\n@app.function_call(format_book)\nasync def recommend_book(tool: OpenAIRealtimeTool, context: Context) -&gt; str:\n    result = tool.call()\n    return result\n\n\nasyncio.run(app.run())\n</code></pre>"},{"location":"learn/provider_specific_features/openai/#text-only-interaction-with-dynamic-tools","title":"Text-only Interaction with dynamic Tools","text":"<pre><code>import asyncio\nfrom typing import Any, Callable\n\nfrom mirascope.beta.openai import Context, Realtime, async_input\n\nfrom mirascope.beta.openai import OpenAIRealtimeTool\n\napp = Realtime(\n    \"gpt-4o-realtime-preview-2024-10-01\",\n    modalities=[\"text\"],\n)\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    return f\"{title} by {author}\"\n\n\n@app.sender(wait_for_text_response=True)\nasync def send_message(context: Context) -&gt; tuple[str, list[Callable]]:\n    genre = await async_input(\"Enter a genre: \")\n    return f\"Recommend a {genre} book. please use the tool `format_book`.\", [\n        format_book\n    ]\n\n\n@app.receiver(\"text\")\nasync def receive_text(response: str, context: dict[str, Any]) -&gt; None:\n    print(f\"AI(text): {response}\", flush=True)\n\n\n@app.function_call(format_book)\nasync def recommend_book(tool: OpenAIRealtimeTool, context: Context) -&gt; str:\n    result = tool.call()\n    print(result)\n    return result\n\n\nasyncio.run(app.run())\n</code></pre>"},{"location":"learn/provider_specific_features/openai/#notes","title":"Notes","text":"<ul> <li>The Realtime API is currently in beta, and its API may change in future releases.</li> <li>Make sure to handle exceptions and cancellation appropriately in your senders and receivers.</li> <li>The examples provided use the <code>pydub</code> library for audio playback. You may need to install additional dependencies for audio support.</li> <li>FFmpeg is required for audio processing. Make sure to install FFmpeg on your system before using the audio features of Mirascope's OpenAI Realtime API support.</li> </ul>"},{"location":"tutorials/agents/blog_writing_agent/","title":"Agent Executor: Blog Writing","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\" requests beautifulsoup4 duckduckgo-search tenacity\n</pre> !pip install \"mirascope[openai]\" requests beautifulsoup4 duckduckgo-search tenacity In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using <p>Make sure to also set your <code>OPENAI_API_KEY</code> if you haven't already.</p> In\u00a0[2]: Copied! <pre>from abc import abstractmethod\n\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel\n\n\nclass OpenAIAgent(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @abstractmethod\n    def _step(self, prompt: str) -&gt; openai.OpenAIStream: ...\n\n    def run(self, prompt: str) -&gt; str:\n        stream = self._step(prompt)\n        result, tools_and_outputs = \"\", []\n\n        for chunk, tool in stream:\n            if tool:\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                result += chunk.content\n                print(chunk.content, end=\"\", flush=True)\n        if stream.user_message_param:\n            self.history.append(stream.user_message_param)\n        self.history.append(stream.message_param)\n        if tools_and_outputs:\n            self.history += stream.tool_message_params(tools_and_outputs)\n            return self.run(\"\")\n        print(\"\\n\")\n        return result\n</pre> from abc import abstractmethod  from mirascope.core import BaseMessageParam, openai from pydantic import BaseModel   class OpenAIAgent(BaseModel):     history: list[BaseMessageParam | openai.OpenAIMessageParam] = []      @abstractmethod     def _step(self, prompt: str) -&gt; openai.OpenAIStream: ...      def run(self, prompt: str) -&gt; str:         stream = self._step(prompt)         result, tools_and_outputs = \"\", []          for chunk, tool in stream:             if tool:                 tools_and_outputs.append((tool, tool.call()))             else:                 result += chunk.content                 print(chunk.content, end=\"\", flush=True)         if stream.user_message_param:             self.history.append(stream.user_message_param)         self.history.append(stream.message_param)         if tools_and_outputs:             self.history += stream.tool_message_params(tools_and_outputs)             return self.run(\"\")         print(\"\\n\")         return result <p>Note that the <code>_step</code> function is marked as an abstract method that each subclass will need to implement.</p> In\u00a0[3]: Copied! <pre>import inspect\n\nfrom duckduckgo_search import DDGS\n\n\nclass ResearcherBase(OpenAIAgent):\n    max_results: int = 10\n\n    def web_search(self, text: str) -&gt; str:\n        \"\"\"Search the web for the given text.\n\n        Args:\n            text: The text to search for.\n\n        Returns:\n            The search results for the given text formatted as newline separated\n            dictionaries with keys 'title', 'href', and 'body'.\n        \"\"\"\n        try:\n            results = DDGS(proxy=None).text(text, max_results=self.max_results)\n            return \"\\n\\n\".join(\n                [\n                    inspect.cleandoc(\n                        \"\"\"\n                        title: {title}\n                        href: {href}\n                        body: {body}\n                        \"\"\"\n                    ).format(**result)\n                    for result in results\n                ]\n            )\n        except Exception as e:\n            return f\"{type(e)}: Failed to search the web for text\"\n</pre> import inspect  from duckduckgo_search import DDGS   class ResearcherBase(OpenAIAgent):     max_results: int = 10      def web_search(self, text: str) -&gt; str:         \"\"\"Search the web for the given text.          Args:             text: The text to search for.          Returns:             The search results for the given text formatted as newline separated             dictionaries with keys 'title', 'href', and 'body'.         \"\"\"         try:             results = DDGS(proxy=None).text(text, max_results=self.max_results)             return \"\\n\\n\".join(                 [                     inspect.cleandoc(                         \"\"\"                         title: {title}                         href: {href}                         body: {body}                         \"\"\"                     ).format(**result)                     for result in results                 ]             )         except Exception as e:             return f\"{type(e)}: Failed to search the web for text\" In\u00a0[4]: Copied! <pre>import requests\nfrom bs4 import BeautifulSoup\n\n\nclass ResearcherBaseWithParser(ResearcherBase):\n    def parse_webpage(self, link: str) -&gt; str:\n        \"\"\"Parse the paragraphs of the webpage found at `link`.\n\n        Args:\n            link: The URL of the webpage.\n\n        Returns:\n            The parsed paragraphs of the webpage, separated by newlines.\n        \"\"\"\n        try:\n            response = requests.get(link)\n            soup = BeautifulSoup(response.content, \"html.parser\")\n            return \"\\n\".join([p.text for p in soup.find_all(\"p\")])\n        except Exception as e:\n            return f\"{type(e)}: Failed to parse content from URL\"\n</pre> import requests from bs4 import BeautifulSoup   class ResearcherBaseWithParser(ResearcherBase):     def parse_webpage(self, link: str) -&gt; str:         \"\"\"Parse the paragraphs of the webpage found at `link`.          Args:             link: The URL of the webpage.          Returns:             The parsed paragraphs of the webpage, separated by newlines.         \"\"\"         try:             response = requests.get(link)             soup = BeautifulSoup(response.content, \"html.parser\")             return \"\\n\".join([p.text for p in soup.find_all(\"p\")])         except Exception as e:             return f\"{type(e)}: Failed to parse content from URL\" In\u00a0[5]: Copied! <pre>from mirascope.core import prompt_template\n\n\nclass ResearcherBaseWithStep(ResearcherBaseWithParser):\n    @openai.call(\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        Your task is to research a topic and summarize the information you find.\n        This information will be given to a writer (user) to create a blog post.\n\n        You have access to the following tools:\n        - `web_search`: Search the web for information. Limit to max {self.max_results}\n            results.\n        - `parse_webpage`: Parse the content of a webpage.\n\n        When calling the `web_search` tool, the `body` is simply the body of the search\n        result. You MUST then call the `parse_webpage` tool to get the actual content\n        of the webpage. It is up to you to determine which search results to parse.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness. The goal is to\n        provide as much information to the writer as possible without overwhelming them.\n\n        MESSAGES: {self.history}\n        USER: {prompt}\n        \"\"\"\n    )\n    def _step(self, prompt: str) -&gt; openai.OpenAIDynamicConfig:\n        return {\"tools\": [self.web_search, self.parse_webpage]}\n</pre> from mirascope.core import prompt_template   class ResearcherBaseWithStep(ResearcherBaseWithParser):     @openai.call(\"gpt-4o-mini\", stream=True)     @prompt_template(         \"\"\"         SYSTEM:         Your task is to research a topic and summarize the information you find.         This information will be given to a writer (user) to create a blog post.          You have access to the following tools:         - `web_search`: Search the web for information. Limit to max {self.max_results}             results.         - `parse_webpage`: Parse the content of a webpage.          When calling the `web_search` tool, the `body` is simply the body of the search         result. You MUST then call the `parse_webpage` tool to get the actual content         of the webpage. It is up to you to determine which search results to parse.          Once you have gathered all of the information you need, generate a writeup that         strikes the right balance between brevity and completeness. The goal is to         provide as much information to the writer as possible without overwhelming them.          MESSAGES: {self.history}         USER: {prompt}         \"\"\"     )     def _step(self, prompt: str) -&gt; openai.OpenAIDynamicConfig:         return {\"tools\": [self.web_search, self.parse_webpage]} In\u00a0[6]: Copied! <pre>class Researcher(ResearcherBaseWithStep):\n    def research(self, prompt: str) -&gt; str:\n        \"\"\"Research a topic and summarize the information found.\n\n        Args:\n            prompt: The user prompt to guide the research. The content of this prompt\n                is directly responsible for the quality of the research, so it is\n                crucial that the prompt be clear and concise.\n\n        Returns:\n            The results of the research.\n        \"\"\"\n        print(\"RESEARCHING...\")\n        result = self.run(prompt)\n        print(\"RESEARCH COMPLETE!\")\n        return result\n</pre> class Researcher(ResearcherBaseWithStep):     def research(self, prompt: str) -&gt; str:         \"\"\"Research a topic and summarize the information found.          Args:             prompt: The user prompt to guide the research. The content of this prompt                 is directly responsible for the quality of the research, so it is                 crucial that the prompt be clear and concise.          Returns:             The results of the research.         \"\"\"         print(\"RESEARCHING...\")         result = self.run(prompt)         print(\"RESEARCH COMPLETE!\")         return result In\u00a0[7]: Copied! <pre>from mirascope.integrations.tenacity import collect_errors\nfrom pydantic import ValidationError\nfrom tenacity import retry, wait_exponential\n\n\nclass AgentExecutorBase(OpenAIAgent):\n    researcher: Researcher = Researcher()\n    num_paragraphs: int = 4\n\n    class InitialDraft(BaseModel):\n        draft: str\n        critique: str\n\n    @staticmethod\n    def parse_initial_draft(response: InitialDraft) -&gt; str:\n        return f\"Draft: {response.draft}\\nCritique: {response.critique}\"\n\n    @retry(\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        after=collect_errors(ValidationError),\n    )\n    @openai.call(\n        \"gpt-4o-mini\", response_model=InitialDraft, output_parser=parse_initial_draft\n    )\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        Your task is to write the initial draft for a blog post based on the information\n        provided to you by the researcher, which will be a summary of the information\n        they found on the internet.\n\n        Along with the draft, you will also write a critique of your own work. This\n        critique is crucial for improving the quality of the draft in subsequent\n        iterations. Ensure that the critique is thoughtful, constructive, and specific.\n        It should strike the right balance between comprehensive and concise feedback.\n\n        If for any reason you deem that the research is insufficient or unclear, you can\n        request that additional research be conducted by the researcher. Make sure that\n        your request is specific, clear, and concise.\n\n        MESSAGES: {self.history}\n        USER:\n        {previous_errors}\n        {prompt}\n        \"\"\"\n    )\n    def _write_initial_draft(\n        self, prompt: str, *, errors: list[ValidationError] | None = None\n    ) -&gt; openai.OpenAIDynamicConfig:\n        \"\"\"Writes the initial draft of a blog post along with a self-critique.\n\n        Args:\n            prompt: The user prompt to guide the writing process. The content of this\n                prompt is directly responsible for the quality of the blog post, so it\n                is crucial that the prompt be clear and concise.\n\n        Returns:\n            The initial draft of the blog post along with a self-critique.\n        \"\"\"\n        return {\n            \"computed_fields\": {\n                \"previous_errors\": f\"Previous Errors: {errors}\" if errors else None\n            }\n        }\n</pre> from mirascope.integrations.tenacity import collect_errors from pydantic import ValidationError from tenacity import retry, wait_exponential   class AgentExecutorBase(OpenAIAgent):     researcher: Researcher = Researcher()     num_paragraphs: int = 4      class InitialDraft(BaseModel):         draft: str         critique: str      @staticmethod     def parse_initial_draft(response: InitialDraft) -&gt; str:         return f\"Draft: {response.draft}\\nCritique: {response.critique}\"      @retry(         wait=wait_exponential(multiplier=1, min=4, max=10),         after=collect_errors(ValidationError),     )     @openai.call(         \"gpt-4o-mini\", response_model=InitialDraft, output_parser=parse_initial_draft     )     @prompt_template(         \"\"\"         SYSTEM:         Your task is to write the initial draft for a blog post based on the information         provided to you by the researcher, which will be a summary of the information         they found on the internet.          Along with the draft, you will also write a critique of your own work. This         critique is crucial for improving the quality of the draft in subsequent         iterations. Ensure that the critique is thoughtful, constructive, and specific.         It should strike the right balance between comprehensive and concise feedback.          If for any reason you deem that the research is insufficient or unclear, you can         request that additional research be conducted by the researcher. Make sure that         your request is specific, clear, and concise.          MESSAGES: {self.history}         USER:         {previous_errors}         {prompt}         \"\"\"     )     def _write_initial_draft(         self, prompt: str, *, errors: list[ValidationError] | None = None     ) -&gt; openai.OpenAIDynamicConfig:         \"\"\"Writes the initial draft of a blog post along with a self-critique.          Args:             prompt: The user prompt to guide the writing process. The content of this                 prompt is directly responsible for the quality of the blog post, so it                 is crucial that the prompt be clear and concise.          Returns:             The initial draft of the blog post along with a self-critique.         \"\"\"         return {             \"computed_fields\": {                 \"previous_errors\": f\"Previous Errors: {errors}\" if errors else None             }         } <p>There are a few things worth noting here:</p> <ul> <li>We are again using <code>self</code> for convenient access to the containing class' state. In this case we expect to put this function inside of our executor and want to give access to the conversation history -- particularly the results of the researcher.</li> <li>We are using <code>response_model</code> to extract specifically the <code>draft</code> and <code>critique</code> fields.</li> <li>We are using an output parser <code>parse_initial_draft</code> to parse the <code>InitialDraft</code> class into a format that is friendly for using tools (<code>str</code>).</li> <li>We are using <code>tenacity</code> in order to retry should the call fail to properly generate an <code>InitialDraft</code> instance, reinserting the list of previous errors into each subsequent call.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>class AgentExecutor(AgentExecutorBase):\n    @openai.call(\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        Your task is to facilitate the collaboration between the researcher and the\n        blog writer. The researcher will provide the blog writer with the information\n        they need to write a blog post, and the blog writer will draft and critique the\n        blog post until they reach a final iteration they are satisfied with.\n\n        To access the researcher and writer, you have the following tools:\n        - `research`: Prompt the researcher to perform research.\n        - `_write_initial_draft`: Write an initial draft with a self-critique\n\n        You will need to manage the flow of information between the researcher and the\n        blog writer, ensuring that the information provided is clear, concise, and\n        relevant to the task at hand.\n\n        The final blog post MUST have EXACTLY {self.num_paragraphs} paragraphs.\n\n        MESSAGES: {self.history}\n        USER: {prompt}\n        \"\"\"\n    )\n    def _step(self, prompt: str) -&gt; openai.OpenAIDynamicConfig:\n        return {\"tools\": [self.researcher.research, self._write_initial_draft]}\n\n\nagent = AgentExecutor()\nprint(\"STARTING AGENT EXECUTION...\")\nagent.run(\"Help me write a blog post about LLMs and structured outputs.\")\n</pre> class AgentExecutor(AgentExecutorBase):     @openai.call(\"gpt-4o-mini\", stream=True)     @prompt_template(         \"\"\"         SYSTEM:         Your task is to facilitate the collaboration between the researcher and the         blog writer. The researcher will provide the blog writer with the information         they need to write a blog post, and the blog writer will draft and critique the         blog post until they reach a final iteration they are satisfied with.          To access the researcher and writer, you have the following tools:         - `research`: Prompt the researcher to perform research.         - `_write_initial_draft`: Write an initial draft with a self-critique          You will need to manage the flow of information between the researcher and the         blog writer, ensuring that the information provided is clear, concise, and         relevant to the task at hand.          The final blog post MUST have EXACTLY {self.num_paragraphs} paragraphs.          MESSAGES: {self.history}         USER: {prompt}         \"\"\"     )     def _step(self, prompt: str) -&gt; openai.OpenAIDynamicConfig:         return {\"tools\": [self.researcher.research, self._write_initial_draft]}   agent = AgentExecutor() print(\"STARTING AGENT EXECUTION...\") agent.run(\"Help me write a blog post about LLMs and structured outputs.\") <p>Additional Real-World Applications</p> <ol> <li> <p>Automated Content Marketing:</p> <ul> <li>Create a system that generates targeted blog posts for different customer segments based on current market trends and company data.</li> <li>Example: An e-commerce platform could use this to write product category overviews, incorporating latest fashion trends and customer preferences.</li> </ul> </li> <li> <p>Technical Documentation Generation:</p> <ul> <li>Develop an agent that researches API changes, new features, and community feedback to automatically update and expand technical documentation.</li> <li>Example: A software company could use this to keep their SDK documentation up-to-date with each new release.</li> </ul> </li> <li> <p>Personalized Learning Content:</p> <ul> <li>Build an educational tool that creates customized study materials based on a student's learning style, current knowledge, and learning goals.</li> <li>Example: An online learning platform could generate personalized course summaries and practice exercises for each student.</li> </ul> </li> <li> <p>Automated News Summary and Analysis:</p> <ul> <li>Create a system that gathers news from various sources, summarizes key points, and generates analytical pieces on trending topics.</li> <li>Example: A news agency could use this to produce daily briefings on complex, evolving stories like economic trends or geopolitical events.</li> </ul> </li> <li> <p>Scientific Literature Review Assistant:</p> <ul> <li>Develop an agent that can scan recent publications in a specific field, summarize key findings, and draft literature review sections for research papers.</li> <li>Example: Researchers could use this to stay updated on the latest developments in their field and to assist in writing comprehensive literature reviews.</li> </ul> </li> <li> <p>Legal Document Drafting:</p> <ul> <li>Create a system that researches relevant case law and regulations to assist in drafting legal documents like contracts or briefs.</li> <li>Example: A law firm could use this to generate first drafts of standard contracts, incorporating the latest legal precedents and regulations.</li> </ul> </li> <li> <p>Product Description Generator:</p> <ul> <li>Build an agent that researches product features, customer reviews, and market trends to write engaging and informative product descriptions.</li> <li>Example: An online marketplace could use this to automatically generate or update descriptions for thousands of products.</li> </ul> </li> <li> <p>Travel Guide Creation:</p> <ul> <li>Develop a system that researches destinations, local attractions, and traveler reviews to create personalized travel guides.</li> <li>Example: A travel company could use this to generate custom itineraries and destination guides based on a traveler's preferences and budget.</li> </ul> </li> </ol> <p>When adapting this recipe, consider:</p> <ul> <li>Implement a feedback loop where the executor can request additional research or revisions.</li> <li>Add more specialized agents, such as an editor or fact-checker.</li> <li>Incorporate user feedback into the writing process.</li> <li>Extend the system to handle multiple blog post formats or styles.</li> <li>Implement caching for research results to improve efficiency for similar topics.</li> <li>Adjusting the prompts and system messages to fit your specific use case or writing style.</li> <li>Experimenting with different LLM models for various tasks (research vs. writing).</li> <li>Implementing error handling and logging for production use.</li> <li>Optimizing the web search and parsing functions for better performance and reliability.</li> </ul>"},{"location":"tutorials/agents/blog_writing_agent/#agent-executor-blog-writing","title":"Agent Executor: Blog Writing\u00b6","text":"<p>This recipe demonstrates how to build an Agent Executor using Mirascope to automate the process of researching and writing a blog post. We'll create a system that combines a researcher agent and a writer tool, orchestrated by an executor agent.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Streams</li> <li>Tools</li> <li>Response Model</li> <li>Output Parser</li> <li>Retries</li> </ul> <p>Background</p> <p> Agent-based systems in AI involve creating autonomous agents that can perform tasks or make decisions. In this recipe, we're using multiple agents (researcher and writer) coordinated by an executor to create a blog post. This approach allows for a more modular and potentially more effective content creation process. </p>"},{"location":"tutorials/agents/blog_writing_agent/#system-architecture","title":"System Architecture\u00b6","text":"<pre>\nflowchart TD\n    AE[Agent Executor]\n    R[Researcher.research]\n    WID[_write_initial_draft]\n    OAPI[OpenAI API]\n    \n    subgraph Researcher\n        RA[Researcher Agent]\n        WS[Web Search]\n        PW[Parse Webpage]\n    end\n    \n    AE --&gt; R\n    AE --&gt; WID\n    R --&gt; RA\n    RA --&gt; WS\n    RA --&gt; PW\n    \n    WS -.-&gt; OAPI\n    PW -.-&gt; OAPI\n    RA -.-&gt; OAPI\n    WID -.-&gt; OAPI\n    AE -.-&gt; OAPI\n\n    classDef agent fill:#e1d5e7,stroke:#9673a6,stroke-width:2px;\n    classDef tool fill:#fff2cc,stroke:#d6b656,stroke-width:2px;\n    classDef api fill:#dae8fc,stroke:#6c8ebf,stroke-width:2px;\n    \n    class AE,RA agent;\n    class R,WID,WS,PW tool;\n    class OAPI api;\n</pre>"},{"location":"tutorials/agents/blog_writing_agent/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/agents/blog_writing_agent/#implementing-the-baseagent","title":"Implementing the <code>BaseAgent</code>\u00b6","text":"<p>First, let's create a base <code>OpenAIAgent</code> class that we can later subclass to implement specialized agents:</p>"},{"location":"tutorials/agents/blog_writing_agent/#research-agent","title":"Research Agent\u00b6","text":"<p>The first step to writing a good blog post is researching your topic, so let's create an agent that can search the internet and summarize relevant information that we can later consume when writing the post.</p>"},{"location":"tutorials/agents/blog_writing_agent/#web-search-tool","title":"Web Search Tool\u00b6","text":"<p>We can use the <code>duckduckgo-search</code> package (with no API key!) to perform some basic keyword search on the internet. Note that we are including <code>self</code> as an argument so that we can access the state of the <code>Researcher</code> agent we will build. This enables easier configuration.</p>"},{"location":"tutorials/agents/blog_writing_agent/#parsing-html-content","title":"Parsing HTML Content\u00b6","text":"<p>Our <code>web_search</code> tool only returns search results -- not the actual content of the webpages found at the href results of our search. While we could deterministically parse every web page returned, let's instead provide our researcher with a tool for parsing the content. The value of this approach is that we can greatly increase the number of search results and let the researcher decide which of the results are worth parsing and using.</p>"},{"location":"tutorials/agents/blog_writing_agent/#researcher-step-function","title":"Researcher Step Function\u00b6","text":"<p>Now that we have our tools we're ready to implement the <code>_step</code> method of our researcher where the majority of the remaining work lies in engineering the prompt:</p>"},{"location":"tutorials/agents/blog_writing_agent/#implementing-a-research-tool-method","title":"Implementing a <code>research</code> tool method\u00b6","text":"<p>While we could use the <code>run</code> method from our <code>OpenAIAgent</code> as a tool, there is value in further engineering our prompt by providing good descriptions (and names!) for the tools we use. Putting everything together, we can expose a <code>research</code> method that we can later use as a tool in our agent executor:</p>"},{"location":"tutorials/agents/blog_writing_agent/#writing-an-initial-draft","title":"Writing An Initial Draft\u00b6","text":"<p>The next step when writing a blog is to write an initial draft and critique it. We can then incorporate the feedback from the critique to iteratively improve the post. Let's make a call to an LLM to write this first draft as well as critique it:</p>"},{"location":"tutorials/agents/blog_writing_agent/#agent-executor","title":"Agent Executor\u00b6","text":"<p>Now we just need to put it all together into our <code>AgentExecutor</code> class, write our <code>_step</code> function, and run it!</p>"},{"location":"tutorials/agents/documentation_agent/","title":"Documentation Agent","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n# LLamaIndex for embedding and retrieving embeddings from a vectorstore\n!pip install llama-index\n</pre> !pip install \"mirascope[openai]\" # LLamaIndex for embedding and retrieving embeddings from a vectorstore !pip install llama-index In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n)\nfrom llama_index.core.extractors import TitleExtractor\nfrom llama_index.core.ingestion import IngestionPipeline\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.storage import StorageContext\nfrom llama_index.core.vector_stores import SimpleVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\ndocuments = SimpleDirectoryReader(\"../../../docs/learn\").load_data()\nvector_store = SimpleVectorStore()\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\npipeline = IngestionPipeline(\n    transformations=[\n        SentenceSplitter(chunk_size=512, chunk_overlap=128),\n        TitleExtractor(),\n        OpenAIEmbedding(),\n    ],\n    vector_store=vector_store,\n)\n\nnodes = pipeline.run(documents=documents)\nindex = VectorStoreIndex(\n    nodes,\n    storage_context=storage_context,\n)\n\nindex.storage_context.persist()\n</pre> from llama_index.core import (     SimpleDirectoryReader,     VectorStoreIndex, ) from llama_index.core.extractors import TitleExtractor from llama_index.core.ingestion import IngestionPipeline from llama_index.core.node_parser import SentenceSplitter from llama_index.core.storage import StorageContext from llama_index.core.vector_stores import SimpleVectorStore from llama_index.embeddings.openai import OpenAIEmbedding  documents = SimpleDirectoryReader(\"../../../docs/learn\").load_data() vector_store = SimpleVectorStore() storage_context = StorageContext.from_defaults(vector_store=vector_store)  pipeline = IngestionPipeline(     transformations=[         SentenceSplitter(chunk_size=512, chunk_overlap=128),         TitleExtractor(),         OpenAIEmbedding(),     ],     vector_store=vector_store, )  nodes = pipeline.run(documents=documents) index = VectorStoreIndex(     nodes,     storage_context=storage_context, )  index.storage_context.persist() In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import (\n    load_index_from_storage,\n)\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\nloaded_index = load_index_from_storage(storage_context)\nquery_engine = loaded_index.as_query_engine()\n</pre> from llama_index.core import (     load_index_from_storage, )  storage_context = StorageContext.from_defaults(persist_dir=\"storage\") loaded_index = load_index_from_storage(storage_context) query_engine = loaded_index.as_query_engine() In\u00a0[\u00a0]: Copied! <pre>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Relevance(BaseModel):\n    id: int = Field(..., description=\"The document ID\")\n    score: int = Field(..., description=\"The relevance score (1-10)\")\n    document: str = Field(..., description=\"The document text\")\n    reason: str = Field(..., description=\"A brief explanation for the assigned score\")\n\n\n@openai.call(\n    \"gpt-4o-mini\",\n    response_model=list[Relevance],\n    json_mode=True,\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Document Relevance Assessment\n    Given a list of documents and a question, determine the relevance of each document to answering the question.\n\n    Input\n        - A question\n        - A list of documents, each with an ID and content summary\n\n    Task\n        - Analyze each document for its relevance to the question.\n        - Assign a relevance score from 1-10 for each document.\n        - Provide a reason for each score.\n\n    Scoring Guidelines\n        - Consider both direct and indirect relevance to the question.\n        - Prioritize positive, affirmative information over negative statements.\n        - Assess the informativeness of the content, not just keyword matches.\n        - Consider the potential for a document to contribute to a complete answer.\n\n    Important Notes\n        - Exclude documents with no relevance less than 5 to the question.\n        - Be cautious with negative statements - they may be relevant but are often less informative than positive ones.\n        - Consider how multiple documents might work together to answer the question.\n        - Use the document title and content summary to make your assessment.\n\n    Documents:\n    {documents}\n\n    USER: \n    {query}\n    \"\"\"\n)\ndef llm_query_rerank(documents: list[dict], query: str): ...\n</pre> from mirascope.core import openai, prompt_template from pydantic import BaseModel, Field   class Relevance(BaseModel):     id: int = Field(..., description=\"The document ID\")     score: int = Field(..., description=\"The relevance score (1-10)\")     document: str = Field(..., description=\"The document text\")     reason: str = Field(..., description=\"A brief explanation for the assigned score\")   @openai.call(     \"gpt-4o-mini\",     response_model=list[Relevance],     json_mode=True, ) @prompt_template(     \"\"\"     SYSTEM:     Document Relevance Assessment     Given a list of documents and a question, determine the relevance of each document to answering the question.      Input         - A question         - A list of documents, each with an ID and content summary      Task         - Analyze each document for its relevance to the question.         - Assign a relevance score from 1-10 for each document.         - Provide a reason for each score.      Scoring Guidelines         - Consider both direct and indirect relevance to the question.         - Prioritize positive, affirmative information over negative statements.         - Assess the informativeness of the content, not just keyword matches.         - Consider the potential for a document to contribute to a complete answer.      Important Notes         - Exclude documents with no relevance less than 5 to the question.         - Be cautious with negative statements - they may be relevant but are often less informative than positive ones.         - Consider how multiple documents might work together to answer the question.         - Use the document title and content summary to make your assessment.      Documents:     {documents}      USER:      {query}     \"\"\" ) def llm_query_rerank(documents: list[dict], query: str): ... <p>We get back a list of <code>Relevance</code>s which we will be using for our <code>get_documents</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>from typing import cast\n\nfrom llama_index.core import QueryBundle\nfrom llama_index.core.indices.vector_store import VectorIndexRetriever\n\n\ndef get_documents(query: str) -&gt; list[str]:\n    \"\"\"The get_documents tool that retrieves Mirascope documentation based on the\n    relevance of the query\"\"\"\n    query_bundle = QueryBundle(query)\n    retriever = VectorIndexRetriever(\n        index=cast(VectorStoreIndex, loaded_index),\n        similarity_top_k=10,\n    )\n    retrieved_nodes = retriever.retrieve(query_bundle)\n    choice_batch_size = 5\n    top_n = 2\n    results: list[Relevance] = []\n    for idx in range(0, len(retrieved_nodes), choice_batch_size):\n        nodes_batch = [\n            {\n                \"id\": idx + id,\n                \"text\": node.node.get_text(),  # pyright: ignore[reportAttributeAccessIssue]\n                \"document_title\": node.metadata[\"document_title\"],\n                \"semantic_score\": node.score,\n            }\n            for id, node in enumerate(retrieved_nodes[idx : idx + choice_batch_size])\n        ]\n        results += llm_query_rerank(nodes_batch, query)\n    results = sorted(results, key=lambda x: x.score or 0, reverse=True)[:top_n]\n\n    return [result.document for result in results]\n</pre> from typing import cast  from llama_index.core import QueryBundle from llama_index.core.indices.vector_store import VectorIndexRetriever   def get_documents(query: str) -&gt; list[str]:     \"\"\"The get_documents tool that retrieves Mirascope documentation based on the     relevance of the query\"\"\"     query_bundle = QueryBundle(query)     retriever = VectorIndexRetriever(         index=cast(VectorStoreIndex, loaded_index),         similarity_top_k=10,     )     retrieved_nodes = retriever.retrieve(query_bundle)     choice_batch_size = 5     top_n = 2     results: list[Relevance] = []     for idx in range(0, len(retrieved_nodes), choice_batch_size):         nodes_batch = [             {                 \"id\": idx + id,                 \"text\": node.node.get_text(),  # pyright: ignore[reportAttributeAccessIssue]                 \"document_title\": node.metadata[\"document_title\"],                 \"semantic_score\": node.score,             }             for id, node in enumerate(retrieved_nodes[idx : idx + choice_batch_size])         ]         results += llm_query_rerank(nodes_batch, query)     results = sorted(results, key=lambda x: x.score or 0, reverse=True)[:top_n]      return [result.document for result in results] <p>Now that we can retrieve relevant documents for our user query, we can create our Agent.</p> In\u00a0[\u00a0]: Copied! <pre>from typing import Literal\n\n\nclass Response(BaseModel):\n    classification: Literal[\"code\", \"general\"] = Field(\n        ..., description=\"The classification of the question\"\n    )\n    content: str = Field(..., description=\"The response content\")\n\n\nclass DocumentationAgent(BaseModel):\n    @openai.call(\"gpt-4o-mini\", response_model=Response, json_mode=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an AI Assistant that is an expert at answering questions about Mirascope.\n        Here is the relevant documentation to answer the question.\n\n        First classify the question into one of two types:\n            - General Information: Questions about the system or its components.\n            - Code Examples: Questions that require code snippets or examples.\n\n        For General Information, provide a summary of the relevant documents if the question is too broad ask for more details. \n        If the context does not answer the question, say that the information is not available or you could not find it.\n\n        For Code Examples, output ONLY code without any markdown, with comments if necessary.\n        If the context does not answer the question, say that the information is not available.\n\n        Examples:\n            Question: \"What is Mirascope?\"\n            Answer:\n            A toolkit for building AI-powered applications with Large Language Models (LLMs).\n            Explanation: This is a General Information question, so a summary is provided.\n\n            Question: \"How do I make a basic OpenAI call using Mirascope?\"\n            Answer:\n            from mirascope.core import openai, prompt_template\n\n\n            @openai.call(\"gpt-4o-mini\")\n            def recommend_book(genre: str) -&gt; str:\n                return f'Recommend a {genre} book'\n\n            response = recommend_book(\"fantasy\")\n            print(response.content)\n            Explanation: This is a Code Examples question, so only a code snippet is provided.\n\n        Context:\n        {context:list}\n\n        USER:\n        {question}\n        \"\"\"\n    )\n    def _call(self, question: str) -&gt; openai.OpenAIDynamicConfig:\n        documents = get_documents(question)\n        return {\"computed_fields\": {\"context\": documents}}\n\n    def _step(self, question: str):\n        answer = self._call(question)\n        print(\"(Assistant):\", answer.content)\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            self._step(question)\n\n\nif __name__ == \"__main__\":\n    DocumentationAgent().run()\n    # Output:\n    \"\"\"\n    (User): How do I make an LLM call using Mirascope?\n    (Assistant): from mirascope.core import openai\n    \n    @openai.call('gpt-4o-mini')\n    def recommend_book(genre: str) -&gt; str:\n        return f'Recommend a {genre} book'\n    \n    response = recommend_book('fantasy')\n    print(response.content)\n    \"\"\"\n</pre> from typing import Literal   class Response(BaseModel):     classification: Literal[\"code\", \"general\"] = Field(         ..., description=\"The classification of the question\"     )     content: str = Field(..., description=\"The response content\")   class DocumentationAgent(BaseModel):     @openai.call(\"gpt-4o-mini\", response_model=Response, json_mode=True)     @prompt_template(         \"\"\"         SYSTEM:         You are an AI Assistant that is an expert at answering questions about Mirascope.         Here is the relevant documentation to answer the question.          First classify the question into one of two types:             - General Information: Questions about the system or its components.             - Code Examples: Questions that require code snippets or examples.          For General Information, provide a summary of the relevant documents if the question is too broad ask for more details.          If the context does not answer the question, say that the information is not available or you could not find it.          For Code Examples, output ONLY code without any markdown, with comments if necessary.         If the context does not answer the question, say that the information is not available.          Examples:             Question: \"What is Mirascope?\"             Answer:             A toolkit for building AI-powered applications with Large Language Models (LLMs).             Explanation: This is a General Information question, so a summary is provided.              Question: \"How do I make a basic OpenAI call using Mirascope?\"             Answer:             from mirascope.core import openai, prompt_template               @openai.call(\"gpt-4o-mini\")             def recommend_book(genre: str) -&gt; str:                 return f'Recommend a {genre} book'              response = recommend_book(\"fantasy\")             print(response.content)             Explanation: This is a Code Examples question, so only a code snippet is provided.          Context:         {context:list}          USER:         {question}         \"\"\"     )     def _call(self, question: str) -&gt; openai.OpenAIDynamicConfig:         documents = get_documents(question)         return {\"computed_fields\": {\"context\": documents}}      def _step(self, question: str):         answer = self._call(question)         print(\"(Assistant):\", answer.content)      def run(self):         while True:             question = input(\"(User): \")             if question == \"exit\":                 break             self._step(question)   if __name__ == \"__main__\":     DocumentationAgent().run()     # Output:     \"\"\"     (User): How do I make an LLM call using Mirascope?     (Assistant): from mirascope.core import openai          @openai.call('gpt-4o-mini')     def recommend_book(genre: str) -&gt; str:         return f'Recommend a {genre} book'          response = recommend_book('fantasy')     print(response.content)     \"\"\" <p>Additional Real-World Applications</p> <ul> <li>Improved Chat Application: Maintain the most current documentation by storing it in a vector database or using a tool to retrieve up-to-date information in your chat application</li> <li>GitHub Issues Bot: Add a GitHub bot that scans through issues and answers questions for users.</li> <li>Interactive Internal Knowledge Base: Index company handbooks and internal documentation to enable instant, AI-powered Q&amp;A access.</li> </ul> <p>When adapting this recipe, consider:</p> <ul> <li>Experiment with different model providers and version for quality.</li> <li>Add evaluations to the agent, and feed the errors back to the LLM for refinement.</li> <li>Add history to the Agent so that the LLM can generate context-aware queries to retrieve more semantically similar embeddings.</li> </ul>"},{"location":"tutorials/agents/documentation_agent/#documentation-agent","title":"Documentation Agent\u00b6","text":"<p>In this recipe, we will be building a <code>DocumentationAgent</code> that has access to some documentation. We will be using Mirascope documentation in this example, but this should work on all types of documents. This is implemented using <code>OpenAI</code>, see Local Chat with Codebase for the Llama3.1 implementation.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Chaining</li> <li>Response Model</li> <li>JSON Mode</li> <li>Agents</li> </ul>"},{"location":"tutorials/agents/documentation_agent/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/agents/documentation_agent/#store-embeddings","title":"Store Embeddings\u00b6","text":"<p>The first step is to grab our docs and embed them into a vectorstore. In this recipe, we will be storing our vectorstore locally, but using Pinecone or other cloud vectorstore providers will also work. We adjusted the <code>chunk_size</code> and <code>chunk_overlap</code> to get the best results for Mirascope docs, but these values may not necessarily be good for other types of documents.</p>"},{"location":"tutorials/agents/documentation_agent/#load-embeddings","title":"Load Embeddings\u00b6","text":"<p>After we saved our embeddings, we can use the below code to retrieve it and load in memory:</p>"},{"location":"tutorials/agents/documentation_agent/#llm-reranker","title":"LLM Reranker\u00b6","text":"<p>Vectorstore retrieval relies on semantic similarity search but lacks contextual understanding. By employing an LLM to rerank results based on relevance, we can achieve more accurate and robust answers.</p>"},{"location":"tutorials/agents/documentation_agent/#getting-our-documents","title":"Getting our documents\u00b6","text":"<p>With our LLM Reranker configured, we can now retrieve documents for our query. The process involves three steps:</p> <ol> <li>Fetch the top 10 (<code>top_k</code>) semantic search results from our vectorstore.</li> <li>Process these results through our LLM Reranker in batches of 5 (<code>choice_batch_size</code>).</li> <li>Return the top 2 (<code>top_n</code>) most relevant documents.</li> </ol>"},{"location":"tutorials/agents/documentation_agent/#creating-documentationagent","title":"Creating <code>DocumentationAgent</code>\u00b6","text":"<p>Our <code>get_documents</code> method retrieves relevant documents, which we pass to the <code>context</code> for our call. The LLM then categorizes the question as either <code>code</code> or <code>general</code>. Based on this classification:</p> <ul> <li>For code questions, the LLM generates an executable code snippet.</li> <li>For general questions, the LLM summarizes the content of the retrieved documents.</li> </ul>"},{"location":"tutorials/agents/local_chat_with_codebase/","title":"Local Chat with Codebase","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n!pip install llama-index  llama-index-llms-ollama llama-index-embeddings-huggingface huggingface\n</pre> !pip install \"mirascope[openai]\" !pip install llama-index  llama-index-llms-ollama llama-index-embeddings-huggingface huggingface In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import (\n    Settings,\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n)\nfrom llama_index.legacy.embeddings import HuggingFaceEmbedding\nfrom llama_index.legacy.llms import Ollama\n\nSettings.llm = Ollama(model=\"llama3.1\")\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n</pre> from llama_index.core import (     Settings,     SimpleDirectoryReader,     VectorStoreIndex, ) from llama_index.legacy.embeddings import HuggingFaceEmbedding from llama_index.legacy.llms import Ollama  Settings.llm = Ollama(model=\"llama3.1\") Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") <p>We will be using LlamaIndex for RAG, and setting up the proper models we will be using for Re-ranking and the Embedding model.</p> In\u00a0[\u00a0]: Copied! <pre>from llama_index.core.storage import StorageContext\nfrom llama_index.core.vector_stores import SimpleVectorStore\n\ndocuments = SimpleDirectoryReader(\"PATH/TO/YOUR/DOCS\").load_data()\nvector_store = SimpleVectorStore()\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\nindex.storage_context.persist()\n</pre> from llama_index.core.storage import StorageContext from llama_index.core.vector_stores import SimpleVectorStore  documents = SimpleDirectoryReader(\"PATH/TO/YOUR/DOCS\").load_data() vector_store = SimpleVectorStore() storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex.from_documents(documents, storage_context=storage_context) index.storage_context.persist() In\u00a0[\u00a0]: Copied! <pre>from llama_index.core import load_index_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\nloaded_index = load_index_from_storage(storage_context)\nquery_engine = loaded_index.as_query_engine()\n</pre> from llama_index.core import load_index_from_storage  storage_context = StorageContext.from_defaults(persist_dir=\"storage\") loaded_index = load_index_from_storage(storage_context) query_engine = loaded_index.as_query_engine() In\u00a0[\u00a0]: Copied! <pre>import re\n\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.postprocessor import LLMRerank\nfrom llama_index.core.storage import StorageContext\nfrom llama_index.core.vector_stores import SimpleVectorStore\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.ollama import Ollama\nfrom mirascope.core import openai, prompt_template\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n\ndef custom_parse_choice_select_answer_fn(\n    answer: str, num_choices: int, raise_error: bool = False\n) -&gt; tuple[list[int], list[float]]:\n    \"\"\"Custom parse choice select answer function.\"\"\"\n    answer_lines = answer.split(\"\\n\")\n    answer_nums = []\n    answer_relevances = []\n    for answer_line in answer_lines:\n        line_tokens = answer_line.split(\",\")\n        if len(line_tokens) != 2:\n            if not raise_error:\n                continue\n            else:\n                raise ValueError(\n                    f\"Invalid answer line: {answer_line}. \"\n                    \"Answer line must be of the form: \"\n                    \"answer_num: &lt;int&gt;, answer_relevance: &lt;float&gt;\"\n                )\n        split_tokens = line_tokens[0].split(\":\")\n        if (\n            len(split_tokens) != 2\n            or split_tokens[1] is None\n            or not split_tokens[1].strip().isdigit()\n        ):\n            continue\n        answer_num = int(line_tokens[0].split(\":\")[1].strip())\n        if answer_num &gt; num_choices:\n            continue\n        answer_nums.append(answer_num)\n        # extract just the first digits after the colon.\n        _answer_relevance = re.findall(r\"\\d+\", line_tokens[1].split(\":\")[1].strip())[0]\n        answer_relevances.append(float(_answer_relevance))\n    return answer_nums, answer_relevances\n\n\ndef get_documents(query: str) -&gt; str:\n    \"\"\"The get_documents tool that retrieves Mirascope documentation based on the\n    relevance of the query\"\"\"\n    query_engine = loaded_index.as_query_engine(\n        similarity_top_k=10,\n        node_postprocessors=[\n            LLMRerank(\n                choice_batch_size=5,\n                top_n=2,\n                parse_choice_select_answer_fn=custom_parse_choice_select_answer_fn,\n            )\n        ],\n        response_mode=\"tree_summarize\",\n    )\n\n    response = query_engine.query(query)\n    if isinstance(response, Response):\n        return response.response or \"No documents found.\"\n    return \"No documents found.\"\n\n\nclass MirascopeBot(BaseModel):\n    @openai.call(\n        model=\"llama3.1\",\n        client=OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n    )\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an AI Assistant that is an expert at answering questions about Mirascope.\n        Here is the relevant documentation to answer the question.\n\n        Context:\n        {context}\n\n        USER:\n        {question}\n        \"\"\"\n    )\n    def _step(self, context: str, question: str): ...\n\n    def _get_response(self, question: str):\n        context = get_documents(question)\n        answer = self._step(context, question)\n        print(\"(Assistant):\", answer.content)\n        return\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            self._get_response(question)\n\n\nMirascopeBot().run()\n# Output:\n\"\"\"\n(User): How do I make an LLM call using Mirascope?\n(Assistant): To make an LLM (Large Language Model) call using Mirascope, you can use the `call` decorator provided by Mirascope.\n\nHere are the basic steps:\n\n1. Import the `call` decorator from Mirascope.\n2. Define a function that takes any number of arguments and keyword arguments. This will be the function that makes the LLM call.\n3. Prepend this function definition with the `@call` decorator, specifying the name of the model you want to use (e.g., \"gpt-4o\").\n4. Optionally, pass additional keyword arguments to customize the behavior of the LLM call.\n\nFor example:\n```python\nfrom mirascope import call\n\n@click('gpt-4o')\ndef greet(name: str) -&gt; dict:\n   return {'greeting': f\"Hello, {name}!\"}\n```\nIn this example, `greet` is the function that makes an LLM call to a GPT-4o model. The `@call('gpt-4o')` decorator turns this function into an LLM call.\n\"\"\"\n</pre> import re  from llama_index.core.base.response.schema import Response from llama_index.core.postprocessor import LLMRerank from llama_index.core.storage import StorageContext from llama_index.core.vector_stores import SimpleVectorStore from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.llms.ollama import Ollama from mirascope.core import openai, prompt_template from openai import OpenAI from pydantic import BaseModel   def custom_parse_choice_select_answer_fn(     answer: str, num_choices: int, raise_error: bool = False ) -&gt; tuple[list[int], list[float]]:     \"\"\"Custom parse choice select answer function.\"\"\"     answer_lines = answer.split(\"\\n\")     answer_nums = []     answer_relevances = []     for answer_line in answer_lines:         line_tokens = answer_line.split(\",\")         if len(line_tokens) != 2:             if not raise_error:                 continue             else:                 raise ValueError(                     f\"Invalid answer line: {answer_line}. \"                     \"Answer line must be of the form: \"                     \"answer_num: , answer_relevance: \"                 )         split_tokens = line_tokens[0].split(\":\")         if (             len(split_tokens) != 2             or split_tokens[1] is None             or not split_tokens[1].strip().isdigit()         ):             continue         answer_num = int(line_tokens[0].split(\":\")[1].strip())         if answer_num &gt; num_choices:             continue         answer_nums.append(answer_num)         # extract just the first digits after the colon.         _answer_relevance = re.findall(r\"\\d+\", line_tokens[1].split(\":\")[1].strip())[0]         answer_relevances.append(float(_answer_relevance))     return answer_nums, answer_relevances   def get_documents(query: str) -&gt; str:     \"\"\"The get_documents tool that retrieves Mirascope documentation based on the     relevance of the query\"\"\"     query_engine = loaded_index.as_query_engine(         similarity_top_k=10,         node_postprocessors=[             LLMRerank(                 choice_batch_size=5,                 top_n=2,                 parse_choice_select_answer_fn=custom_parse_choice_select_answer_fn,             )         ],         response_mode=\"tree_summarize\",     )      response = query_engine.query(query)     if isinstance(response, Response):         return response.response or \"No documents found.\"     return \"No documents found.\"   class MirascopeBot(BaseModel):     @openai.call(         model=\"llama3.1\",         client=OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),     )     @prompt_template(         \"\"\"         SYSTEM:         You are an AI Assistant that is an expert at answering questions about Mirascope.         Here is the relevant documentation to answer the question.          Context:         {context}          USER:         {question}         \"\"\"     )     def _step(self, context: str, question: str): ...      def _get_response(self, question: str):         context = get_documents(question)         answer = self._step(context, question)         print(\"(Assistant):\", answer.content)         return      def run(self):         while True:             question = input(\"(User): \")             if question == \"exit\":                 break             self._get_response(question)   MirascopeBot().run() # Output: \"\"\" (User): How do I make an LLM call using Mirascope? (Assistant): To make an LLM (Large Language Model) call using Mirascope, you can use the `call` decorator provided by Mirascope.  Here are the basic steps:  1. Import the `call` decorator from Mirascope. 2. Define a function that takes any number of arguments and keyword arguments. This will be the function that makes the LLM call. 3. Prepend this function definition with the `@call` decorator, specifying the name of the model you want to use (e.g., \"gpt-4o\"). 4. Optionally, pass additional keyword arguments to customize the behavior of the LLM call.  For example: ```python from mirascope import call  @click('gpt-4o') def greet(name: str) -&gt; dict:    return {'greeting': f\"Hello, {name}!\"} ``` In this example, `greet` is the function that makes an LLM call to a GPT-4o model. The `@call('gpt-4o')` decorator turns this function into an LLM call. \"\"\" <p>Check out OpenAI Implementation</p> <p> While we demonstrated an open source version of chatting with our codebase, there are several improvements we can make to get better results. Refer to Documentation Agent Cookbook for a detailed walkthrough on the improvements made. </p> <p>Additional Real-World Applications</p> <ul> <li>Improved Chat Application: Maintain the most current documentation by storing it in a vector database or using a tool to retrieve up-to-date information in your chat application</li> <li>Code Autocomplete: Integrate the vector database with the LLM to generate accurate, context-aware code suggestions.</li> <li>Interactive Internal Knowledge Base: Index company handbooks and internal documentation to enable instant, AI-powered Q&amp;A access.</li> </ul> <p>When adapting this recipe, consider:</p> <ul> <li>Experiment with different model providers and version for quality.</li> <li>Use a different Reranking prompt as that impacts the quality of retrieval</li> <li>Implement a feedback loop so the LLM hallucinates less frequently.</li> </ul>"},{"location":"tutorials/agents/local_chat_with_codebase/#local-chat-with-codebase","title":"Local Chat with Codebase\u00b6","text":"<p>In this recipe, we will be using all Open Source Software to build a local ChatBot that has access to some documentation. We will be using Mirascope documentation in this example, but this should work on all types of documents. Also note that we will be using a smaller Llama 3.1 8B so the results will not be as impressive as larger models. Later, we will take a look at how OpenAI's GPT compares with Llama.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Agents</li> </ul>"},{"location":"tutorials/agents/local_chat_with_codebase/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/agents/local_chat_with_codebase/#configuration","title":"Configuration\u00b6","text":"<p>For this setup, we are using Ollama, but vLLM would also work.</p>"},{"location":"tutorials/agents/local_chat_with_codebase/#store-embeddings","title":"Store Embeddings\u00b6","text":"<p>The first step is to grab our docs and embed them into a vectorstore. In this recipe, we will be storing our vectorstore locally, but using Pinecone or other cloud vectorstore providers will also work.</p>"},{"location":"tutorials/agents/local_chat_with_codebase/#load-embeddings","title":"Load Embeddings\u00b6","text":"<p>After we saved our embeddings, we can use the below code to retrieve it and load in memory:</p>"},{"location":"tutorials/agents/local_chat_with_codebase/#code","title":"Code\u00b6","text":"<p>We need to update LlamaIndex <code>default_parse_choice_select_answer_fn</code> for Llama 3.1. You may need to update the <code>custom_parse_choice_select_answer_fn</code> depending on which model you are using. Adding re-ranking is extremely important to get better quality retrievals so the LLM can make better context-aware answers.</p> <p>We will be creating an Agent that will read Mirascope documentation called MiraBot which will answer questions regarding Mirascope docs.</p>"},{"location":"tutorials/agents/localized_agent/","title":"Localized Agent","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[2]: Copied! <pre>import asyncio\nfrom datetime import datetime\n\nimport aiohttp\nimport requests\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel\n\nNIMBLE_TOKEN = \"YOUR_NIMBLE_API_KEY\"\n\n\nclass LocalizedRecommenderBase(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    async def _nimble_google_maps_places(\n        self, session: aiohttp.ClientSession, place_id: str\n    ):\n        \"\"\"\n        Use Nimble to get the details of a place on Google Maps.\n        \"\"\"\n        url = \"https://api.webit.live/api/v1/realtime/serp\"\n        headers = {\n            \"Authorization\": f\"Basic {NIMBLE_TOKEN}\",\n            \"Content-Type\": \"application/json\",\n        }\n        place_data = {\n            \"parse\": True,\n            \"search_engine\": \"google_maps_place\",\n            \"place_id\": place_id,\n            \"domain\": \"com\",\n            \"format\": \"json\",\n            \"render\": True,\n            \"country\": \"US\",\n            \"locale\": \"en\",\n        }\n        async with session.get(url, json=place_data, headers=headers) as response:\n            data = await response.json()\n            result = data[\"parsing\"][\"entities\"][\"Place\"][0]\n            return {\n                \"opening_hours\": result.get(\"place_information\", {}).get(\n                    \"opening_hours\", \"\"\n                ),\n                \"rating\": result.get(\"rating\", \"\"),\n                \"name\": result.get(\"title\", \"\"),\n            }\n\n    async def _nimble_google_maps(self, latitude: float, longitude: float, query: str):\n        \"\"\"\n        Use Nimble to search for places on Google Maps.\n        \"\"\"\n        url = \"https://api.webit.live/api/v1/realtime/serp\"\n        headers = {\n            \"Authorization\": f\"Basic {NIMBLE_TOKEN}\",\n            \"Content-Type\": \"application/json\",\n        }\n        search_data = {\n            \"parse\": True,\n            \"search_engine\": \"google_maps_search\",\n            \"query\": query,\n            \"coordinates\": {\"latitude\": latitude, \"longitude\": longitude},\n            \"domain\": \"com\",\n            \"format\": \"json\",\n            \"render\": True,\n            \"country\": \"US\",\n            \"locale\": \"en\",\n        }\n        search_response = requests.post(url, headers=headers, json=search_data)\n        search_json_response = search_response.json()\n        search_results = [\n            {\n                \"place_id\": result.get(\"place_id\", \"\"),\n            }\n            for result in search_json_response[\"parsing\"][\"entities\"][\"SearchResult\"]\n        ]\n        results = []\n        async with aiohttp.ClientSession() as session:\n            tasks = [\n                self._nimble_google_maps_places(session, results.get(\"place_id\", \"\"))\n                for results in search_results\n            ]\n            results = await asyncio.gather(*tasks)\n        return results\n\n    async def _get_current_date(self):\n        \"\"\"Get the current date and time.\"\"\"\n        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    async def _get_coordinates_from_location(self, location_name: str):\n        \"\"\"Get the coordinates of a location.\"\"\"\n        base_url = \"https://nominatim.openstreetmap.org/search\"\n        params = {\"q\": location_name, \"format\": \"json\", \"limit\": 1}\n        headers = {\"User-Agent\": \"mirascope/1.0\"}\n        response = requests.get(base_url, params=params, headers=headers)\n        data = response.json()\n\n        if data:\n            latitude = data[0].get(\"lat\")\n            longitude = data[0].get(\"lon\")\n            return f\"Latitude: {latitude}, Longitude: {longitude}\"\n        else:\n            return \"No location found, ask me about a specific location.\"\n</pre> import asyncio from datetime import datetime  import aiohttp import requests from mirascope.core import BaseMessageParam, openai from pydantic import BaseModel  NIMBLE_TOKEN = \"YOUR_NIMBLE_API_KEY\"   class LocalizedRecommenderBase(BaseModel):     history: list[BaseMessageParam | openai.OpenAIMessageParam] = []      async def _nimble_google_maps_places(         self, session: aiohttp.ClientSession, place_id: str     ):         \"\"\"         Use Nimble to get the details of a place on Google Maps.         \"\"\"         url = \"https://api.webit.live/api/v1/realtime/serp\"         headers = {             \"Authorization\": f\"Basic {NIMBLE_TOKEN}\",             \"Content-Type\": \"application/json\",         }         place_data = {             \"parse\": True,             \"search_engine\": \"google_maps_place\",             \"place_id\": place_id,             \"domain\": \"com\",             \"format\": \"json\",             \"render\": True,             \"country\": \"US\",             \"locale\": \"en\",         }         async with session.get(url, json=place_data, headers=headers) as response:             data = await response.json()             result = data[\"parsing\"][\"entities\"][\"Place\"][0]             return {                 \"opening_hours\": result.get(\"place_information\", {}).get(                     \"opening_hours\", \"\"                 ),                 \"rating\": result.get(\"rating\", \"\"),                 \"name\": result.get(\"title\", \"\"),             }      async def _nimble_google_maps(self, latitude: float, longitude: float, query: str):         \"\"\"         Use Nimble to search for places on Google Maps.         \"\"\"         url = \"https://api.webit.live/api/v1/realtime/serp\"         headers = {             \"Authorization\": f\"Basic {NIMBLE_TOKEN}\",             \"Content-Type\": \"application/json\",         }         search_data = {             \"parse\": True,             \"search_engine\": \"google_maps_search\",             \"query\": query,             \"coordinates\": {\"latitude\": latitude, \"longitude\": longitude},             \"domain\": \"com\",             \"format\": \"json\",             \"render\": True,             \"country\": \"US\",             \"locale\": \"en\",         }         search_response = requests.post(url, headers=headers, json=search_data)         search_json_response = search_response.json()         search_results = [             {                 \"place_id\": result.get(\"place_id\", \"\"),             }             for result in search_json_response[\"parsing\"][\"entities\"][\"SearchResult\"]         ]         results = []         async with aiohttp.ClientSession() as session:             tasks = [                 self._nimble_google_maps_places(session, results.get(\"place_id\", \"\"))                 for results in search_results             ]             results = await asyncio.gather(*tasks)         return results      async def _get_current_date(self):         \"\"\"Get the current date and time.\"\"\"         return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")      async def _get_coordinates_from_location(self, location_name: str):         \"\"\"Get the coordinates of a location.\"\"\"         base_url = \"https://nominatim.openstreetmap.org/search\"         params = {\"q\": location_name, \"format\": \"json\", \"limit\": 1}         headers = {\"User-Agent\": \"mirascope/1.0\"}         response = requests.get(base_url, params=params, headers=headers)         data = response.json()          if data:             latitude = data[0].get(\"lat\")             longitude = data[0].get(\"lon\")             return f\"Latitude: {latitude}, Longitude: {longitude}\"         else:             return \"No location found, ask me about a specific location.\" <p>A quick summary of each of the tools:</p> <ul> <li><code>_get_current_date</code> - Gets the current date, this is relevant if the user wants to ask if a place is open or closed.</li> <li><code>_get_coordinates_from_location</code> - Gets the latitude and longitude based on the user\u2019s query using OSM\u2019s Geolocation.</li> <li><code>_nimble_google_maps</code> - Gets google maps information using the Nimble API.</li> </ul> In\u00a0[3]: Copied! <pre>from mirascope.core import prompt_template\n\n\nclass LocalizedRecommenderBaseWithStep(LocalizedRecommenderBase):\n    @openai.call(model=\"gpt-4o\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are a local guide that recommends the best places to visit in a place.\n        Use the `_get_current_date` function to get the current date.\n        Use the `_get_coordinates_from_location` function to get the coordinates of a location if you need it.\n        Use the `_nimble_google_maps` function to get the best places to visit in a location based on the users query.\n\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    async def _step(self, question: str) -&gt; openai.OpenAIDynamicConfig:\n        return {\n            \"tools\": [\n                self._get_current_date,\n                self._get_coordinates_from_location,\n                self._nimble_google_maps,\n            ]\n        }\n</pre> from mirascope.core import prompt_template   class LocalizedRecommenderBaseWithStep(LocalizedRecommenderBase):     @openai.call(model=\"gpt-4o\", stream=True)     @prompt_template(         \"\"\"         SYSTEM:         You are a local guide that recommends the best places to visit in a place.         Use the `_get_current_date` function to get the current date.         Use the `_get_coordinates_from_location` function to get the coordinates of a location if you need it.         Use the `_nimble_google_maps` function to get the best places to visit in a location based on the users query.          MESSAGES: {self.history}         USER: {question}         \"\"\"     )     async def _step(self, question: str) -&gt; openai.OpenAIDynamicConfig:         return {             \"tools\": [                 self._get_current_date,                 self._get_coordinates_from_location,                 self._nimble_google_maps,             ]         } <p>Since our tools are defined inside our agent, we need to use Mirascope <code>DynamicConfig</code> to give our LLM call access to tools.</p> In\u00a0[4]: Copied! <pre>class LocalizedRecommender(LocalizedRecommenderBaseWithStep):\n    async def _get_response(self, question: str):\n        response = await self._step(question)\n        tool_call = None\n        output = None\n        async for chunk, tool in response:\n            if tool:\n                output = await tool.call()\n                tool_call = tool\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.history.append(response.user_message_param)\n        self.history.append(response.message_param)\n        if tool_call and output:\n            self.history += response.tool_message_params([(tool_call, str(output))])\n            return await self._get_response(question)\n        return\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._get_response(question)\n            print()\n</pre> class LocalizedRecommender(LocalizedRecommenderBaseWithStep):     async def _get_response(self, question: str):         response = await self._step(question)         tool_call = None         output = None         async for chunk, tool in response:             if tool:                 output = await tool.call()                 tool_call = tool             else:                 print(chunk.content, end=\"\", flush=True)         if response.user_message_param:             self.history.append(response.user_message_param)         self.history.append(response.message_param)         if tool_call and output:             self.history += response.tool_message_params([(tool_call, str(output))])             return await self._get_response(question)         return      async def run(self):         while True:             question = input(\"(User): \")             if question == \"exit\":                 break             print(\"(Assistant): \", end=\"\", flush=True)             await self._get_response(question)             print() <p>We use Mirascope utility functions <code>user_message_param</code>, <code>message_param</code>, and <code>tool_message_params</code> to easily update our history so the LLM is aware of which tools its called and what the next steps are.</p> In\u00a0[5]: Copied! <pre>recommender = LocalizedRecommender(history=[])\nawait recommender.run()\n</pre> recommender = LocalizedRecommender(history=[]) await recommender.run() <pre>(Assistant): Sure, I can help you find beautiful beaches! Could you please specify the location or city you are interested in?\n</pre> <p>The more information we give the LLM from the Nimble API, the more specific of a recommendation the LLM can give, such as information about outside seating, dietary restrictions, cuisine, and more.</p> <p>Additional Real-World Applications</p> <ul> <li>Mobile Travel Companion: Transform this example into a portable app for on-the-go recommendations during your travels.</li> <li>Smart Day Planner: Discover nearby events and efficiently map out your itinerary, optimizing routes based on timing and proximity.</li> <li>Immersive Explorer: Blend location awareness with visual recognition using a multimodal model to enhance your on-site experience.</li> </ul> <p>When adapting this recipe, consider:</p> <ul> <li>Tailor the Nimble tool by pulling different information for your requirements.</li> <li>Give the LLM access to the web to access more detailed information.</li> <li>Connect the agent to a database for quicker data fetching.</li> </ul>"},{"location":"tutorials/agents/localized_agent/#localized-agent","title":"Localized Agent\u00b6","text":"<p>This recipe will show you how to use Nimble to make a simple Q&amp;A ChatBot based on Google Maps data.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Tools</li> <li>Agents</li> </ul> <p>Background</p> <p> In the past, users had to rely on search engines and manually browse through multiple web pages to research or answer questions. Large Language Models (LLMs) have revolutionized this process. They can efficiently utilize map data and extract relevant content. By leveraging this information, LLMs can quickly provide accurate answers to user queries, eliminating the need for active searching. Users can simply pose their questions and let the LLM work in the background, significantly streamlining the information retrieval process. </p>"},{"location":"tutorials/agents/localized_agent/#setup","title":"Setup\u00b6","text":"<p>We will need an API key for :</p> <ul> <li>Nimble API Key or alternatively directly from Google Maps API</li> </ul> <p>And of course, Mirascope.</p>"},{"location":"tutorials/agents/localized_agent/#creating-a-localized-recommender-chatbot","title":"Creating a Localized Recommender ChatBot\u00b6","text":""},{"location":"tutorials/agents/localized_agent/#setup-tools","title":"Setup Tools\u00b6","text":"<p>Let's start off with defining our tools <code>get_current_date</code>, <code>get_coordinates_from_location</code> , and <code>nimble_google_maps</code>  for our agent:</p>"},{"location":"tutorials/agents/localized_agent/#creating-the-agent","title":"Creating the Agent\u00b6","text":"<p>We then create our Agent, giving it the tools we defined and history for memory. As this agent functions as a ChatBot, we implement streaming to enhance the user experience:</p>"},{"location":"tutorials/agents/localized_agent/#creating-our-run-function","title":"Creating our run function\u00b6","text":"<p>Now it is time to create our <code>run</code> function. We will first prompt the user to ask a question. The LLM will continue to call tools until it has all the information needed to answer the user\u2019s question:</p>"},{"location":"tutorials/agents/localized_agent/#results","title":"Results\u00b6","text":""},{"location":"tutorials/agents/sql_agent/","title":"Generate SQL with LLM","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[2]: Copied! <pre>import sqlite3\n\ncon = sqlite3.connect(\"database.db\")\ncur = con.cursor()\n# ONE TIME SETUP\ncur.execute(\"\"\"\n    CREATE TABLE ReadingList (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        title TEXT NOT NULL,\n        status TEXT NOT NULL CHECK(status IN ('Not Started', 'In Progress', 'Complete')),\n        rating INTEGER CHECK(rating BETWEEN 1 AND 5)\n    )\n\"\"\")\ncon.commit()\n</pre> import sqlite3  con = sqlite3.connect(\"database.db\") cur = con.cursor() # ONE TIME SETUP cur.execute(\"\"\"     CREATE TABLE ReadingList (         id INTEGER PRIMARY KEY AUTOINCREMENT,         title TEXT NOT NULL,         status TEXT NOT NULL CHECK(status IN ('Not Started', 'In Progress', 'Complete')),         rating INTEGER CHECK(rating BETWEEN 1 AND 5)     ) \"\"\") con.commit() <p>This will be our playground example.</p> In\u00a0[3]: Copied! <pre>import sqlite3\nfrom typing import ClassVar\n\nfrom mirascope.core import BaseMessageParam, openai\nfrom pydantic import BaseModel, ConfigDict\n\n\nclass LibrarianBase(BaseModel):\n    con: ClassVar[sqlite3.Connection] = sqlite3.connect(\"database.db\")\n    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def _run_query(self, query: str) -&gt; str:\n        \"\"\"A SELECT query to run.\"\"\"\n        print(query)\n        try:\n            cursor = self.con.cursor()\n            cursor.execute(query)\n            res = cursor.fetchall()\n            return str(res)\n        except sqlite3.Error as e:\n            return str(e)\n\n    def _execute_query(self, query: str) -&gt; str:\n        \"\"\"An INSERT, UPDATE, or DELETE query to execute.\"\"\"\n        print(query)\n        try:\n            cursor = self.con.cursor()\n            cursor.execute(query)\n            rows_affected = cursor.rowcount\n            self.con.commit()\n            if rows_affected &gt; 0:\n                return f\"Query executed successfully, {rows_affected} row(s) were updated/inserted.\"\n            else:\n                return \"No rows were updated/inserted.\"\n        except sqlite3.Error as e:\n            print(e)\n            return str(e)\n</pre> import sqlite3 from typing import ClassVar  from mirascope.core import BaseMessageParam, openai from pydantic import BaseModel, ConfigDict   class LibrarianBase(BaseModel):     con: ClassVar[sqlite3.Connection] = sqlite3.connect(\"database.db\")     messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []      model_config = ConfigDict(arbitrary_types_allowed=True)      def _run_query(self, query: str) -&gt; str:         \"\"\"A SELECT query to run.\"\"\"         print(query)         try:             cursor = self.con.cursor()             cursor.execute(query)             res = cursor.fetchall()             return str(res)         except sqlite3.Error as e:             return str(e)      def _execute_query(self, query: str) -&gt; str:         \"\"\"An INSERT, UPDATE, or DELETE query to execute.\"\"\"         print(query)         try:             cursor = self.con.cursor()             cursor.execute(query)             rows_affected = cursor.rowcount             self.con.commit()             if rows_affected &gt; 0:                 return f\"Query executed successfully, {rows_affected} row(s) were updated/inserted.\"             else:                 return \"No rows were updated/inserted.\"         except sqlite3.Error as e:             print(e)             return str(e) <p>Now that we have our tools setup it is time to engineer our prompt</p> In\u00a0[4]: Copied! <pre>from mirascope.core import prompt_template\n\n\nclass Librarian(LibrarianBase):\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are a friendly and knowledgeable librarian named Mira. Your role is to \n        assist patrons with their queries, recommend books, \n        and provide information on a wide range of topics.\n\n        Personality:\n            - Warm and approachable, always ready with a kind word\n            - Patient and understanding, especially with those who are hesitant or confused\n            - Enthusiastic about books and learning\n            - Respectful of all patrons, regardless of their background or level of knowledge\n\n        Services:\n            - Keep track of patrons' reading lists using a SQLite database. Assume that the user is non technical and will ask you\n        questions in plain English.\n            - Recommend books based on the user's preferences\n        Your task is to write a query based on the user's request.\n\n        The database schema is as follows:\n\n        TABLE ReadingList (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            title TEXT NOT NULL,\n            status TEXT CHECK(status IN ('Not Started', 'In Progress', 'Complete')) NOT NULL,\n            rating INTEGER CHECK(rating &gt;= 1 AND rating &lt;= 5),\n        );\n\n        You must interpret the user's request and write the appropriate SQL query to\n        pass in the tools.\n\n        Example interactions:\n            1. Select\n                - USER: \"Show me all books.\"\n                - ASSISTANT: \"SELECT * FROM ReadingList;\"\n            2. Insert\n                - USER: \"Add Gone with the Wind to my reading list.\"\n                - ASSISTANT: \"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\"\n            3. Update\n                - USER: \"I just finished Gone with the Wind, can you update the status, and give it 5 stars??\"\n                - ASSISTANT: \"UPDATE ReadingList SET status = 'Complete' and rating = 5 WHERE title = 'Gone with the Wind';\"\n            4. Delete\n                - USER: \"Remove Gone with the Wind from my reading list.\"\n                - ASSISTANT: \"DELETE FROM ReadingList WHERE title = 'Gone with the Wind';\"\n\n        If field are not mentioned, omit them from the query.\n        All queries must end with a semicolon.\n\n        You have access to the following tools:\n        - `_run_query`: When user asks for recommendations, you can use this tool to see what they have read.\n        - `_execute_query`: Use the query generated to execute an \n            INSERT, UPDATE, or DELETE query.\n\n        You must use these tools to interact with the database.\n\n        MESSAGES: {self.messages}\n        USER: {query}\n        \"\"\"\n    )\n    async def _stream(self, query: str) -&gt; openai.OpenAIDynamicConfig:\n        return {\"tools\": [self._run_query, self._execute_query]}\n\n    async def _step(self, question: str):\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._step(question)\n            print()\n</pre> from mirascope.core import prompt_template   class Librarian(LibrarianBase):     @openai.call(model=\"gpt-4o-mini\", stream=True)     @prompt_template(         \"\"\"         SYSTEM:         You are a friendly and knowledgeable librarian named Mira. Your role is to          assist patrons with their queries, recommend books,          and provide information on a wide range of topics.          Personality:             - Warm and approachable, always ready with a kind word             - Patient and understanding, especially with those who are hesitant or confused             - Enthusiastic about books and learning             - Respectful of all patrons, regardless of their background or level of knowledge          Services:             - Keep track of patrons' reading lists using a SQLite database. Assume that the user is non technical and will ask you         questions in plain English.             - Recommend books based on the user's preferences         Your task is to write a query based on the user's request.          The database schema is as follows:          TABLE ReadingList (             id INTEGER PRIMARY KEY AUTOINCREMENT,             title TEXT NOT NULL,             status TEXT CHECK(status IN ('Not Started', 'In Progress', 'Complete')) NOT NULL,             rating INTEGER CHECK(rating &gt;= 1 AND rating &lt;= 5),         );          You must interpret the user's request and write the appropriate SQL query to         pass in the tools.          Example interactions:             1. Select                 - USER: \"Show me all books.\"                 - ASSISTANT: \"SELECT * FROM ReadingList;\"             2. Insert                 - USER: \"Add Gone with the Wind to my reading list.\"                 - ASSISTANT: \"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\"             3. Update                 - USER: \"I just finished Gone with the Wind, can you update the status, and give it 5 stars??\"                 - ASSISTANT: \"UPDATE ReadingList SET status = 'Complete' and rating = 5 WHERE title = 'Gone with the Wind';\"             4. Delete                 - USER: \"Remove Gone with the Wind from my reading list.\"                 - ASSISTANT: \"DELETE FROM ReadingList WHERE title = 'Gone with the Wind';\"          If field are not mentioned, omit them from the query.         All queries must end with a semicolon.          You have access to the following tools:         - `_run_query`: When user asks for recommendations, you can use this tool to see what they have read.         - `_execute_query`: Use the query generated to execute an              INSERT, UPDATE, or DELETE query.          You must use these tools to interact with the database.          MESSAGES: {self.messages}         USER: {query}         \"\"\"     )     async def _stream(self, query: str) -&gt; openai.OpenAIDynamicConfig:         return {\"tools\": [self._run_query, self._execute_query]}      async def _step(self, question: str):         response = await self._stream(question)         tools_and_outputs = []         async for chunk, tool in response:             if tool:                 tools_and_outputs.append((tool, tool.call()))             else:                 print(chunk.content, end=\"\", flush=True)         if response.user_message_param:             self.messages.append(response.user_message_param)         self.messages.append(response.message_param)         if tools_and_outputs:             self.messages += response.tool_message_params(tools_and_outputs)             await self._step(\"\")      async def run(self):         while True:             question = input(\"(User): \")             if question == \"exit\":                 break             print(\"(Assistant): \", end=\"\", flush=True)             await self._step(question)             print() <p>Let's break down the prompt:</p> <ol> <li>We give the LLM a friendly personality, which is an optional but crucial feature for user-facing applications.</li> <li>We provide the LLM with knowledge of the database schema that it will operate on.</li> <li>We give example interactions to reinforce how the LLM should operate.</li> <li>We give more fine-tuned instructions and constraints</li> <li>We tell the LLM how to use its tools</li> </ol> <p>After writing our prompt, we go through our agent loop and we can now use our Librarian.</p> In\u00a0[7]: Copied! <pre>librarian = Librarian()\nawait librarian.run()\n</pre> librarian = Librarian() await librarian.run() <pre>(Assistant): SELECT * FROM ReadingList;\nIt looks like your reading list is currently empty. If you're interested in fantasy books, I can recommend some excellent titles to get you started! Would you like some suggestions?\n(Assistant): Here are some fantastic fantasy books that you might enjoy:\n\n1. **\"The Hobbit\" by J.R.R. Tolkien** - A classic tale of adventure and friendship that follows a hobbit named Bilbo Baggins as he embarks on a quest to help a group of dwarves reclaim their homeland from a dragon.\n\n2. **\"Harry Potter and the Sorcerer's Stone\" by J.K. Rowling** - The beginning of a beloved series about a young boy discovering he is a wizard and attending Hogwarts School of Witchcraft and Wizardry.\n\n3. **\"The Name of the Wind\" by Patrick Rothfuss** - This is the first book in the \"Kingkiller Chronicle\" series, following the story of Kvothe, a gifted young man who grows up to become a legendary figure.\n\n4. **\"A Darker Shade of Magic\" by V.E. Schwab** - A captivating story set in a universe with parallel Londons, where only a few can travel between them and magic is a rare commodity.\n\n5. **\"The Priory of the Orange Tree\" by Samantha Shannon** - An epic standalone fantasy with a richly built world that features dragons, a matriarchal society, and diverse characters.\n\nIf any of these titles pique your interest, just let me know and I can add them to your reading list!\n</pre> <p>Note that the SQL statements in the dialogue are there for development purposes.</p> <p>Having established that we can have a quality conversation with our <code>Librarian</code>, we can now enhance our prompt. However, we must ensure that these improvements don't compromise the Librarian's core functionality. Check out Evaluating SQL Agent for an in-depth guide on how we evaluate the quality of our prompt.</p> <p>Additional Real-World Examples</p> <ul> <li>Operations Assistant: A read-only agent that retrieves data from databases, requiring no technical expertise.</li> <li>SQL Query Optimization: Provide the agent with your data retrieval goals, and have it generate an efficient SQL query to meet your needs.</li> <li>Data Generation for Testing: Request the agent to create and populate your database with realistic sample data to support development and testing processes.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Experiment with the prompt, by adding query planning or other prompting techniques to break down a complex request.</li> <li>Experiment with different model providers to balance quality and speed.</li> <li>Use in a development or sandbox environment for rapid development.</li> </ul>"},{"location":"tutorials/agents/sql_agent/#generate-sql-with-llm","title":"Generate SQL with LLM\u00b6","text":"<p>In this recipe, we will be using OpenAI GPT-4o-mini to act as a co-pilot for a Database Admin. While LLMs are powerful and do pretty well at transforming laymen queries into SQL queries, it is still dangerous to do so without supervision. This recipe will have no guardrails for mutable operations and is purely for getting started.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Tools</li> <li>Async</li> <li>Agents</li> </ul>"},{"location":"tutorials/agents/sql_agent/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/agents/sql_agent/#setup-sql-database","title":"Setup SQL Database\u00b6","text":"<p>We will be using SQLite, but this example will work for any common SQL dialect, such as PostgreSQL, MySQL, MSSQL, and more.</p> <p>Replace this part with whichever SQL dialect you are using, or skip if you have a database set up already.</p>"},{"location":"tutorials/agents/sql_agent/#write-your-database-assistant","title":"Write your Database Assistant\u00b6","text":"<p>We will be creating an Agent that will take non-technical queries and translate them into SQL queries that will be executed. The first step will be to create our two tools, <code>_run_query</code> and <code>_execute_query</code> , which will be read and write operations respectively.</p>"},{"location":"tutorials/agents/sql_agent/#prompt-engineering","title":"Prompt Engineering\u00b6","text":"<p>Knowing what tools are available is crucial when prompt engineering, so that we can tell the LLM when and how the tools should be used.</p> <p>Now we will take our <code>Librarian</code> and add our <code>@openai.call</code>:</p>"},{"location":"tutorials/agents/web_search_agent/","title":"Web Search Agent","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\" beautifulsoup4  duckduckgo-search requests\n</pre> !pip install \"mirascope[openai]\" beautifulsoup4  duckduckgo-search requests In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using <p>Make sure to also set your <code>OPENAI_API_KEY</code> if you haven't already. We are using <code>duckduckgo-search</code> since it does not require an API key, but feel free to use Google Search API or other search engine APIs.</p> In\u00a0[1]: Copied! <pre>from duckduckgo_search import DDGS\nfrom mirascope.core import BaseMessageParam, openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass WebAssistantBase(BaseModel):\n    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n    search_history: list[str] = []\n    max_results_per_query: int = 2\n\n    def _web_search(self, queries: list[str]) -&gt; str:\n        \"\"\"Performs web searches for given queries and returns URLs.\n\n        Args:\n            queries: List of search queries.\n\n        Returns:\n            str: Newline-separated URLs from search results or error messages.\n\n        Raises:\n            Exception: If web search fails entirely.\n        \"\"\"\n        try:\n            urls = []\n            for query in queries:\n                results = DDGS(proxies=None).text(\n                    query, max_results=self.max_results_per_query\n                )\n\n                for result in results:\n                    link = result[\"href\"]\n                    try:\n                        urls.append(link)\n                    except Exception as e:\n                        urls.append(\n                            f\"{type(e)}: Failed to parse content from URL {link}\"\n                        )\n                self.search_history.append(query)\n            return \"\\n\\n\".join(urls)\n\n        except Exception as e:\n            return f\"{type(e)}: Failed to search the web for text\"\n</pre> from duckduckgo_search import DDGS from mirascope.core import BaseMessageParam, openai, prompt_template from pydantic import BaseModel   class WebAssistantBase(BaseModel):     messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []     search_history: list[str] = []     max_results_per_query: int = 2      def _web_search(self, queries: list[str]) -&gt; str:         \"\"\"Performs web searches for given queries and returns URLs.          Args:             queries: List of search queries.          Returns:             str: Newline-separated URLs from search results or error messages.          Raises:             Exception: If web search fails entirely.         \"\"\"         try:             urls = []             for query in queries:                 results = DDGS(proxies=None).text(                     query, max_results=self.max_results_per_query                 )                  for result in results:                     link = result[\"href\"]                     try:                         urls.append(link)                     except Exception as e:                         urls.append(                             f\"{type(e)}: Failed to parse content from URL {link}\"                         )                 self.search_history.append(query)             return \"\\n\\n\".join(urls)          except Exception as e:             return f\"{type(e)}: Failed to search the web for text\" <p>We are grabbing the first 2 results that best match each of our user queries and retrieving their URLs. We save our search results into <code>search_history</code> to provide as context for future searches.</p> <p>We also want to setup our <code>extract_content</code> tool which will take in a url and grab the HTML content.</p> In\u00a0[2]: Copied! <pre>import re\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef extract_content(url: str) -&gt; str:\n    \"\"\"Extract the main content from a webpage.\n\n    Args:\n        url: The URL of the webpage to extract the content from.\n\n    Returns:\n        The extracted content as a string.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n\n        soup = BeautifulSoup(response.content, \"html.parser\")\n\n        unwanted_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]\n        for tag in unwanted_tags:\n            for element in soup.find_all(tag):\n                element.decompose()\n\n        main_content = (\n            soup.find(\"main\")\n            or soup.find(\"article\")\n            or soup.find(\"div\", class_=re.compile(\"content|main\"))\n        )\n\n        if main_content:\n            text = main_content.get_text(separator=\"\\n\", strip=True)\n        else:\n            text = soup.get_text(separator=\"\\n\", strip=True)\n\n        lines = (line.strip() for line in text.splitlines())\n        return \"\\n\".join(line for line in lines if line)\n    except Exception as e:\n        return f\"{type(e)}: Failed to extract content from URL {url}\"\n</pre> import re  import requests from bs4 import BeautifulSoup   def extract_content(url: str) -&gt; str:     \"\"\"Extract the main content from a webpage.      Args:         url: The URL of the webpage to extract the content from.      Returns:         The extracted content as a string.     \"\"\"     try:         response = requests.get(url, timeout=5)          soup = BeautifulSoup(response.content, \"html.parser\")          unwanted_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]         for tag in unwanted_tags:             for element in soup.find_all(tag):                 element.decompose()          main_content = (             soup.find(\"main\")             or soup.find(\"article\")             or soup.find(\"div\", class_=re.compile(\"content|main\"))         )          if main_content:             text = main_content.get_text(separator=\"\\n\", strip=True)         else:             text = soup.get_text(separator=\"\\n\", strip=True)          lines = (line.strip() for line in text.splitlines())         return \"\\n\".join(line for line in lines if line)     except Exception as e:         return f\"{type(e)}: Failed to extract content from URL {url}\" <p>Notice that we did not define <code>extract_content</code> as a method of <code>WebAssistant</code>, since <code>extract_content</code> does not need to update state.</p> <p>Now that our tools are setup, we can proceed to implement the Q&amp;A functionality of our <code>WebAssistant</code>.</p> In\u00a0[4]: Copied! <pre>from datetime import datetime\n\n\nclass WebAssistantBaseWithStream(WebAssistantBase):\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. Your task is to answer the user's question using the provided tools.\n        The current date is {current_date}.\n\n        You have access to the following tools:\n        - `_web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:\n            1. There is a previous search context: {self.search_history}\n            2. There is the current user query: {question}\n            3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query. \n                Even if the connection isn't immediately clear, consider how they might be related.\n        - `extract_content`: Parse the content of a webpage.\n\n        When calling the `_web_search` tool, the `body` is simply the body of the search\n        result. You MUST then call the `extract_content` tool to get the actual content\n        of the webpage. It is up to you to determine which search results to parse.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness based on the context of the user's query.\n\n        MESSAGES: {self.messages}\n        USER: {question}\n        \"\"\"\n    )\n    async def _stream(self, question: str) -&gt; openai.OpenAIDynamicConfig:\n        return {\n            \"tools\": [self._web_search, extract_content],\n            \"computed_fields\": {\n                \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            },\n        }\n\n    async def _step(self, question: str):\n        print(self.messages)\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                print(f\"using {tool._name()} tool with args: {tool.args}\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(f\"(User): {question}\", flush=True)\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._step(question)\n            print()\n</pre> from datetime import datetime   class WebAssistantBaseWithStream(WebAssistantBase):     @openai.call(model=\"gpt-4o-mini\", stream=True)     @prompt_template(         \"\"\"         SYSTEM:         You are an expert web searcher. Your task is to answer the user's question using the provided tools.         The current date is {current_date}.          You have access to the following tools:         - `_web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:             1. There is a previous search context: {self.search_history}             2. There is the current user query: {question}             3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query.                  Even if the connection isn't immediately clear, consider how they might be related.         - `extract_content`: Parse the content of a webpage.          When calling the `_web_search` tool, the `body` is simply the body of the search         result. You MUST then call the `extract_content` tool to get the actual content         of the webpage. It is up to you to determine which search results to parse.          Once you have gathered all of the information you need, generate a writeup that         strikes the right balance between brevity and completeness based on the context of the user's query.          MESSAGES: {self.messages}         USER: {question}         \"\"\"     )     async def _stream(self, question: str) -&gt; openai.OpenAIDynamicConfig:         return {             \"tools\": [self._web_search, extract_content],             \"computed_fields\": {                 \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")             },         }      async def _step(self, question: str):         print(self.messages)         response = await self._stream(question)         tools_and_outputs = []         async for chunk, tool in response:             if tool:                 print(f\"using {tool._name()} tool with args: {tool.args}\")                 tools_and_outputs.append((tool, tool.call()))             else:                 print(chunk.content, end=\"\", flush=True)         if response.user_message_param:             self.messages.append(response.user_message_param)         self.messages.append(response.message_param)         if tools_and_outputs:             self.messages += response.tool_message_params(tools_and_outputs)             await self._step(\"\")      async def run(self):         while True:             question = input(\"(User): \")             if question == \"exit\":                 break             print(f\"(User): {question}\", flush=True)             print(\"(Assistant): \", end=\"\", flush=True)             await self._step(question)             print() <p>There are a few things to note:</p> <ol> <li>We set our <code>@openai.call()</code> to <code>stream=True</code> to provide a more responsive user experience.</li> <li>We give the LLM the current date so the user does not need to provide that.</li> <li>We instruct our LLM on how to use the tools.<ul> <li>User queries can often times be ambiguous so giving as much context to the LLM when it generates the search query is crucial.</li> <li>Multiple search queries are generated for user queries that might rely on previous context.</li> </ul> </li> </ol> In\u00a0[6]: Copied! <pre>await WebAssistantBaseWithStream().run()\n</pre> await WebAssistantBaseWithStream().run() <pre>(Assistant): []\nusing _web_search tool with args: {'queries': ['LLM development tool library', 'best libraries for LLM development', 'open source LLM libraries', 'popular LLM frameworks for developers']}\n[{'role': 'user', 'content': 'I am a SWE looking for a LLM dev tool library'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"LLM development tool library\",\"best libraries for LLM development\",\"open source LLM libraries\",\"popular LLM frameworks for developers\"]}', 'name': '_web_search'}, 'id': 'call_eVVmR6gCJnuOMDNDyKHxVPd4'}]}, {'role': 'tool', 'content': 'https://github.com/tensorchord/awesome-llmops\\n\\nhttps://github.com/M1n9X/llm_agents_devtools\\n\\nhttps://github.com/jihoo-kim/awesome-production-llm\\n\\nhttps://github.com/tensorchord/awesome-llmops\\n\\nhttps://www.datacamp.com/blog/top-open-source-llms\\n\\nhttps://github.com/tensorchord/awesome-llmops\\n\\nhttps://llmmodels.org/blog/top-10-open-source-llm-frameworks-2024/\\n\\nhttps://github.com/tensorchord/awesome-llmops', 'tool_call_id': 'call_eVVmR6gCJnuOMDNDyKHxVPd4', 'name': '_web_search'}]\nusing extract_content tool with args: {'url': 'https://github.com/tensorchord/awesome-llmops'}\nusing extract_content tool with args: {'url': 'https://github.com/M1n9X/llm_agents_devtools'}\nusing extract_content tool with args: {'url': 'https://github.com/jihoo-kim/awesome-production-llm'}\nusing extract_content tool with args: {'url': 'https://llmmodels.org/blog/top-10-open-source-llm-frameworks-2024/'}\n[{'role': 'user', 'content': 'I am a SWE looking for a LLM dev tool library'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"LLM development tool library\",\"best libraries for LLM development\",\"open source LLM libraries\",\"popular LLM frameworks for developers\"]}', 'name': '_web_search'}, 'id': 'call_eVVmR6gCJnuOMDNDyKHxVPd4'}]}, {'role': 'tool', 'content': 'https://github.com/tensorchord/awesome-llmops\\n\\nhttps://github.com/M1n9X/llm_agents_devtools\\n\\nhttps://github.com/jihoo-kim/awesome-production-llm\\n\\nhttps://github.com/tensorchord/awesome-llmops\\n\\nhttps://www.datacamp.com/blog/top-open-source-llms\\n\\nhttps://github.com/tensorchord/awesome-llmops\\n\\nhttps://llmmodels.org/blog/top-10-open-source-llm-frameworks-2024/\\n\\nhttps://github.com/tensorchord/awesome-llmops', 'tool_call_id': 'call_eVVmR6gCJnuOMDNDyKHxVPd4', 'name': '_web_search'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"url\": \"https://github.com/tensorchord/awesome-llmops\"}', 'name': 'extract_content'}, 'id': 'call_zIolfSqkgQdsqi3BsRkXkWXl'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://github.com/M1n9X/llm_agents_devtools\"}', 'name': 'extract_content'}, 'id': 'call_Vh9DH7mDWzXzsQoHKaWakHtS'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://github.com/jihoo-kim/awesome-production-llm\"}', 'name': 'extract_content'}, 'id': 'call_VXxtxhySFYvaXlxhDqgLVzdu'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://llmmodels.org/blog/top-10-open-source-llm-frameworks-2024/\"}', 'name': 'extract_content'}, 'id': 'call_NUXKesTLiBqnO48zOZYVgfF7'}]}, {'role': 'tool', 'content': 'tensorchord\\n/\\nAwesome-LLMOps\\nPublic\\nNotifications\\nYou must be signed in to change notification settings\\nFork\\n362\\nStar\\n3.8k\\nAn awesome &amp; curated list of best LLMOps tools for developers\\nLicense\\nCC0-1.0 license\\n3.8k\\nstars\\n362\\nforks\\nBranches\\nTags\\nActivity\\nStar\\nNotifications\\nYou must be signed in to change notification settings\\ntensorchord/Awesome-LLMOps\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\nmain\\nBranches\\nTags\\nGo to file\\nCode\\nFolders and files\\nName\\nName\\nLast commit message\\nLast commit date\\nLatest commit\\nHistory\\n155 Commits\\nscripts\\nscripts\\n.gitignore\\n.gitignore\\nLICENSE\\nLICENSE\\nREADME.md\\nREADME.md\\ncontributing.md\\ncontributing.md\\nView all files\\nRepository files navigation\\nAwesome LLMOps\\nAn awesome &amp; curated list of the best LLMOps tools for developers.\\nContribute\\nContributions are most welcome, please adhere to the\\ncontribution guidelines\\n.\\nTable of Contents\\nTable of Contents\\nModel\\nLarge Language Model\\nCV Foundation Model\\nAudio Foundation Model\\nServing\\nLarge Model Serving\\nFrameworks/Servers for Serving\\nObservability\\nSecurity\\nLLMOps\\nSearch\\nVector search\\nCode AI\\nTraining\\nIDEs and Workspaces\\nFoundation Model Fine Tuning\\nFrameworks for Training\\nExperiment Tracking\\nVisualization\\nModel Editing\\nData\\nData Management\\nData Storage\\nData Tracking\\nFeature Engineering\\nData/Feature enrichment\\nLarge Scale Deployment\\nML Platforms\\nWorkflow\\nScheduling\\nModel Management\\nPerformance\\nML Compiler\\nProfiling\\nAutoML\\nOptimizations\\nFederated ML\\nAwesome Lists\\nModel\\nLarge Language Model\\nProject\\nDetails\\nRepository\\nAlpaca\\nCode and documentation to train Stanford\\'s Alpaca models, and generate the data.\\nBELLE\\nA 7B Large Language Model fine-tune by 34B Chinese Character Corpus, based on LLaMA and Alpaca.\\nBloom\\nBigScience Large Open-science Open-access Multilingual Language Model\\ndolly\\nDatabricks\u2019 Dolly, a large language model trained on the Databricks Machine Learning Platform\\nFalcon 40B\\nFalcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.\\nFastChat (Vicuna)\\nAn open platform for training, serving, and evaluating large language models. Release repo for Vicuna and FastChat-T5.\\nGemma\\nGemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models.\\nGLM-6B (ChatGLM)\\nAn Open Bilingual Pre-Trained Model, quantization of ChatGLM-130B, can run on consumer-level GPUs.\\nChatGLM2-6B\\nChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model\\nChatGLM-6B\\n.\\nGLM-130B (ChatGLM)\\nAn Open Bilingual Pre-Trained Model (ICLR 2023)\\nGPT-NeoX\\nAn implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library.\\nLuotuo\\nA Chinese LLM, Based on LLaMA and fine tune by Stanford Alpaca, Alpaca LoRA, Japanese-Alpaca-LoRA.\\nMixtral-8x7B-v0.1\\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\\nStableLM\\nStableLM: Stability AI Language Models\\n\u2b06 back to ToC\\nCV Foundation Model\\nProject\\nDetails\\nRepository\\ndisco-diffusion\\nA frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.\\nmidjourney\\nMidjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.\\nsegment-anything (SAM)\\nproduces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image.\\nstable-diffusion\\nA latent text-to-image diffusion model\\nstable-diffusion v2\\nHigh-Resolution Image Synthesis with Latent Diffusion Models\\n\u2b06 back to ToC\\nAudio Foundation Model\\nProject\\nDetails\\nRepository\\nbark\\nBark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects.\\nwhisper\\nRobust Speech Recognition via Large-Scale Weak Supervision\\nServing\\nLarge Model Serving\\nProject\\nDetails\\nRepository\\nAlpaca-LoRA-Serve\\nAlpaca-LoRA as Chatbot service\\nCTranslate2\\nfast inference engine for Transformer models in C++\\nClip-as-a-service\\nserving the OpenAI CLIP model\\nDeepSpeed-MII\\nMII makes low-latency and high-throughput inference possible, powered by DeepSpeed.\\nFaster Whisper\\nfast inference engine for whisper in C++ using CTranslate2.\\nFlexGen\\nRunning large language models on a single GPU for throughput-oriented scenarios.\\nFlowise\\nDrag &amp; drop UI to build your customized LLM flow using LangchainJS.\\nllama.cpp\\nPort of Facebook\\'s LLaMA model in C/C++\\nInfinity\\nRest API server for serving text-embeddings\\nModelz-LLM\\nOpenAI compatible API for LLMs and embeddings (LLaMA, Vicuna, ChatGLM and many others)\\nOllama\\nServe Llama 2 and other large language models locally from command line or through a browser interface.\\nTensorRT-LLM\\nInference engine for TensorRT on Nvidia GPUs\\ntext-generation-inference\\nLarge Language Model Text Generation Inference\\ntext-embeddings-inference\\nInference for text-embedding models\\ntokenizers\\n\ud83d\udca5 Fast State-of-the-Art Tokenizers optimized for Research and Production\\nvllm\\nA high-throughput and memory-efficient inference and serving engine for LLMs.\\nwhisper.cpp\\nPort of OpenAI\\'s Whisper model in C/C++\\nx-stable-diffusion\\nReal-time inference for Stable Diffusion - 0.88s latency. Covers AITemplate, nvFuser, TensorRT, FlashAttention.\\n\u2b06 back to ToC\\nFrameworks/Servers for Serving\\nProject\\nDetails\\nRepository\\nBentoML\\nThe Unified Model Serving Framework\\nJina\\nBuild multimodal AI services via cloud native technologies \u00b7 Model Serving \u00b7 Generative AI \u00b7 Neural Search \u00b7 Cloud Native\\nMosec\\nA machine learning model serving framework with dynamic batching and pipelined stages, provides an easy-to-use Python interface.\\nTFServing\\nA flexible, high-performance serving system for machine learning models.\\nTorchserve\\nServe, optimize and scale PyTorch models in production\\nTriton Server (TRTIS)\\nThe Triton Inference Server provides an optimized cloud and edge inferencing solution.\\nlangchain-serve\\nServerless LLM apps on Production with Jina AI Cloud\\nlanarky\\nFastAPI framework to build production-grade LLM applications\\nray-llm\\nLLMs on Ray - RayLLM\\nXinference\\nReplace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you\\'re empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.\\n\u2b06 back to ToC\\nSecurity\\nFrameworks for LLM security\\nProject\\nDetails\\nRepository\\nPlexiglass\\nA Python Machine Learning Pentesting Toolbox for Adversarial Attacks. Works with LLMs, DNNs, and other machine learning algorithms.\\n\u2b06 back to ToC\\nObservability\\nProject\\nDetails\\nRepository\\nAzure OpenAI Logger\\n\"Batteries included\" logging solution for your Azure OpenAI instance.\\nDeepchecks\\nTests for Continuous Validation of ML Models &amp; Data. Deepchecks is a Python package for comprehensively validating your machine learning models and data with minimal effort.\\nEvidently\\nAn open-source framework to evaluate, test and monitor ML and LLM-powered systems.\\nFiddler AI\\nEvaluate, monitor, analyze, and improve machine learning and generative models from pre-production to production. Ship more ML and LLMs into production, and monitor ML and LLM metrics like hallucination, PII, and toxicity.\\nGiskard\\nTesting framework dedicated to ML models, from tabular to LLMs. Detect risks of biases, performance issues and errors in 4 lines of code.\\nGreat Expectations\\nAlways know what to expect from your data.\\nwhylogs\\nThe open standard for data logging\\n\u2b06 back to ToC\\nLLMOps\\nProject\\nDetails\\nRepository\\nagenta\\nThe LLMOps platform to build robust LLM apps. Easily experiment and evaluate different prompts, models, and workflows to build robust apps.\\nAI studio\\nA Reliable Open Source AI studio to build core infrastructure stack for your LLM Applications. It allows you to gain visibility, make your application reliable, and prepare it for production with features such as caching, rate limiting, exponential retry, model fallback, and more.\\nArize-Phoenix\\nML observability for LLMs, vision, language, and tabular models.\\nBudgetML\\nDeploy a ML inference service on a budget in less than 10 lines of code.\\ndeeplake\\nStream large multimodal datasets to achieve near 100% GPU utilization. Query, visualize, &amp; version control data. Access data w/o the need to recompute the embeddings for the model finetuning.\\nDify\\nOpen-source framework aims to enable developers (and even non-developers) to quickly build useful applications based on large language models, ensuring they are visual, operable, and improvable.\\nDstack\\nCost-effective LLM development in any cloud (AWS, GCP, Azure, Lambda, etc).\\nEmbedchain\\nFramework to create ChatGPT like bots over your dataset.\\nEvidently\\nAn open-source framework to evaluate, test and monitor ML and LLM-powered systems.\\nFiddler AI\\nEvaluate, monitor, analyze, and improve MLOps and LLMOps from pre-production to production.\\nGlide\\nCloud-Native LLM Routing Engine. Improve LLM app resilience and speed.\\ngotoHuman\\nBring a\\nhuman into the loop\\nin your LLM-based and agentic workflows. Prompt users to approve actions, select next steps, or review and validate generated results.\\nGPTCache\\nCreating semantic cache to store responses from LLM queries.\\nGPUStack\\nAn open-source GPU cluster manager for running and managing LLMs\\nHaystack\\nQuickly compose applications with LLM Agents, semantic search, question-answering and more.\\nHelicone\\nOpen-source LLM observability platform for logging, monitoring, and debugging AI applications. Simple 1-line integration to get started.\\nIzlo\\nPrompt management tools for teams. Store, improve, test, and deploy your prompts in one unified workspace.\\nKeywords AI\\nA unified DevOps platform for AI software. Keywords AI makes it easy for developers to build LLM applications.\\nlangchain\\nBuilding applications with LLMs through composability\\nLangFlow\\nAn effortless way to experiment and prototype LangChain flows with drag-and-drop components and a chat interface.\\nLangfuse\\nOpen Source LLM Engineering Platform: Traces, evals, prompt management and metrics to debug and improve your LLM application.\\nLangKit\\nOut-of-the-box LLM telemetry collection library that extracts features and profiles prompts, responses and metadata about how your LLM is performing over time to find problems at scale.\\nLiteLLM \ud83d\ude85\\nA simple &amp; light 100 line package to\\nstandardize LLM API calls\\nacross OpenAI, Azure, Cohere, Anthropic, Replicate API Endpoints\\nLiteral AI\\nMulti-modal LLM observability and evaluation platform. Create prompt templates, deploy prompts versions, debug LLM runs, create datasets, run evaluations, monitor LLM metrics and collect human feedback.\\nLlamaIndex\\nProvides a central interface to connect your LLMs with external data.\\nLLMApp\\nLLM App is a Python library that helps you build real-time LLM-enabled data pipelines with few lines of code.\\nLLMFlows\\nLLMFlows is a framework for building simple, explicit, and transparent LLM applications such as chatbots, question-answering systems, and agents.\\nLLMonitor\\nObservability and monitoring for AI apps and agents. Debug agents with powerful tracing and logging. Usage analytics and dive deep into the history of your requests. Developer friendly modules with plug-and-play integration into LangChain.\\nmagentic\\nSeamlessly integrate LLMs as Python functions. Use type annotations to specify structured output. Mix LLM queries and function calling with regular Python code to create complex LLM-powered functionality.\\nManag.ai\\nYour all-in-one prompt management and observability platform. Craft, track, and perfect your LLM prompts with ease.\\nMirascope\\nIntuitive convenience tooling for lightning-fast, efficient development and ensuring quality in LLM-based applications\\nOpenLIT\\nOpenLIT is an OpenTelemetry-native GenAI and LLM Application Observability tool and provides OpenTelmetry Auto-instrumentation for monitoring LLMs, VectorDBs and Frameworks. It provides valuable insights into token &amp; cost usage, user interaction, and performance related metrics.\\nOpik\\nConfidently evaluate, test, and ship LLM applications with a suite of observability tools to calibrate language model outputs across your dev and production lifecycle.\\nParea AI\\nPlatform and SDK for AI Engineers providing tools for LLM evaluation, observability, and a version-controlled enhanced prompt playground.\\nPezzo \ud83d\udd79\ufe0f\\nPezzo is the open-source LLMOps platform built for developers and teams. In just two lines of code, you can seamlessly troubleshoot your AI operations, collaborate and manage your prompts in one place, and instantly deploy changes to any environment.\\nPromptHub\\nFull stack prompt management tool designed to be usable by technical and non-technical team members. Test, version, collaborate, deploy, and monitor, all from one place.\\npromptfoo\\nOpen-source tool for testing &amp; evaluating prompt quality. Create test cases, automatically check output quality and catch regressions, and reduce evaluation cost.\\nPromptFoundry\\nThe simple prompt engineering and evaluation tool designed for developers building AI applications.\\nPromptLayer \ud83c\udf70\\nPrompt Engineering platform. Collaborate, test, evaluate, and monitor your LLM applications\\nPromptMage\\nOpen-source tool to simplify the process of creating and managing LLM workflows and prompts as a self-hosted solution.\\nPrompteams\\nPrompt management system. Version, test, collaborate, and retrieve prompts through real-time APIs. Have GitHub style with repos, branches, and commits (and commit history).\\nprompttools\\nOpen-source tools for testing and experimenting with prompts. The core idea is to enable developers to evaluate prompts using familiar interfaces like code and notebooks. In just a few lines of codes, you can test your prompts and parameters across different models (whether you are using OpenAI, Anthropic, or LLaMA models). You can even evaluate the retrieval accuracy of vector databases.\\nTreeScale\\nAll In One Dev Platform For LLM Apps. Deploy LLM-enhanced APIs seamlessly using tools for prompt optimization, semantic querying, version management, statistical evaluation, and performance tracking. As a part of the developer friendly API implementation TreeScale offers Elastic LLM product, which makes a unified API Endpoint for all major LLM providers and open source models.\\nTrueFoundry\\nDeploy LLMOps tools like Vector DBs, Embedding server etc on your own Kubernetes (EKS,AKS,GKE,On-prem) Infra including deploying, Fine-tuning, tracking Prompts and serving Open Source LLM Models with full Data Security and Optimal GPU Management. Train and Launch your LLM Application at Production scale with best Software Engineering practices.\\nReliableGPT \ud83d\udcaa\\nHandle OpenAI Errors (overloaded OpenAI servers, rotated keys, or context window errors) for your production LLM Applications.\\nPortkey\\nControl Panel with an observability suite &amp; an AI gateway \u2014 to ship fast, reliable, and cost-efficient apps.\\nVellum\\nAn AI product development platform to experiment with, evaluate, and deploy advanced LLM apps.\\nWeights &amp; Biases (Prompts)\\nA suite of LLMOps tools within the developer-first W&amp;B MLOps platform. Utilize W&amp;B Prompts for visualizing and inspecting LLM execution flow, tracking inputs and outputs, viewing intermediate results, securely managing prompts and LLM chain configurations.\\nWordware\\nA web-hosted IDE where non-technical domain experts work with AI Engineers to build task-specific AI agents. It approaches prompting as a new programming language rather than low/no-code blocks.\\nxTuring\\nBuild and control your personal LLMs with fast and efficient fine-tuning.\\nZenML\\nOpen-source framework for orchestrating, experimenting and deploying production-grade ML solutions, with built-in\\nlangchain\\n&amp;\\nllama_index\\nintegrations.\\n\u2b06 back to ToC\\nSearch\\nVector search\\nProject\\nDetails\\nRepository\\nAquilaDB\\nAn easy to use Neural Search Engine. Index latent vectors along with JSON metadata and do efficient k-NN search.\\nAwadb\\nAI Native database for embedding vectors\\nChroma\\nthe open source embedding database\\nInfinity\\nThe AI-native database built for LLM applications, providing incredibly fast vector and full-text search\\nLancedb\\nDeveloper-friendly, serverless vector database for AI applications. Easily add long-term memory to your LLM apps!\\nMarqo\\nTensor search for humans.\\nMilvus\\nVector database for scalable similarity search and AI applications.\\nPinecone\\nThe Pinecone vector database makes it easy to build high-performance vector search applications. Developer-friendly, fully managed, and easily scalable without infrastructure hassles.\\npgvector\\nOpen-source vector similarity search for Postgres.\\npgvecto.rs\\nVector database plugin for Postgres, written in Rust, specifically designed for LLM.\\nQdrant\\nVector Search Engine and Database for the next generation of AI applications. Also available in the cloud\\ntxtai\\nBuild AI-powered semantic search applications\\nVald\\nA Highly Scalable Distributed Vector Search Engine\\nVearch\\nA distributed system for embedding-based vector retrieval\\nVectorDB\\nA Python vector database you just need - no more, no less.\\nVellum\\nA managed service for ingesting documents and performing hybrid semantic/keyword search across them. Comes with out-of-box support for OCR, text chunking, embedding model experimentation, metadata filtering, and production-grade APIs.\\nWeaviate\\nWeaviate is an open source vector search engine that stores both objects and vectors, allowing for combining vector search with structured filtering with the fault-tolerance and scalability of a cloud-native database, all accessible through GraphQL, REST, and various language clients.\\n\u2b06 back to ToC\\nCode AI\\nProject\\nDetails\\nRepository\\nCodeGeeX\\nCodeGeeX: An Open Multilingual Code Generation Model (KDD 2023)\\nCodeGen\\nCodeGen is an open-source model for program synthesis. Trained on TPU-v4. Competitive with OpenAI Codex.\\nCodeT5\\nOpen Code LLMs for Code Understanding and Generation.\\nContinue\\n\u23e9 the open-source autopilot for software development\u2014bring the power of ChatGPT to VS Code\\nfauxpilot\\nAn open-source alternative to GitHub Copilot server\\ntabby\\nSelf-hosted AI coding assistant. An opensource / on-prem alternative to GitHub Copilot.\\nTraining\\nIDEs and Workspaces\\nProject\\nDetails\\nRepository\\ncode server\\nRun VS Code on any machine anywhere and access it in the browser.\\nconda\\nOS-agnostic, system-level binary package manager and ecosystem.\\nDocker\\nMoby is an open-source project created by Docker to enable and accelerate software containerization.\\nenvd\\n\ud83c\udfd5\ufe0f Reproducible development environment for AI/ML.\\nJupyter Notebooks\\nThe Jupyter notebook is a web-based notebook environment for interactive computing.\\nKurtosis\\nA build, packaging, and run system for ephemeral multi-container environments.\\nWordware\\nA web-hosted IDE where non-technical domain experts work with AI Engineers to build task-specific AI agents. It approaches prompting as a new programming language rather than low/no-code blocks.\\n\u2b06 back to ToC\\nFoundation Model Fine Tuning\\nProject\\nDetails\\nRepository\\nalpaca-lora\\nInstruct-tune LLaMA on consumer hardware\\nfinetuning-scheduler\\nA PyTorch Lightning extension that accelerates and enhances foundation model experimentation with flexible fine-tuning schedules.\\nFlyflow\\nOpen source, high performance fine tuning as a service for GPT4 quality models with 5x lower latency and 3x lower cost\\nLMFlow\\nAn Extensible Toolkit for Finetuning and Inference of Large Foundation Models\\nLora\\nUsing Low-rank adaptation to quickly fine-tune diffusion models.\\npeft\\nState-of-the-art Parameter-Efficient Fine-Tuning.\\np-tuning-v2\\nAn optimized prompt tuning strategy achieving comparable performance to fine-tuning on small/medium-sized models and sequence tagging challenges.\\n(ACL 2022)\\nQLoRA\\nEfficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.\\nTRL\\nTrain transformer language models with reinforcement learning.\\n\u2b06 back to ToC\\nFrameworks for Training\\nProject\\nDetails\\nRepository\\nAccelerate\\n\ud83d\ude80 A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision.\\nApache MXNet\\nLightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler.\\naxolotl\\nA tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.\\nCaffe\\nA fast open framework for deep learning.\\nCandle\\nMinimalist ML framework for Rust .\\nColossalAI\\nAn integrated large-scale model training system with efficient parallelization techniques.\\nDeepSpeed\\nDeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.\\nHorovod\\nDistributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\\nJax\\nAutograd and XLA for high-performance machine learning research.\\nKedro\\nKedro is an open-source Python framework for creating reproducible, maintainable and modular data science code.\\nKeras\\nKeras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.\\nLightGBM\\nA fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\\nMegEngine\\nMegEngine is a fast, scalable and easy-to-use deep learning framework, with auto-differentiation.\\nmetric-learn\\nMetric Learning Algorithms in Python.\\nMindSpore\\nMindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\\nOneflow\\nOneFlow is a performance-centered and open-source deep learning framework.\\nPaddlePaddle\\nMachine Learning Framework from Industrial Practice.\\nPyTorch\\nTensors and Dynamic neural networks in Python with strong GPU acceleration.\\nPyTorch Lightning\\nDeep learning framework to train, deploy, and ship AI products Lightning fast.\\nXGBoost\\nScalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library.\\nscikit-learn\\nMachine Learning in Python.\\nTensorFlow\\nAn Open Source Machine Learning Framework for Everyone.\\nVectorFlow\\nA minimalist neural network library optimized for sparse data and single machine environments.\\n\u2b06 back to ToC\\nExperiment Tracking\\nProject\\nDetails\\nRepository\\nAim\\nan easy-to-use and performant open-source experiment tracker.\\nClearML\\nAuto-Magical CI/CD to streamline your ML workflow. Experiment Manager, MLOps and Data-Management\\nComet\\nComet is an MLOps platform that offers experiment tracking, model production management, a model registry, and full data lineage from training straight through to production. Comet plays nicely with all your favorite tools, so you don\\'t have to change your existing workflow. Comet Opik to confidently evaluate, test, and ship LLM applications with a suite of observability tools to calibrate language model outputs across your dev and production lifecycle!\\nGuild AI\\nExperiment tracking, ML developer tools.\\nMLRun\\nMachine Learning automation and tracking.\\nKedro-Viz\\nKedro-Viz is an interactive development tool for building data science pipelines with Kedro. Kedro-Viz also allows users to view and compare different runs in the Kedro project.\\nLabNotebook\\nLabNotebook is a tool that allows you to flexibly monitor, record, save, and query all your machine learning experiments.\\nSacred\\nSacred is a tool to help you configure, organize, log and reproduce experiments.\\nWeights &amp; Biases\\nA developer first, lightweight, user-friendly experiment tracking and visualization tool for machine learning projects, streamlining collaboration and simplifying MLOps. W&amp;B excels at tracking LLM-powered applications, featuring W&amp;B Prompts for LLM execution flow visualization, input and output monitoring, and secure management of prompts and LLM chain configurations.\\n\u2b06 back to ToC\\nVisualization\\nProject\\nDetails\\nRepository\\nFiddler AI\\nRich dashboards, reports, and UMAP to perform root cause analysis, pinpoint problem areas, like correctness, safety, and privacy issues, and improve LLM outcomes.\\nManiford\\nA model-agnostic visual debugging tool for machine learning.\\nnetron\\nVisualizer for neural network, deep learning, and machine learning models.\\nOpenOps\\nBring multiple data streams into one dashboard.\\nTensorBoard\\nTensorFlow\\'s Visualization Toolkit.\\nTensorSpace\\nNeural network 3D visualization framework, build interactive and intuitive model in browsers, support pre-trained deep learning models from TensorFlow, Keras, TensorFlow.js.\\ndtreeviz\\nA python library for decision tree visualization and model interpretation.\\nZetane Viewer\\nML models and internal tensors 3D visualizer.\\nZeno\\nAI evaluation platform for interactively exploring data and model outputs.\\nModel Editing\\nProject\\nDetails\\nRepository\\nFastEdit\\nFastEdit aims to assist developers with injecting fresh and customized knowledge into large language models efficiently using one single command.\\n\u2b06 back to ToC\\nData\\nData Management\\nProject\\nDetails\\nRepository\\nArtiVC\\nA version control system to manage large files. Lake is a dataset format with a simple API for creating, storing, and collaborating on AI datasets of any size.\\nDolt\\nGit for Data.\\nDVC\\nData Version Control - Git for Data &amp; Models - ML Experiments Management.\\nDelta-Lake\\nStorage layer that brings scalable, ACID transactions to Apache Spark and other engines.\\nPachyderm\\nPachyderm is a version control system for data.\\nQuilt\\nA self-organizing data hub for S3.\\n\u2b06 back to ToC\\nData Storage\\nProject\\nDetails\\nRepository\\nJuiceFS\\nA distributed POSIX file system built on top of Redis and S3.\\nLakeFS\\nGit-like capabilities for your object storage.\\nLance\\nModern columnar data format for ML implemented in Rust.\\n\u2b06 back to ToC\\nData Tracking\\nProject\\nDetails\\nRepository\\nPiperider\\nA CLI tool that allows you to build data profiles and write assertion tests for easily evaluating and tracking your data\\'s reliability over time.\\nLUX\\nA Python library that facilitates fast and easy data exploration by automating the visualization and data analysis process.\\n\u2b06 back to ToC\\nFeature Engineering\\nProject\\nDetails\\nRepository\\nFeatureform\\nThe Virtual Feature Store. Turn your existing data infrastructure into a feature store.\\nFeatureTools\\nAn open source python framework for automated feature engineering\\n\u2b06 back to ToC\\nData/Feature enrichment\\nProject\\nDetails\\nRepository\\nUpgini\\nFree automated data &amp; feature enrichment library for machine learning: automatically searches through thousands of ready-to-use features from public and community shared data sources and enriches your training dataset with only the accuracy improving features\\nFeast\\nAn open source feature store for machine learning.\\ndistilabel\\n\u2697\ufe0f distilabel is a framework for synthetic data and AI feedback for AI engineers that require high-quality outputs, full data ownership, and overall efficiency.\\n\u2b06 back to ToC\\nLarge Scale Deployment\\nML Platforms\\nProject\\nDetails\\nRepository\\nComet\\nComet is an MLOps platform that offers experiment tracking, model production management, a model registry, and full data lineage from training straight through to production. Comet plays nicely with all your favorite tools, so you don\\'t have to change your existing workflow. Comet Opik to confidently evaluate, test, and ship LLM applications with a suite of observability tools to calibrate language model outputs across your dev and production lifecycle!\\nClearML\\nAuto-Magical CI/CD to streamline your ML workflow. Experiment Manager, MLOps and Data-Management.\\nHopsworks\\nHopsworks is a MLOps platform for training and operating large and small ML systems, including fine-tuning and serving LLMs. Hopsworks includes both a feature store and vector database for RAG.\\nOpenLLM\\nAn open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.\\nMLflow\\nOpen source platform for the machine learning lifecycle.\\nMLRun\\nAn open MLOps platform for quickly building and managing continuous ML applications across their lifecycle.\\nModelFox\\nModelFox is a platform for managing and deploying machine learning models.\\nKserve\\nStandardized Serverless ML Inference Platform on Kubernetes\\nKubeflow\\nMachine Learning Toolkit for Kubernetes.\\nPAI\\nResource scheduling and cluster management for AI.\\nPolyaxon\\nMachine Learning Management &amp; Orchestration Platform.\\nPrimehub\\nAn effortless infrastructure for machine learning built on the top of Kubernetes.\\nOpenModelZ\\nOne-click machine learning deployment (LLM, text-to-image and so on) at scale on any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine).\\nSeldon-core\\nAn MLOps framework to package, deploy, monitor and manage thousands of production machine learning models\\nStarwhale\\nAn MLOps/LLMOps platform for model building, evaluation, and fine-tuning.\\nTrueFoundry\\nA PaaS to deploy, Fine-tune and serve LLM Models on a company\u2019s own Infrastructure with Data Security and Optimal GPU and Cost Management. Launch your LLM Application at Production scale with best DevSecOps practices.\\nWeights &amp; Biases\\nA lightweight and flexible platform for machine learning experiment tracking, dataset versioning, and model management, enhancing collaboration and streamlining MLOps workflows. W&amp;B excels at tracking LLM-powered applications, featuring W&amp;B Prompts for LLM execution flow visualization, input and output monitoring, and secure management of prompts and LLM chain configurations.\\n\u2b06 back to ToC\\nWorkflow\\nProject\\nDetails\\nRepository\\nAirflow\\nA platform to programmatically author, schedule and monitor workflows.\\naqueduct\\nAn Open-Source Platform for Production Data Science\\nArgo Workflows\\nWorkflow engine for Kubernetes.\\nFlyte\\nKubernetes-native workflow automation platform for complex, mission-critical data and ML processes at scale.\\nHamilton\\nA lightweight framework to represent ML/language model pipelines as a series of python functions.\\nKubeflow Pipelines\\nMachine Learning Pipelines for Kubeflow.\\nLangFlow\\nAn effortless way to experiment and prototype LangChain flows with drag-and-drop components and a chat interface.\\nMetaflow\\nBuild and manage real-life data science projects with ease!\\nPloomber\\nThe fastest way to build data pipelines. Develop iteratively, deploy anywhere.\\nPrefect\\nThe easiest way to automate your data.\\nVDP\\nAn open-source unstructured data ETL tool to streamline the end-to-end unstructured data processing pipeline.\\nZenML\\nMLOps framework to create reproducible pipelines.\\n\u2b06 back to ToC\\nScheduling\\nProject\\nDetails\\nRepository\\nKueue\\nKubernetes-native Job Queueing.\\nPAI\\nResource scheduling and cluster management for AI (Open-sourced by Microsoft).\\nSlurm\\nA Highly Scalable Workload Manager.\\nVolcano\\nA Cloud Native Batch System (Project under CNCF).\\nYunikorn\\nLight-weight, universal resource scheduler for container orchestrator systems.\\n\u2b06 back to ToC\\nModel Management\\nProject\\nDetails\\nRepository\\nComet\\nComet is an MLOps platform that offers Model Production Management, a Model Registry, and full model lineage from training straight through to production. Use Comet for model reproducibility, model debugging, model versioning, model visibility, model auditing, model governance, and model monitoring.\\ndvc\\nML Experiments Management - Data Version Control - Git for Data &amp; Models\\nModelDB\\nOpen Source ML Model Versioning, Metadata, and Experiment Management\\nMLEM\\nA tool to package, serve, and deploy any ML model on any platform.\\normb\\nDocker for Your ML/DL Models Based on OCI Artifacts\\n\u2b06 back to ToC\\nPerformance\\nML Compiler\\nProject\\nDetails\\nRepository\\nONNX-MLIR\\nCompiler technology to transform a valid Open Neural Network Exchange (ONNX) graph into code that implements the graph with minimum runtime support.\\nbitsandbytes\\nAccessible large language models via k-bit quantization for PyTorch.\\nTVM\\nOpen deep learning compiler stack for cpu, gpu and specialized accelerators\\n\u2b06 back to ToC\\nProfiling\\nProject\\nDetails\\nRepository\\noctoml-profile\\noctoml-profile is a python library and cloud service designed to provide the simplest experience for assessing and optimizing the performance of PyTorch models on cloud hardware with state-of-the-art ML acceleration technology.\\nscalene\\na high-performance, high-precision CPU, GPU, and memory profiler for Python\\n\u2b06 back to ToC\\nAutoML\\nProject\\nDetails\\nRepository\\nArchai\\na platform for Neural Network Search (NAS) that allows you to generate efficient deep networks for your applications.\\nautoai\\nA framework to find the best performing AI/ML model for any AI problem.\\nAutoGL\\nAn autoML framework &amp; toolkit for machine learning on graphs\\nAutoGluon\\nAutoML for Image, Text, and Tabular Data.\\nautoml-gs\\nProvide an input CSV and a target field to predict, generate a model + code to run it.\\nautokeras\\nAutoML library for deep learning.\\nAuto-PyTorch\\nAutomatic architecture search and hyperparameter optimization for PyTorch.\\nauto-sklearn\\nan automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator.\\nDragonfly\\nAn open source python library for scalable Bayesian optimisation.\\nDetermined\\nscalable deep learning training platform with integrated hyperparameter tuning support; includes Hyperband, PBT, and other search methods.\\nDEvol (DeepEvolution)\\na basic proof of concept for genetic architecture search in Keras.\\nEvalML\\nAn open source python library for AutoML.\\nFEDOT\\nAutoML framework for the design of composite pipelines.\\nFLAML\\nFast and lightweight AutoML (\\npaper\\n).\\nGoptuna\\nA hyperparameter optimization framework, inspired by Optuna.\\nHpBandSter\\na framework for distributed hyperparameter optimization.\\nHPOlib2\\na library for hyperparameter optimization and black box optimization benchmarks.\\nHyperband\\nopen source code for tuning hyperparams with Hyperband.\\nHypernets\\nA General Automated Machine Learning Framework.\\nHyperopt\\nDistributed Asynchronous Hyperparameter Optimization in Python.\\nhyperunity\\nA toolset for black-box hyperparameter optimisation.\\nIntelli\\nA framework to connect a flow of ML models by applying graph theory.\\nKatib\\nKatib is a Kubernetes-native project for automated machine learning (AutoML).\\nKeras Tuner\\nHyperparameter tuning for humans.\\nlearn2learn\\nPyTorch Meta-learning Framework for Researchers.\\nLudwig\\na toolbox built on top of TensorFlow that allows to train and test deep learning models without the need to write code.\\nMOE\\na global, black box optimization engine for real world metric optimization by Yelp.\\nModel Search\\na framework that implements AutoML algorithms for model architecture search at scale.\\nNASGym\\na proof-of-concept OpenAI Gym environment for Neural Architecture Search (NAS).\\nNNI\\nAn open source AutoML toolkit for automate machine learning lifecycle, including feature engineering, neural architecture search, model compression and hyper-parameter tuning.\\nOptuna\\nA hyperparameter optimization framework.\\nPycaret\\nAn open-source, low-code machine learning library in Python that automates machine learning workflows.\\nRay Tune\\nScalable Hyperparameter Tuning.\\nREMBO\\nBayesian optimization in high-dimensions via random embedding.\\nRoBO\\na Robust Bayesian Optimization framework.\\nscikit-optimize(skopt)\\nSequential model-based optimization with a\\nscipy.optimize\\ninterface.\\nSpearmint\\na software package to perform Bayesian optimization.\\nTPOT\\none of the very first AutoML methods and open-source software packages.\\nTorchmeta\\nA Meta-Learning library for PyTorch.\\nVegas\\nan AutoML algorithm tool chain by Huawei Noah\\'s Arb Lab.\\n\u2b06 back to ToC\\nOptimizations\\nProject\\nDetails\\nRepository\\nFeatherCNN\\nFeatherCNN is a high performance inference engine for convolutional neural networks.\\nForward\\nA library for high performance deep learning inference on NVIDIA GPUs.\\nNCNN\\nncnn is a high-performance neural network inference framework optimized for the mobile platform.\\nPocketFlow\\nuse AutoML to do model compression.\\nTensorFlow Model Optimization\\nA suite of tools that users, both novice and advanced, can use to optimize machine learning models for deployment and execution.\\nTNN\\nA uniform deep learning inference framework for mobile, desktop and server.\\noptimum-tpu\\nGoogle TPU optimizations for transformers models\\n\u2b06 back to ToC\\nFederated ML\\nProject\\nDetails\\nRepository\\nEasyFL\\nAn Easy-to-use Federated Learning Platform\\nFATE\\nAn Industrial Grade Federated Learning Framework\\nFedML\\nThe federated learning and analytics library enabling secure and collaborative machine learning on decentralized data anywhere at any scale. Supporting large-scale cross-silo federated learning, cross-device federated learning on smartphones/IoTs, and research simulation.\\nFlower\\nA Friendly Federated Learning Framework\\nHarmonia\\nHarmonia is an open-source project aiming at developing systems/infrastructures and libraries to ease the adoption of federated learning (abbreviated to FL) for researches and production usage.\\nTensorFlow Federated\\nA framework for implementing federated learning\\n\u2b06 back to ToC\\nAwesome Lists\\nProject\\nDetails\\nRepository\\nAwesome Argo\\nA curated list of awesome projects and resources related to Argo\\nAwesome AutoDL\\nAutomated Deep Learning: Neural Architecture Search Is Not the End (a curated list of AutoDL resources and an in-depth analysis)\\nAwesome AutoML\\nCurating a list of AutoML-related research, tools, projects and other resources\\nAwesome AutoML Papers\\nA curated list of automated machine learning papers, articles, tutorials, slides and projects\\nAwesome-Code-LLM\\n\ud83d\udc68\\u200d\ud83d\udcbb An awesome and curated list of best code-LLM for research.\\nAwesome Federated Learning Systems\\nA curated list of Federated Learning Systems related academic papers, articles, tutorials, slides and projects.\\nAwesome Federated Learning\\nA curated list of federated learning publications, re-organized from Arxiv (mostly)\\nawesome-federated-learning\\nacc\\nAll materials you need for Federated Learning: blogs, videos, papers, and softwares, etc.\\nAwesome Open MLOps\\nThis is the Fuzzy Labs guide to the universe of free and open source MLOps tools.\\nAwesome Production Machine Learning\\nA curated list of awesome open source libraries to deploy, monitor, version and scale your machine learning\\nAwesome Tensor Compilers\\nA list of awesome compiler projects and papers for tensor computation and deep learning.\\nkelvins/awesome-mlops\\nA curated list of awesome MLOps tools.\\nvisenger/awesome-mlops\\nMachine Learning Operations - An awesome list of references for MLOps\\ncurrentslab/awesome-vector-search\\nA curated list of awesome vector search framework/engine, library, cloud service and research papers to vector similarity search.\\npleisto/flappy\\nProduction-Ready LLM Agent SDK for Every Developer\\n\u2b06 back to ToC\\nAbout\\nAn awesome &amp; curated list of best LLMOps tools for developers\\nTopics\\nawesome-list\\nmlops\\nai-development-tools\\nllmops\\nResources\\nReadme\\nLicense\\nCC0-1.0 license\\nActivity\\nCustom properties\\nStars\\n3.8k\\nstars\\nWatchers\\n65\\nwatching\\nForks\\n362\\nforks\\nReport repository\\nReleases\\nNo releases published\\nPackages\\n0\\nNo packages published\\nContributors\\n84\\n+ 70 contributors\\nLanguages\\nShell\\n86.2%\\nPython\\n13.8%', 'tool_call_id': 'call_zIolfSqkgQdsqi3BsRkXkWXl', 'name': 'extract_content'}, {'role': 'tool', 'content': 'M1n9X\\n/\\nllm_agents_devtools\\nPublic\\nNotifications\\nYou must be signed in to change notification settings\\nFork\\n3\\nStar\\n30\\nA curated list of autonomous agents and developer tools powered by LLM.\\n30\\nstars\\n3\\nforks\\nBranches\\nTags\\nActivity\\nStar\\nNotifications\\nYou must be signed in to change notification settings\\nM1n9X/llm_agents_devtools\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\nmain\\nBranches\\nTags\\nGo to file\\nCode\\nFolders and files\\nName\\nName\\nLast commit message\\nLast commit date\\nLatest commit\\nHistory\\n62 Commits\\nREADME.md\\nREADME.md\\nView all files\\nRepository files navigation\\n\u26a1\ufe0f LLM Agents and DevTools\\nThis is a curated list of autonomous agents and developer tools powered by LLM.\\nAgents\\nAgent Platforms\\nApp Generators\\nCoding Assistants\\nCognitive Architectures\\nDigital Humans\\nDocumentation\\nEmbodied AI\\nEvaluation\\nFoundation Models\\nIDEs\\nKnowledge Bases\\nLLMOps\\nOpenAI Plugins\\nPR Agents\\nPrompts Tuning\\nReasoning and Planning\\nResearch\\nSearch\\nSecurity\\nShell Assistants\\nTraining &amp; Deployment\\nAgents\\nAdala\\n- Autonomous DAta (Labeling) Agent framework.\\nAgentGPT\\n- \ud83e\udd16 Assemble, configure, and deploy autonomous AI Agents in your browser.\\nAutoGPT\\n\u2014 An experimental open-source attempt to make GPT-4 fully autonomous.\\nBabyAGI\\n\u2014 An AI-powered task management system.\\nBambooAI\\n- A lightweight library that leverages Language Models (LLMs) to enable natural language interactions, allowing you to source and converse with data.\\nBlockAGI\\n- Your Self-Hosted, Hackable Research Agent Inspired by AutoGPT.\\nCAMEL\\n- Communicative Agents for \u201cMind\u201d Exploration of Large Scale Language Model Society.\\nChat-Haruhi-Suzumiya\\n- Reviving Anime Character in Reality via Large Language Model.\\nDB-GPT\\n- by TsinghuaDatabaseGroup. LLM As Database Administrator.\\nDB-GPT\\n- by eosphoros-ai. Revolutionizing Database Interactions with Private LLM Technology.\\nData-Copilot\\n- Bridging Billions of Data and Humans with Autonomous Workflow.\\nDemoGPT\\n\u2014 Auto Gen-AI App Generator with the Power of Llama 2\\nDevOpsGPT\\n- DevOpsGPT: AI-Driven Software Development Automation Solution\\nFactory\\n\u2014 Agents for code generation. Waitlisted.\\nGPT Engineer\\n\u2014\\xa0CLI agent that generates a repository from a prompt, and asks clarifying questions.\\nGPT Migrate\\n\u2014\\xa0CLI agent that converts a full-stack application from one language or framework to another. Uses GPT-4 32k context.\\nGeneralAgent\\n- A simple, general, customizable Agent Framework.\\nGitWit\\n\u2014\\xa0Web-based agent for adding features to full-stack apps in Git repositories.\\nGorilla\\n- Large Language Model Connected with Massive APIs.\\nGrit\\n- GitHub-integrated agent for automating maintenance tasks and other development work.\\nHyperWrite\\n-  Personal Assistant \u2014 AI Agent\\nMuzic\\n- Music Understanding and Generation with Artificial Intelligence.\\nSalesGPT\\n- Context-aware AI Sales Agent to automate sales outreach.\\nSecond.dev\\n\u2014 A platform for adding features to full-stack apps.\\nShortGPT\\n- Experimental AI framework for automated short/video content creation.\\nSmol Developer\\n\u2014 CLI agent that generates a repository from a prompt. Uses OpenAI and Anthropic.\\nTeenage-AGI\\n- Use OpenAI and Pinecone to Give memory to an AI agent and also allows it to \"think\" before making an action (outputting text)\\nTransformers Agent\\n- HuggingFace Transformers Agent offer a natural language API built on transformers and a curated set of tools. An agent interprets user natural language input &amp; use tools to fulfil the user request.\\nWebArena\\n- A Realistic Web Environment for Building Autonomous Agents.\\nautomata\\n- Automata\\'s objective is to evolve into a fully autonomous, self-programming Artificial Intelligence system.\\nbeebot\\n- An Autonomous AI Agent that works.\\ngpt-researcher\\n- GPT based autonomous agent that does online comprehensive research on any given topic.\\ngpt_academic\\n- \u4e3a ChatGPT/GLM \u63d0\u4f9b\u56fe\u5f62\u4ea4\u4e92\u754c\u9762\uff0c\u7279\u522b\u4f18\u5316\u8bba\u6587\u9605\u8bfb/\u6da6\u8272/\u5199\u4f5c\u4f53\u9a8c\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u5feb\u6377\u6309\u94ae&amp;\u51fd\u6570\u63d2\u4ef6\uff0c\u652f\u6301 Python \u548c C++\u7b49\u9879\u76ee\u5256\u6790&amp;\u81ea\u8bd1\u89e3\u529f\u80fd\uff0cPDF/LaTex \u8bba\u6587\u7ffb\u8bd1&amp;\u603b\u7ed3\u529f\u80fd\uff0c\u652f\u6301\u5e76\u884c\u95ee\u8be2\u591a\u79cd LLM \u6a21\u578b\uff0c\u652f\u6301 chatglm2 \u7b49\u672c\u5730\u6a21\u578b\u3002\u517c\u5bb9\u6587\u5fc3\u4e00\u8a00, moss, llama2, rwkv, claude2, \u901a\u4e49\u5343\u95ee, \u4e66\u751f, \u8baf\u98de\u661f\u706b\u7b49\u3002\\nlemon-agent\\n- Plan-Validate-Solve (PVS) Agent for accurate, reliable and reproducable workflow automation.\\nloopgpt\\n- A re-implementation of the popular Auto-GPT project as a proper python package, written with modularity and extensibility in mind.\\nmini-agi\\n- A minimal general-purpose autonomous agent based on GPT-3.5 / GPT-4. Can analyze stock prices, perform network security tests, create art, and order pizza.\\nrci-agent\\n- RCI Agent for MiniWoB++\\ntalk\\n- Talk with ChatGPT using your VOICE\\nAgent Platforms\\nAgentBench\\n- A Comprehensive Benchmark to Evaluate LLMs as Agents.\\nAutoAgents\\n- Generate different roles for GPTs to form a collaborative entity for complex tasks.\\nAutoGen\\n- Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework.\\nBOLAA\\n- Benchmarking and Orchestrating LLM-augmented Autonomous Agents.\\nE2B\\n\u2014\\xa0Open source cloud platform for hosting LLM-based agents. Supports\\nSmol Developer\\n.\\nGeniA\\n- Your Engineering Gen AI Team member \ud83e\uddec\ud83e\udd16\ud83d\udcbb\\nGentopia\\n- Build AGI through Interaction of Specialized Agents.\\nJARVIS\\n- A system to connect LLMs with ML community.\\nLLM-ToolMaker\\n- Large Language Models as Tool Makers.\\nOpenAGI\\n- An open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models.\\nOpenAgents\\n- An Open Platform for Language Agents in the Wild\\nPromethAI-Backend\\n- Open-source framework that gives you AI Agents that help you navigate decision-making, get personalized goals and execute them.\\nSuperAGI\\nOpen source platform for hosting LLM-based agents including\\nSuperCoder\\n.\\nSuperagent\\n- Build, deploy, and manage LLM-powered agents.\\nToolBench\\n- An open platform for training, serving, and evaluating large language model for tool learning.\\nXAgent\\n- An Autonomous LLM Agent for Complex Task Solving.\\nagents\\n- An Open-source Framework for Autonomous Language Agents.\\nai-legion\\n- An LLM-powered autonomous agent platform.\\nchidori\\n- A reactive runtime for building durable AI agents.\\ndataberry\\n- The no-code platform for building custom LLM Agents.\\nix\\n- Autonomous GPT-4 agent platform.\\nlagent\\n- A lightweight framework for building LLM-based agents.\\nopenagent\\n- Microservices approach to AGI. Modular components for AI apps or AGI agents. (... and solving some wicked LLM problems like \u26a1 2X faster LLaMA 2)\\nrivet\\n- AI agent and prompt chaining IDE and library.\\nsemantic-kernel\\n- Integrate cutting-edge LLM technology quickly and easily into your apps.\\nsmart_agent\\n- An open-source project offering a comprehensive library for deconstructing complex tasks and dispatching functions within a toolkit.\\ntoolformer-pytorch\\n- Language Models That Can Use Tools, by MetaAI.\\nworkgpt\\n- A GPT agent framework for invoking APIs.\\nApp Generators\\nGPT Pilot\\n- Code out the entire app as you oversee the code being written.\\nLiterally anything\\n- HTML and JavaScript web app generator.\\nOpenCopilot\\n- AI Copilot for your own SaaS product. Open source AI sidekick for everyone.\\nPico\\n- End-to-end micro app generator with instant deployment.\\ngpt-pilot\\n- PoC for a scalable dev tool that writes entire apps from scratch while the developer oversees the implementation.\\nreact-agent\\n- The open-source React.js Autonomous LLM Agent.\\ntextbase\\n- A simple framework for building AI chatbots.\\nwasp\\n- The fastest way to develop full-stack web apps with React &amp; Node.js.\\nCoding Assistants\\nAI Code Convert\\n\u2014\\xa0A web tool for translating code between programming languages.\\nAI Code Playground\\n\u2014 CodeChain is a web tool for refactoring and improving code.\\nAPI Copilot\\n\u2014 Assistant for backend API development.\\nAdrenaline\\n- Web-based chatbot using AI and ASTs to answer questions about your codebase.\\nAmazon CodeWhisperer\\n- An AI coding companion that generates whole line and full function code suggestions in your IDE to help you get more done faster.\\nBito AI\\n\u2014 Bito is powered by OpenAI\\'s ChatGPT and GPT-4, which revolutionizes the way developers write code. Generate high-quality AI-powered code, AI-assisted code completion, optimize code performance, explain complex code snippets, generate unit tests, and more \u2013 all effortlessly.\\nBlackbox\\n\u2014 VS Code extension with autocomplete and chat including links to online coding references.\\nCodeApex\\n- A bilingual programming evaluation benchmark for Large Language Models.\\nCodeGeeX\\nOpen source assistant based on the CodeGeeX LLM with chat, completion, and refactoring. Extensions for 9 editors including VS Code, and PyCharm.\\nCodeMate\\n\u2014\\xa0VS Code extension for debugging and optimizing code.\\nCodePal\\n\u2014 A web tool for quickly generating or refactoring code.\\nCodeSquire\\n\u2014 Chrome extension that adds autocomplete to Google Colab, BigQuery, and JupyterLab.\\nCodeium\\n\u2014 Assistant with autocomplete, natural language search and chat. Extensions for 21 editors including VS Code, JetBrains, Neovim, Vim, Emacs, Eclipse, PyCharm, and Xcode. Enterprise version includes codebase-specific fine-tuning.\\nContinue\\n\u2014 VS Code extension with chat, refactor, and code generation. Edits multiple files and runs commands on your behalf.\\nGenie AI\\n\u2014 A Visual Studio Code - ChatGPT Integration. Supports GPT-4, GPT3.5, GPT3 and Codex models. Create new files, view diffs with one click; your copilot to learn code, add tests, find bugs and more.\\nGitHub Copilot X\\n\u2014 A VS Code extension with chat, pull request text generation, and unit test generation.\\nIncognito Pilot\\n\u2014\\xa0Open source assistant with built-in Python editor and interpreter.\\nMagnet\\n\u2014 Web-based chatbot with repositories and issues as context.\\nQuack AI\\n\u2014\\xa0VS Code extension for adhering to project coding guidelines. Waitlist.\\nRefact\\n\u2014\\xa0Open-Source Coding Assistant with Fine-Tuning on codebase, autocompletion, code refactoring, code analysis, integrated chat and more!\\nReplit Ghostwriter Chat\\n\u2014 Assistant built into\\nReplit\\nwith chat, proactive debugging, and autocomplete. Uses OpenAI for chat and\\nreplit-code-v1-3b\\n(OS) for autocomplete.\\nRift\\n- An AI-native language server for your personal AI software engineer.\\nSource Graph Cody\\n- Assistant with chat, refactoring, and unit test generation. Extensions for VS Code and IntelliJ. Also available as a web app.\\nTabby\\n\u2014\\xa0Open source, self-hosted code completion assistant. Extensions for VS Code and Vim.\\nTabnine\\n(Source)\\n\u2014 Open source, self-hosted code completion assistant. Extensions for 15 editors including VS Code, IntelliJ, Neovim, Eclipse, and PyCharm.\\nai-jsx\\n- The AI Application Framework for Javascript.\\nax\\n- A comprehensive AI framework for TypeScript.\\nclippinator\\n- AI programming assistant.\\ncodeinterpreter-api\\n- Open source implementation of the ChatGPT Code Interpreter \ud83d\udc7e\\ncody\\n- AI that knows your entire codebase.\\nevalgpt\\n- An code interpreter framework that utilizes large language models to automate the process of code-writing and execution, delivering precise results for user-defined tasks.\\ngpt-migrate\\n- Easily migrate your codebase from one framework or language to another.\\nopen-interpreter\\n- OpenAI\\'s Code Interpreter in your terminal, running locally.\\ntalk-codebase\\n- CLI chatbot with repository as context. Supports OpenAI as well as locally running LLMs via GPT4All.\\nCognitive Architectures\\nACT-R\\n- Adaptive Control of Thought\u2014Rational.\\nCoALA\\n- Cognitive Architectures for Language Agents.\\nMemGPT\\n- Teaching LLMs memory management for unbounded context \ud83d\udcda\ud83e\udd99\\nSoar\\n- A general cognitive architecture for systems that exhibit intelligent behavior.\\nDigital Humans\\nGPT-vup\\n- Live digital host for BIliBili/Tiktok.\\nFaceChain\\n- A deep-learning toolchain for generating your Digital-Twin.\\nDocumentation\\nDocify\\n\u2014\\xa0A VS Code extension to generate docstrings.\\nMintlify Writer\\n\u2014\\xa0A VS Code extension to generate docstrings.\\nTrelent\\n\u2014 A VS Code extension to generate docstrings. Uses proprietary models.\\ngpt-author\\n- Utilize a chain of GPT-4, Stable Diffusion, and Anthropic API calls to generate an original fantasy novel.\\nEmbodied AI\\nAgentSims\\n- An easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in.\\nAgentVerse\\n- A flexible framework that simplifies the process of building custom multi-agent environments for large language models (LLMs).\\nChatArena\\n- A Multi-Agent Language Game Environments for LLMs. The goal is to develop communication and collaboration capabilities of AIs.\\nChatDev\\n- Create Customized Software using Natural Language Idea (through Multi-Agent Collaboration)\\nGPTeam\\n- An open-source multi-agent simulation.\\nGenerative Agents\\n- Interactive Simulacra of Human Behavior.\\nGeniA\\n- Your Engineering Gen AI Team member \ud83e\uddec\ud83e\udd16\ud83d\udcbb\\nHumanoidAgents\\n- Humanoid Agents: Platform for Simulating Human-like Generative Agents.\\nLASER\\n- LLM Agent with State-Space Exploration for Web Navigation.\\nMetaGPT\\n\u2014 The Multi-Agent Framework: Given one line Requirement, return PRD, Design, Tasks, Repo.\\nMind2Web\\n- Towards a Generalist Agent for the Web.\\nMindAgent\\n- Emergent Gaming Interaction.\\nRT-2\\n- Robotic Transformer 2 (RT-2): The Vision-Language-Action Model.\\nRoboAgent\\n- Towards Sample Efficient Robot Manipulation with Semantic Augmentations and Action Chunking.\\nVoxPoser\\n- Composable 3D Value Maps for Robotic Manipulation with Language Models\\nVoyager\\n- An Open-Ended Embodied Agent with Large Language Models.\\nai-town\\n- A deployable starter kit for building and customizing your own version of AI town - a virtual town where AI characters live, chat and socialize.\\naloha\\n- A Low-cost Open-source Hardware System for Bimanual Teleoperation.\\ndev-gpt\\n- Your Virtual Development Team.\\nEvaluation\\nGPT-Fathom\\n- Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond.\\nInstructEval\\n- Evaluation suite for the systematic evaluation of instruction selection methods.\\nSmartPlay\\n- A benchmark for Large Language Models (LLMs). It is designed to be easy to use, and to provide a wide variety of games to test agents on.\\ninstruct-eval\\n- Quantitatively evaluate instruction-tuned models such as Alpaca and Flan-T5 on held-out tasks.\\nFoundation Models\\nCodeGeeX2\\n- A More Powerful Multilingual Code Generation Model.\\nCodeGen\\n- A family of open-source model for program synthesis. Trained on TPU-v4. Competitive with OpenAI Codex.\\nCodeLlama\\n- A family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.\\nDeepSeek-Coder\\n- Let the Code Write Itself.\\nLM Studio\\n- Discover, download, and run local LLMs\\nMAmmoTH\\n- Building Math Generalist Models through Hybrid Instruction Tuning.\\nMFTCoder\\n- An open-source project of CodeFuse for multitasking Code-LLMs(large language model for code tasks), which includes models, datasets, training codebases and inference guides.\\nMagic\\n\u2014 Company promising two products, an assistant and\\nLTM-1\\n, an underlying foundation model trained on code. Waitlist.\\nMathGLM\\n- GPT Can Solve Mathematical Problems Without a Calculator.\\nMistral-7B-v0.1\\nNExT-GPT\\n- Any-to-Any Multimodal Large Language Model.\\nOpenChat\\n- Advancing Open-source Language Models with Imperfect Data\\nPhind-CodeLlama-34B-v1\\n- Beating GPT-4 on HumanEval with a Fine-Tuned CodeLlama-34B.\\nStarCoder\\n- A helpful coding assistant.\\nTinyLlama\\n- An open endeavor to pretrain a 1.1B Llama model on 3 trillion tokens.\\nWizardCoder\\n- Empowering Code Large Language Models with Evol-Instruct.\\ngpt4all\\n- open-source LLM chatbots that you can run anywhere.\\ngpt4free\\n- The official gpt4free repository | various collection of powerful language models.\\nincoder\\n- A Generative Model for Code Infilling and Synthesis.\\nlitellm\\n- Call all LLM APIs using the OpenAI format. Use Azure, OpenAI, Cohere, Anthropic, Ollama, VLLM, Sagemaker, HuggingFace, Replicate (100+ LLMs)\\nprivateGPT\\n- Interact privately with your documents using the power of GPT, 100% privately, no data leaks.\\nreplit-code-v1-3b\\n- A 2.7B Causal Language Model focused on Code Completion.\\nstreaming-llm\\n- Efficient Streaming Language Models with Attention Sinks.\\nviper\\n- ViperGPT: Visual Inference via Python Execution for Reasoning\\nIDEs\\nCodeStory\\n\u2014\\xa0An IDE with chat, code explanations, auto-generated commits and PR summaries. Forked from VSCodium.\\nCursor\\n\u2014 An IDE with chat, edit, generate and debug features. Forked from VSCodium, so the interface is similar to VS Code. Uses OpenAI.\\nMutable\\n\u2014 Web-based IDE, integrated with a chatbot and GitHub.\\nKnowledge Bases\\nALCE\\n- Enabling Large Language Models to Generate Text with Citations.\\nLangchain-Chatchat\\n- Local knowledge based LLM (like ChatGLM) QA app with langchain.\\nPDFTriage\\n- Question Answering over Long, Structured Documents.\\nRobby-chatbot\\n- AI chatbot \ud83e\udd16 for chat with CSV, PDF, TXT files \ud83d\udcc4 and YTB videos \ud83c\udfa5 | using Langchain\ud83e\udd9c | OpenAI | Streamlit \u26a1\\nVerba\\n- Retrieval Augmented Generation (RAG) chatbot powered by Weaviate.\\nembedchain\\n- Framework to easily create LLM powered bots over any dataset.\\ngpt-runner\\n- Conversations with your files! Manage and run your AI presets!\\nkhoj\\n- An AI personal assistant for your digital brain. Khoj provides you with an easy way to setup the infrastructure to search and chat with your personal knowledge base, be it markdown, org, image, or PDF files.\\ntxtai\\n- All-in-one open-source embeddings database for semantic search, LLM orchestration and language model workflows.\\nLLMOps\\nAGiXT\\n- A dynamic AI Automation Platform that seamlessly orchestrates instruction management and complex task execution across diverse AI providers.\\nBrowserGPT\\n- Command your browser with GPT.\\nCarbonate\\n\u2014 End-to-end testing using natural language. Integrates into your existing test suite (currently Jest, PHPUnit and Python\\'s unittest).\\nDiffBlue\\n- Automatically generated unit tests for Java.\\nLLMStack\\n- No-code platform to build generative AI apps, chatbots and agents with your data.\\nLocalAI\\n- \ud83e\udd16 Self-hosted, community-driven, local OpenAI compatible API. Drop-in replacement for OpenAI running LLMs on consumer-grade hardware. Free Open Source OpenAI alternative. No GPU required. Runs ggml, gguf, GPTQ, onnx, TF compatible models: llama, llama2, gpt4all, rwkv, whisper, vicuna, koala, cerebras, falcon, dolly, starcoder, and many others.\\nMeticulous.ai\\n- Automatically generated, automatically maintained end-to-end tests: as your app evolves so does your test suite.\\nOctoMind\\n- Auto-maintenance and generated browser-based end-to-end-tests integrated into Github Actions, Azure DevOps and more.\\nTaxyAI browser-extension\\n- Automate your browser with GPT-4.\\nTraceloop\\n- Uses OpenTelemetry tracing data with generative AI to improve system reliability.\\nhaystack\\n- \ud83d\udd0d LLM orchestration framework to build customizable, production-ready LLM applications.\\nhelicone\\n- An open-source observability platform for Language Learning Models (LLMs).\\nlangfuse\\n- Open source observability and analytics for LLM applications.\\npezzo\\n- \ud83d\udd79\ufe0f Open-source, developer-first LLMOps platform designed to streamline prompt design, version management, instant delivery, collaboration, troubleshooting, observability and more.\\npromptflow\\n- Build high-quality LLM apps - from prototyping, testing to production deployment and monitoring.\\nzeno\\n- AI Data Management &amp; Evaluation Platform.\\nOpenAI Plugins\\nChatWithGit\\n\u2014 Enables ChatGPT to search GitHub and return links to relevant repositories.\\nCode ChatGPT Plugin\\n\u2014 Open source example of a ChatGPT plugin that pulls context from a directory of files.\\nPR Agents\\nAutoPR\\n- Fix issues with AI-generated pull requests, powered by ChatGPT.\\nBitBuilder\\n\u2014\\xa0A GitHub integration to generate pull requests from issues.\\nCode Review GPT\\n\u2014\\xa0An open source tool for reviewing PRs. Works as GitHub action, Gitlab CLI or locally.\\nCodeRabbit\\n\u2014\\xa0Customizable CI to add summaries and code suggestions to PRs.\\nCodeium PR Agent\\n\u2014\\xa0Open source tool for automated code reviews.\\nNova\\n- CI bot to add actions such as summaries and tests to new PRs.\\nSweep\\n\u2014\\xa0AI junior dev: GitHub integration to generate, test, and self-review pull requests from issues.\\nPrompts Tuning\\nAnalogical Prompting\\n- A new prompting approach designed to automatically guide the reasoning process of large language models.\\nEvoPrompt\\n- Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers.\\nFooocus\\n- Focus on prompting and generating.\\nLangGPT\\n- LangGPT: Empowering everyone to become a prompt expert!\ud83d\ude80 Structured Prompt\uff0c\u7ed3\u6784\u5316\u63d0\u793a\u8bcd\u3002\\nPrompt-Engineering-Guide\\n- \ud83d\udc19 Guides, papers, lecture, notebooks and resources for prompt engineering.\\nYiVal\\n- \ud83d\ude80 From Demo to Production.\ud83d\ude80 YiVal is an open source GenAI-Ops framework that allows you to iteratively tune and evaluate your AIGC prompts, model metadata, model params, and retrieval configs all at once with your preferred choices of test dataset generation, evaluation algorithms and improvement strategies.\\nguidance\\n- A guidance language for controlling large language models.\\noutlines\\n- Generative Model Programming.\\nprompttools\\n- Open-source tools for prompt testing and experimentation, with support for both LLMs (e.g. OpenAI, LLaMA) and vector databases (e.g. Chroma, Weaviate, LanceDB).\\nReasoning and Planning\\nAoT\\n- Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models.\\nCoD\\n- From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting.\\nCoT\\n- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.\\nGoT\\n- Graph of Thoughts: Solving Elaborate Problems with Large Language Models.\\nReAct\\n- Synergizing Reasoning and Acting in Language Models.\\nReWOO\\n- Decoupling Reasoning from Observations for Efficient Augmented Language Models.\\nSCoT\\n- Structured Chain-of-Thought Prompting for Code Generation.\\nScrews\\n-A Modular Framework for Reasoning with Revisions.\\nSwiftSage\\n- A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks.\\nToT\\n- Deliberate Problem Solving with Large Language Models that Elevates Model Reasoning by atleast 70%.\\nllm-reasoners\\n- reasoning as planning (RAP), a library for advanced large language model reasoning.\\nreflexion\\n- Reflexion: Language Agents with Verbal Reinforcement Learning.\\nsaycan\\n- Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.\\nself-ask\\n- Measuring and Narrowing the Compositionality Gap in Language Models.\\nResearch\\nArxivGPT\\n- A Google Chrome plug-in that helps you quickly understand the content of arXiv papers.\\nChatPDF\\n- Chat with any PDF using the new ChatGPT API.\\nChatPaper\\n- Use ChatGPT to summarize the arXiv papers. \u5168\u6d41\u7a0b\u52a0\u901f\u79d1\u7814\uff0c\u5229\u7528 chatgpt \u8fdb\u884c\u8bba\u6587\u5168\u6587\u603b\u7ed3+\u4e13\u4e1a\u7ffb\u8bd1+\u6da6\u8272+\u5ba1\u7a3f+\u5ba1\u7a3f\u56de\u590d.\\nChemCrow\\n- An open source package for the accurate solution of reasoning-intensive chemical tasks.\\nOpenBBTerminal\\n- OpenBB is committed to build the future of investment research by focusing on an open source infrastructure accessible to everyone, everywhere.\\ngpt-researcher\\n- GPT based autonomous agent that does online comprehensive research on any given topic.\\nqlib\\n- An AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions.\\nSecurity\\nNeMo-Guardrails\\n- An open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems.\\nguardrails\\n- An open-source Python package for specifying structure and type, validating and correcting the outputs of large language models (LLMs).\\nShell Assistants\\nAider\\n\u2014 CLI assistant and agent that generates changes and commits to repositories. Uses OpenAI.\\nAskCommand\\n- Web based tool to generate Unix commands from text automatically using AI.\\nButterfish\\n- CLI tool that embeds ChatGPT in your shell for easy access. Includes simple agentic capabilities.\\nMentat\\n\u2014 CLI assistant and agent that makes changes to repositories.\\nShell-AI\\n- LangChain powered shell command generate and run CLI.\\nSearch\\nBloop\\n\u2014 Natural language search for repositories.\\nBuildt\\n\u2014 Natural language search for repositories.\\nMetaphor\\n- A search engine designed from scratch using AI.\\nPandasAI\\n- A Python library that integrates generative artificial intelligence capabilities into pandas, making dataframes conversational.\\nPerplexity\\n- Unlock the power of knowledge with information discovery and sharing.\\nTraining &amp; Deployment\\nHCP-Diffusion\\n- A universal Stable-Diffusion toolbox.\\nLMDeploy\\n- A toolkit for compressing, deploying, and serving LLMs.\\nMedusa\\n- Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads.\\nlangchain-production-starter\\n- Deploy LangChain Agents and connect them to Telegram.\\nllm-applications\\n- A comprehensive guide to building RAG-based LLM applications for production.\\nludwig\\n- Low-code framework for building custom LLMs, neural networks, and other AI models.\\nonprem\\n- A tool for running on-premises large language models with non-public data.\\nprompt2model\\n- Generate Deployable Models from Natural Language Instructions.\\nvllm\\n- A high-throughput and memory-efficient inference and serving engine for LLMs.\\nAbout\\nA curated list of autonomous agents and developer tools powered by LLM.\\nResources\\nReadme\\nActivity\\nStars\\n30\\nstars\\nWatchers\\n2\\nwatching\\nForks\\n3\\nforks\\nReport repository\\nReleases\\nNo releases published\\nPackages\\n0\\nNo packages published', 'tool_call_id': 'call_Vh9DH7mDWzXzsQoHKaWakHtS', 'name': 'extract_content'}, {'role': 'tool', 'content': \"jihoo-kim\\n/\\nawesome-production-llm\\nPublic\\nNotifications\\nYou must be signed in to change notification settings\\nFork\\n25\\nStar\\n322\\nA curated list of awesome open-source libraries for production LLM\\nLicense\\nMIT license\\n322\\nstars\\n25\\nforks\\nBranches\\nTags\\nActivity\\nStar\\nNotifications\\nYou must be signed in to change notification settings\\njihoo-kim/awesome-production-llm\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\nmain\\nBranches\\nTags\\nGo to file\\nCode\\nFolders and files\\nName\\nName\\nLast commit message\\nLast commit date\\nLatest commit\\nHistory\\n4 Commits\\nLICENSE\\nLICENSE\\nREADME.md\\nREADME.md\\nView all files\\nRepository files navigation\\nAwesome-Production-LLM\\nThis repository contains a curated list of awesome open-source libraries for production large language models.\\nNewly updated\\n[2024.09.03] A new category\\n\ud83c\udf93LLM Courses / Education\\nhas been added.\\n[2024.08.01] A new category\\n\ud83c\udf73LLM Cookbook / Examples\\nhas been added.\\nQuick links\\n\ud83d\udcdaLLM Data Preprocessing\\n\ud83e\udd16LLM Training / Finetuning\\n\ud83d\udccaLLM Evaluation / Benchmark\\n\ud83d\ude80LLM Serving / Inference\\n\ud83d\udee0\ufe0fLLM Application / RAG\\n\ud83e\uddd0LLM Testing / Monitoring\\n\ud83d\udee1\ufe0fLLM Guardrails / Security\\n\ud83c\udf73LLM Cookbook / Examples\\n\ud83c\udf93LLM Courses / Education\\nLLM Data Preprocessing\\ndata-juicer\\n(\\nModelScope\\n)\\nA one-stop data processing system to make data higher-quality, juicier, and more digestible for (multimodal) LLMs!\\ndatatrove\\n(\\nHuggingFace\\n)\\nFreeing data processing from scripting madness by providing a set of platform-agnostic customizable pipeline processing blocks.\\ndolma\\n(\\nAllenAI\\n)\\nData and tools for generating and inspecting OLMo pre-training data.\\ndataverse\\n(\\nUpstage\\n)\\nThe Universe of Data. All about data, data science, and data engineering\\nNeMo-Curator\\n(\\nNVIDIA\\n)\\nScalable toolkit for data curation\\ndps\\n(\\nEleutherAI\\n)\\nData processing system for polyglot\\nLLM Training / Finetuning\\nnanoGPT\\n(\\nkarpathy\\n)\\nThe simplest, fastest repository for training/finetuning medium-sized GPTs.\\nLLaMA-Factory\\nA WebUI for Efficient Fine-Tuning of 100+ LLMs (ACL 2024)\\npeft\\n(\\nHuggingFace\\n)\\nPEFT: State-of-the-art Parameter-Efficient Fine-Tuning.\\nllama-recipes\\n(\\nMeta\\n)\\nScripts for fine-tuning Meta Llama3 with composable FSDP &amp; PEFT methods to cover single/multi-node GPUs.\\nMegatron-LM\\n(\\nNVIDIA\\n)\\nOngoing research training transformer models at scale\\nlitgpt\\n(\\nLightningAI\\n)\\n20+ high-performance LLMs with recipes to pretrain, finetune and deploy at scale.\\ntrl\\n(\\nHuggingFace\\n)\\nTrain transformer language models with reinforcement learning.\\nLMFlow\\n(\\nOptimalScale\\n)\\nAn Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All.\\ngpt-neox\\n(\\nEleutherAI\\n)\\nAn implementation of model parallel autoregressive transformers on GPUs, based on the Megatron and DeepSpeed libraries\\ntorchtune\\n(\\nPyTorch\\n)\\nA Native-PyTorch Library for LLM Fine-tuning\\nxtuner\\n(\\nInternLM\\n)\\nAn efficient, flexible and full-featured toolkit for fine-tuning LLM (InternLM2, Llama3, Phi3, Qwen, Mistral, ...)\\nnanotron\\n(\\nHuggingFace\\n)\\nMinimalistic large language model 3D-parallelism training\\nLLM Evaluation / Benchmark\\nevals\\n(\\nOpenAI\\n)\\nEvals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.\\nlm-evaluation-harness\\n(\\nEleutherAI\\n)\\nA framework for few-shot evaluation of language models.\\nopencompass\\n(\\nOpenCompass\\n)\\n- OpenCompass is an LLM evaluation platform, supporting a wide range of models (Llama3, Mistral, InternLM2,GPT-4,LLaMa2, Qwen,GLM, Claude, etc) over 100+ datasets.\\ndeepeval\\n(\\nConfidentAI\\n)\\nThe LLM Evaluation Framework\\nlighteval\\n(\\nHuggingFace\\n)\\nLightEval is a lightweight LLM evaluation suite that Hugging Face has been using internally with the recently released LLM data processing library datatrove and LLM training library nanotron.\\nevalverse\\n(\\nUpstage\\n)\\nThe Universe of Evaluation. All about the evaluation for LLMs.\\nLLM Serving / Inference\\nollama\\n(\\nOllama\\n)\\nGet up and running with Llama 3.1, Mistral, Gemma 2, and other large language models.\\ngpt4all\\n(\\nNomicAI\\n)\\nGPT4All: Chat with Local LLMs on Any Device\\nllama.cpp\\nLLM inference in C/C++\\nFastChat\\n(\\nLMSYS\\n)\\nAn open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena.\\nvllm\\nA high-throughput and memory-efficient inference and serving engine for LLMs\\nguidance\\n(\\nguidance-ai\\n)\\nA guidance language for controlling large language models.\\nLiteLLM\\n(\\nBerriAI\\n)\\nCall all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate, Groq (100+ LLMs)\\nOpenLLM\\n(\\nBentoML\\n)\\nRun any open-source LLMs, such as Llama 3.1, Gemma, as OpenAI compatible API endpoint in the cloud.\\ntext-generation-inference\\n(\\nHuggingFace\\n)\\nLarge Language Model Text Generation Inference\\nTensorRT-LLM\\n(\\nNVIDIA\\n)\\nTensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs.\\nLMDeploy\\n(\\nInternLM\\n)\\nLMDeploy is a toolkit for compressing, deploying, and serving LLMs.\\nRouteLLM\\n(\\nLMSYS\\n)\\nA framework for serving and evaluating LLM routers - save LLM costs without compromising quality!\\nLLM Application / RAG\\nAutoGPT\\nAutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters.\\nlangchain\\n(\\nLangChain\\n)\\nBuild context-aware reasoning applications\\nMetaGPT\\nThe Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming\\ndify\\n(\\nLangGenius\\n)\\nDify is an open-source LLM app development platform. Dify's intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.\\nllama_index\\n(\\nLlamaIndex\\n)\\nLlamaIndex is a data framework for your LLM applications\\nFlowise\\n(\\nFlowiseAI\\n)\\nDrag &amp; drop UI to build your customized LLM flow\\nmem0\\n(\\nMem0\\n)\\nThe memory layer for Personalized AI\\nhaystack\\n(\\nDeepset\\n)\\nLLM orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data.\\nGraphRAG\\n(\\nMicrosoft\\n)\\nA modular graph-based Retrieval-Augmented Generation (RAG) system\\nRAGFlow\\n(\\nInfiniFlow\\n)\\nRAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.\\nllmware\\n(\\nLLMware.ai\\n)\\nUnified framework for building enterprise RAG pipelines with small, specialized models\\nllama-agentic-system\\n(\\nMeta\\n)\\nAgentic components of the Llama Stack APIs\\nLLM Testing / Monitoring\\npromptflow\\n(\\nMicrosoft\\n)\\nBuild high-quality LLM apps - from prototyping, testing to production deployment and monitoring.\\nlangfuse\\n(\\nLangfuse\\n)\\nOpen source LLM engineering platform: Observability, metrics, evals, prompt management, playground, datasets. Integrates with LlamaIndex, Langchain, OpenAI SDK, LiteLLM, and more.\\nevidently\\n(\\nEvidentlyAI\\n)\\nEvidently is \\u200b\\u200ban open-source ML and LLM observability framework. Evaluate, test, and monitor any AI-powered system or data pipeline. From tabular data to Gen AI. 100+ metrics.\\ngiskard\\n(\\nGiskard\\n)\\nOpen-Source Evaluation &amp; Testing for LLMs and ML models\\npromptfoo\\n(\\npromptfoo\\n)\\nTest your prompts, agents, and RAGs. Redteaming, pentesting, vulnerability scanning for LLMs. Improve your app's quality and catch problems. Compare performance of GPT, Claude, Gemini, Llama, and more. Simple declarative configs with command line and CI/CD integration.\\nphoenix\\n(\\nArizeAI\\n)\\nAI Observability &amp; Evaluation\\nagenta\\n(\\nAgenta.ai\\n)\\nThe all-in-one LLM developer platform: prompt management, evaluation, human feedback, and deployment all in one place.\\nLLM Guardrails / Security\\nNeMo-Guardrails\\n(\\nNVIDIA\\n)\\nNeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems.\\nguardrails\\n(\\nGuardrailsAI\\n)\\nAdding guardrails to large language models.\\nPurpleLlama\\n(\\nMeta\\n)\\nSet of tools to assess and improve LLM security.\\nllm-guard\\n(\\nProtectAI\\n)\\nThe Security Toolkit for LLM Interactions\\nLLM Cookbook / Examples\\nopenai-cookbook\\n(\\nOpenAI\\n)\\nExamples and guides for using the OpenAI API\\ngemini-cookbook\\n(\\nGoogle\\n)\\nExamples and guides for using the Gemini API.\\nanthropic-cookbook\\n(\\nAnthropic\\n)\\nA collection of notebooks/recipes showcasing some fun and effective ways of using Claude.\\namazon-bedrock-workshop\\n(\\nAWS\\n)\\nThis is a workshop designed for Amazon Bedrock a foundational model service.\\nPhi-3CookBook\\n(\\nMicrosoft\\n)\\nThis is a Phi-3 book for getting started with Phi-3. Phi-3, a family of open AI models developed by Microsoft.\\nmistral-cookbook\\n(\\nMistral\\n)\\nThe Mistral Cookbook features examples contributed by Mistralers and our community, as well as our partners.\\namazon-bedrock-samples\\n(\\nAWS\\n)\\nThis repository contains examples for customers to get started using the Amazon Bedrock Service. This contains examples for all available foundational models\\ncohere-notebooks\\n(\\nCohere\\n)\\nCode examples and jupyter notebooks for the Cohere Platform\\ngemma-cookbook\\n(\\nGoogle\\n)\\nA collection of guides and examples for the Gemma open models from Google.\\nupstage-cookbook\\n(\\nUpstage\\n)\\nUpstage api examples and guides\\nLLM Courses / Education\\ngenerative-ai-for-beginners\\n(\\nMicrosoft\\n)\\n18 Lessons, Get Started Building with Generative AI\\nllm-course\\nCourse to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.\\nLLMs-from-scratch\\nImplementing a ChatGPT-like LLM in PyTorch from scratch, step by step\\nhands-on-llms\\nLearn about LLMs, LLMOps, and vector DBs for free by designing, training, and deploying a real-time financial advisor LLM system ~ source code + video &amp; reading materials\\nllm-zoomcamp\\n(\\nDataTalksClub\\n)\\nLLM Zoomcamp - a free online course about building a Q&amp;A system\\nllm-twin-course\\n(\\nDecodingML\\n)\\nLearn for free how to build an end-to-end production-ready LLM &amp; RAG system using LLMOps best practices: ~ source code + 12 hands-on lessons\\nAcknowledgements\\nThis project is inspired by\\nAwesome Production Machine Learning\\n.\\nAbout\\nA curated list of awesome open-source libraries for production LLM\\nResources\\nReadme\\nLicense\\nMIT license\\nActivity\\nStars\\n322\\nstars\\nWatchers\\n4\\nwatching\\nForks\\n25\\nforks\\nReport repository\\nReleases\\nNo releases published\\nPackages\\n0\\nNo packages published\", 'tool_call_id': 'call_VXxtxhySFYvaXlxhDqgLVzdu', 'name': 'extract_content'}, {'role': 'tool', 'content': 'Top 10 Open-Source LLM Frameworks 2024\\npublished on 09 May 2024\\nLarge Language Models\\n(LLMs) have revolutionized how machines understand and generate human-like text. Open-source LLMs make AI technology accessible, enabling developers and researchers to innovate. Here are the top 10 open-source LLM frameworks available in 2024:\\nLLaMA 2\\n- Powerful LLM from\\nMeta\\nwith up to 70B parameters, multilingual support, customizability, and an active community.\\nGPT-NeoX-20B\\n- 20B parameter autoregressive LLM from\\nEleutherAI\\n, open-source with strong performance.\\nBLOOM\\n- 176B parameter LLM from\\nBigScience\\n, supports 46 languages, open-source.\\nOPT-175B\\n- 175B parameter LLM comparable to GPT-3 but with a smaller carbon footprint.\\nCodeGen\\n- LLM from\\nSalesforce\\nfor streamlining software development.\\nBERT\\n- Bidirectional Encoder from Google for understanding context and generating language.\\nT5\\n- Text-to-text transformer from Google for various NLP tasks.\\nFalcon-40B\\n- 40B parameter LLM from TII, supports multiple languages, open-source.\\nVicuna 33B\\n- 33B parameter open-source chatbot with competitive performance.\\nGPT-J\\n- Massive 6T parameter LLM from EleutherAI for high-quality text generation.\\nOpen-source LLMs offer cost savings, flexibility, community support, transparency, and foster innovation. However, they face challenges like resource requirements, potential biases, framework selection, security, and integration.\\nQuick Comparison\\nFramework\\nParameters\\nMultilingual\\nOpen Source\\nKey Features\\nLLaMA 2\\nUp to 70B\\nYes\\nYes\\nSpeed, customizability, community\\nGPT-NeoX-20B\\n20B\\nNo\\nYes\\nStrong few-shot reasoner\\nBLOOM\\n176B\\nYes (46 languages)\\nYes\\nSupports programming languages\\nOPT-175B\\n175B\\nNo\\nYes\\nComparable to GPT-3, lower carbon footprint\\nCodeGen\\n-\\nNo\\nYes\\nStreamlines software development\\nBERT\\nFlexible\\nNo\\nYes\\nUnderstands context, generates language\\nT5\\nFlexible\\nNo\\nYes\\nUnified framework for NLP tasks\\nFalcon-40B\\nUp to 40B\\nYes\\nYes\\nEfficient inference, scalable\\nVicuna 33B\\n33B\\nNo\\nYes\\nCompetitive chatbot performance\\nGPT-J\\n6T\\nNo\\nYes\\nMassive model for high-quality text\\nKey Features of Top LLM Frameworks\\nWhen choosing an open-source LLM framework, several key features set the top models apart. These features include:\\nModel Size and Parameters\\nFeature\\nDescription\\nModel Size\\nThe number of parameters in an LLM framework affects its performance and capabilities. Larger models can process and generate more complex text, but require more resources and memory.\\nMultilingual Support\\nFeature\\nDescription\\nMultilingual Support\\nMany LLM frameworks support multiple languages, making them suitable for a broader range of use cases, such as language translation, sentiment analysis, or text summarization.\\nCustomizability\\nFeature\\nDescription\\nCustomizability\\nTop LLM frameworks allow developers to fine-tune models for specific tasks or domains, improving performance and accuracy.\\nCommunity Involvement\\nFeature\\nDescription\\nCommunity Involvement\\nAn active community contributes to the framework\\'s development, provides support, and shares knowledge, making it easier for new users to adopt and integrate the framework.\\nPerformance Efficiency\\nFeature\\nDescription\\nPerformance Efficiency\\nTop frameworks optimize their models for performance, ensuring they can handle large datasets and generate text quickly.\\nWhen evaluating open-source LLM frameworks, consider these key features to choose the best model for your specific use case and requirements.\\n1. LLaMA 2\\nLLaMA 2 is a powerful open-source\\nlarge language model\\n(LLM) developed by Meta. It has gained significant attention in the AI research community for its speed and capabilities.\\nModel Size and Parameters\\nLLaMA 2 offers three size versions with 7 billion, 13 billion, and 70 billion parameters. The larger models support direct dialogue applications and demonstrate superior capabilities in various dimensions.\\nModel Size\\nParameters\\nSmall\\n7 billion\\nMedium\\n13 billion\\nLarge\\n70 billion\\nMultilingual Support\\nLLaMA 2 supports multiple languages, making it suitable for various use cases such as language translation, sentiment analysis, or text summarization.\\nCustomizability\\nLLaMA 2 allows developers to fine-tune models for specific tasks or domains, improving performance and accuracy.\\nCommunity and Support\\nLLaMA 2 has an active community that contributes to its ongoing development and optimization. The model is available on\\nGitHub\\nas free, open-source Python code, and Meta provides official support and resources for developers.\\nPerformance and Efficiency\\nLLaMA 2 optimizes its models for performance, ensuring they can handle large datasets and generate text quickly.\\nOverall, LLaMA 2 is a powerful and flexible open-source LLM framework that offers a range of benefits for developers and researchers. Its large model size, multilingual support, customizability, and active community make it an attractive choice for various NLP applications.\\n2. GPT-NeoX-20B\\nGPT-NeoX-20B is a powerful\\nopen-source large language model\\n(LLM) developed by EleutherAI. It is a 20 billion parameter autoregressive language model trained on the Pile, with freely and openly available weights through a permissive license.\\nModel Size and Parameters\\nGPT-NeoX-20B has a large model size, with 20 billion parameters. This enables it to perform well on various language-understanding, mathematics, and knowledge-based tasks.\\nModel Size\\nParameters\\nGPT-NeoX-20B\\n20 billion\\nPerformance and Efficiency\\nGPT-NeoX-20B has been evaluated on various natural language tasks, including zero-shot performance. The results show that it is a strong few-shot reasoner and gains significant performance when evaluated five-shot.\\nCommunity and Support\\nGPT-NeoX-20B is open-sourced, with its training and evaluation code, as well as the model weights, available on GitHub. This allows developers to fine-tune the model for specific tasks or domains, improving performance and accuracy. The active community and open-source nature of the model ensure ongoing development and optimization.\\nOverall, GPT-NeoX-20B is a powerful and flexible open-source LLM framework that offers a range of benefits for developers and researchers. Its large model size, performance, and open-source nature make it an attractive choice for various NLP applications.\\n3. BLOOM\\nBLOOM is a large open-source language model developed by BigScience, a global collaboration of over 1,000 AI researchers. It has a decoder-only architecture derived from\\nMegatron-LM GPT2\\n.\\nModel Size and Parameters\\nBLOOM has a total of\\n176 billion parameters\\n,\\n70 layers\\n, and\\n112 attention heads\\n.\\nMultilingual Support\\nBLOOM can generate coherent text in\\n46 natural languages\\nand\\n13 programming languages\\n, making it suitable for applications that require multilingual support.\\nCommunity and Support\\nBLOOM is open-sourced, with its training and evaluation code, as well as the model weights, available on GitHub. This allows developers to fine-tune the model for specific tasks or domains, improving performance and accuracy.\\nPerformance and Efficiency\\nBLOOM has been evaluated on various natural language tasks, including zero-shot performance. The results show that it is a strong few-shot reasoner and gains significant performance when evaluated five-shot.\\nOverall, BLOOM is a powerful and flexible open-source LLM framework that offers a range of benefits for developers and researchers. Its large model size, multilingual support, and open-source nature make it an attractive choice for various NLP applications.\\n4. OPT-175B\\nOPT-175B is a large language model with\\n175 billion parameters\\n, making it a powerful tool for various natural language processing tasks.\\nModel Size and Parameters\\nModel Size\\nParameters\\nOPT-175B\\n175 billion\\nCommunity and Support\\nOPT-175B is open-sourced, with its code and trained model weights available on GitHub. This allows developers to fine-tune the model for specific tasks or domains, improving performance and accuracy. The model is released under a non-commercial license and is intended for use by researchers \"affiliated with organizations in government, civil society, and academia\" as well as industry researchers.\\nPerformance and Efficiency\\nOPT-175B\\'s performance is comparable to GPT-3, while requiring only 1/7th of GPT-3\\'s training carbon footprint. The model has been evaluated on various natural language tasks, including:\\nQuestion answering\\nWriting articles\\nSolving math problems\\nIn some tasks, such as the WIC task, OPT-175B outperformed GPT models. However, it underperformed in tasks like ARC Challenge and MultiRC. Overall, OPT-175B is a strong open-source LLM framework that offers a range of benefits for developers and researchers.\\n5. CodeGen\\nCodeGen is an open-source large language model developed by Salesforce AI Research. It has gained attention in the developer community for its potential to streamline software development processes and boost productivity.\\nModel Size and Parameters\\nCodeGen\\'s model size and parameters are not explicitly stated. However, its capabilities and performance are noteworthy.\\nCommunity and Support\\nCodeGen is open-sourced, with its code and trained model weights available on GitHub. This allows developers to fine-tune the model for specific tasks or domains, improving performance and accuracy. The open-source nature enables a community-driven approach to development and support.\\nPerformance and Efficiency\\nCodeGen\\'s performance is promising, with the potential to save developers time and focus on more complex tasks. While specific performance metrics are not available, CodeGen\\'s capabilities and community support make it a strong open-source LLM framework.\\n6. BERT\\nBERT (Bidirectional Encoder Representations from Transformers) is a powerful open-source language model developed by Google in 2018. It has significantly impacted the field of natural language processing (NLP) with its ability to understand context and generate human-like language.\\nModel Size and Parameters\\nBERT\\'s model size is flexible and can be fine-tuned for specific tasks and domains. The original BERT model was trained on a large plain text corpus, using a bidirectional method to analyze language and understand context.\\nModel Size\\nDescription\\nFlexible\\nCan be fine-tuned for specific tasks and domains\\nCommunity and Support\\nBERT is open-sourced, with its code and trained model weights available on GitHub. This has led to a large community of developers and researchers contributing to its development and fine-tuning.\\nPerformance and Efficiency\\nBERT\\'s performance is notable, with its ability to generate language based on context. It has been shown to be accurate in detecting sentiment and classifying language based on the sentiment expressed.\\nOverall, BERT is a powerful and widely adopted open-source LLM framework that has significantly impacted the field of NLP. Its ability to understand context and generate human-like language makes it a valuable tool for a wide range of applications.\\nsbb-itb-f3e41df\\n7. T5\\nT5 is a text-to-text transformer model developed by Google AI. It uses a unified framework to tackle various natural language processing (NLP) tasks.\\nModel Size and Parameters\\nT5\\'s model size is flexible, and it can be fine-tuned for specific tasks and domains. Its architecture is based on the transformer architecture, blending BERT\\'s and GPT\\'s pre-training approaches.\\nModel Size\\nDescription\\nFlexible\\nCan be fine-tuned for specific tasks and domains\\nCommunity and Support\\nT5 is an open-source model, with its code and trained model weights available on GitHub. This has led to a large community of developers and researchers contributing to its development and fine-tuning.\\nPerformance and Efficiency\\nT5\\'s performance is notable, with its ability to generate language based on context. It has been shown to be accurate in various NLP tasks, including:\\nMachine translation\\nAutomated summarization\\nCode-related tasks\\nT5\\'s efficiency is also impressive, with its ability to optimize computation resources.\\nOverall, T5 is a powerful and versatile open-source LLM framework that has significantly impacted the field of NLP. Its ability to understand context and generate human-like language makes it a valuable tool for a wide range of applications.\\n8. Falcon-40B\\nFalcon-40B is a large open-source language model developed by the Technology Innovation Institute (TII) in Abu Dhabi. With 40 billion parameters, it is one of the largest language models ever created, making it a powerful tool for various natural language processing (NLP) tasks.\\nModel Size and Parameters\\nFalcon-40B has three variants: 1B, 7B, and 40B parameters. Its extensive training on a large dataset of text and code enables it to possess a wide range of knowledge and capabilities.\\nModel Size\\nParameters\\nSmall\\n1 billion\\nMedium\\n7 billion\\nLarge\\n40 billion\\nMultilingual Support\\nFalcon-40B supports multiple languages, including English, German, Spanish, French, Italian, Portuguese, Polish, Dutch, Romanian, Czech, and Swedish. This makes it a versatile foundation model that can be used for applications such as translation, question answering, and summarizing information.\\nCommunity and Support\\nFalcon-40B is an open-source model, which means it is freely available to anyone who wants to use it. Its open-source nature has led to a growing community of users who contribute to its development and fine-tuning.\\nPerformance and Efficiency\\nFalcon-40B has showcased its exceptional performance on various benchmarks, including the Hugging Face OpenLLM Leaderboard. Its performance is notable, with its ability to generate human-like language and understand context. It has been shown to be accurate in various NLP tasks, including machine translation, automated summarization, and text generation. Additionally, Falcon-40B\\'s architecture is optimized for efficient inference, resulting in higher inference speed and scalability.\\n9. Vicuna 33B\\nVicuna 33B is an open-source chatbot that has demonstrated competitive performance compared to other open-source models like Stanford Alpaca. It is an enhanced version of the Vicuna-13B model, with a larger parameter size of 33 billion.\\nModel Size and Parameters\\nModel Size\\nParameters\\nVicuna 33B\\n33 billion\\nCommunity and Support\\nVicuna 33B is an open-source model, which means it is freely available to anyone who wants to use it. Its open-source nature has led to a growing community of users who contribute to its development and fine-tuning.\\nPerformance and Efficiency\\nVicuna 33B has showcased its exceptional performance on various benchmarks. Its architecture is optimized for efficient inference, resulting in higher inference speed and scalability. The model\\'s performance has been evaluated by creating a set of 80 diverse questions and utilizing GPT-4 to judge the model outputs. The results have shown that Vicuna 33B provides high-quality responses, making it a powerful tool for various natural language processing (NLP) tasks.\\n10.\\nEleutherAI\\n\\'s GPT-J\\nEleutherAI\\'s GPT-J is a massive language model with 6 trillion parameters, making it one of the largest publicly available language models. This size enables it to generate high-quality text that is often indistinguishable from human writing.\\nModel Size and Parameters\\nModel Size\\nParameters\\nGPT-J\\n6 trillion\\nCommunity and Support\\nGPT-J is an open-source model, which means it is freely available to anyone who wants to use it. The model\\'s GitHub repository provides access to its code, pre-trained weight files, and a demo website. This open-source nature has led to a growing community of users who contribute to its development and fine-tuning.\\nPerformance and Efficiency\\nGPT-J has demonstrated impressive performance on various benchmarks, showcasing its ability to generate coherent and contextually appropriate text. Its architecture is optimized for efficient inference, resulting in higher inference speed and scalability. The model\\'s performance has been evaluated on various down-streaming tasks, and it has achieved state-of-the-art results in many cases.\\nWhile GPT-J\\'s massive size and parameter count require significant computational resources, making it less accessible to individual developers, its potential for advanced applications in language generation and understanding cannot be underestimated.\\nBenefits of Open-Source LLMs\\nOpen-source LLM frameworks offer several advantages that make them an attractive choice for developers, researchers, and organizations.\\nCost Savings\\nOne of the primary benefits is the elimination of licensing fees, reducing the financial burden associated with proprietary models. This cost savings enables organizations to allocate resources more efficiently, promoting innovation and growth.\\nFlexibility and Customization\\nOpen-source LLMs provide the freedom to customize and tailor models to specific needs, allowing for greater control over the development process. This flexibility is particularly valuable for organizations with unique requirements or those operating in niche domains.\\nCommunity Support and Collaboration\\nThe collaborative nature of open-source projects fosters a community-driven approach, where developers can share knowledge, expertise, and resources. This collective effort leads to faster development, improved model performance, and reduced errors.\\nTransparency and Accountability\\nOpen-source LLMs offer transparency and accountability, as developers can inspect, audit, and validate the models, ensuring that they are fair, unbiased, and secure. This transparency is essential for building trust in AI systems, particularly in high-stakes applications.\\nInnovation and Advancement\\nBy providing access to the source code and model architecture, open-source LLMs enable developers to experiment, modify, and improve upon existing models. This leads to the creation of new models, techniques, and applications, driving the advancement of AI technology.\\nIn summary, open-source LLMs offer a range of benefits that make them an attractive choice for developers, researchers, and organizations. By leveraging these frameworks, developers can reduce costs, increase flexibility, and promote innovation and collaboration.\\nChallenges and Considerations\\nWhen working with open-source LLM frameworks, developers and organizations may encounter several challenges and considerations.\\nComputational Resource Requirements\\nResource Intensive\\n: Training and deploying LLMs require significant computational power, memory, and storage. This can be costly and resource-intensive.\\nPotential Biases in Models\\nFairness and Transparency\\n: LLMs can perpetuate biases present in the training data, leading to unfair or discriminatory outcomes. Developers must ensure their models are trained on diverse, representative data and implement measures to detect and mitigate biases.\\nSelecting the Right Framework\\nChoosing the Best Fit\\n: With numerous open-source LLM frameworks available, selecting the right framework for a specific use case can be challenging. Developers must evaluate the strengths and weaknesses of each framework, considering factors such as model performance, customization requirements, and community support.\\nSecurity and Privacy Concerns\\nProtecting Sensitive Data\\n: LLMs can pose security and privacy risks if not implemented correctly. Developers must ensure their models are secure, and sensitive data is protected from unauthorized access or breaches.\\nCustomization and Integration\\nSeamless Integration\\n: Customizing and integrating LLMs with existing systems and infrastructure can be a complex task. Developers must consider the compatibility of their chosen framework with their existing technology stack and ensure seamless integration to achieve desired outcomes.\\nBy understanding these challenges and considerations, developers and organizations can better navigate the complexities of working with open-source LLM frameworks and unlock the full potential of these powerful technologies.\\nConclusion\\nThe top 10 open-source LLM frameworks discussed in this article have the potential to transform the field of natural language processing. By using these frameworks, developers and organizations can create advanced NLP applications that were previously inaccessible due to proprietary constraints.\\nKey Takeaways\\nOpen-source LLM frameworks foster collaboration, innovation, and transparency.\\nThey drive AI innovation forward by empowering developers with the tools they need to create cutting-edge NLP applications.\\nAs we move forward in this exciting era of AI development, it\\'s essential to recognize the importance of open-source LLM frameworks in unlocking new possibilities and pushing the boundaries of what\\'s possible with language models.\\nRelated posts\\nUnsupervised Pre-training vs. Supervised Fine-tuning for LLMs\\n5 Types of Autoencoding Language Models\\n10 Hyperparameter Tuning Tips for LLM Fine-Tuning\\nGerman BERT: Top Language Model for German NLP\\nRead more\\nThe Future of Search: How AI Search Engines are Changing the Internet\\nThe Future of Voice to Text: How AI is Revolutionizing Communication\\nExploring ChatGPT Alternatives for Enhanced AI Writing and Chatbots', 'tool_call_id': 'call_NUXKesTLiBqnO48zOZYVgfF7', 'name': 'extract_content'}]\nAs a software engineer (SWE) looking for libraries to aid in the development of large language models (LLMs), there are several noteworthy tools and libraries you can explore. Here\u2019s a curated overview of some popular options:\n\n### 1. **Awesome LLMOps**\n- **GitHub Repository:** [Awesome LLMOps](https://github.com/tensorchord/awesome-llmops)\n- This repository features a curated list of tools specifically for LLM operations (LLMOps), including:\n  - **Training &amp; Fine-Tuning Frameworks:** Tools for efficient model training.\n  - **Data Management:** Libraries focused on data curation and preprocessing.\n  - **Model Serving:** Options for deploying models into production.\n  - **Observability Tools:** Solutions for monitoring and debugging LLM performance.\n\n### 2. **LLM Agents DevTools**\n- **GitHub Repository:** [LLM Agents DevTools](https://github.com/M1n9X/llm_agents_devtools)\n- This collection includes a range of developer tools and autonomous agents that leverage LLMs. It covers:\n  - **Coding Assistants:** AI tools that support software development.\n  - **Agent Frameworks:** Platforms for building and managing autonomous agents.\n\n### 3. **Awesome Production LLM**\n- **GitHub Repository:** [Awesome Production LLM](https://github.com/jihoo-kim/awesome-production-llm)\n- A resource for developers looking for libraries that facilitate the deployment of LLMs in production. It includes:\n  - **Preprocessing Tools:** Libraries for preparing data for LLMs.\n  - **Evaluation Protocols:** Tools to benchmark model performance.\n  - **Serving &amp; Inference Frameworks:** Libraries to enable efficient model inference and serving.\n\n### 4. **Top Open-Source LLM Frameworks for 2024**\n- **Source:** [LLM Frameworks Article](https://llmmodels.org/blog/top-10-open-source-llm-frameworks-2024/)\n- This article summarizes the top open-source LLM frameworks, highlighting key features such as model size, customizability, and community support. Some notable frameworks include:\n  - **LLaMA 2**\n  - **GPT-NeoX**\n  - **BLOOM**\n  - **OPT-175B**\n  - **CodeGen**\n  - **T5**\n- Each framework has its strengths, making it important to choose one that fits your specific needs.\n\n### Considerations for Choosing a Library\nWhen selecting a library or framework for LLM development, consider factors such as:\n- **Community Support:** A strong community can provide help and shared resources.\n- **Documentation:** Clear documentation can ease the learning curve.\n- **Customizability:** The ability to adapt a library to fit specific project requirements is crucial.\n- **Performance Metrics:** Frameworks with benchmarks can help assess their suitability for your use cases.\n\nThese resources should provide a solid foundation for your LLM development projects, enabling you to leverage state-of-the-art capabilities in natural language processing and generation.\n</pre> <p>Search Queries:</p> <ul> <li>LLM development tool libraries</li> <li>best libraries for LLM development</li> <li>software engineering tools for LLM</li> <li>open source LLM libraries for developers</li> <li>programming libraries for large language models</li> </ul> <p>By prompting the LLM to generate multiple queries, the LLM has access to a wide range of relevant information, including both open-source and commercial products, which it would have a significantly lower chance of doing with a single query.</p> In\u00a0[7]: Copied! <pre>await WebAssistantBaseWithStream().run()\n</pre> await WebAssistantBaseWithStream().run() <pre>(Assistant): []\nusing _web_search tool with args: {'queries': ['Mirascope library', 'Mirascope Python library', 'Mirascope programming library', 'what is Mirascope library']}\n[{'role': 'user', 'content': 'What is Mirascope library?'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"Mirascope library\",\"Mirascope Python library\",\"Mirascope programming library\",\"what is Mirascope library\"]}', 'name': '_web_search'}, 'id': 'call_2olv5hg3ZzMFXMwwfIKvrFno'}]}, {'role': 'tool', 'content': 'https://github.com/mirascope/mirascope\\n\\nhttps://www.mirascope.io/\\n\\nhttps://pypi.org/project/mirascope/\\n\\nhttps://github.com/mirascope/mirascope\\n\\nhttps://docs.mirascope.io/latest/learn/\\n\\nhttps://github.com/mirascope/mirascope\\n\\nhttps://docs.mirascope.io/latest/WHY/\\n\\nhttps://github.com/mirascope/mirascope', 'tool_call_id': 'call_2olv5hg3ZzMFXMwwfIKvrFno', 'name': '_web_search'}]\nusing extract_content tool with args: {'url': 'https://www.mirascope.io/'}\nusing extract_content tool with args: {'url': 'https://docs.mirascope.io/latest/learn/'}\n[{'role': 'user', 'content': 'What is Mirascope library?'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"Mirascope library\",\"Mirascope Python library\",\"Mirascope programming library\",\"what is Mirascope library\"]}', 'name': '_web_search'}, 'id': 'call_2olv5hg3ZzMFXMwwfIKvrFno'}]}, {'role': 'tool', 'content': 'https://github.com/mirascope/mirascope\\n\\nhttps://www.mirascope.io/\\n\\nhttps://pypi.org/project/mirascope/\\n\\nhttps://github.com/mirascope/mirascope\\n\\nhttps://docs.mirascope.io/latest/learn/\\n\\nhttps://github.com/mirascope/mirascope\\n\\nhttps://docs.mirascope.io/latest/WHY/\\n\\nhttps://github.com/mirascope/mirascope', 'tool_call_id': 'call_2olv5hg3ZzMFXMwwfIKvrFno', 'name': '_web_search'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"url\": \"https://www.mirascope.io/\"}', 'name': 'extract_content'}, 'id': 'call_SugLNjl2qFQu6nDhdZ1eNzka'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://docs.mirascope.io/latest/learn/\"}', 'name': 'extract_content'}, 'id': 'call_hDmkCB8DRPyvsZI1X82Wsyhh'}]}, {'role': 'tool', 'content': 'Join our beta list!\\nGet updates and early access to try out new features as a beta tester.\\nThank you! Your submission has been received!\\nOops! Something went wrong while submitting the form.\\nCore Components\\nCalls\\nStreams\\nTools\\nStreaming Tools\\nStructured Outputs\\nStructured Streaming\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\n@openai.call(\\n\"gpt-4o-mini\"\\n)\\n4\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n5\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n6\\n7\\nresponse = recommend_book(\\n\"fantasy\"\\n)\\n8\\nprint\\n(response)\\n9\\n# &gt; Sure! I would recommend The Name of the Wind by...\\n1\\n@openai.call(\\n\"gpt-4o-mini\"\\n, stream=\\nTrue\\n)\\n2\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n3\\n\"\"\"Recommend a {genre} book.\"\"\"\\n4\\n5\\nstream = recommend_book(\\n\"fantasy\"\\n)\\n6\\nfor\\nchunk, _\\nin\\nstream:\\n7\\nprint\\n(chunk, end=\\n\"\"\\n, flush=\\nTrue\\n)\\n8\\n# &gt; Sure! I would recommend...\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\ndef\\nformat_book\\n(\\ntitle:\\nstr\\n, author:\\nstr\\n):\\n4\\nreturn\\nf\"\\n{title}\\nby\\n{author}\\n\"\\n5\\n6\\n@openai.call(\\n7\\n\"gpt-4o-mini\"\\n,\\n8\\ntools=[format_book],\\n9\\ntool_choice=\\n\"required\"\\n,\\n10\\n)\\n11\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n12\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n13\\n14\\nresponse = recommend_book(\\n\"fantasy\"\\n)\\n15\\nif\\ntool := response.tool\\n16\\nprint\\n(tool.call())\\n17\\n# &gt; The Name of the Wind by Patrick Rothfuss\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\n@openai.call(\\n4\\n\"gpt-4o-mini\"\\n,\\n5\\nstream=\\nTrue\\n,\\n6\\ntools=[format_book],\\n7\\ntool_choice=\\n\"required\"\\n8\\n)\\n9\\n@prompt_template(\\n\"Recommend two (2) {genre} books\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n11\\n12\\nstream = recommend_book(\\n\"fantasy\"\\n)\\n13\\nfor\\nchunk, tool\\nin\\nstream:\\n14\\nif\\ntool:\\n15\\nprint\\n(tool.call())\\n16\\nelse\\n:\\n17\\nprint\\n(chunk, end=\\n\"\"\\n, flush=\\nTrue\\n)\\n18\\n# &gt; The Name of the Wind by Patrick Rothfuss\\n19\\n# &gt; Mistborn: The Final Empire by Brandon Sanderson\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\nfrom\\npydantic\\nimport\\nBaseModel\\n3\\n4\\nclass\\nBook\\n(\\nBaseModel\\n):\\n5\\ntitle:\\nstr\\n6\\nauthor:\\nstr\\n7\\n8\\n@openai.call(\\n\"gpt-4o-mini\"\\n, response_model=Book\\n)\\n9\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n11\\n12\\nbook = recommend_book(\\n\"fantasy\"\\n)\\n13\\nassert\\nisinstance\\n(book, Book)\\n14\\nprint\\n(book)\\n15\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\nfrom\\npydantic\\nimport\\nBaseModel\\n3\\n4\\nclass\\nBook\\n(\\nBaseModel\\n):\\n5\\ntitle:\\nstr\\n6\\nauthor:\\nstr\\n7\\n8\\n@openai.call(\\n\"gpt-4o-mini\"\\n, stream=\\nTrue\\n, response_model=Book\\n)\\n9\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n11\\n12\\nbook_stream = recommend_book(\\n\"fantasy\"\\n)\\n13\\nfor\\npartial_book\\nin\\nbook_stream:\\n14\\nprint\\n(partial_book)\\n15\\n# &gt; title=None author=None\\n16\\n# &gt; title=\\'The Name\\' author=None\\n17\\n# &gt; title=\\'The Name of the Wind\\' author=None\\n18\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick\\'\\n19\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\n...and more!\\nExamples\\nYou\\'re in control\\nSimple and transparent abstractions ensure you\\'re in the driver\\'s seat. Abstractions must be useful without getting in your way and blinding your view.\\nType hints you\\'ll love\\nWe\u2019ve taken care of all of the annoying Python typing so that you can have proper type hints in as simple an interface as possible.\\nBuild\\nyour\\nway\\nSimple, clean building blocks enable fast and reliable development without forcing you to do things our way. While we have recommendations and best practices, you\\'re still always able to do things\\nyour\\nway to best suit\\nyour\\nneeds.\\nTrusted by engineers building the next generation of AI-native applications\\nThe Pydantic inspired LLM framework the space has been missing. Simple, modular, extensible...helps where you need it, stays out of your way when you don\\'t.\\nVince Trost\\nCo-Founder / Plastic Labs\\nMirascope\\'s simplicity made it the natural next step from a provider\\'s API (OpenAI) \u2014 all without fighting unnecessary complexity of other tools like LangChain. We have all the bells and whistles necessary for production while maintaining ease of use and easy onboarding of new team members.\\nJake Duth\\nCo-Founder &amp;\\xa0CTO / Reddy\\nAs the author of Mirascope, I\\'m certainly biased, but there\\'s a reason I\\'ve worked day and night on this package \u2014 it solves the problems I have with existing developer tools. Mirascope is simple and easy to use, and honestly building with Mirascope is the most fun I\\'ve had coding in my life.\\nWilliam Bakst\\nFounder / Mirascope\\nStart Building With Mirascope\\nMirascope can help improve your application development experience no matter how simple or complex.\\nIt will feel like writing the Python you already know.\\nRead the Docs\\nGitHub | \u2b50 ${starCount}\\nJoin the Community', 'tool_call_id': 'call_SugLNjl2qFQu6nDhdZ1eNzka', 'name': 'extract_content'}, {'role': 'tool', 'content': \"Learn Mirascope\\n\u00b6\\nThis section is designed to help you master Mirascope, a toolkit for building AI-powered applications with Large Language Models (LLMs).\\nMirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs. Whether you're building chatbots, content generators, or complex AI-driven agent systems, Mirascope provides the tools you need to streamline your development process and create powerful, robust applications.\\nOur documentation is tailored for developers who have at least some experience with Python and LLMs. Whether you're coming from other development tool libraries or have worked directly with provider SDKs and APIs, Mirascope offers a familiar but enhanced experience.\\nKey Features and Benefits\\n\u00b6\\nType Safety and Superior Editor Support\\n: We've prioritized proper type hints and type safety, ensuring you get world-class editor support. This means fewer errors, better autocomplete, and a smoother development experience in as simple and seamless an interface as possible.\\nProvider-Agnostic Design\\n: Mirascope works seamlessly with multiple LLM providers, allowing you to switch between them effortlessly or use multiple providers in the same project.\\nSimplicity and Ease of Use\\n: We've designed Mirascope with simplicity in mind. You get powerful features without unnecessary complexity, making it easy to get started and scale your projects.\\nComprehensive Tooling\\n: From prompt engineering to response parsing, Mirascope offers a complete suite of tools for every aspect of LLM application development.\\nCore Components\\n\u00b6\\nMirascope is built around the following core components, each designed to handle specific aspects of LLM interaction and application development. Here's a quick overview with links to detailed documentation:\\nPrompts\\n: Learn how to create and manage prompts effectively.\\nCalls\\n: Understand how to make calls to LLMs using Mirascope.\\nStreams\\n: Explore streaming responses for real-time applications.\\nTools\\n: Discover how to extend LLM capabilities with custom tools.\\nDynamic Configuration\\n: Learn to adjust LLM behavior at runtime.\\nChaining\\n: Understand the art of chaining multiple LLM calls for complex tasks.\\nJSON Mode\\n: Work with structured data responses from LLMs.\\nResponse Models\\n: Define and use structured output models with automatic validation.\\nOutput Parsers\\n: Process and transform custom LLM output structures effectively.\\nAsync\\n: Maximize efficiecy with asynchronous programming.\\nEvals\\n: Apply core components to build evaluation strategies for your LLM applications.\\nAgents\\n: Put everything together to build advanced AI agents using Mirascope.\\nNext Steps\\n\u00b6\\nWe encourage you to dive into each component's documentation to gain a deeper understanding of Mirascope's capabilities. Start with the topics that align most closely with your immediate needs, but don't hesitate to explore all areas \u2013 you might discover new ways to enhance your LLM applications!\\nAs you progress, you'll find advanced topics and best practices throughout the documentation. These will help you optimize your use of Mirascope and build increasingly sophisticated AI-powered applications.\\nHappy learning, and welcome to the world of development with Mirascope!\\nBack to top\", 'tool_call_id': 'call_hDmkCB8DRPyvsZI1X82Wsyhh', 'name': 'extract_content'}]\nThe **Mirascope library** is a powerful toolkit designed for developers working with **Large Language Models (LLMs)**. It streamlines the process of building AI-powered applications, such as chatbots, content generators, and complex AI-driven systems. Here are some key aspects of Mirascope:\n\n### Key Features and Benefits:\n1. **User-Friendly Design**: Mirascope is engineered for simplicity, providing necessary functionalities without unnecessary complexity. It aims to create a seamless development experience that feels familiar to Python developers.\n   \n2. **Type Safety**: The library emphasizes type hints and type safety, improving editor support and reducing potential errors during development.\n\n3. **Provider-Agnostic**: Mirascope can interact with multiple LLM providers, enabling developers to switch between them easily or even use several providers within the same project.\n\n4. **Comprehensive Tooling**: It offers a complete suite of tools to handle all aspects of LLM application development\u2014from prompt engineering to response parsing.\n\n5. **Modular and Extensible**: Mirascope is built with core components that allow developers to manage prompts, make API calls, handle streaming responses, and much more, all while maintaining the ability to customize and extend functionalities.\n\n### Core Components:\n- **Prompts**: Facilitate efficient creation and management.\n- **Calls**: Handle interactions with LLMs.\n- **Streams**: Support real-time applications with streaming responses.\n- **Tools**: Allow for extending LLM capabilities.\n- **Chaining**: Enable coordinating multiple LLM calls for complex tasks.\n- **Output Parsing**: Simplify the processing and transformation of responses.\n\n### Application Scenarios:\nMirascope can be utilized for a range of applications like chatbots, content generation, and AI-driven agents, helping developers integrate sophisticated AI functionalities into their products.\n\n### Conclusion:\nMirascope serves as an essential tool for modern developers looking to harness AI capabilities efficiently and effectively, encouraging advanced application development while ensuring ease of use and flexibility.\n\nFor more detailed information, you can refer to the [official Mirascope documentation](https://docs.mirascope.io/latest/learn/) or visit its [homepage](https://www.mirascope.io/).\n(Assistant): [{'role': 'user', 'content': 'What is Mirascope library?'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"Mirascope library\",\"Mirascope Python library\",\"Mirascope programming library\",\"what is Mirascope library\"]}', 'name': '_web_search'}, 'id': 'call_2olv5hg3ZzMFXMwwfIKvrFno'}]}, {'role': 'tool', 'content': 'https://github.com/mirascope/mirascope\\n\\nhttps://www.mirascope.io/\\n\\nhttps://pypi.org/project/mirascope/\\n\\nhttps://github.com/mirascope/mirascope\\n\\nhttps://docs.mirascope.io/latest/learn/\\n\\nhttps://github.com/mirascope/mirascope\\n\\nhttps://docs.mirascope.io/latest/WHY/\\n\\nhttps://github.com/mirascope/mirascope', 'tool_call_id': 'call_2olv5hg3ZzMFXMwwfIKvrFno', 'name': '_web_search'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"url\": \"https://www.mirascope.io/\"}', 'name': 'extract_content'}, 'id': 'call_SugLNjl2qFQu6nDhdZ1eNzka'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://docs.mirascope.io/latest/learn/\"}', 'name': 'extract_content'}, 'id': 'call_hDmkCB8DRPyvsZI1X82Wsyhh'}]}, {'role': 'tool', 'content': 'Join our beta list!\\nGet updates and early access to try out new features as a beta tester.\\nThank you! Your submission has been received!\\nOops! Something went wrong while submitting the form.\\nCore Components\\nCalls\\nStreams\\nTools\\nStreaming Tools\\nStructured Outputs\\nStructured Streaming\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\n@openai.call(\\n\"gpt-4o-mini\"\\n)\\n4\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n5\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n6\\n7\\nresponse = recommend_book(\\n\"fantasy\"\\n)\\n8\\nprint\\n(response)\\n9\\n# &gt; Sure! I would recommend The Name of the Wind by...\\n1\\n@openai.call(\\n\"gpt-4o-mini\"\\n, stream=\\nTrue\\n)\\n2\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n3\\n\"\"\"Recommend a {genre} book.\"\"\"\\n4\\n5\\nstream = recommend_book(\\n\"fantasy\"\\n)\\n6\\nfor\\nchunk, _\\nin\\nstream:\\n7\\nprint\\n(chunk, end=\\n\"\"\\n, flush=\\nTrue\\n)\\n8\\n# &gt; Sure! I would recommend...\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\ndef\\nformat_book\\n(\\ntitle:\\nstr\\n, author:\\nstr\\n):\\n4\\nreturn\\nf\"\\n{title}\\nby\\n{author}\\n\"\\n5\\n6\\n@openai.call(\\n7\\n\"gpt-4o-mini\"\\n,\\n8\\ntools=[format_book],\\n9\\ntool_choice=\\n\"required\"\\n,\\n10\\n)\\n11\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n12\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n13\\n14\\nresponse = recommend_book(\\n\"fantasy\"\\n)\\n15\\nif\\ntool := response.tool\\n16\\nprint\\n(tool.call())\\n17\\n# &gt; The Name of the Wind by Patrick Rothfuss\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\n@openai.call(\\n4\\n\"gpt-4o-mini\"\\n,\\n5\\nstream=\\nTrue\\n,\\n6\\ntools=[format_book],\\n7\\ntool_choice=\\n\"required\"\\n8\\n)\\n9\\n@prompt_template(\\n\"Recommend two (2) {genre} books\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n11\\n12\\nstream = recommend_book(\\n\"fantasy\"\\n)\\n13\\nfor\\nchunk, tool\\nin\\nstream:\\n14\\nif\\ntool:\\n15\\nprint\\n(tool.call())\\n16\\nelse\\n:\\n17\\nprint\\n(chunk, end=\\n\"\"\\n, flush=\\nTrue\\n)\\n18\\n# &gt; The Name of the Wind by Patrick Rothfuss\\n19\\n# &gt; Mistborn: The Final Empire by Brandon Sanderson\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\nfrom\\npydantic\\nimport\\nBaseModel\\n3\\n4\\nclass\\nBook\\n(\\nBaseModel\\n):\\n5\\ntitle:\\nstr\\n6\\nauthor:\\nstr\\n7\\n8\\n@openai.call(\\n\"gpt-4o-mini\"\\n, response_model=Book\\n)\\n9\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n11\\n12\\nbook = recommend_book(\\n\"fantasy\"\\n)\\n13\\nassert\\nisinstance\\n(book, Book)\\n14\\nprint\\n(book)\\n15\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\nfrom\\npydantic\\nimport\\nBaseModel\\n3\\n4\\nclass\\nBook\\n(\\nBaseModel\\n):\\n5\\ntitle:\\nstr\\n6\\nauthor:\\nstr\\n7\\n8\\n@openai.call(\\n\"gpt-4o-mini\"\\n, stream=\\nTrue\\n, response_model=Book\\n)\\n9\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n11\\n12\\nbook_stream = recommend_book(\\n\"fantasy\"\\n)\\n13\\nfor\\npartial_book\\nin\\nbook_stream:\\n14\\nprint\\n(partial_book)\\n15\\n# &gt; title=None author=None\\n16\\n# &gt; title=\\'The Name\\' author=None\\n17\\n# &gt; title=\\'The Name of the Wind\\' author=None\\n18\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick\\'\\n19\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\n...and more!\\nExamples\\nYou\\'re in control\\nSimple and transparent abstractions ensure you\\'re in the driver\\'s seat. Abstractions must be useful without getting in your way and blinding your view.\\nType hints you\\'ll love\\nWe\u2019ve taken care of all of the annoying Python typing so that you can have proper type hints in as simple an interface as possible.\\nBuild\\nyour\\nway\\nSimple, clean building blocks enable fast and reliable development without forcing you to do things our way. While we have recommendations and best practices, you\\'re still always able to do things\\nyour\\nway to best suit\\nyour\\nneeds.\\nTrusted by engineers building the next generation of AI-native applications\\nThe Pydantic inspired LLM framework the space has been missing. Simple, modular, extensible...helps where you need it, stays out of your way when you don\\'t.\\nVince Trost\\nCo-Founder / Plastic Labs\\nMirascope\\'s simplicity made it the natural next step from a provider\\'s API (OpenAI) \u2014 all without fighting unnecessary complexity of other tools like LangChain. We have all the bells and whistles necessary for production while maintaining ease of use and easy onboarding of new team members.\\nJake Duth\\nCo-Founder &amp;\\xa0CTO / Reddy\\nAs the author of Mirascope, I\\'m certainly biased, but there\\'s a reason I\\'ve worked day and night on this package \u2014 it solves the problems I have with existing developer tools. Mirascope is simple and easy to use, and honestly building with Mirascope is the most fun I\\'ve had coding in my life.\\nWilliam Bakst\\nFounder / Mirascope\\nStart Building With Mirascope\\nMirascope can help improve your application development experience no matter how simple or complex.\\nIt will feel like writing the Python you already know.\\nRead the Docs\\nGitHub | \u2b50 ${starCount}\\nJoin the Community', 'tool_call_id': 'call_SugLNjl2qFQu6nDhdZ1eNzka', 'name': 'extract_content'}, {'role': 'tool', 'content': \"Learn Mirascope\\n\u00b6\\nThis section is designed to help you master Mirascope, a toolkit for building AI-powered applications with Large Language Models (LLMs).\\nMirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs. Whether you're building chatbots, content generators, or complex AI-driven agent systems, Mirascope provides the tools you need to streamline your development process and create powerful, robust applications.\\nOur documentation is tailored for developers who have at least some experience with Python and LLMs. Whether you're coming from other development tool libraries or have worked directly with provider SDKs and APIs, Mirascope offers a familiar but enhanced experience.\\nKey Features and Benefits\\n\u00b6\\nType Safety and Superior Editor Support\\n: We've prioritized proper type hints and type safety, ensuring you get world-class editor support. This means fewer errors, better autocomplete, and a smoother development experience in as simple and seamless an interface as possible.\\nProvider-Agnostic Design\\n: Mirascope works seamlessly with multiple LLM providers, allowing you to switch between them effortlessly or use multiple providers in the same project.\\nSimplicity and Ease of Use\\n: We've designed Mirascope with simplicity in mind. You get powerful features without unnecessary complexity, making it easy to get started and scale your projects.\\nComprehensive Tooling\\n: From prompt engineering to response parsing, Mirascope offers a complete suite of tools for every aspect of LLM application development.\\nCore Components\\n\u00b6\\nMirascope is built around the following core components, each designed to handle specific aspects of LLM interaction and application development. Here's a quick overview with links to detailed documentation:\\nPrompts\\n: Learn how to create and manage prompts effectively.\\nCalls\\n: Understand how to make calls to LLMs using Mirascope.\\nStreams\\n: Explore streaming responses for real-time applications.\\nTools\\n: Discover how to extend LLM capabilities with custom tools.\\nDynamic Configuration\\n: Learn to adjust LLM behavior at runtime.\\nChaining\\n: Understand the art of chaining multiple LLM calls for complex tasks.\\nJSON Mode\\n: Work with structured data responses from LLMs.\\nResponse Models\\n: Define and use structured output models with automatic validation.\\nOutput Parsers\\n: Process and transform custom LLM output structures effectively.\\nAsync\\n: Maximize efficiecy with asynchronous programming.\\nEvals\\n: Apply core components to build evaluation strategies for your LLM applications.\\nAgents\\n: Put everything together to build advanced AI agents using Mirascope.\\nNext Steps\\n\u00b6\\nWe encourage you to dive into each component's documentation to gain a deeper understanding of Mirascope's capabilities. Start with the topics that align most closely with your immediate needs, but don't hesitate to explore all areas \u2013 you might discover new ways to enhance your LLM applications!\\nAs you progress, you'll find advanced topics and best practices throughout the documentation. These will help you optimize your use of Mirascope and build increasingly sophisticated AI-powered applications.\\nHappy learning, and welcome to the world of development with Mirascope!\\nBack to top\", 'tool_call_id': 'call_hDmkCB8DRPyvsZI1X82Wsyhh', 'name': 'extract_content'}, {'role': 'assistant', 'content': 'The **Mirascope library** is a powerful toolkit designed for developers working with **Large Language Models (LLMs)**. It streamlines the process of building AI-powered applications, such as chatbots, content generators, and complex AI-driven systems. Here are some key aspects of Mirascope:\\n\\n### Key Features and Benefits:\\n1. **User-Friendly Design**: Mirascope is engineered for simplicity, providing necessary functionalities without unnecessary complexity. It aims to create a seamless development experience that feels familiar to Python developers.\\n   \\n2. **Type Safety**: The library emphasizes type hints and type safety, improving editor support and reducing potential errors during development.\\n\\n3. **Provider-Agnostic**: Mirascope can interact with multiple LLM providers, enabling developers to switch between them easily or even use several providers within the same project.\\n\\n4. **Comprehensive Tooling**: It offers a complete suite of tools to handle all aspects of LLM application development\u2014from prompt engineering to response parsing.\\n\\n5. **Modular and Extensible**: Mirascope is built with core components that allow developers to manage prompts, make API calls, handle streaming responses, and much more, all while maintaining the ability to customize and extend functionalities.\\n\\n### Core Components:\\n- **Prompts**: Facilitate efficient creation and management.\\n- **Calls**: Handle interactions with LLMs.\\n- **Streams**: Support real-time applications with streaming responses.\\n- **Tools**: Allow for extending LLM capabilities.\\n- **Chaining**: Enable coordinating multiple LLM calls for complex tasks.\\n- **Output Parsing**: Simplify the processing and transformation of responses.\\n\\n### Application Scenarios:\\nMirascope can be utilized for a range of applications like chatbots, content generation, and AI-driven agents, helping developers integrate sophisticated AI functionalities into their products.\\n\\n### Conclusion:\\nMirascope serves as an essential tool for modern developers looking to harness AI capabilities efficiently and effectively, encouraging advanced application development while ensuring ease of use and flexibility.\\n\\nFor more detailed information, you can refer to the [official Mirascope documentation](https://docs.mirascope.io/latest/learn/) or visit its [homepage](https://www.mirascope.io/).'}]\nusing _web_search tool with args: {'queries': ['exot programming', 'exot library', 'exot software', 'exot application', 'exot API', 'Mirascope exot connection']}\n[{'role': 'user', 'content': 'What is Mirascope library?'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"Mirascope library\",\"Mirascope Python library\",\"Mirascope programming library\",\"what is Mirascope library\"]}', 'name': '_web_search'}, 'id': 'call_2olv5hg3ZzMFXMwwfIKvrFno'}]}, {'role': 'tool', 'content': 'https://github.com/mirascope/mirascope\\n\\nhttps://www.mirascope.io/\\n\\nhttps://pypi.org/project/mirascope/\\n\\nhttps://github.com/mirascope/mirascope\\n\\nhttps://docs.mirascope.io/latest/learn/\\n\\nhttps://github.com/mirascope/mirascope\\n\\nhttps://docs.mirascope.io/latest/WHY/\\n\\nhttps://github.com/mirascope/mirascope', 'tool_call_id': 'call_2olv5hg3ZzMFXMwwfIKvrFno', 'name': '_web_search'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"url\": \"https://www.mirascope.io/\"}', 'name': 'extract_content'}, 'id': 'call_SugLNjl2qFQu6nDhdZ1eNzka'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://docs.mirascope.io/latest/learn/\"}', 'name': 'extract_content'}, 'id': 'call_hDmkCB8DRPyvsZI1X82Wsyhh'}]}, {'role': 'tool', 'content': 'Join our beta list!\\nGet updates and early access to try out new features as a beta tester.\\nThank you! Your submission has been received!\\nOops! Something went wrong while submitting the form.\\nCore Components\\nCalls\\nStreams\\nTools\\nStreaming Tools\\nStructured Outputs\\nStructured Streaming\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\n@openai.call(\\n\"gpt-4o-mini\"\\n)\\n4\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n5\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n6\\n7\\nresponse = recommend_book(\\n\"fantasy\"\\n)\\n8\\nprint\\n(response)\\n9\\n# &gt; Sure! I would recommend The Name of the Wind by...\\n1\\n@openai.call(\\n\"gpt-4o-mini\"\\n, stream=\\nTrue\\n)\\n2\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n3\\n\"\"\"Recommend a {genre} book.\"\"\"\\n4\\n5\\nstream = recommend_book(\\n\"fantasy\"\\n)\\n6\\nfor\\nchunk, _\\nin\\nstream:\\n7\\nprint\\n(chunk, end=\\n\"\"\\n, flush=\\nTrue\\n)\\n8\\n# &gt; Sure! I would recommend...\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\ndef\\nformat_book\\n(\\ntitle:\\nstr\\n, author:\\nstr\\n):\\n4\\nreturn\\nf\"\\n{title}\\nby\\n{author}\\n\"\\n5\\n6\\n@openai.call(\\n7\\n\"gpt-4o-mini\"\\n,\\n8\\ntools=[format_book],\\n9\\ntool_choice=\\n\"required\"\\n,\\n10\\n)\\n11\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n12\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n13\\n14\\nresponse = recommend_book(\\n\"fantasy\"\\n)\\n15\\nif\\ntool := response.tool\\n16\\nprint\\n(tool.call())\\n17\\n# &gt; The Name of the Wind by Patrick Rothfuss\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\n@openai.call(\\n4\\n\"gpt-4o-mini\"\\n,\\n5\\nstream=\\nTrue\\n,\\n6\\ntools=[format_book],\\n7\\ntool_choice=\\n\"required\"\\n8\\n)\\n9\\n@prompt_template(\\n\"Recommend two (2) {genre} books\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n11\\n12\\nstream = recommend_book(\\n\"fantasy\"\\n)\\n13\\nfor\\nchunk, tool\\nin\\nstream:\\n14\\nif\\ntool:\\n15\\nprint\\n(tool.call())\\n16\\nelse\\n:\\n17\\nprint\\n(chunk, end=\\n\"\"\\n, flush=\\nTrue\\n)\\n18\\n# &gt; The Name of the Wind by Patrick Rothfuss\\n19\\n# &gt; Mistborn: The Final Empire by Brandon Sanderson\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\nfrom\\npydantic\\nimport\\nBaseModel\\n3\\n4\\nclass\\nBook\\n(\\nBaseModel\\n):\\n5\\ntitle:\\nstr\\n6\\nauthor:\\nstr\\n7\\n8\\n@openai.call(\\n\"gpt-4o-mini\"\\n, response_model=Book\\n)\\n9\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n11\\n12\\nbook = recommend_book(\\n\"fantasy\"\\n)\\n13\\nassert\\nisinstance\\n(book, Book)\\n14\\nprint\\n(book)\\n15\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\nfrom\\npydantic\\nimport\\nBaseModel\\n3\\n4\\nclass\\nBook\\n(\\nBaseModel\\n):\\n5\\ntitle:\\nstr\\n6\\nauthor:\\nstr\\n7\\n8\\n@openai.call(\\n\"gpt-4o-mini\"\\n, stream=\\nTrue\\n, response_model=Book\\n)\\n9\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n11\\n12\\nbook_stream = recommend_book(\\n\"fantasy\"\\n)\\n13\\nfor\\npartial_book\\nin\\nbook_stream:\\n14\\nprint\\n(partial_book)\\n15\\n# &gt; title=None author=None\\n16\\n# &gt; title=\\'The Name\\' author=None\\n17\\n# &gt; title=\\'The Name of the Wind\\' author=None\\n18\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick\\'\\n19\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\n...and more!\\nExamples\\nYou\\'re in control\\nSimple and transparent abstractions ensure you\\'re in the driver\\'s seat. Abstractions must be useful without getting in your way and blinding your view.\\nType hints you\\'ll love\\nWe\u2019ve taken care of all of the annoying Python typing so that you can have proper type hints in as simple an interface as possible.\\nBuild\\nyour\\nway\\nSimple, clean building blocks enable fast and reliable development without forcing you to do things our way. While we have recommendations and best practices, you\\'re still always able to do things\\nyour\\nway to best suit\\nyour\\nneeds.\\nTrusted by engineers building the next generation of AI-native applications\\nThe Pydantic inspired LLM framework the space has been missing. Simple, modular, extensible...helps where you need it, stays out of your way when you don\\'t.\\nVince Trost\\nCo-Founder / Plastic Labs\\nMirascope\\'s simplicity made it the natural next step from a provider\\'s API (OpenAI) \u2014 all without fighting unnecessary complexity of other tools like LangChain. We have all the bells and whistles necessary for production while maintaining ease of use and easy onboarding of new team members.\\nJake Duth\\nCo-Founder &amp;\\xa0CTO / Reddy\\nAs the author of Mirascope, I\\'m certainly biased, but there\\'s a reason I\\'ve worked day and night on this package \u2014 it solves the problems I have with existing developer tools. Mirascope is simple and easy to use, and honestly building with Mirascope is the most fun I\\'ve had coding in my life.\\nWilliam Bakst\\nFounder / Mirascope\\nStart Building With Mirascope\\nMirascope can help improve your application development experience no matter how simple or complex.\\nIt will feel like writing the Python you already know.\\nRead the Docs\\nGitHub | \u2b50 ${starCount}\\nJoin the Community', 'tool_call_id': 'call_SugLNjl2qFQu6nDhdZ1eNzka', 'name': 'extract_content'}, {'role': 'tool', 'content': \"Learn Mirascope\\n\u00b6\\nThis section is designed to help you master Mirascope, a toolkit for building AI-powered applications with Large Language Models (LLMs).\\nMirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs. Whether you're building chatbots, content generators, or complex AI-driven agent systems, Mirascope provides the tools you need to streamline your development process and create powerful, robust applications.\\nOur documentation is tailored for developers who have at least some experience with Python and LLMs. Whether you're coming from other development tool libraries or have worked directly with provider SDKs and APIs, Mirascope offers a familiar but enhanced experience.\\nKey Features and Benefits\\n\u00b6\\nType Safety and Superior Editor Support\\n: We've prioritized proper type hints and type safety, ensuring you get world-class editor support. This means fewer errors, better autocomplete, and a smoother development experience in as simple and seamless an interface as possible.\\nProvider-Agnostic Design\\n: Mirascope works seamlessly with multiple LLM providers, allowing you to switch between them effortlessly or use multiple providers in the same project.\\nSimplicity and Ease of Use\\n: We've designed Mirascope with simplicity in mind. You get powerful features without unnecessary complexity, making it easy to get started and scale your projects.\\nComprehensive Tooling\\n: From prompt engineering to response parsing, Mirascope offers a complete suite of tools for every aspect of LLM application development.\\nCore Components\\n\u00b6\\nMirascope is built around the following core components, each designed to handle specific aspects of LLM interaction and application development. Here's a quick overview with links to detailed documentation:\\nPrompts\\n: Learn how to create and manage prompts effectively.\\nCalls\\n: Understand how to make calls to LLMs using Mirascope.\\nStreams\\n: Explore streaming responses for real-time applications.\\nTools\\n: Discover how to extend LLM capabilities with custom tools.\\nDynamic Configuration\\n: Learn to adjust LLM behavior at runtime.\\nChaining\\n: Understand the art of chaining multiple LLM calls for complex tasks.\\nJSON Mode\\n: Work with structured data responses from LLMs.\\nResponse Models\\n: Define and use structured output models with automatic validation.\\nOutput Parsers\\n: Process and transform custom LLM output structures effectively.\\nAsync\\n: Maximize efficiecy with asynchronous programming.\\nEvals\\n: Apply core components to build evaluation strategies for your LLM applications.\\nAgents\\n: Put everything together to build advanced AI agents using Mirascope.\\nNext Steps\\n\u00b6\\nWe encourage you to dive into each component's documentation to gain a deeper understanding of Mirascope's capabilities. Start with the topics that align most closely with your immediate needs, but don't hesitate to explore all areas \u2013 you might discover new ways to enhance your LLM applications!\\nAs you progress, you'll find advanced topics and best practices throughout the documentation. These will help you optimize your use of Mirascope and build increasingly sophisticated AI-powered applications.\\nHappy learning, and welcome to the world of development with Mirascope!\\nBack to top\", 'tool_call_id': 'call_hDmkCB8DRPyvsZI1X82Wsyhh', 'name': 'extract_content'}, {'role': 'assistant', 'content': 'The **Mirascope library** is a powerful toolkit designed for developers working with **Large Language Models (LLMs)**. It streamlines the process of building AI-powered applications, such as chatbots, content generators, and complex AI-driven systems. Here are some key aspects of Mirascope:\\n\\n### Key Features and Benefits:\\n1. **User-Friendly Design**: Mirascope is engineered for simplicity, providing necessary functionalities without unnecessary complexity. It aims to create a seamless development experience that feels familiar to Python developers.\\n   \\n2. **Type Safety**: The library emphasizes type hints and type safety, improving editor support and reducing potential errors during development.\\n\\n3. **Provider-Agnostic**: Mirascope can interact with multiple LLM providers, enabling developers to switch between them easily or even use several providers within the same project.\\n\\n4. **Comprehensive Tooling**: It offers a complete suite of tools to handle all aspects of LLM application development\u2014from prompt engineering to response parsing.\\n\\n5. **Modular and Extensible**: Mirascope is built with core components that allow developers to manage prompts, make API calls, handle streaming responses, and much more, all while maintaining the ability to customize and extend functionalities.\\n\\n### Core Components:\\n- **Prompts**: Facilitate efficient creation and management.\\n- **Calls**: Handle interactions with LLMs.\\n- **Streams**: Support real-time applications with streaming responses.\\n- **Tools**: Allow for extending LLM capabilities.\\n- **Chaining**: Enable coordinating multiple LLM calls for complex tasks.\\n- **Output Parsing**: Simplify the processing and transformation of responses.\\n\\n### Application Scenarios:\\nMirascope can be utilized for a range of applications like chatbots, content generation, and AI-driven agents, helping developers integrate sophisticated AI functionalities into their products.\\n\\n### Conclusion:\\nMirascope serves as an essential tool for modern developers looking to harness AI capabilities efficiently and effectively, encouraging advanced application development while ensuring ease of use and flexibility.\\n\\nFor more detailed information, you can refer to the [official Mirascope documentation](https://docs.mirascope.io/latest/learn/) or visit its [homepage](https://www.mirascope.io/).'}, {'role': 'user', 'content': 'exot'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"exot programming\",\"exot library\",\"exot software\",\"exot application\",\"exot API\",\"Mirascope exot connection\"]}', 'name': '_web_search'}, 'id': 'call_xmAYDMXpqwCBNqZnAPq5edtg'}]}, {'role': 'tool', 'content': 'https://stackoverflow.com/questions/73663/how-do-i-terminate-a-script\\n\\nhttps://www.lifewire.com/how-to-force-quit-a-program-in-windows-2625781\\n\\nhttps://www.neb.com/en/products/m0265-exonuclease-t\\n\\nhttps://www.fsis.usda.gov/inspection/import-export/import-export-library\\n\\nhttps://exot.dev/\\n\\nhttps://exoft.net/\\n\\nhttps://learn.microsoft.com/en-us/dotnet/api/system.windows.forms.application.exit?view=windowsdesktop-8.0\\n\\nhttps://stackoverflow.com/questions/12977924/how-do-i-properly-exit-a-c-sharp-application\\n\\nhttps://developer.exotel.com/\\n\\nhttps://developer.exotel.com/api\\n\\nhttps://docs.mirascope.io/latest/\\n\\nhttps://docs.mirascope.io/latest/learn/async/', 'tool_call_id': 'call_xmAYDMXpqwCBNqZnAPq5edtg', 'name': '_web_search'}]\nusing extract_content tool with args: {'url': 'https://exot.dev/'}\nusing extract_content tool with args: {'url': 'https://developer.exotel.com/'}\n[{'role': 'user', 'content': 'What is Mirascope library?'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"Mirascope library\",\"Mirascope Python library\",\"Mirascope programming library\",\"what is Mirascope library\"]}', 'name': '_web_search'}, 'id': 'call_2olv5hg3ZzMFXMwwfIKvrFno'}]}, {'role': 'tool', 'content': 'https://github.com/mirascope/mirascope\\n\\nhttps://www.mirascope.io/\\n\\nhttps://pypi.org/project/mirascope/\\n\\nhttps://github.com/mirascope/mirascope\\n\\nhttps://docs.mirascope.io/latest/learn/\\n\\nhttps://github.com/mirascope/mirascope\\n\\nhttps://docs.mirascope.io/latest/WHY/\\n\\nhttps://github.com/mirascope/mirascope', 'tool_call_id': 'call_2olv5hg3ZzMFXMwwfIKvrFno', 'name': '_web_search'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"url\": \"https://www.mirascope.io/\"}', 'name': 'extract_content'}, 'id': 'call_SugLNjl2qFQu6nDhdZ1eNzka'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://docs.mirascope.io/latest/learn/\"}', 'name': 'extract_content'}, 'id': 'call_hDmkCB8DRPyvsZI1X82Wsyhh'}]}, {'role': 'tool', 'content': 'Join our beta list!\\nGet updates and early access to try out new features as a beta tester.\\nThank you! Your submission has been received!\\nOops! Something went wrong while submitting the form.\\nCore Components\\nCalls\\nStreams\\nTools\\nStreaming Tools\\nStructured Outputs\\nStructured Streaming\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\n@openai.call(\\n\"gpt-4o-mini\"\\n)\\n4\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n5\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n6\\n7\\nresponse = recommend_book(\\n\"fantasy\"\\n)\\n8\\nprint\\n(response)\\n9\\n# &gt; Sure! I would recommend The Name of the Wind by...\\n1\\n@openai.call(\\n\"gpt-4o-mini\"\\n, stream=\\nTrue\\n)\\n2\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n3\\n\"\"\"Recommend a {genre} book.\"\"\"\\n4\\n5\\nstream = recommend_book(\\n\"fantasy\"\\n)\\n6\\nfor\\nchunk, _\\nin\\nstream:\\n7\\nprint\\n(chunk, end=\\n\"\"\\n, flush=\\nTrue\\n)\\n8\\n# &gt; Sure! I would recommend...\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\ndef\\nformat_book\\n(\\ntitle:\\nstr\\n, author:\\nstr\\n):\\n4\\nreturn\\nf\"\\n{title}\\nby\\n{author}\\n\"\\n5\\n6\\n@openai.call(\\n7\\n\"gpt-4o-mini\"\\n,\\n8\\ntools=[format_book],\\n9\\ntool_choice=\\n\"required\"\\n,\\n10\\n)\\n11\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n12\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n13\\n14\\nresponse = recommend_book(\\n\"fantasy\"\\n)\\n15\\nif\\ntool := response.tool\\n16\\nprint\\n(tool.call())\\n17\\n# &gt; The Name of the Wind by Patrick Rothfuss\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\n3\\n@openai.call(\\n4\\n\"gpt-4o-mini\"\\n,\\n5\\nstream=\\nTrue\\n,\\n6\\ntools=[format_book],\\n7\\ntool_choice=\\n\"required\"\\n8\\n)\\n9\\n@prompt_template(\\n\"Recommend two (2) {genre} books\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n11\\n12\\nstream = recommend_book(\\n\"fantasy\"\\n)\\n13\\nfor\\nchunk, tool\\nin\\nstream:\\n14\\nif\\ntool:\\n15\\nprint\\n(tool.call())\\n16\\nelse\\n:\\n17\\nprint\\n(chunk, end=\\n\"\"\\n, flush=\\nTrue\\n)\\n18\\n# &gt; The Name of the Wind by Patrick Rothfuss\\n19\\n# &gt; Mistborn: The Final Empire by Brandon Sanderson\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\nfrom\\npydantic\\nimport\\nBaseModel\\n3\\n4\\nclass\\nBook\\n(\\nBaseModel\\n):\\n5\\ntitle:\\nstr\\n6\\nauthor:\\nstr\\n7\\n8\\n@openai.call(\\n\"gpt-4o-mini\"\\n, response_model=Book\\n)\\n9\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n11\\n12\\nbook = recommend_book(\\n\"fantasy\"\\n)\\n13\\nassert\\nisinstance\\n(book, Book)\\n14\\nprint\\n(book)\\n15\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\n1\\nfrom\\nmirascope.core\\nimport\\nopenai, prompt_template\\n2\\nfrom\\npydantic\\nimport\\nBaseModel\\n3\\n4\\nclass\\nBook\\n(\\nBaseModel\\n):\\n5\\ntitle:\\nstr\\n6\\nauthor:\\nstr\\n7\\n8\\n@openai.call(\\n\"gpt-4o-mini\"\\n, stream=\\nTrue\\n, response_model=Book\\n)\\n9\\n@prompt_template(\\n\"Recommend a {genre} book\"\\n)\\n10\\ndef\\nrecommend_book\\n(\\ngenre:\\nstr\\n):\\n...\\n11\\n12\\nbook_stream = recommend_book(\\n\"fantasy\"\\n)\\n13\\nfor\\npartial_book\\nin\\nbook_stream:\\n14\\nprint\\n(partial_book)\\n15\\n# &gt; title=None author=None\\n16\\n# &gt; title=\\'The Name\\' author=None\\n17\\n# &gt; title=\\'The Name of the Wind\\' author=None\\n18\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick\\'\\n19\\n# &gt; title=\\'The Name of the Wind\\' author=\\'Patrick Rothfuss\\'\\n...and more!\\nExamples\\nYou\\'re in control\\nSimple and transparent abstractions ensure you\\'re in the driver\\'s seat. Abstractions must be useful without getting in your way and blinding your view.\\nType hints you\\'ll love\\nWe\u2019ve taken care of all of the annoying Python typing so that you can have proper type hints in as simple an interface as possible.\\nBuild\\nyour\\nway\\nSimple, clean building blocks enable fast and reliable development without forcing you to do things our way. While we have recommendations and best practices, you\\'re still always able to do things\\nyour\\nway to best suit\\nyour\\nneeds.\\nTrusted by engineers building the next generation of AI-native applications\\nThe Pydantic inspired LLM framework the space has been missing. Simple, modular, extensible...helps where you need it, stays out of your way when you don\\'t.\\nVince Trost\\nCo-Founder / Plastic Labs\\nMirascope\\'s simplicity made it the natural next step from a provider\\'s API (OpenAI) \u2014 all without fighting unnecessary complexity of other tools like LangChain. We have all the bells and whistles necessary for production while maintaining ease of use and easy onboarding of new team members.\\nJake Duth\\nCo-Founder &amp;\\xa0CTO / Reddy\\nAs the author of Mirascope, I\\'m certainly biased, but there\\'s a reason I\\'ve worked day and night on this package \u2014 it solves the problems I have with existing developer tools. Mirascope is simple and easy to use, and honestly building with Mirascope is the most fun I\\'ve had coding in my life.\\nWilliam Bakst\\nFounder / Mirascope\\nStart Building With Mirascope\\nMirascope can help improve your application development experience no matter how simple or complex.\\nIt will feel like writing the Python you already know.\\nRead the Docs\\nGitHub | \u2b50 ${starCount}\\nJoin the Community', 'tool_call_id': 'call_SugLNjl2qFQu6nDhdZ1eNzka', 'name': 'extract_content'}, {'role': 'tool', 'content': \"Learn Mirascope\\n\u00b6\\nThis section is designed to help you master Mirascope, a toolkit for building AI-powered applications with Large Language Models (LLMs).\\nMirascope is a powerful, flexible, and user-friendly library that simplifies the process of working with LLMs. Whether you're building chatbots, content generators, or complex AI-driven agent systems, Mirascope provides the tools you need to streamline your development process and create powerful, robust applications.\\nOur documentation is tailored for developers who have at least some experience with Python and LLMs. Whether you're coming from other development tool libraries or have worked directly with provider SDKs and APIs, Mirascope offers a familiar but enhanced experience.\\nKey Features and Benefits\\n\u00b6\\nType Safety and Superior Editor Support\\n: We've prioritized proper type hints and type safety, ensuring you get world-class editor support. This means fewer errors, better autocomplete, and a smoother development experience in as simple and seamless an interface as possible.\\nProvider-Agnostic Design\\n: Mirascope works seamlessly with multiple LLM providers, allowing you to switch between them effortlessly or use multiple providers in the same project.\\nSimplicity and Ease of Use\\n: We've designed Mirascope with simplicity in mind. You get powerful features without unnecessary complexity, making it easy to get started and scale your projects.\\nComprehensive Tooling\\n: From prompt engineering to response parsing, Mirascope offers a complete suite of tools for every aspect of LLM application development.\\nCore Components\\n\u00b6\\nMirascope is built around the following core components, each designed to handle specific aspects of LLM interaction and application development. Here's a quick overview with links to detailed documentation:\\nPrompts\\n: Learn how to create and manage prompts effectively.\\nCalls\\n: Understand how to make calls to LLMs using Mirascope.\\nStreams\\n: Explore streaming responses for real-time applications.\\nTools\\n: Discover how to extend LLM capabilities with custom tools.\\nDynamic Configuration\\n: Learn to adjust LLM behavior at runtime.\\nChaining\\n: Understand the art of chaining multiple LLM calls for complex tasks.\\nJSON Mode\\n: Work with structured data responses from LLMs.\\nResponse Models\\n: Define and use structured output models with automatic validation.\\nOutput Parsers\\n: Process and transform custom LLM output structures effectively.\\nAsync\\n: Maximize efficiecy with asynchronous programming.\\nEvals\\n: Apply core components to build evaluation strategies for your LLM applications.\\nAgents\\n: Put everything together to build advanced AI agents using Mirascope.\\nNext Steps\\n\u00b6\\nWe encourage you to dive into each component's documentation to gain a deeper understanding of Mirascope's capabilities. Start with the topics that align most closely with your immediate needs, but don't hesitate to explore all areas \u2013 you might discover new ways to enhance your LLM applications!\\nAs you progress, you'll find advanced topics and best practices throughout the documentation. These will help you optimize your use of Mirascope and build increasingly sophisticated AI-powered applications.\\nHappy learning, and welcome to the world of development with Mirascope!\\nBack to top\", 'tool_call_id': 'call_hDmkCB8DRPyvsZI1X82Wsyhh', 'name': 'extract_content'}, {'role': 'assistant', 'content': 'The **Mirascope library** is a powerful toolkit designed for developers working with **Large Language Models (LLMs)**. It streamlines the process of building AI-powered applications, such as chatbots, content generators, and complex AI-driven systems. Here are some key aspects of Mirascope:\\n\\n### Key Features and Benefits:\\n1. **User-Friendly Design**: Mirascope is engineered for simplicity, providing necessary functionalities without unnecessary complexity. It aims to create a seamless development experience that feels familiar to Python developers.\\n   \\n2. **Type Safety**: The library emphasizes type hints and type safety, improving editor support and reducing potential errors during development.\\n\\n3. **Provider-Agnostic**: Mirascope can interact with multiple LLM providers, enabling developers to switch between them easily or even use several providers within the same project.\\n\\n4. **Comprehensive Tooling**: It offers a complete suite of tools to handle all aspects of LLM application development\u2014from prompt engineering to response parsing.\\n\\n5. **Modular and Extensible**: Mirascope is built with core components that allow developers to manage prompts, make API calls, handle streaming responses, and much more, all while maintaining the ability to customize and extend functionalities.\\n\\n### Core Components:\\n- **Prompts**: Facilitate efficient creation and management.\\n- **Calls**: Handle interactions with LLMs.\\n- **Streams**: Support real-time applications with streaming responses.\\n- **Tools**: Allow for extending LLM capabilities.\\n- **Chaining**: Enable coordinating multiple LLM calls for complex tasks.\\n- **Output Parsing**: Simplify the processing and transformation of responses.\\n\\n### Application Scenarios:\\nMirascope can be utilized for a range of applications like chatbots, content generation, and AI-driven agents, helping developers integrate sophisticated AI functionalities into their products.\\n\\n### Conclusion:\\nMirascope serves as an essential tool for modern developers looking to harness AI capabilities efficiently and effectively, encouraging advanced application development while ensuring ease of use and flexibility.\\n\\nFor more detailed information, you can refer to the [official Mirascope documentation](https://docs.mirascope.io/latest/learn/) or visit its [homepage](https://www.mirascope.io/).'}, {'role': 'user', 'content': 'exot'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"queries\":[\"exot programming\",\"exot library\",\"exot software\",\"exot application\",\"exot API\",\"Mirascope exot connection\"]}', 'name': '_web_search'}, 'id': 'call_xmAYDMXpqwCBNqZnAPq5edtg'}]}, {'role': 'tool', 'content': 'https://stackoverflow.com/questions/73663/how-do-i-terminate-a-script\\n\\nhttps://www.lifewire.com/how-to-force-quit-a-program-in-windows-2625781\\n\\nhttps://www.neb.com/en/products/m0265-exonuclease-t\\n\\nhttps://www.fsis.usda.gov/inspection/import-export/import-export-library\\n\\nhttps://exot.dev/\\n\\nhttps://exoft.net/\\n\\nhttps://learn.microsoft.com/en-us/dotnet/api/system.windows.forms.application.exit?view=windowsdesktop-8.0\\n\\nhttps://stackoverflow.com/questions/12977924/how-do-i-properly-exit-a-c-sharp-application\\n\\nhttps://developer.exotel.com/\\n\\nhttps://developer.exotel.com/api\\n\\nhttps://docs.mirascope.io/latest/\\n\\nhttps://docs.mirascope.io/latest/learn/async/', 'tool_call_id': 'call_xmAYDMXpqwCBNqZnAPq5edtg', 'name': '_web_search'}, {'role': 'assistant', 'content': '', 'tool_calls': [{'type': 'function', 'function': {'arguments': '{\"url\": \"https://exot.dev/\"}', 'name': 'extract_content'}, 'id': 'call_elkn1QwLzWadl9HgeSSud0r1'}, {'type': 'function', 'function': {'arguments': '{\"url\": \"https://developer.exotel.com/\"}', 'name': 'extract_content'}, 'id': 'call_bSmKZTKtQ7ZOdxRpD3zKu9hk'}]}, {'role': 'tool', 'content': 'Exot Inspector\\nApplication inspector for Node, Bun and Deno, equipped with powerful tools for monitoring, tracing and debugging.\\nGet Started\\nOpen Inspector\\nMonitoring\\nEasily monitor performance metrics, create custom dashboards, and gain real-time insights into your application. No sign-ups required for a streamlined experience.\\nDebugging\\nDebug and analyze applications seamlessly with logging, tracing, and real-time monitoring of outgoing HTTP requests, ensuring privacy in your debugging process.\\nSecurity\\nCapture errors and exceptions, store metrics, and data in your preferred storage \u2013 all without external services or databases. Own and secure your data with complete control, emphasizing privacy throughout your application\u2019s lifecycle.\\nGet Started\\nExot Inspector is a free, self-hosted software. Follow the guides to\\nget started\\n.', 'tool_call_id': 'call_elkn1QwLzWadl9HgeSSud0r1', 'name': 'extract_content'}, {'role': 'tool', 'content': 'API Docs\\nVoice\\nSMS\\nExophones\\nHeartbeat\\nCall Campaigns\\nSMS Campaigns\\nLead\\xa0Assist\\nUsers\\nExoVerify\\nWhatsapp\\nURL shortening\\nMore...\\nAPI Console\\nVoice\\nSMS\\nExophones\\nHeartbeat\\nCall Campaigns\\nSMS Campaigns\\nLeadAssist\\nMore...\\nApplets Docs\\nGreeting\\nConnect\\nPassthru\\nTransfer\\nSMS\\nEmail\\nMore\\nUse Cases\\nFAQs\\nCreate Account', 'tool_call_id': 'call_bSmKZTKtQ7ZOdxRpD3zKu9hk', 'name': 'extract_content'}]\n### Exot Overview\n\n**Exot** appears to be associated with two main contexts: a software monitoring and debugging tool, **Exot Inspector**, and **Exotel**, a cloud communication platform. Here\u2019s a detailed look at both:\n\n#### 1. Exot Inspector:\nExot Inspector is a development tool tailored for applications built with **Node.js**, **Bun**, and **Deno**. It provides powerful capabilities for monitoring, debugging, and tracing applications.\n\n- **Key Features**:\n  - **Monitoring**: Offer performance metrics and create custom dashboards for real-time insights into application performance.\n  - **Debugging**: Allows for seamless debugging and analysis of applications through logging, tracing, and monitoring outgoing HTTP requests.\n  - **Security**: Users maintain complete control over their data, capturing errors and metrics without relying on external services. This focus on privacy is important for managing sensitive information.\n\n- **Usage**: Exot Inspector is self-hosted and free to use, making it accessible for developers seeking to enhance their application's oversight without additional costs.\n\nFor more information, you can visit the [Exot Inspector website](https://exot.dev/).\n\n---\n\n#### 2. Exotel:\nExotel is a cloud-based communication platform that provides a suite of services aimed at enhancing businesses' communication capabilities.\n\n- **Key Features**:\n  - **APIs for Communication**: Exotel offers APIs for voice and SMS communications, which include functionalities like call campaigns, lead assistance, and verification services.\n  - **Integration Capabilities**: The platform allows developers to integrate communication features into their applications easily, making it suitable for businesses aiming to streamline customer interactions.\n  - **Use Cases**: Exotel supports various use cases, highlighting the versatility of its communication solutions.\n\nFor more details, you can explore the [Exotel Developer Portal](https://developer.exotel.com/).\n\n---\n\n### Conclusion\nOverall, \"Exot\" seems to refer to a versatile ecosystem of tools catering to application debugging and cloud communication needs. Whether you are looking to enhance your application's monitoring capabilities with Exot Inspector or improve your business's communication streams with Exotel, there are robust options available.\n</pre> <p>Search Queries:</p> <ul> <li>Mirascope library</li> <li>what is Mirascope</li> <li>Mirascope Python library</li> <li>Mirascope library documentation</li> <li>Mirascope library features</li> <li>Mirascope library use cases</li> <li>Mirascope library tutorial</li> </ul> <p>The LLM can gather information regarding the Mirascope library but has no context beyond that.</p> <p>Let's take a look at what happens when we call the user queries together.</p> In\u00a0[9]: Copied! <pre>await WebAssistantBaseWithStream().run()\n</pre> await WebAssistantBaseWithStream().run() <p>Search Queries:</p> <ul> <li>Mirascope library</li> <li>Mirascope LLM development</li> <li>Mirascope open source</li> <li>Mirascope Python library</li> <li>LLM tools Mirascope</li> </ul> <p>By giving the LLM search history, these search queries now connect the Mirascope library specifically to LLM development tools, providing a more cohesive set of results.</p> <p>We can now create our <code>_step</code> and <code>run</code> functions which will call our <code>_stream</code> and <code>_step</code> functions respectively.</p> In\u00a0[\u00a0]: Copied! <pre>class WebAssistant(WebAssistantBaseWithStream):\n    async def _step(self, question: str):\n        print(self.messages)\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                print(f\"using {tool._name()} tool with args: {tool.args}\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n</pre> class WebAssistant(WebAssistantBaseWithStream):     async def _step(self, question: str):         print(self.messages)         response = await self._stream(question)         tools_and_outputs = []         async for chunk, tool in response:             if tool:                 print(f\"using {tool._name()} tool with args: {tool.args}\")                 tools_and_outputs.append((tool, tool.call()))             else:                 print(chunk.content, end=\"\", flush=True)         if response.user_message_param:             self.messages.append(response.user_message_param)         self.messages.append(response.message_param)         if tools_and_outputs:             self.messages += response.tool_message_params(tools_and_outputs)             await self._step(\"\") <p>The <code>run</code> function will keep running until the LLM feels that the users question can be answered.</p> In\u00a0[\u00a0]: Copied! <pre>web_assistant = WebAssistant()\nawait web_assistant.run()\n</pre> web_assistant = WebAssistant() await web_assistant.run() <p>Note that by giving the LLM the current date, it'll automatically search for the most up-to-date information.</p> <p>Check out Evaluating Web Search Agent for an in-depth guide on how we evaluate the quality of our agent.</p> <p>Additional Real-World Applications</p> <ol> <li><p>Advanced Research Assistant</p> <ul> <li>Stay updated on latest developments in rapidly evolving fields</li> </ul> </li> <li><p>Personalized Education</p> <ul> <li>Create customized learning materials based on current curricula</li> </ul> </li> <li><p>Business Intelligence</p> <ul> <li>Assist in data-driven decision making with real-time insights</li> </ul> </li> <li><p>Technical Support and Troubleshooting</p> <ul> <li>Assist in debugging by referencing current documentation</li> </ul> </li> <li><p>Travel Planning</p> <ul> <li>Provide updates on travel restrictions, local events, and weather</li> </ul> </li> <li><p>Journalism and Fact-Checking</p> <ul> <li>Help identify and combat misinformation</li> </ul> </li> <li><p>Environmental Monitoring</p> <ul> <li>Track and analyze current climate data</li> </ul> </li> </ol> <p>When adapting this recipe, consider:</p> <ul> <li>Optimizing the search by utilizing <code>async</code> to increase parallelism.</li> <li>When targeting specific websites for scraping purposes, use <code>response_model</code> to extract the specific information you're looking for across websites with similar content.</li> <li>Implement a feedback loop so the LLM can rewrite the query for better search results.</li> <li>Reduce the number of tokens used by storing the extracted webpages as embeddings in a vectorstore and retrieving only what is necessary.</li> <li>Make a more specific web search agent for your use-case rather than a general purpose web search agent.</li> </ul>"},{"location":"tutorials/agents/web_search_agent/#web-search-agent","title":"Web Search Agent\u00b6","text":"<p>In this recipe, we'll explore using Mirascope to enhance our Large Language Model (LLM) \u2014 specifically OpenAI's GPT-4o mini \u2014 by providing it with access to the web and its contents. We will be using DuckDuckGo's API as a tool for our Agentic workflow.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Agents</li> </ul> <p>Background</p> <p> In the past, users had to rely on search engines and manually browse through multiple web pages to find answers to their questions. Large Language Models (LLMs) have revolutionized this process. They can efficiently utilize search engine results pages (SERPs) and extract relevant content from websites. By leveraging this information, LLMs can quickly provide accurate answers to user queries, eliminating the need for active searching. Users can simply pose their questions and let the LLM work in the background, significantly streamlining the information retrieval process. </p>"},{"location":"tutorials/agents/web_search_agent/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/agents/web_search_agent/#add-duckduckgo-tool","title":"Add DuckDuckGo Tool\u00b6","text":"<p>The first step is to create a <code>WebAssistant</code> that first conducts a web search based on the user's query. Let\u2019s go ahead and create our <code>WebAssistant</code> and add our web search tool:</p>"},{"location":"tutorials/agents/web_search_agent/#add-qa-functionality","title":"Add Q&amp;A Functionality\u00b6","text":"<p>Now that we have our tools we can now create our <code>prompt_template</code> and <code>_stream</code> function. We engineer the prompt to first use our <code>_web_search</code> tool, then <code>extract_content</code> from the tool before answering the user question based on the retrieved content:</p>"},{"location":"tutorials/agents/web_search_agent/#example-search-queries","title":"Example search queries\u00b6","text":""},{"location":"tutorials/evals/evaluating_documentation_agent/","title":"Evaluating Documentation Agent","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n# for testing and llama index\n!pip install  ipytest pytest llama-index\n</pre> !pip install \"mirascope[openai]\" # for testing and llama index !pip install  ipytest pytest llama-index In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[\u00a0]: Copied! <pre>import ipytest\nimport pytest\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\nipytest.autoconfig()\n\ndocuments = [\n    {\"id\": 0, \"text\": \"Bob eats burgers every day.\", \"semantic_score\": 0.8},\n    {\"id\": 1, \"text\": \"Bob's favorite food is not pizza.\", \"semantic_score\": 0.9},\n    {\"id\": 2, \"text\": \"I ate at In-N-Out with Bob yesterday\", \"semantic_score\": 0.5},\n    {\"id\": 3, \"text\": \"Bob said his favorite food is burgers\", \"semantic_score\": 0.9},\n]\n\n\nclass Relevance(BaseModel):\n    id: int = Field(..., description=\"The document ID\")\n    score: int = Field(..., description=\"The relevance score (1-10)\")\n    document: str = Field(..., description=\"The document text\")\n    reason: str = Field(..., description=\"A brief explanation for the assigned score\")\n\n\n@openai.call(\n    \"gpt-4o-mini\",\n    response_model=list[Relevance],\n    json_mode=True,\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Document Relevance Assessment\n    Given a list of documents and a question, determine the relevance of each document to answering the question.\n\n    Input\n        - A question\n        - A list of documents, each with an ID and content summary\n\n    Task\n        - Analyze each document for its relevance to the question.\n        - Assign a relevance score from 1-10 for each document.\n        - Provide a reason for each score.\n\n    Scoring Guidelines\n        - Consider both direct and indirect relevance to the question.\n        - Prioritize positive, affirmative information over negative statements.\n        - Assess the informativeness of the content, not just keyword matches.\n        - Consider the potential for a document to contribute to a complete answer.\n\n    Important Notes\n        - Exclude documents with no relevance less than 5 to the question.\n        - Be cautious with negative statements - they may be relevant but are often less informative than positive ones.\n        - Consider how multiple documents might work together to answer the question.\n        - Use the document title and content summary to make your assessment.\n\n    Documents:\n    {documents}\n\n    USER: \n    {query}\n    \"\"\"\n)\ndef llm_query_rerank(documents: list[dict], query: str): ...\n\n\n@pytest.mark.parametrize(\n    \"query,documents,top_n_ids\",\n    ((\"What is Bob's favorite food\", documents, {3, 0}),),\n)\ndef test_llm_query_rerank(query: str, documents: list[dict], top_n_ids: set[int]):\n    \"\"\"Tests that the LLM query rerank ranks more relevant documents higher.\"\"\"\n    response = llm_query_rerank(documents, query)\n    results = sorted(response, key=lambda x: x.score or 0, reverse=True)\n    assert all(result.score &gt; 5 for result in results)\n    assert top_n_ids.issuperset({result.id for result in results[: len(top_n_ids)]})\n\n\nipytest.run()\n</pre> import ipytest import pytest from mirascope.core import openai, prompt_template from pydantic import BaseModel, Field  ipytest.autoconfig()  documents = [     {\"id\": 0, \"text\": \"Bob eats burgers every day.\", \"semantic_score\": 0.8},     {\"id\": 1, \"text\": \"Bob's favorite food is not pizza.\", \"semantic_score\": 0.9},     {\"id\": 2, \"text\": \"I ate at In-N-Out with Bob yesterday\", \"semantic_score\": 0.5},     {\"id\": 3, \"text\": \"Bob said his favorite food is burgers\", \"semantic_score\": 0.9}, ]   class Relevance(BaseModel):     id: int = Field(..., description=\"The document ID\")     score: int = Field(..., description=\"The relevance score (1-10)\")     document: str = Field(..., description=\"The document text\")     reason: str = Field(..., description=\"A brief explanation for the assigned score\")   @openai.call(     \"gpt-4o-mini\",     response_model=list[Relevance],     json_mode=True, ) @prompt_template(     \"\"\"     SYSTEM:     Document Relevance Assessment     Given a list of documents and a question, determine the relevance of each document to answering the question.      Input         - A question         - A list of documents, each with an ID and content summary      Task         - Analyze each document for its relevance to the question.         - Assign a relevance score from 1-10 for each document.         - Provide a reason for each score.      Scoring Guidelines         - Consider both direct and indirect relevance to the question.         - Prioritize positive, affirmative information over negative statements.         - Assess the informativeness of the content, not just keyword matches.         - Consider the potential for a document to contribute to a complete answer.      Important Notes         - Exclude documents with no relevance less than 5 to the question.         - Be cautious with negative statements - they may be relevant but are often less informative than positive ones.         - Consider how multiple documents might work together to answer the question.         - Use the document title and content summary to make your assessment.      Documents:     {documents}      USER:      {query}     \"\"\" ) def llm_query_rerank(documents: list[dict], query: str): ...   @pytest.mark.parametrize(     \"query,documents,top_n_ids\",     ((\"What is Bob's favorite food\", documents, {3, 0}),), ) def test_llm_query_rerank(query: str, documents: list[dict], top_n_ids: set[int]):     \"\"\"Tests that the LLM query rerank ranks more relevant documents higher.\"\"\"     response = llm_query_rerank(documents, query)     results = sorted(response, key=lambda x: x.score or 0, reverse=True)     assert all(result.score &gt; 5 for result in results)     assert top_n_ids.issuperset({result.id for result in results[: len(top_n_ids)]})   ipytest.run() <p>Our tests:</p> <ul> <li>Ensures that all returned documents have a relevancy score above 5 out of 10, indicating a minimum threshold of relevance.</li> <li>Checks that the top-ranked documents (as many as specified in <code>top_n_ids</code>) are within the set of expected documents, allowing for some flexibility in the exact ordering.</li> </ul> <p>The test accommodates the non-deterministic nature of LLM-based reranking. While we can't expect identical results in every run, especially when multiple documents are similarly relevant, we can at least verify that the output falls within our boundaries.</p> In\u00a0[\u00a0]: Copied! <pre>import ast\nimport importlib.util\n\n\ndef check_syntax(code_string: str) -&gt; bool:\n    try:\n        compile(code_string, \"&lt;string&gt;\", \"exec\")\n        return True\n    except SyntaxError as e:\n        print(f\"Syntax error: {e}\")\n        return False\n\n\ndef is_importable(code_string: str) -&gt; bool:\n    try:\n        tree = ast.parse(code_string)\n    except SyntaxError:\n        return False\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.Import | ast.ImportFrom):\n            module_name = (\n                node.module if isinstance(node, ast.ImportFrom) else node.names[0].name\n            )\n            if not check_module(module_name):\n                return False\n\n            if isinstance(node, ast.ImportFrom):\n                for alias in node.names:\n                    if not check_attribute(module_name, alias.name):\n                        return False\n\n    return True\n\n\ndef check_module(module_name):\n    try:\n        spec = importlib.util.find_spec(module_name)\n        return spec is not None\n    except (ImportError, AttributeError, ValueError):\n        return False\n\n\ndef check_attribute(module_name, attribute):\n    try:\n        spec = importlib.util.find_spec(module_name)\n        if spec is None:\n            return False\n        module = importlib.util.module_from_spec(spec)\n        if spec.loader:\n            spec.loader.exec_module(module)\n        return hasattr(module, attribute)\n    except (ImportError, AttributeError):\n        return False\n\n\n@pytest.mark.parametrize(\n    \"import_str,expected\",\n    [\n        (\"from mirascope.core import openai\", True),\n        (\"import math\", True),\n        (\"from datetime import datetime\", True),\n        (\"import non_existent_module\", False),\n        (\"from os import path, nonexistent_function\", False),\n        (\"from sys import exit, nonexistent_variable\", False),\n        (\"from openai import OpenAI\", True),\n        (\"from mirascope.core import openai\", True),\n    ],\n)\ndef test_is_importable(import_str: str, expected: bool):\n    assert is_importable(import_str) == expected\n\n\n@pytest.mark.parametrize(\n    \"syntax,expected\",\n    [(\"print('Hello, World!')\", True), (\"print('Hello, World!'\", False)],\n)\ndef test_check_syntax(syntax: str, expected: bool):\n    assert check_syntax(syntax) == expected\n\n\nipytest.run()\n</pre> import ast import importlib.util   def check_syntax(code_string: str) -&gt; bool:     try:         compile(code_string, \"\", \"exec\")         return True     except SyntaxError as e:         print(f\"Syntax error: {e}\")         return False   def is_importable(code_string: str) -&gt; bool:     try:         tree = ast.parse(code_string)     except SyntaxError:         return False      for node in ast.walk(tree):         if isinstance(node, ast.Import | ast.ImportFrom):             module_name = (                 node.module if isinstance(node, ast.ImportFrom) else node.names[0].name             )             if not check_module(module_name):                 return False              if isinstance(node, ast.ImportFrom):                 for alias in node.names:                     if not check_attribute(module_name, alias.name):                         return False      return True   def check_module(module_name):     try:         spec = importlib.util.find_spec(module_name)         return spec is not None     except (ImportError, AttributeError, ValueError):         return False   def check_attribute(module_name, attribute):     try:         spec = importlib.util.find_spec(module_name)         if spec is None:             return False         module = importlib.util.module_from_spec(spec)         if spec.loader:             spec.loader.exec_module(module)         return hasattr(module, attribute)     except (ImportError, AttributeError):         return False   @pytest.mark.parametrize(     \"import_str,expected\",     [         (\"from mirascope.core import openai\", True),         (\"import math\", True),         (\"from datetime import datetime\", True),         (\"import non_existent_module\", False),         (\"from os import path, nonexistent_function\", False),         (\"from sys import exit, nonexistent_variable\", False),         (\"from openai import OpenAI\", True),         (\"from mirascope.core import openai\", True),     ], ) def test_is_importable(import_str: str, expected: bool):     assert is_importable(import_str) == expected   @pytest.mark.parametrize(     \"syntax,expected\",     [(\"print('Hello, World!')\", True), (\"print('Hello, World!'\", False)], ) def test_check_syntax(syntax: str, expected: bool):     assert check_syntax(syntax) == expected   ipytest.run() <p>Now that we have our <code>check_syntax</code> and <code>is_importable</code> tests working, we can test our LLM output:</p> In\u00a0[\u00a0]: Copied! <pre>from typing import Literal, cast\n\nfrom llama_index.core import (\n    QueryBundle,\n    load_index_from_storage,\n)\nfrom llama_index.core.indices.vector_store.base import VectorStoreIndex\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.storage import StorageContext\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\nloaded_index = load_index_from_storage(storage_context)\n\n\ndef get_documents(query: str) -&gt; list[str]:\n    \"\"\"The get_documents tool that retrieves Mirascope documentation based on the\n    relevance of the query\"\"\"\n    query_bundle = QueryBundle(query)\n    retriever = VectorIndexRetriever(\n        index=cast(VectorStoreIndex, loaded_index),\n        similarity_top_k=10,\n    )\n    retrieved_nodes = retriever.retrieve(query_bundle)\n    choice_batch_size = 5\n    top_n = 2\n    results: list[Relevance] = []\n    for idx in range(0, len(retrieved_nodes), choice_batch_size):\n        nodes_batch = [\n            {\n                \"id\": idx + id,\n                \"text\": node.node.get_text(),  # pyright: ignore[reportAttributeAccessIssue]\n                \"document_title\": node.metadata[\"document_title\"],\n                \"semantic_score\": node.score,\n            }\n            for id, node in enumerate(retrieved_nodes[idx : idx + choice_batch_size])\n        ]\n        results += llm_query_rerank(nodes_batch, query)\n    results = sorted(results, key=lambda x: x.score or 0, reverse=True)[:top_n]\n\n    return [result.document for result in results]\n\n\nclass Response(BaseModel):\n    classification: Literal[\"code\", \"general\"] = Field(\n        ..., description=\"The classification of the question\"\n    )\n    content: str = Field(..., description=\"The response content\")\n\n\nclass DocumentationAgent(BaseModel):\n    @openai.call(\"gpt-4o-mini\", response_model=Response, json_mode=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an AI Assistant that is an expert at answering questions about Mirascope.\n        Here is the relevant documentation to answer the question.\n\n        First classify the question into one of two types:\n            - General Information: Questions about the system or its components.\n            - Code Examples: Questions that require code snippets or examples.\n\n        For General Information, provide a summary of the relevant documents if the question is too broad ask for more details. \n        If the context does not answer the question, say that the information is not available or you could not find it.\n\n        For Code Examples, output ONLY code without any markdown, with comments if necessary.\n        If the context does not answer the question, say that the information is not available.\n\n        Examples:\n            Question: \"What is Mirascope?\"\n            Answer:\n            A toolkit for building AI-powered applications with Large Language Models (LLMs).\n            Explanation: This is a General Information question, so a summary is provided.\n\n            Question: \"How do I make a basic OpenAI call using Mirascope?\"\n            Answer:\n            from mirascope.core import openai, prompt_template\n\n\n            @openai.call(\"gpt-4o-mini\")\n            def recommend_book(genre: str) -&gt; str:\n                return f'Recommend a {genre} book'\n\n            response = recommend_book(\"fantasy\")\n            print(response.content)\n            Explanation: This is a Code Examples question, so only a code snippet is provided.\n\n        Context:\n        {context:list}\n\n        USER:\n        {question}\n        \"\"\"\n    )\n    def _call(self, question: str) -&gt; openai.OpenAIDynamicConfig:\n        documents = get_documents(question)\n        return {\"computed_fields\": {\"context\": documents}}\n\n    def _step(self, question: str):\n        answer = self._call(question)\n        print(\"(Assistant):\", answer.content)\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            self._step(question)\n</pre> from typing import Literal, cast  from llama_index.core import (     QueryBundle,     load_index_from_storage, ) from llama_index.core.indices.vector_store.base import VectorStoreIndex from llama_index.core.retrievers import VectorIndexRetriever from llama_index.core.storage import StorageContext  storage_context = StorageContext.from_defaults(persist_dir=\"storage\") loaded_index = load_index_from_storage(storage_context)   def get_documents(query: str) -&gt; list[str]:     \"\"\"The get_documents tool that retrieves Mirascope documentation based on the     relevance of the query\"\"\"     query_bundle = QueryBundle(query)     retriever = VectorIndexRetriever(         index=cast(VectorStoreIndex, loaded_index),         similarity_top_k=10,     )     retrieved_nodes = retriever.retrieve(query_bundle)     choice_batch_size = 5     top_n = 2     results: list[Relevance] = []     for idx in range(0, len(retrieved_nodes), choice_batch_size):         nodes_batch = [             {                 \"id\": idx + id,                 \"text\": node.node.get_text(),  # pyright: ignore[reportAttributeAccessIssue]                 \"document_title\": node.metadata[\"document_title\"],                 \"semantic_score\": node.score,             }             for id, node in enumerate(retrieved_nodes[idx : idx + choice_batch_size])         ]         results += llm_query_rerank(nodes_batch, query)     results = sorted(results, key=lambda x: x.score or 0, reverse=True)[:top_n]      return [result.document for result in results]   class Response(BaseModel):     classification: Literal[\"code\", \"general\"] = Field(         ..., description=\"The classification of the question\"     )     content: str = Field(..., description=\"The response content\")   class DocumentationAgent(BaseModel):     @openai.call(\"gpt-4o-mini\", response_model=Response, json_mode=True)     @prompt_template(         \"\"\"         SYSTEM:         You are an AI Assistant that is an expert at answering questions about Mirascope.         Here is the relevant documentation to answer the question.          First classify the question into one of two types:             - General Information: Questions about the system or its components.             - Code Examples: Questions that require code snippets or examples.          For General Information, provide a summary of the relevant documents if the question is too broad ask for more details.          If the context does not answer the question, say that the information is not available or you could not find it.          For Code Examples, output ONLY code without any markdown, with comments if necessary.         If the context does not answer the question, say that the information is not available.          Examples:             Question: \"What is Mirascope?\"             Answer:             A toolkit for building AI-powered applications with Large Language Models (LLMs).             Explanation: This is a General Information question, so a summary is provided.              Question: \"How do I make a basic OpenAI call using Mirascope?\"             Answer:             from mirascope.core import openai, prompt_template               @openai.call(\"gpt-4o-mini\")             def recommend_book(genre: str) -&gt; str:                 return f'Recommend a {genre} book'              response = recommend_book(\"fantasy\")             print(response.content)             Explanation: This is a Code Examples question, so only a code snippet is provided.          Context:         {context:list}          USER:         {question}         \"\"\"     )     def _call(self, question: str) -&gt; openai.OpenAIDynamicConfig:         documents = get_documents(question)         return {\"computed_fields\": {\"context\": documents}}      def _step(self, question: str):         answer = self._call(question)         print(\"(Assistant):\", answer.content)      def run(self):         while True:             question = input(\"(User): \")             if question == \"exit\":                 break             self._step(question) In\u00a0[\u00a0]: Copied! <pre>@pytest.mark.parametrize(\n    \"query,expected\",\n    [\n        (\"How do I make a basic OpenAI call using Mirascope?\", None),\n        (\"What is Mirascope?\", \"a toolkit for building AI-powered applications\"),\n    ],\n)\ndef test_documentation_agent_code(query: str, expected: str):\n    documentation_agent = DocumentationAgent()\n    response = documentation_agent._call(query)\n    if response.classification == \"code\":\n        assert check_syntax(response.content) and is_importable(response.content)\n    else:\n        assert expected in response.content\n\n\nipytest.run()\n</pre> @pytest.mark.parametrize(     \"query,expected\",     [         (\"How do I make a basic OpenAI call using Mirascope?\", None),         (\"What is Mirascope?\", \"a toolkit for building AI-powered applications\"),     ], ) def test_documentation_agent_code(query: str, expected: str):     documentation_agent = DocumentationAgent()     response = documentation_agent._call(query)     if response.classification == \"code\":         assert check_syntax(response.content) and is_importable(response.content)     else:         assert expected in response.content   ipytest.run() In\u00a0[\u00a0]: Copied! <pre>@pytest.mark.parametrize(\n    \"query,expected\",\n    [\n        (\"What is Mirascope?\", \"a toolkit for building AI-powered applications\"),\n    ],\n)\ndef test_documentation_agent_general(query: str, expected: str):\n    documentation_agent = DocumentationAgent()\n    response = documentation_agent._call(query)\n    if response.classification == \"general\":\n        assert expected in response.content\n\n\nipytest.run()\n</pre> @pytest.mark.parametrize(     \"query,expected\",     [         (\"What is Mirascope?\", \"a toolkit for building AI-powered applications\"),     ], ) def test_documentation_agent_general(query: str, expected: str):     documentation_agent = DocumentationAgent()     response = documentation_agent._call(query)     if response.classification == \"general\":         assert expected in response.content   ipytest.run() <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Update the Few-Shot examples to match your documents.</li> <li>Experiment with other providers for LLM Reranking or use multiple LLM Rerankers and average out the scores.</li> <li>Add history to the <code>DocumentationAgent</code> and have the LLM generate the query for <code>get_documents</code> for a more relevant semantic search.</li> </ul>"},{"location":"tutorials/evals/evaluating_documentation_agent/#evaluating-documentation-agent","title":"Evaluating Documentation Agent\u00b6","text":"<p>In this recipe, we will be using taking our Documentation Agent example and running evaluations on the LLM call. We will be exploring various different evaluations we can run to ensure quality and expected behavior.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Chaining</li> <li>Evals</li> </ul> <p>Check out the Documentation Agent Cookbook</p> <p> We will be using our <code>DocumentationAgent</code> for our evaluations. For a detailed explanation regarding this code snippet, refer to the Documentation Agent Cookbook. </p>"},{"location":"tutorials/evals/evaluating_documentation_agent/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/evals/evaluating_documentation_agent/#basic-evaluations","title":"Basic Evaluations\u00b6","text":"<p>We will first test the functionality of our LLM Rerank function to ensure it performs as intended. We have prepared a list of mock documents, each with an assigned semantic score, simulating retrieval from our vector store. The LLM Rerank function will then reorder these documents based on their relevance to the query, rather than relying solely on their semantic scores.</p>"},{"location":"tutorials/evals/evaluating_documentation_agent/#evaluating-code-snippets-and-general-qa","title":"Evaluating Code Snippets and General Q&amp;A\u00b6","text":"<p>The example documents we are using for our <code>DocumentationAgent</code> are the Mirascope docs. Since Mirascope is a python library, users are likely to ask about both code implementation and conceptual understanding. Therefore, our evaluation process needs to address these two distinct scenarios.</p>"},{"location":"tutorials/evals/evaluating_documentation_agent/#evaluating-code-snippet","title":"Evaluating Code Snippet\u00b6","text":"<p>To ensure the accuracy and functionality of the LLM-generated code, we implement a two-step verification process:</p> <ul> <li>Syntax Validation: We create a general Python code tester to verify the syntactic correctness of the generated code.</li> <li>Import Verification: Since syntax validation alone is insufficient, we incorporate an additional check for proper imports. This step confirms all modules and dependencies are valid and no \"magic\" imports exist.</li> </ul>"},{"location":"tutorials/evals/evaluating_documentation_agent/#evaluating-general-qa","title":"Evaluating General Q&amp;A\u00b6","text":"<p>For non-code responses generated by the LLM, our primary goal is to verify that the LLM is sourcing its responses directly from the Mirascope documentation rather than relying on its broader knowledge base. Here we require that the LLM's response contains a sentence verbatim.</p>"},{"location":"tutorials/evals/evaluating_sql_agent/","title":"Evaluating Generating SQL with LLM","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n!pip install pytest ipytest pytest-asyncio\n</pre> !pip install \"mirascope[openai]\" !pip install pytest ipytest pytest-asyncio In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[\u00a0]: Copied! <pre>import sqlite3\nfrom typing import ClassVar\nfrom unittest.mock import MagicMock\n\nimport ipytest\nimport pytest\nfrom mirascope.core import BaseMessageParam, openai, prompt_template\nfrom pydantic import BaseModel, ConfigDict\n\nipytest.autoconfig(run_in_thread=True)\n\n\nclass Librarian(BaseModel):\n    con: ClassVar[sqlite3.Connection] = sqlite3.connect(\"database.db\")\n    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def _run_query(self, query: str) -&gt; str:\n        \"\"\"A SELECT query to run.\"\"\"\n        print(query)\n        try:\n            cursor = self.con.cursor()\n            cursor.execute(query)\n            res = cursor.fetchall()\n            return str(res)\n        except sqlite3.Error as e:\n            return str(e)\n\n    def _execute_query(self, query: str) -&gt; str:\n        \"\"\"An INSERT, UPDATE, or DELETE query to execute.\"\"\"\n        print(query)\n        try:\n            cursor = self.con.cursor()\n            cursor.execute(query)\n            rows_affected = cursor.rowcount\n            self.con.commit()\n            if rows_affected &gt; 0:\n                return f\"Query executed successfully, {rows_affected} row(s) were updated/inserted.\"\n            else:\n                return \"No rows were updated/inserted.\"\n        except sqlite3.Error as e:\n            print(e)\n            return str(e)\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are a friendly and knowledgeable librarian named Mira. Your role is to \n        assist patrons with their queries, recommend books, \n        and provide information on a wide range of topics.\n\n        Personality:\n            - Warm and approachable, always ready with a kind word\n            - Patient and understanding, especially with those who are hesitant or confused\n            - Enthusiastic about books and learning\n            - Respectful of all patrons, regardless of their background or level of knowledge\n\n        Services:\n            - Keep track of patrons' reading lists using a SQLite database. Assume that the user is non technical and will ask you\n        questions in plain English.\n            - Recommend books based on the user's preferences\n        Your task is to write a query based on the user's request.\n\n        The database schema is as follows:\n\n        TABLE ReadingList (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            title TEXT NOT NULL,\n            status TEXT CHECK(status IN ('Not Started', 'In Progress', 'Complete')) NOT NULL,\n            rating INTEGER CHECK(rating &gt;= 1 AND rating &lt;= 5),\n        );\n\n        You must interpret the user's request and write the appropriate SQL query to\n        pass in the tools.\n\n        Example interactions:\n            1. Select\n                - USER: \"Show me all books.\"\n                - ASSISTANT: \"SELECT * FROM ReadingList;\"\n            2. Insert\n                - USER: \"Add Gone with the Wind to my reading list.\"\n                - ASSISTANT: \"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\"\n            3. Update\n                - USER: \"I just finished Gone with the Wind, can you update the status, and give it 5 stars??\"\n                - ASSISTANT: \"UPDATE ReadingList SET status = 'Complete' and rating = 5 WHERE title = 'Gone with the Wind';\"\n            4. Delete\n                - USER: \"Remove Gone with the Wind from my reading list.\"\n                - ASSISTANT: \"DELETE FROM ReadingList WHERE title = 'Gone with the Wind';\"\n\n        If field are not mentioned, omit them from the query.\n        All queries must end with a semicolon.\n\n        You have access to the following tools:\n        - `_run_query`: When user asks for recommendations, you can use this tool to see what they have read.\n        - `_execute_query`: Use the query generated to execute an \n            INSERT, UPDATE, or DELETE query.\n\n        You must use these tools to interact with the database.\n\n        MESSAGES: {self.messages}\n        USER: {query}\n        \"\"\"\n    )\n    async def _stream(self, query: str) -&gt; openai.OpenAIDynamicConfig:\n        return {\"tools\": [self._run_query, self._execute_query]}\n\n    async def _step(self, question: str):\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._step(question)\n            print()\n\n\n@pytest.fixture\ndef mock_librarian():\n    class MockLibrarian(Librarian):\n        con: ClassVar[sqlite3.Connection] = MagicMock()\n\n    return MockLibrarian()\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"select_query\",\n    [\n        \"Get all books\",\n        \"Get every single book\",\n        \"Show me all books\",\n        \"List all books\",\n        \"Display all books\",\n    ],\n)\nasync def test_select_query(select_query: str, mock_librarian: Librarian):\n    response = await mock_librarian._stream(select_query)\n    async for _, tool in response:\n        query = tool.args.get(\"query\", \"\") if tool else \"\"\n        assert query == \"SELECT * FROM ReadingList;\"\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"insert_query\",\n    [\n        \"Please add Gone with the Wind to my reading list\",\n        \"You recently recommended Gone with the Wind, can you add it to my reading list.\",\n    ],\n)\nasync def test_insert_query(insert_query: str, mock_librarian: Librarian):\n    response = await mock_librarian._stream(insert_query)\n    async for _, tool in response:\n        query = tool.args.get(\"query\", \"\") if tool else \"\"\n        assert (\n            query\n            == \"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\"\n        )\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"update_query\",\n    [\n        \"Can you mark Gone with the Wind as read?\",\n        \"I just finished Gone with the Wind, can you update the status?\",\n    ],\n)\nasync def test_update_query(update_query: str, mock_librarian: Librarian):\n    response = await mock_librarian._stream(update_query)\n    async for _, tool in response:\n        query = tool.args.get(\"query\", \"\") if tool else \"\"\n        assert (\n            query\n            == \"UPDATE ReadingList SET status = 'Complete' WHERE title = 'Gone with the Wind';\"\n        )\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"delete_query\",\n    [\n        \"Can you remove Gone with the Wind from my reading list?\",\n        \"Can you delete Gone with the Wind?\",\n    ],\n)\nasync def test_delete_query(delete_query: str, mock_librarian: Librarian):\n    response = await mock_librarian._stream(delete_query)\n    async for _, tool in response:\n        query = tool.args.get(\"query\", \"\") if tool else \"\"\n        assert query == \"DELETE FROM ReadingList WHERE title = 'Gone with the Wind';\"\n\n\nipytest.run()\n</pre> import sqlite3 from typing import ClassVar from unittest.mock import MagicMock  import ipytest import pytest from mirascope.core import BaseMessageParam, openai, prompt_template from pydantic import BaseModel, ConfigDict  ipytest.autoconfig(run_in_thread=True)   class Librarian(BaseModel):     con: ClassVar[sqlite3.Connection] = sqlite3.connect(\"database.db\")     messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []      model_config = ConfigDict(arbitrary_types_allowed=True)      def _run_query(self, query: str) -&gt; str:         \"\"\"A SELECT query to run.\"\"\"         print(query)         try:             cursor = self.con.cursor()             cursor.execute(query)             res = cursor.fetchall()             return str(res)         except sqlite3.Error as e:             return str(e)      def _execute_query(self, query: str) -&gt; str:         \"\"\"An INSERT, UPDATE, or DELETE query to execute.\"\"\"         print(query)         try:             cursor = self.con.cursor()             cursor.execute(query)             rows_affected = cursor.rowcount             self.con.commit()             if rows_affected &gt; 0:                 return f\"Query executed successfully, {rows_affected} row(s) were updated/inserted.\"             else:                 return \"No rows were updated/inserted.\"         except sqlite3.Error as e:             print(e)             return str(e)      @openai.call(model=\"gpt-4o-mini\", stream=True)     @prompt_template(         \"\"\"         SYSTEM:         You are a friendly and knowledgeable librarian named Mira. Your role is to          assist patrons with their queries, recommend books,          and provide information on a wide range of topics.          Personality:             - Warm and approachable, always ready with a kind word             - Patient and understanding, especially with those who are hesitant or confused             - Enthusiastic about books and learning             - Respectful of all patrons, regardless of their background or level of knowledge          Services:             - Keep track of patrons' reading lists using a SQLite database. Assume that the user is non technical and will ask you         questions in plain English.             - Recommend books based on the user's preferences         Your task is to write a query based on the user's request.          The database schema is as follows:          TABLE ReadingList (             id INTEGER PRIMARY KEY AUTOINCREMENT,             title TEXT NOT NULL,             status TEXT CHECK(status IN ('Not Started', 'In Progress', 'Complete')) NOT NULL,             rating INTEGER CHECK(rating &gt;= 1 AND rating &lt;= 5),         );          You must interpret the user's request and write the appropriate SQL query to         pass in the tools.          Example interactions:             1. Select                 - USER: \"Show me all books.\"                 - ASSISTANT: \"SELECT * FROM ReadingList;\"             2. Insert                 - USER: \"Add Gone with the Wind to my reading list.\"                 - ASSISTANT: \"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\"             3. Update                 - USER: \"I just finished Gone with the Wind, can you update the status, and give it 5 stars??\"                 - ASSISTANT: \"UPDATE ReadingList SET status = 'Complete' and rating = 5 WHERE title = 'Gone with the Wind';\"             4. Delete                 - USER: \"Remove Gone with the Wind from my reading list.\"                 - ASSISTANT: \"DELETE FROM ReadingList WHERE title = 'Gone with the Wind';\"          If field are not mentioned, omit them from the query.         All queries must end with a semicolon.          You have access to the following tools:         - `_run_query`: When user asks for recommendations, you can use this tool to see what they have read.         - `_execute_query`: Use the query generated to execute an              INSERT, UPDATE, or DELETE query.          You must use these tools to interact with the database.          MESSAGES: {self.messages}         USER: {query}         \"\"\"     )     async def _stream(self, query: str) -&gt; openai.OpenAIDynamicConfig:         return {\"tools\": [self._run_query, self._execute_query]}      async def _step(self, question: str):         response = await self._stream(question)         tools_and_outputs = []         async for chunk, tool in response:             if tool:                 tools_and_outputs.append((tool, tool.call()))             else:                 print(chunk.content, end=\"\", flush=True)         if response.user_message_param:             self.messages.append(response.user_message_param)         self.messages.append(response.message_param)         if tools_and_outputs:             self.messages += response.tool_message_params(tools_and_outputs)             await self._step(\"\")      async def run(self):         while True:             question = input(\"(User): \")             if question == \"exit\":                 break             print(\"(Assistant): \", end=\"\", flush=True)             await self._step(question)             print()   @pytest.fixture def mock_librarian():     class MockLibrarian(Librarian):         con: ClassVar[sqlite3.Connection] = MagicMock()      return MockLibrarian()   @pytest.mark.asyncio @pytest.mark.parametrize(     \"select_query\",     [         \"Get all books\",         \"Get every single book\",         \"Show me all books\",         \"List all books\",         \"Display all books\",     ], ) async def test_select_query(select_query: str, mock_librarian: Librarian):     response = await mock_librarian._stream(select_query)     async for _, tool in response:         query = tool.args.get(\"query\", \"\") if tool else \"\"         assert query == \"SELECT * FROM ReadingList;\"   @pytest.mark.asyncio @pytest.mark.parametrize(     \"insert_query\",     [         \"Please add Gone with the Wind to my reading list\",         \"You recently recommended Gone with the Wind, can you add it to my reading list.\",     ], ) async def test_insert_query(insert_query: str, mock_librarian: Librarian):     response = await mock_librarian._stream(insert_query)     async for _, tool in response:         query = tool.args.get(\"query\", \"\") if tool else \"\"         assert (             query             == \"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\"         )   @pytest.mark.asyncio @pytest.mark.parametrize(     \"update_query\",     [         \"Can you mark Gone with the Wind as read?\",         \"I just finished Gone with the Wind, can you update the status?\",     ], ) async def test_update_query(update_query: str, mock_librarian: Librarian):     response = await mock_librarian._stream(update_query)     async for _, tool in response:         query = tool.args.get(\"query\", \"\") if tool else \"\"         assert (             query             == \"UPDATE ReadingList SET status = 'Complete' WHERE title = 'Gone with the Wind';\"         )   @pytest.mark.asyncio @pytest.mark.parametrize(     \"delete_query\",     [         \"Can you remove Gone with the Wind from my reading list?\",         \"Can you delete Gone with the Wind?\",     ], ) async def test_delete_query(delete_query: str, mock_librarian: Librarian):     response = await mock_librarian._stream(delete_query)     async for _, tool in response:         query = tool.args.get(\"query\", \"\") if tool else \"\"         assert query == \"DELETE FROM ReadingList WHERE title = 'Gone with the Wind';\"   ipytest.run() <p>The golden dataset serves as our test fixtures, allowing us to verify basic text-to-SQL conversions and ensure our Agent maintains this functionality.</p> <p>As we expand our tools or enhance our prompt, we should consistently implement additional prompt regression tests to maintain quality and functionality.</p> <p>Tip</p> <p> SQL queries can get complex so be sure to add very specific tests, especially for mutable operations like INSERTS, UPDATES, and DELETES. </p> In\u00a0[5]: Copied! <pre>from openai.types.chat import (\n    ChatCompletionAssistantMessageParam,\n    ChatCompletionUserMessageParam,\n)\n\n\n@pytest.mark.asyncio\nasync def test_conversation(mock_librarian: Librarian):\n    mock_librarian.messages = [\n        ChatCompletionUserMessageParam(\n            role=\"user\", content=\"Can you recommend me a fantasy book?\"\n        ),\n        ChatCompletionAssistantMessageParam(\n            role=\"assistant\",\n            content=\"I would recommend 'The Name of the Wind' by Patrick Rothfuss. It\u2019s the first book in 'The Kingkiller Chronicle' series and features a beautifully written story about a gifted young man who grows up to be a legendary figure. It's filled with magic, adventure, and rich character development. I believe you'll enjoy it! Would you like to add it to your reading list?\",\n        ),\n    ]\n    response = await mock_librarian._stream(\"Can you add it to my reading list?\")\n    async for _, tool in response:\n        query = tool.args.get(\"query\", \"\") if tool else \"\"\n        assert (\n            query\n            == \"INSERT INTO ReadingList (title, status) VALUES ('The Name of the Wind', 'Not Started');\"\n        )\n\n\nipytest.run()\n</pre> from openai.types.chat import (     ChatCompletionAssistantMessageParam,     ChatCompletionUserMessageParam, )   @pytest.mark.asyncio async def test_conversation(mock_librarian: Librarian):     mock_librarian.messages = [         ChatCompletionUserMessageParam(             role=\"user\", content=\"Can you recommend me a fantasy book?\"         ),         ChatCompletionAssistantMessageParam(             role=\"assistant\",             content=\"I would recommend 'The Name of the Wind' by Patrick Rothfuss. It\u2019s the first book in 'The Kingkiller Chronicle' series and features a beautifully written story about a gifted young man who grows up to be a legendary figure. It's filled with magic, adventure, and rich character development. I believe you'll enjoy it! Would you like to add it to your reading list?\",         ),     ]     response = await mock_librarian._stream(\"Can you add it to my reading list?\")     async for _, tool in response:         query = tool.args.get(\"query\", \"\") if tool else \"\"         assert (             query             == \"INSERT INTO ReadingList (title, status) VALUES ('The Name of the Wind', 'Not Started');\"         )   ipytest.run() <pre>/Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages/pytest_asyncio/plugin.py:208: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n</pre> <pre>............                                                                                 [100%]\n========================================= warnings summary =========================================\n../../../.venv_notebook/lib/python3.12/site-packages/_pytest/config/__init__.py:1277\n  /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages/_pytest/config/__init__.py:1277: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n    self._mark_plugins_for_rewrite(hook)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n12 passed, 1 warning in 13.13s\n</pre> Out[5]: <pre>&lt;ExitCode.OK: 0&gt;</pre> <p>We expect that the LLM understands what \"it\" means in this case. If history was not working, then the LLM would not know what \"it\" refers to and would not be able to use the tool properly. This is particular useful because we can now test user flows easily to make sure certain flows behave properly.</p> <p>Next, we'll be taking a look at using LLMs as judges for testing the tone we expect the LLM to take.</p> In\u00a0[7]: Copied! <pre>import asyncio\n\nfrom mirascope.core import BasePrompt, anthropic\nfrom openai.types.chat import ChatCompletionToolMessageParam\nfrom pydantic import Field\n\n\n@prompt_template(\n    \"\"\"\n    Evaluate the friendliness of generated text on a continuous scale from 1 to 5.\n    A generation is friendly (score &gt; 1) if it contains content that is:\n\n    - Warm, welcoming, and positive in tone\n    - Respectful, considerate, and empathetic towards others\n    - Encouraging, supportive, or promotes kindness\n    - Inclusive and mindful of diverse perspectives\n\n    Provide a brief reasoning for your assigned score.\n    Make sure to highlight the specific aspects that impacted your score.\n\n    Use the following friendliness scale:\n\n    1 - No friendliness; cold, distant, or neutral tone\n    2 - Mild friendliness; slightly warm or polite language\n    3 - Moderate friendliness; noticeably kind and welcoming content\n    4 - High friendliness; very warm, supportive, and empathetic language\n    5 - Extreme friendliness; exceptionally kind, caring, and uplifting content\n\n    Input Query: {input_query}\n    Output Content: {output_content}\n    \"\"\"\n)\nclass FriendlinessEvaluator(BasePrompt):\n    input_query: str\n    output_content: str\n\n\n@pytest.mark.asyncio\nasync def test_friendliness(mock_librarian: Librarian):\n    input_query = \"Can you add gone with the wind\"\n    mock_librarian.messages = [\n        ChatCompletionUserMessageParam(role=\"user\", content=input_query),\n        ChatCompletionAssistantMessageParam(\n            role=\"assistant\",\n            tool_calls=[\n                {\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"arguments\": \"{\\\"query\\\":\\\"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\\\"}\",\n                        \"name\": \"_execute_query\",\n                    },\n                    \"id\": \"1\",\n                }\n            ],\n        ),\n        ChatCompletionToolMessageParam(\n            role=\"tool\",\n            content=\"Query executed successfully, 1 row(s) were updated/inserted.\",\n            tool_call_id=\"1\",\n        ),\n    ]\n    response = await mock_librarian._stream(\"\")\n    output_content = \"\"\n    async for chunk, _ in response:\n        output_content += chunk.content\n    prompt = FriendlinessEvaluator(\n        input_query=input_query, output_content=output_content\n    )\n\n    class Eval(BaseModel):\n        score: float = Field(..., description=\"A score between [1.0, 5.0]\")\n        reasoning: str = Field(..., description=\"The reasoning for the score\")\n\n    async def run_evals() -&gt; list[Eval]:\n        judges = [\n            openai.call(\n                \"gpt-4o-mini\",\n                response_model=Eval,\n                json_mode=True,\n            ),\n            anthropic.call(\n                \"claude-3-5-sonnet-20240620\",\n                response_model=Eval,\n                json_mode=True,\n            ),\n        ]\n        calls = [prompt.run_async(judge) for judge in judges]\n        return await asyncio.gather(*calls)\n\n    evals = await run_evals()\n    total_score = sum(eval.score for eval in evals)\n    average_score = total_score / len(evals)\n    assert average_score &gt; 2\n\n\nipytest.run()\n</pre> import asyncio  from mirascope.core import BasePrompt, anthropic from openai.types.chat import ChatCompletionToolMessageParam from pydantic import Field   @prompt_template(     \"\"\"     Evaluate the friendliness of generated text on a continuous scale from 1 to 5.     A generation is friendly (score &gt; 1) if it contains content that is:      - Warm, welcoming, and positive in tone     - Respectful, considerate, and empathetic towards others     - Encouraging, supportive, or promotes kindness     - Inclusive and mindful of diverse perspectives      Provide a brief reasoning for your assigned score.     Make sure to highlight the specific aspects that impacted your score.      Use the following friendliness scale:      1 - No friendliness; cold, distant, or neutral tone     2 - Mild friendliness; slightly warm or polite language     3 - Moderate friendliness; noticeably kind and welcoming content     4 - High friendliness; very warm, supportive, and empathetic language     5 - Extreme friendliness; exceptionally kind, caring, and uplifting content      Input Query: {input_query}     Output Content: {output_content}     \"\"\" ) class FriendlinessEvaluator(BasePrompt):     input_query: str     output_content: str   @pytest.mark.asyncio async def test_friendliness(mock_librarian: Librarian):     input_query = \"Can you add gone with the wind\"     mock_librarian.messages = [         ChatCompletionUserMessageParam(role=\"user\", content=input_query),         ChatCompletionAssistantMessageParam(             role=\"assistant\",             tool_calls=[                 {                     \"type\": \"function\",                     \"function\": {                         \"arguments\": \"{\\\"query\\\":\\\"INSERT INTO ReadingList (title, status) VALUES ('Gone with the Wind', 'Not Started');\\\"}\",                         \"name\": \"_execute_query\",                     },                     \"id\": \"1\",                 }             ],         ),         ChatCompletionToolMessageParam(             role=\"tool\",             content=\"Query executed successfully, 1 row(s) were updated/inserted.\",             tool_call_id=\"1\",         ),     ]     response = await mock_librarian._stream(\"\")     output_content = \"\"     async for chunk, _ in response:         output_content += chunk.content     prompt = FriendlinessEvaluator(         input_query=input_query, output_content=output_content     )      class Eval(BaseModel):         score: float = Field(..., description=\"A score between [1.0, 5.0]\")         reasoning: str = Field(..., description=\"The reasoning for the score\")      async def run_evals() -&gt; list[Eval]:         judges = [             openai.call(                 \"gpt-4o-mini\",                 response_model=Eval,                 json_mode=True,             ),             anthropic.call(                 \"claude-3-5-sonnet-20240620\",                 response_model=Eval,                 json_mode=True,             ),         ]         calls = [prompt.run_async(judge) for judge in judges]         return await asyncio.gather(*calls)      evals = await run_evals()     total_score = sum(eval.score for eval in evals)     average_score = total_score / len(evals)     assert average_score &gt; 2   ipytest.run() <pre>/Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages/pytest_asyncio/plugin.py:208: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n</pre> <pre>.............                                                                                [100%]\n========================================= warnings summary =========================================\n../../../.venv_notebook/lib/python3.12/site-packages/_pytest/config/__init__.py:1277\n  /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages/_pytest/config/__init__.py:1277: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n    self._mark_plugins_for_rewrite(hook)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n13 passed, 1 warning in 17.07s\n</pre> Out[7]: <pre>&lt;ExitCode.OK: 0&gt;</pre> <p>Similar to the conversations test, we simulate a user interaction by passing a messages array to a mock librarian. To assess the friendliness of this response, we utilize a custom <code>FriendlinessEvaluator</code>, which is a Mirascope <code>BasePrompt</code> that scores friendliness on a scale from 1 to 5 based on specific criteria. Leveraging Mirascope's model-agnostic capabilities, we send this evaluation prompt to both GPT-4 and Claude 3.5 Sonnet and average out these results.</p> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Experiment with the prompt, add specific examples to the <code>FriendlinessEvaluator</code> for more accurate evaluations</li> <li>Experiment with the call prompt, add more examples of friendly conversation or persona.</li> <li>Add evaluations for retrieval if you're using RAG, or evaluations for business metrics. e</li> </ul>"},{"location":"tutorials/evals/evaluating_sql_agent/#evaluating-generating-sql-with-llm","title":"Evaluating Generating SQL with LLM\u00b6","text":"<p>In this recipe, we will be using taking our SQL Agent example and running evaluations on LLM call. We will be exploring various different evaluations we can run to ensure quality and expected behavior.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Tools</li> <li>Async</li> <li>Evals</li> </ul> <p>Check out the SQL Agent Cookbook</p> <p> We will be using our <code>LibrarianAgent</code> for our evaluations. For a detailed explanation regarding this code snippet, refer to the SQL Agent Cookbook. </p>"},{"location":"tutorials/evals/evaluating_sql_agent/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/evals/evaluating_sql_agent/#evaluating-the-prompt-using-a-golden-dataset","title":"Evaluating the prompt using a golden dataset\u00b6","text":"<p>One effective approach is to establish a golden dataset that the prompt must successfully pass. We'll leverage <code>pytest</code> for this purpose, as it offers numerous testing conveniences.</p>"},{"location":"tutorials/evals/evaluating_sql_agent/#evaluating-conversations","title":"Evaluating conversations\u00b6","text":"<p>With the above techniques, we've shown how to evaluate simple input and output pairs, but we also want to test conversations with history. We can define some messages we want the LLM to have and get some expected output, like so:</p>"},{"location":"tutorials/evals/evaluating_sql_agent/#llms-as-judges","title":"LLMs as judges\u00b6","text":"<p>With tools, it's easy to do an equality comparison to check that the LLM outputs proper SQL, but tone is a bit more complex. Thankfully, using LLMs makes this much simpler. We can use the LLM to evaluate the <code>friendliness</code> of the LLM and adjust the prompt as necessary.</p>"},{"location":"tutorials/evals/evaluating_web_search_agent/","title":"Evaluating Web Search Agent with LLM","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n# for web search functionality\n!pip install requests beautifulsoup4 duckduckgo-search ipytest\n</pre> !pip install \"mirascope[openai]\" # for web search functionality !pip install requests beautifulsoup4 duckduckgo-search ipytest In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[\u00a0]: Copied! <pre>import re\nfrom datetime import datetime\n\nimport ipytest\nimport pytest\nimport requests\nfrom bs4 import BeautifulSoup\nfrom duckduckgo_search import DDGS\nfrom mirascope.core import BaseMessageParam, openai, prompt_template\nfrom pydantic import BaseModel\n\nipytest.autoconfig(run_in_thread=True)\n\n\ndef extract_content(url: str) -&gt; str:\n    \"\"\"Extract the main content from a webpage.\n\n    Args:\n        url: The URL of the webpage to extract the content from.\n\n    Returns:\n        The extracted content as a string.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n\n        soup = BeautifulSoup(response.content, \"html.parser\")\n\n        unwanted_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]\n        for tag in unwanted_tags:\n            for element in soup.find_all(tag):\n                element.decompose()\n\n        main_content = (\n            soup.find(\"main\")\n            or soup.find(\"article\")\n            or soup.find(\"div\", class_=re.compile(\"content|main\"))\n        )\n\n        if main_content:\n            text = main_content.get_text(separator=\"\\n\", strip=True)\n        else:\n            text = soup.get_text(separator=\"\\n\", strip=True)\n\n        lines = (line.strip() for line in text.splitlines())\n        return \"\\n\".join(line for line in lines if line)\n    except Exception as e:\n        return f\"{type(e)}: Failed to extract content from URL {url}\"\n\n\nclass WebAssistant(BaseModel):\n    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n    search_history: list[str] = []\n    max_results_per_query: int = 2\n\n    def _web_search(self, queries: list[str]) -&gt; str:\n        \"\"\"Performs web searches for given queries and returns URLs.\n\n        Args:\n            queries: List of search queries.\n\n        Returns:\n            str: Newline-separated URLs from search results or error messages.\n\n        Raises:\n            Exception: If web search fails entirely.\n        \"\"\"\n        try:\n            urls = []\n            for query in queries:\n                results = DDGS(proxies=None).text(\n                    query, max_results=self.max_results_per_query\n                )\n\n                for result in results:\n                    link = result[\"href\"]\n                    try:\n                        urls.append(link)\n                    except Exception as e:\n                        urls.append(\n                            f\"{type(e)}: Failed to parse content from URL {link}\"\n                        )\n                self.search_history.append(query)\n            return \"\\n\\n\".join(urls)\n\n        except Exception as e:\n            return f\"{type(e)}: Failed to search the web for text\"\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. Your task is to answer the user's question using the provided tools.\n        The current date is {current_date}.\n\n        You have access to the following tools:\n        - `_web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:\n            1. There is a previous search context: {self.search_history}\n            2. There is the current user query: {question}\n            3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query. \n                Even if the connection isn't immediately clear, consider how they might be related.\n        - `extract_content`: Parse the content of a webpage.\n\n        When calling the `_web_search` tool, the `body` is simply the body of the search\n        result. You MUST then call the `extract_content` tool to get the actual content\n        of the webpage. It is up to you to determine which search results to parse.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness based on the context of the user's query.\n\n        MESSAGES: {self.messages}\n        USER: {question}\n        \"\"\"\n    )\n    async def _stream(self, question: str) -&gt; openai.OpenAIDynamicConfig:\n        return {\n            \"tools\": [self._web_search, extract_content],\n            \"computed_fields\": {\n                \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            },\n        }\n\n    async def _step(self, question: str):\n        print(self.messages)\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                print(f\"using {tool._name()} tool with args: {tool.args}\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._step(question)\n            print()\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\n    \"user_query\",\n    [\n        \"How is the weather in New York?\",\n        \"What is the capital of France?\",\n        \"Who is the president of the United States?\",\n        \"What is the population of India?\",\n        \"What is an apple?\",\n    ],\n)\nasync def test_web_search(user_query: str):\n    \"\"\"Tests that the web search agent always uses the web search tool.\"\"\"\n    web_assistant = WebAssistant()\n    response = await web_assistant._stream(user_query)\n    tools = []\n    async for _, tool in response:\n        if tool:\n            tools.append(tool)\n    assert len(tools) == 1 and tools[0]._name() == \"_web_search\"\n\n\nipytest.run()\n</pre> import re from datetime import datetime  import ipytest import pytest import requests from bs4 import BeautifulSoup from duckduckgo_search import DDGS from mirascope.core import BaseMessageParam, openai, prompt_template from pydantic import BaseModel  ipytest.autoconfig(run_in_thread=True)   def extract_content(url: str) -&gt; str:     \"\"\"Extract the main content from a webpage.      Args:         url: The URL of the webpage to extract the content from.      Returns:         The extracted content as a string.     \"\"\"     try:         response = requests.get(url, timeout=5)          soup = BeautifulSoup(response.content, \"html.parser\")          unwanted_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]         for tag in unwanted_tags:             for element in soup.find_all(tag):                 element.decompose()          main_content = (             soup.find(\"main\")             or soup.find(\"article\")             or soup.find(\"div\", class_=re.compile(\"content|main\"))         )          if main_content:             text = main_content.get_text(separator=\"\\n\", strip=True)         else:             text = soup.get_text(separator=\"\\n\", strip=True)          lines = (line.strip() for line in text.splitlines())         return \"\\n\".join(line for line in lines if line)     except Exception as e:         return f\"{type(e)}: Failed to extract content from URL {url}\"   class WebAssistant(BaseModel):     messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []     search_history: list[str] = []     max_results_per_query: int = 2      def _web_search(self, queries: list[str]) -&gt; str:         \"\"\"Performs web searches for given queries and returns URLs.          Args:             queries: List of search queries.          Returns:             str: Newline-separated URLs from search results or error messages.          Raises:             Exception: If web search fails entirely.         \"\"\"         try:             urls = []             for query in queries:                 results = DDGS(proxies=None).text(                     query, max_results=self.max_results_per_query                 )                  for result in results:                     link = result[\"href\"]                     try:                         urls.append(link)                     except Exception as e:                         urls.append(                             f\"{type(e)}: Failed to parse content from URL {link}\"                         )                 self.search_history.append(query)             return \"\\n\\n\".join(urls)          except Exception as e:             return f\"{type(e)}: Failed to search the web for text\"      @openai.call(model=\"gpt-4o-mini\", stream=True)     @prompt_template(         \"\"\"         SYSTEM:         You are an expert web searcher. Your task is to answer the user's question using the provided tools.         The current date is {current_date}.          You have access to the following tools:         - `_web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:             1. There is a previous search context: {self.search_history}             2. There is the current user query: {question}             3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query.                  Even if the connection isn't immediately clear, consider how they might be related.         - `extract_content`: Parse the content of a webpage.          When calling the `_web_search` tool, the `body` is simply the body of the search         result. You MUST then call the `extract_content` tool to get the actual content         of the webpage. It is up to you to determine which search results to parse.          Once you have gathered all of the information you need, generate a writeup that         strikes the right balance between brevity and completeness based on the context of the user's query.          MESSAGES: {self.messages}         USER: {question}         \"\"\"     )     async def _stream(self, question: str) -&gt; openai.OpenAIDynamicConfig:         return {             \"tools\": [self._web_search, extract_content],             \"computed_fields\": {                 \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")             },         }      async def _step(self, question: str):         print(self.messages)         response = await self._stream(question)         tools_and_outputs = []         async for chunk, tool in response:             if tool:                 print(f\"using {tool._name()} tool with args: {tool.args}\")                 tools_and_outputs.append((tool, tool.call()))             else:                 print(chunk.content, end=\"\", flush=True)         if response.user_message_param:             self.messages.append(response.user_message_param)         self.messages.append(response.message_param)         if tools_and_outputs:             self.messages += response.tool_message_params(tools_and_outputs)             await self._step(\"\")      async def run(self):         while True:             question = input(\"(User): \")             if question == \"exit\":                 break             print(\"(Assistant): \", end=\"\", flush=True)             await self._step(question)             print()   @pytest.mark.asyncio @pytest.mark.parametrize(     \"user_query\",     [         \"How is the weather in New York?\",         \"What is the capital of France?\",         \"Who is the president of the United States?\",         \"What is the population of India?\",         \"What is an apple?\",     ], ) async def test_web_search(user_query: str):     \"\"\"Tests that the web search agent always uses the web search tool.\"\"\"     web_assistant = WebAssistant()     response = await web_assistant._stream(user_query)     tools = []     async for _, tool in response:         if tool:             tools.append(tool)     assert len(tools) == 1 and tools[0]._name() == \"_web_search\"   ipytest.run() <p>It's recommended to continually expand our golden dataset until we can confidently assert that the LLM uses web search when appropriate.</p> In\u00a0[\u00a0]: Copied! <pre>test_extract_content_messages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"queries\":[\"capital of France\",\"capital city of France\",\"France\",\"Paris\",\"France capital\"]}',\n                    \"name\": \"_web_search\",\n                },\n                \"id\": \"call_ddhSpVcNeDg9bLz4C7HEw85P\",\n            }\n        ],\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": \"https://en.wikipedia.org/wiki/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://en.wikipedia.org/wiki/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://en.wikipedia.org/wiki/France\\n\\nhttps://www.britannica.com/place/France\\n\\nhttps://en.wikipedia.org/wiki/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://en.wikipedia.org/wiki/Paris\",\n        \"tool_call_id\": \"call_ddhSpVcNeDg9bLz4C7HEw85P\",\n        \"name\": \"_web_search\",\n    },\n]\n\n\n@pytest.mark.asyncio\nasync def test_extract_content():\n    \"\"\"Tests that the extract content tool gets called once.\"\"\"\n    user_query = \"What is the capital of France?\"\n    web_assistant = WebAssistant(messages=test_extract_content_messages)\n    response = await web_assistant._stream(user_query)\n    tools = []\n    async for _, tool in response:\n        if tool:\n            tools.append(tool)\n    assert len(tools) == 1 and tools[0]._name() == \"extract_content\"\n\n\nipytest.run()\n</pre> test_extract_content_messages = [     {\"role\": \"user\", \"content\": \"What is the capital of France?\"},     {         \"role\": \"assistant\",         \"content\": \"\",         \"tool_calls\": [             {                 \"type\": \"function\",                 \"function\": {                     \"arguments\": '{\"queries\":[\"capital of France\",\"capital city of France\",\"France\",\"Paris\",\"France capital\"]}',                     \"name\": \"_web_search\",                 },                 \"id\": \"call_ddhSpVcNeDg9bLz4C7HEw85P\",             }         ],     },     {         \"role\": \"tool\",         \"content\": \"https://en.wikipedia.org/wiki/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://en.wikipedia.org/wiki/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://en.wikipedia.org/wiki/France\\n\\nhttps://www.britannica.com/place/France\\n\\nhttps://en.wikipedia.org/wiki/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://www.britannica.com/place/Paris\\n\\nhttps://en.wikipedia.org/wiki/Paris\",         \"tool_call_id\": \"call_ddhSpVcNeDg9bLz4C7HEw85P\",         \"name\": \"_web_search\",     }, ]   @pytest.mark.asyncio async def test_extract_content():     \"\"\"Tests that the extract content tool gets called once.\"\"\"     user_query = \"What is the capital of France?\"     web_assistant = WebAssistant(messages=test_extract_content_messages)     response = await web_assistant._stream(user_query)     tools = []     async for _, tool in response:         if tool:             tools.append(tool)     assert len(tools) == 1 and tools[0]._name() == \"extract_content\"   ipytest.run() <p>For brevity, we've included just one example from our golden dataset, as the full messages array would be too lengthy to show.</p> <p>Now that we have our simple tests, let's take a look at a more complex evaluation-based test.</p> In\u00a0[5]: Copied! <pre>from mirascope.core import anthropic\nfrom pydantic import Field\n\n\nclass ContextRelevant(BaseModel):\n    is_context_relevant: bool = Field(\n        description=\"Whether the LLM-generated query is context-relevant\"\n    )\n    explanation: str = Field(description=\"The reasoning for the context relevance\")\n\n\n@anthropic.call(\n    model=\"claude-3-5-sonnet-20240620\", response_model=ContextRelevant, json_mode=True\n)\n@prompt_template(\n    \"\"\"\n    Given:\n\n    Search history: {search_history}\n    User query: {user_query}\n    LLM-generated query: {llm_query}\n\n    Evaluate if the LLM-generated query is context-relevant using the following criteria:\n\n    Bridging Relevance:\n\n    Does {llm_query} effectively bridge the gap between {search_history} and {user_query}?\n    Does it incorporate elements from both {search_history} and {user_query} meaningfully?\n\n\n    Intent Preservation:\n\n    Does {llm_query} maintain the apparent intent of {user_query}?\n    Does it also consider the broader context established by {search_history}?\n\n\n    Topical Consistency:\n\n    Is {llm_query} consistent with the overall topic or theme of {search_history}?\n    If there's a shift in topic from {search_history} to {user_query}, does {llm_query} handle this transition logically?\n\n\n    Specificity and Relevance:\n\n    Is {llm_query} specific enough to be useful, considering both {search_history} and {user_query}?\n    Does it avoid being overly broad or tangential?\n\n\n    Contextual Enhancement:\n\n    Does {llm_query} add value by incorporating relevant context from {search_history}?\n    Does it expand on {user_query} in a way that's likely to yield more relevant results?\n\n\n    Handling of Non-Sequiturs:\n\n    If {user_query} is completely unrelated to {search_history}, does {llm_query} appropriately pivot to the new topic?\n    Does it still attempt to maintain any relevant context from {search_history}, if possible?\n\n\n    Semantic Coherence:\n\n    Do the terms and concepts in {llm_query} relate logically to both {search_history} and {user_query}?\n    Is there a clear semantic path from {search_history} through {user_query} to {llm_query}?\n\n\n\n    Evaluation:\n\n    Assess {llm_query} against each criterion, noting how well it performs.\n    Consider the balance between maintaining context from {search_history} and addressing the specific intent of {user_query}.\n    Evaluate how {llm_query} handles any topic shift between {search_history} and {user_query}.\n\n    Provide a final assessment of whether {llm_query} is context-relevant, with a brief explanation of your reasoning.\n    \"\"\"\n)\nasync def check_context_relevance(\n    search_history: list[str], user_query: str, llm_query: str\n): ...\n</pre> from mirascope.core import anthropic from pydantic import Field   class ContextRelevant(BaseModel):     is_context_relevant: bool = Field(         description=\"Whether the LLM-generated query is context-relevant\"     )     explanation: str = Field(description=\"The reasoning for the context relevance\")   @anthropic.call(     model=\"claude-3-5-sonnet-20240620\", response_model=ContextRelevant, json_mode=True ) @prompt_template(     \"\"\"     Given:      Search history: {search_history}     User query: {user_query}     LLM-generated query: {llm_query}      Evaluate if the LLM-generated query is context-relevant using the following criteria:      Bridging Relevance:      Does {llm_query} effectively bridge the gap between {search_history} and {user_query}?     Does it incorporate elements from both {search_history} and {user_query} meaningfully?       Intent Preservation:      Does {llm_query} maintain the apparent intent of {user_query}?     Does it also consider the broader context established by {search_history}?       Topical Consistency:      Is {llm_query} consistent with the overall topic or theme of {search_history}?     If there's a shift in topic from {search_history} to {user_query}, does {llm_query} handle this transition logically?       Specificity and Relevance:      Is {llm_query} specific enough to be useful, considering both {search_history} and {user_query}?     Does it avoid being overly broad or tangential?       Contextual Enhancement:      Does {llm_query} add value by incorporating relevant context from {search_history}?     Does it expand on {user_query} in a way that's likely to yield more relevant results?       Handling of Non-Sequiturs:      If {user_query} is completely unrelated to {search_history}, does {llm_query} appropriately pivot to the new topic?     Does it still attempt to maintain any relevant context from {search_history}, if possible?       Semantic Coherence:      Do the terms and concepts in {llm_query} relate logically to both {search_history} and {user_query}?     Is there a clear semantic path from {search_history} through {user_query} to {llm_query}?        Evaluation:      Assess {llm_query} against each criterion, noting how well it performs.     Consider the balance between maintaining context from {search_history} and addressing the specific intent of {user_query}.     Evaluate how {llm_query} handles any topic shift between {search_history} and {user_query}.      Provide a final assessment of whether {llm_query} is context-relevant, with a brief explanation of your reasoning.     \"\"\" ) async def check_context_relevance(     search_history: list[str], user_query: str, llm_query: str ): ... <p>We use an LLM to evaluate context-awareness and define a series of questions the LLM will answer to determine if the <code>llm_query</code> generated makes sense given the <code>user_query</code>.</p> In\u00a0[7]: Copied! <pre>async def run(search_history: list[str], user_query: str, llm_query: str):\n    return await check_context_relevance(search_history, user_query, llm_query)\n\n\nsearch_history = [\"Best beaches in Thailand\", \"Thai cuisine must-try dishes\"]\nuser_query = \"How to book flights?\"\nllm_query = \"How to book flights to Thailand for a beach and culinary vacation\"\n\nawait run(search_history, user_query, llm_query)\n</pre> async def run(search_history: list[str], user_query: str, llm_query: str):     return await check_context_relevance(search_history, user_query, llm_query)   search_history = [\"Best beaches in Thailand\", \"Thai cuisine must-try dishes\"] user_query = \"How to book flights?\" llm_query = \"How to book flights to Thailand for a beach and culinary vacation\"  await run(search_history, user_query, llm_query) Out[7]: <pre>ContextRelevant(is_context_relevant=True, explanation=\"The LLM-generated query 'How to book flights to Thailand for a beach and culinary vacation' effectively bridges the gap between the search history and the user query. It maintains the intent of booking flights while incorporating the context of Thai beaches and cuisine from the search history. The query is specific, relevant, and adds value by combining the user's immediate need (booking flights) with their apparent interest in Thai travel. It handles the topic shift smoothly and creates a semantically coherent link between all elements.\")</pre> <p>Now let's update our <code>llm_query</code>:</p> In\u00a0[8]: Copied! <pre>llm_query = \"General steps for booking flights online\"\nawait run(search_history, user_query, llm_query)\n</pre> llm_query = \"General steps for booking flights online\" await run(search_history, user_query, llm_query) Out[8]: <pre>ContextRelevant(is_context_relevant=False, explanation=\"The LLM-generated query 'General steps for booking flights online' focuses solely on the user's new query about booking flights, without incorporating any context from the previous search history about Thai beaches and cuisine. While it addresses the user's immediate question, it fails to bridge the contextual gap or maintain topical consistency with the established travel theme. A more context-relevant query might have been 'How to book flights to Thailand for a beach and culinary vacation', which would have preserved the intent while incorporating the previous search context.\")</pre> <p>We can verify that the <code>llm_query</code> does not mention anything related to the <code>search_history</code>, and therefore is properly labeled as context irrelevant.</p> <p>However, it's important to note that not all user queries need to be context-relevant to previous searches. Users may intentionally shift topics or ask unrelated questions in succession, which is a natural part of chatbot interactions.</p> In\u00a0[\u00a0]: Copied! <pre>test_conversation_messages = [\n    {\"role\": \"user\", \"content\": \"I am a SWE looking for a LLM dev tool library\"},\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"queries\":[\"best LLM development tools\",\"top libraries for LLM development\",\"LLM libraries for software engineers\",\"LLM dev tools for machine learning\",\"most popular libraries for LLM development\"]}',\n                    \"name\": \"_web_search\",\n                },\n                \"id\": \"call_1\",\n            }\n        ],\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": \"https://www.blog.aiport.tech/p/top-9-libraries-to-accelerate-llm\\n\\nhttps://github.com/tensorchord/awesome-llmops\\n\\nhttps://www.blog.aiport.tech/p/top-9-libraries-to-accelerate-llm\\n\\nhttps://medium.com/pythonforall/essential-python-libraries-for-llms-and-application-development-in-2024-17c64b672421\\n\\nhttps://www.datacamp.com/blog/top-open-source-llms\\n\\nhttps://machinelearningmastery.com/5-essential-free-tools-getting-started-llms/\\n\\nhttps://github.com/princeton-nlp/swe-agent\\n\\nhttps://arxiv.org/html/2407.01489v1\\n\\nhttps://www.datacamp.com/blog/top-open-source-llms\\n\\nhttps://llmmodels.org/blog/top-10-open-source-llm-frameworks-2024/\",\n        \"tool_call_id\": \"call_1\",\n        \"name\": \"_web_search\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://research.aimultiple.com/llmops-tools/\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_UXnnadcCnki8qvCxrzRI1fXA\",\n            },\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://www.techradar.com/computing/artificial-intelligence/best-large-language-models-llms-for-coding\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_S0OnNQqxtPH5HtDb1buzjedV\",\n            },\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://www.blog.aipo.rt.tech/p/top-9-libraries-to-accelerate-llm\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_aCyaCFXUWMWloDkETTrxHyoJ\",\n            },\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://medium.com/pythonforall/essential-python-libraries-for-llms-and-application-development-in-2024-17c64b672421\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_71e9AkvKjIuEp3QceqO4DCUK\",\n            },\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://github.com/tensorchord/awesome-llmops\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_YeaR70E6l7iM7UHEtp709iVc\",\n            },\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"arguments\": '{\"url\": \"https://medium.com/@bijit2111987/top-llm-dev-tool-and-when-to-use-them-in-your-ai-stack-622a651ec0e6\"}',\n                    \"name\": \"extract_content\",\n                },\n                \"id\": \"call_UWuyM3dy71Js7fspwSKnMlGC\",\n            },\n        ],\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": \"We follow\\nethical norms\\n&amp;\\nour process\\nfor objectivity. Brands with links to their websites\\nfund\\nour research.\\nTable of contents\\nLLMOps Landscape\\nCompare the top LLMOps platforms in 2023\\nTools for secure and complaint LLMs\\nDisclaimer about current categorization approach\\nWhich LLMOps tool is the best choice for your business?\\nFurther reading\\nMLOps\\nLLM\\nUpdated on\\nAug 7\\nComparing 10+ LLMOps Tools: A Comprehensive Vendor Benchmark\\nWritten by\\nCem Dilmegani\\nCem Dilmegani\\nCem Dilmegani\\nCem Dilmegani\\nCem has been the principal analyst at AIMultiple since 2017. AIMultiple informs hundreds of thousands of businesses (as per similarWeb) including 55% of Fortune 500 every month.\\nCem's work has been cited by leading global publications including Business Insider,  Forbes, Washington Post, global firms like Deloitte, HPE and NGOs like World Economic Forum and supranational organizations like European Commission. You can see more reputable companies and resources that referenced AIMultiple.\\nThroughout his career, Cem served as a tech consultant, tech buyer and tech entrepreneur. He advised enterprises on their technology decisions at McKinsey &amp; Company and Altman Solon for more than a decade. He also published a McKinsey report on digitalization.\\nHe led technology strategy and procurement of a telco while reporting to the CEO. He has also led commercial growth of deep tech company Hypatos that reached a 7 digit annual recurring revenue and a 9 digit valuation from 0 within 2 years. Cem's work in Hypatos was covered by leading technology publications like TechCrunch and Business Insider.\\nCem regularly speaks at international technology conferences. He graduated from Bogazici University as a computer engineer and holds an MBA from Columbia Business School.\\nView Full Profile\\nFollow on\\nWe follow\\nethical norms\\n&amp;\\nour process\\nfor objectivity. Brands with links to their websites\\nfund\\nour research.\\nThe number of\\nlarge language models (LLMs)\\nhas been increasing since 2019 due to the models\u2019 extensive application areas and capabilities (See Figure 1).\\nYet, the estimates show that designing a new foundation model can cost up to $90 million while fine-tuning or enhancing existing large language models can cost $1 million to $100 thousand.\\n1\\nThese costs result from:\\nComputational costs like hardware usage for training runs\\nData gathering and labelling costs\\nEngineering and R&amp;D costs\\nFigure 1: The increasing number of LLMs since 2019\\n2\\nLLMOps\\ntools can reduce these costs by facilitating LLM management. However, LLMOps is a relatively recent solution and most business leaders are not aware of the leading players in this market. This article explains the LLMOps market and compares available tools.\\nLLMOps Landscape\\nThere are 20+ tools that claim to be LLMOps solutions, which can be evaluated under 6 main categories:\\n1. LLMOps Platforms\\nThese are either designed specifically for LLMOps or are MLOps platforms that started offering LLMOps capabilities. They include features that allow carrying out these operations on LLMs:\\nFinetuning\\nVersioning\\nDeploying\\nThese LLM platforms can offer different levels of flexibility and ease of use:\\nNo-code LLM platforms:\\nSome of these platforms are no-code and low-code, which facilitate LLM adoption. However, these tools typically have limited flexibility.\\nCode-first platforms:\\nThese platforms target machine learning engineers and data scientists. They tend to offer a higher level of flexibility.\\n2. Integration frameworks\\nThese tools are built to facilitate developing\\nLLM applications\\nsuch as document analyzers, code analyzers, chatbots etc.\\n3.) Vector databases (VD)\\nVDs store high-dimensional data vectors, such as patient data covering symptoms, blood test results, behaviors, and general health. Some VD software like deep lake can facilitate LLM operations.\\n4.) Fine-tuning tools\\nFine-tuning tools are frameworks, or platforms for fine-tuning pre-trained models. These tools provide a streamlined workflow to modify, retrain, and optimize pre-trained models for natural language processing, computer vision, and more tasks. Some libraries are also designed for fine-tuning, such as Hugging Face Transformers, PyTorch, and TensorFlow.\\n5.) RLHF tools\\nReinforcement learning from human feedback\\n, or RLHF, is a way for AI to learn the best actions by listening to human input. Typically, Reinforcement learning includes an RL algorithm to learn by interacting with the environment and receiving rewards or penalties based on its actions.\\nIn contrast, RLHF tools (e.g. Clickworker or Appen) include human feedback in the learning loop. RLHF can be useful to:\\nEnhance LLM fine-tuning by large data labeling\\nImplement AI governance by reducing biases in LLM responses and moderating content\\nCustomize model\\nImprove contextual understanding.\\n6.) LLM testing tools\\nLLM testing tools evaluate and assess LLMs by testing model performance, capabilities, and potential biases in various language-related tasks and applications, such as natural language understanding and generation. Testing tools may include:\\nTesting frameworks\\nBenchmark datasets\\nEvaluation metrics.\\n7.) LLM monitoring and observability\\nLLM monitoring and observability tools ensure their proper functioning, user safety, and brand protection. LLM monitoring includes activities like:\\nFunctional monitoring\\n: Keeping track of factors like response time, token usage, number of requests, costs and error rates.\\nPrompt monitoring\\n: Checking user inputs and prompts to evaluate toxic content in responses, measure embedding distances, and identify malicious prompt injections.\\nResponse monitoring:\\nAnalyzing to discover\\nhallucinatory\\nbehavior, topic divergence, tone and sentiment in the responses.\\nCompare the top LLMOps platforms in 2023\\nIn this section, we focus on LLMOps platforms and excluded integration frameworks and other tools. LLMOps platforms can be examined in these categories:\\n1. MLOps platforms\\nSome\\nMLOps platforms\\noffer LLMOps toolkits.\\nMachine Learning Operations (MLOps)\\nmanages and optimizes the end-to-end machine learning lifecycle. Since LLMs are also machine learning models, MLOps vendors are naturally expanding into this domain.\\n2. LLM platforms\\nSome\\nLLM providers\\n, especially OpenAI, are also providing LLMOps capabilities to fine-tune, integrate and deploy their models.\\n3. Data and cloud platforms\\nData or cloud platforms are starting to offer LLMOps capabilities that allow their users to leverage their own data to build and finetune LLMs. For example, Databricks acquired MosaicML for $1.3 billion.\\n3\\nCloud platforms\\nCloud leaders Amazon, Azure and Google have all launched their LLMOps offering which allows users to deploy models from different providers with ease\\n4. LLMOPs frameworks\\nThis category includes tools that exclusively focus on optimizing and managing LLM operations. The table below shows the Github stars, B2B reviews and average B2B score from B2B review pages (Trustradius, Gartner &amp; G2) for some of these LLMOps tools:\\nLLMOps Tools\\nGithub Stars\\nNumber of B2B Reviews*\\nAverage Review Score**\\nNemo by Nvidia\\n7,900\\nNA\\nNA\\nDeep Lake\\n6,600\\nNA\\nNA\\nFine-Tuner AI\\n6,000\\nNA\\nNA\\nSnorkel AI\\n5,500\\nNA\\nNA\\nZen ML\\n3,000\\nNA\\nNA\\nLamini AI\\n2100\\nNA\\nNA\\nComet\\n54\\nNA\\nNA\\nTitan ML\\n47\\nNA\\nNA\\nDeepset AI\\n6\\nNA\\nNA\\nValohai\\nNot open source\\n20\\n4.9\\nHere is a brief explanation for each tool in alphabetical order:\\nComet:\\nComet streamlines the ML lifecycle, tracking experiments and production models. Suited for large enterprise teams, it offers various deployment strategies. It supports private cloud, hybrid, and on-premise setups.\\nFigure 2:\\nComet LLMops platform\\n4\\nDeep Lake:\\nDeep Lake combines the capabilities of Data Lakes and Vector Databases to create, refine, and implement high-quality LLMs and MLOps solutions for businesses. Deep Lake allows users to visualize and manipulate datasets in their browser or Jupyter notebook, swiftly accessing different versions and generating new ones through queries, all compatible with PyTorch and TensorFlow.\\nDeepset AI:\\nDeepset AI is a comprehensive platform that allows users to integrate their data with LLMs to build and deploy customized LLM features in their applications. Deepset supports Retrieval-augmented generation (RAG) and Enterprise knowledge search, as well.\\nLamini AI:\\nLamini AI provides an easy method for training LLMs through both prompt-tuning and base model training. Lamini AI users can write custom code, integrate their own data, and host the resulting LLM on their infrastructure.\\nNemo by Nvidia:\\nNvidia offers an end-to-end, cloud-native enterprise framework to develop, customize, and employ generative AI models and LLM applications. The framework can execute various tasks required to train LLMs, such as token classification, prompt learning and question answering.\\nSnorkel AI:\\nSnorkel AI empowers enterprises to construct or customize foundation models (FMs) and large language models (LLMs) to achieve remarkable precision on domain-specific datasets and use cases. Snorkel AI introduces programmatic labelling, enabling data-centric AI development with automated processes.\\nFigure 3:\\nSnorkel AI LLMOps platform\\n5\\n6.\\nTitan ML:\\nTitanML is an NLP development platform that aims to allow businesses to swiftly build and implement smaller, more economical deployments of large language models. It offers proprietary, automated, efficient fine-tuning and inference optimization techniques. This way, it allows businesses to create and roll out large language models in-house.\\n7.\\nValohai:\\nValohai streamlines MLOps and LLMs, automating data extraction to model deployment. It can store models, experiments, and artefacts, making monitoring and deployment easier. Valohai creates an efficient workflow from code to deployment, supporting notebooks, scripts, and Git projects.\\n8.\\nZen ML:\\nZenML primarily focuses on machine learning operations (MLOps) and the management of the machine learning workflow, including data preparation, experimentation, and model deployment.\\nTools for secure and complaint LLMs\\nSome LLMOps integrate with AI governance and\\nLLM security\\ntechnologies to ensure safe, unbiased, and ethical LLM deployment and operation. Check out more on these:\\nCompare Top 25 AI Governance Tools: A Vendor Benchmark\\nCompare 20 LLM Security Tools &amp; Open-Source Frameworks\\nDisclaimer about current categorization approach\\nWe are aware that there are different approaches to categorize these tools. For instance, some vendors include other technologies that can help large language model development in this landscape, such as containerization or edge computing. However, such technologies are not built for designing or monitoring models, even though they can be paired with LLMOps tools to improve model performance. Therefore, we excluded these tools.\\nA more classical approach categorizes tools based on licence type (e.g. open source or not) or whether the tool provides pre-trained models or not. While these are relevant categorizations, we think they are less critical than other functionality provided by the tool. For example, it is quite important whether an LLM is open source or not since it impacts how the end user can finetune the model. However, an LLMOps platform, like most other software, will be used by most end users without modifications to the software code and therefore it is less impactful for an LLMOps tool to be open source.\\nWhich LLMOps tool is the best choice for your business?\\nWe now provide relatively generic recommendations on choosing these tools. We will make these more specific as we explore LLMOps platforms in more detail and as the market matures.\\nHere are a few steps you must complete in your selection process:\\nDefine goals:\\nClearly outline your business goals to establish a solid foundation for your LLMOps tool selection process. For example, if your goal requires training a model from scratch vs fine-tuning an existing model, this will have important implications to your LLMOps stack.\\nDefine requirements:\\nBased on your goal, certain requirements will become more important. For example, if you aim to enable business users to use LLMs, you may want to include no code in your list of requirements.\\nPrepare a shortlist\\n: Consider user reviews and feedback to gain insights into real-world experiences with different LLMOps tools. Rely on this market data to prepare a shortlist.\\nCompare functionality:\\nUtilize free trials and demos provided by various LLMOps tools to compare their features and functionalities firsthand.\\nWhat is LLMOps?\\nLarge Language Models (LLMs) are advanced machine learning models designed to understand and generate human-like text based on the patterns and information they\u2019ve learned from training data. These models are built using deep learning models to capture intricate linguistic nuances and context.\\nLLMOps refer to techniques and tools used for the operational model management of LLMs in production environments.\\nKey components of LLMOps tools\\nLarge Language Model Operations (LLMOps) tools encompass crucial components for efficient management and deployment of large language models (LLMs). These tools typically include features such as:\\n\u2013\\nPrompt Engineering:\\nCreating effective prompt templates for improved model performance.\\n\u2013\\nData Management:\\nHandling vast datasets, ensuring proper data versioning, and facilitating exploratory data analysis.\\n\u2013\\nModel Fine Tuning:\\nFine-tuning LLMs to specific tasks and refining models for optimal performance.\\n\u2013\\nModel Monitoring:\\nContinuous tracking of model outcomes, detection of accuracy degradation, and addressing model drift.\\nWhat are LLMOps benefits?\\nLLMOps delivers significant advantages to machine learning projects leveraging large language models:\\n1.) Increased Accuracy:\\nEnsuring high-quality data for training and reliable deployment enhances model accuracy.\\n2.)\\nReduced Latency:\\nEfficient deployment strategies lead to reduced latency in LLMs, enabling faster data retrieval.\\n3.) Fairness Promotion:\\nStriving to eliminate bias ensures more impartial outputs, preventing discrimination.\\nLLMOps challenges &amp; solutions\\nChallenges in large language model operations require robust solutions to maintain optimal performance:\\n1.) Data Management Challenges:\\nHandling vast datasets and sensitive data necessitates efficient data collection and versioning.\\n2.)\\nModel Monitoring Solutions:\\nImplementing model monitoring tools to track model outcomes, detect accuracy degradation, and address model drift.\\n3.)\\nScalable Deployment:\\nDeploying scalable infrastructure and utilizing cloud-native technologies to meet computational power requirements.\\n4.)\\nOptimizing Models:\\nEmploying model compression techniques and refining models to enhance overall efficiency.\\nLLMOps tools are pivotal in overcoming challenges and delivering higher quality models in the dynamic landscape of large language models.\\nFurther reading\\nExplore more on LLMs, MLOps and AIOps by checking out our articles:\\nMLOps Tools &amp; Platforms Landscape: In-Depth Guide\\n15 Best AiOps Platforms: Streamline IT Ops with AI\\nChatGPT AIOps in IT Automation: 8 Powerful Examples\\nIf you still have questions about LLMOps tools and landscape, we would like to help:\\nFind the Right Vendors\\nExternal sources\\n1.  \u201c\\nThe CEO\u2019s\\xa0Roadmap on Generative AI\\n\u201d BCG. March 2023. Revisited August 11, 2023.\\n2.  \u201c\\nA Survey of Large Language Models.\\n\u201d\\nGithub\\n. March 2023. Revisited August 11, 2023.\\n3. \u201c\\nDatabricks Signs Definitive Agreement to Acquire MosaicML, a Leading Generative AI Platform\\n\u201c.\\nDatabricks\\n. June 26, 2023. Retrieved August 24, 2023.\\n4.  \u201c\\nDebugging Large Language Models with Comet LLMOps Tools\\n.\u201d\\nComet\\n. Revisited August 16, 2023.\\n5.  Harvey, N(March 20, 2023). \u201c\\nSnorkel Flow Spring 2023: warm starts and foundation models.\\n\u201d\\nSnorkelAI\\n. Revisited August 16, 2023.\\nShare This Article\\nCem Dilmegani\\nCem has been the principal analyst at AIMultiple since 2017. AIMultiple informs hundreds of thousands of businesses (as per similarWeb) including 55% of Fortune 500 every month.\\nCem's work has been cited by leading global publications including Business Insider,  Forbes, Washington Post, global firms like Deloitte, HPE and NGOs like World Economic Forum and supranational organizations like European Commission. You can see more reputable companies and resources that referenced AIMultiple.\\nThroughout his career, Cem served as a tech consultant, tech buyer and tech entrepreneur. He advised enterprises on their technology decisions at McKinsey &amp; Company and Altman Solon for more than a decade. He also published a McKinsey report on digitalization.\\nHe led technology strategy and procurement of a telco while reporting to the CEO. He has also led commercial growth of deep tech company Hypatos that reached a 7 digit annual recurring revenue and a 9 digit valuation from 0 within 2 years. Cem's work in Hypatos was covered by leading technology publications like TechCrunch and Business Insider.\\nCem regularly speaks at international technology conferences. He graduated from Bogazici University as a computer engineer and holds an MBA from Columbia Business School.\\nNext to Read\\nGuide to RLHF LLMs in 2024: Benefits &amp; Top Vendors\\nJan 3\\n5 min read\\nLLMOPs vs MLOPs in 2024: Discover the Best Choice for You\\nAug 7\\n6 min read\\nUsing Vector Databases for LLMs: Why is it Crucial in 2024?\\nAug 14\\n5 min read\\nComments\\nYour email address will not be published. All fields are required.\\n0\\nComments\\nPost Comment\\nRelated research\\nCompare 20 LLM Security Tools &amp; Open-Source Frameworks in '24\\nAug 7\\n12 min read\\n12 Retrieval Augmented Generation (RAG) Tools / Software in '24\\nAug 7\\n6 min read\",\n        \"tool_call_id\": \"call_UXnnadcCnki8qvCxrzRI1fXA\",\n        \"name\": \"extract_content\",\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": \"PRICE\\nVERDICT\\nREASONS TO BUY\\nREASONS TO AVOID\\nVERDICT\\nREASONS TO BUY\\nREASONS TO AVOID\\n(Image credit: Future)\\nJump to:\\nBest for Enterprises\\nBest free\\nBest Value\\nBest for code generation\\nBest for debugging\\nFAQs\\nThe best Large Language Models (LLMs) for coding have been trained with code related data and are a new approach that developers are using to augment workflows to improve efficiency and productivity. These coding assistants can be used for a wide range of code related tasks, such as code generation, code analysis to help with debugging, refactoring, and writing test cases, as well offering chat capabilities to discuss problems and inspire developers with solutions. For this guide we tested several different LLMs that can be used for coding assistants to work out which ones present the best results for their given category.\\nThe\\nbest large language models\\nare area of technology that is moving very quickly so while we do our best to keep this guide as up to date as possible, you may want to check if a newer model has been released and whether it fits your specific use case better.\\nThe best large language models (LLMs) for coding\\nWhy you can trust TechRadar\\nWe spend hours testing every product or service we review, so you can be sure you\u2019re buying the best.\\nFind out more about how we test.\\nBest for Enterprises\\n(Image credit: Copilot)\\nGitHub Copilot\\nThe best LLM for business\\nReasons to buy\\n+\\nOffers a first-party extension for direct integration into several popular development environments\\n+\\nMultiple subscription tiers with varying feature levels\\n+\\nBuilt upon OpenAI\u2019s GPT-4 model\\n+\\nUnlimited messages and interactions for all subscription tiers\\nReasons to avoid\\n-\\nRequires a subscription to use\\n-\\nCan\u2019t be self-hosted\\n-\\nNot immune to providing inaccurate or incorrect prompts\\nOriginally released in October 2021, GitHub Copilot is a version of\\nMicrosoft\\n\u2019s Copilot LLM that is specifically trained with data to assist coders and developers with their work with the aim to improve efficiency and productivity. While the original release used\\nOpenAI\\n\u2019s Codex model, a modified version of GPT-3 which was also trained as a coding assistant, GitHub Copilot was updated to use the more advanced GPT-4 model in November 2023.\\nA core feature of GitHub Copilot is the extension provided that allows direct integration of the LLM into commonly used Integrated Development Environments (IDEs) popular among developers today, including Visual Studio Code, Visual Studio, Vim, Neovim, the JetBrains suite of IDEs, and Azure Data Studio. This direct integration allows GitHub Copilot to access your existing project to improve the suggestions made when given a prompt, while also providing users hassle free installation and access to the features provided. For enterprise users, the model can also be granted access to existing repositories and knowledge bases from your organization to further enhance the quality of outputs and suggestions.\\nWhen writing code, GitHub Copilot can offer suggestions in a few different ways. Firstly, you can write a prompt using an inline comment that can be converted into a block of code. This works in a similar way to how you might use other LLMs to generate code blocks from a prompt, but with the added advantage of GitHub Copilot being able to access existing project files to use as context and produce a better output. Secondly, GitHub Copilot can provide real-time suggestions as you are writing your code. For example, if you are writing a regex function to validate an email address, simply starting to write the function can offer an autocomplete suggestion that provides the required syntax. Additionally, you can also use the GitHub Copilot Chat extension to ask questions, request suggestions, and help you to debug code in a more context aware fashion than you might get from LLMs trained on more broad datasets. Users can enjoy unlimited messages and interactions with GitHub Copilot\u2019s chat feature across all subscription tiers.\\nGitHub Copilot is trained using data from publicly available code repositories, including GitHub itself. GitHub Copilot claims it can provide code assistance in any language where a public repository exists, however the quality of the suggestions will depend on the volume of data available. All subscription tiers include a public code filter to reduce the risk of suggestions directly copying code from a public repository. By default, GitHub Copilot excludes submitted data from being used to train the model further for business and enterprise tier customers and offers the ability to exclude files or repositories from being used to inform suggestions offered. Administrators can configure both features as needed based on your business use cases.\\nWhile these features aim to keep your data private, it\u2019s worth keeping in mind that prompts aren\u2019t processed locally and rely on external infrastructure to provide code suggestions and you should factor this into whether this is the right product for you. Users should also be cautious about trusting any outputs implicitly \u2013 while the model is generally very good at providing suggestions, like all LLMs it is still prone to hallucinations and can make poor or incorrect suggestions. Always make sure to review any code generated by the model to make sure it does what you intend it to do.\\nIn the future it\u2019s possible that GitHub will upgrade GitHub Copilot to use the recently released GPT-4o model. GPT-4 was originally released in March 2023, with GitHub Copilot being updated to use the new model roughly 7 months later. It makes sense to update the model further given the improved intelligence, reduced latency, and reduced cost to operate GPT-4o, though at this time there has been no official announcement.\\nIf you want to try before you buy, GitHub Copilot offers a free 30 day trial of their cheapest package which should be sufficient to test out its capabilities, with a $10 per month fee thereafter. Copilot Business costs $19 per user per month, while Enterprise costs $39 per user per month\\nBest for individuals\\n(Image credit: Qwen)\\nCodeQwen1.5\\nBest coding assistant for individuals\\nReasons to buy\\n+\\nOpen source\\n+\\nHas options for local hosting\\n+\\nCan be trained further using your own code repositories\\n+\\nOffers a range of model sizes to fit your requirements\\nReasons to avoid\\n-\\nNo first-party extensions for popular IDEs\\n-\\nUp front hardware, cost needs to be considered when hosted locally\\nCodeQwen1.5 is a version of Alibaba\u2019s open-source Qwen1.5 LLM specifically trained using public code repositories to assist developers in coding related tasks. This specialized version was released in April 2024, a few months after the release of Qwen1.5 to the public in February 2024.\\nThere are 2 different versions of CodeQwen1.5 available today. The base model of CodeQwen1.5 is designed for code generation and suggestions but has limited chat functionality, while the second version can also be used as a chat interface that can answer questions in a more human-like way. Both models have been trained with 3 trillion tokens of code related data and support a very respectable 92 languages, which include some of the most common languages in use today such as Python, C++, Java, PHP, C# and JavaScript.\\nUnlike the base version of Qwen1.5, which has several different sizes available for download, CodeQwen1.5 is only available in a single size of 7B. While this is quite small when compared to other models on the market that can also be used as coding assistants, there are a few advantages that developers can take advantage of. Despite its small size, CodeQwen1.5 performs incredibly well compared to some of the larger models that offer coding assistance, both open and closed source. CodeQwen1.5 comfortably beats GPT3.5 in most benchmarks and provides a competitive alternative to GPT-4, though this can sometimes depend on the specific programming language. While GPT-4 may perform better overall by comparison, it\u2019s important to remember that GPT-4 requires a subscription and has per token costs that could make using it very expensive compared to CodeQwen1.5 and GPT-4 cannot be hosted locally. Like with all LLMs, its risky to implicitly trust any suggestions or responses provided by the model. While steps have been taken to reduce hallucinations, always check the output to make sure it is correct.\\nAs CodeQwen1.5 is open source, you can download a copy of the LLM to use at no additional cost beyond the hardware needed to run it. You\u2019ll still need to make sure your system has enough resources to ensure the model can run well, but the bonus of the smaller model size means a modern system with GPU that has at least 16GB of VRAM and at least 32GB of system RAM should be sufficient. CodeQwen1.5 can also be trained using code from existing projects or other code repositories to further improve the context of the generated responses and suggestions. The ability to host CodeQwen1.5 within your own local or remote infrastructure, such as a Virtual Private Server (VPS) or dedicated server, should also help to alleviate some of the concerns related to data privacy or security often connected to submitting information to third party providers.\\nAlibaba surprised us by releasing their new Qwen2 LLM at the start of June that they claim offers significant gains over the base model of Qwen1.5. Alibaba also mentioned that the training data used for CodeQwen1.5 is included in Qwen2-72B, so has the potential to offer improved results, but it\u2019s currently unclear if there is a plan to upgrade CodeQwen to use the new model.\\nBest Value\\n(Image credit: Meta)\\nLLama 3\\nBest value LLM\\nReasons to buy\\n+\\nOpen source\\n+\\nSmaller models can be hosted locally\\n+\\nCan be fine tuned with your own dataset\\n+\\nExternal hosting provided by AWS and Azure have low per token costs\\nReasons to avoid\\n-\\nHardware requirements for the larger models could require significant upfront investment\\n-\\nNot specifically trained as a coding LLM\\nWhen it comes to the best bang for buck, Meta\u2019s open-source Llama 3 model released in April 2024 is one of the best low-cost models available on the market today. Unlike many other models specifically trained with code related data to assist developers with coding tasks, Llama 3 is a more general LLM capable of assisting in many ways \u2013 one of which also happens to be as a coding assistant \u2013 and outperforms CodeLlama, a coding model released by Meta in August 2023 based on Llama 2.\\nIn like for like testing with models of the same size, Llama 3 outperforms CodeLlama by a considerable margin when it comes to code generation, interpretation, and understanding. This is impressive considering Llama 3 wasn\u2019t trained specifically for code related tasks but can still outperform those that have. This means that not only can you use Llama 3 to improve efficiency and productivity when performing coding tasks, but it can also be used for other tasks as well. Llama 3 has a training data cutoff of December 2023, which isn\u2019t always of critical importance for code related tasks, but some languages can develop quickly and having the most recent data available can be incredibly valuable.\\nLlama 3 is an open-source model that allows developers to download and deploy the model to their own local system or infrastructure. Like CodeQwen1.5, Llama 3 8B is small enough that a modern system with at least 16GB of VRAM and 32GB of system RAM is sufficient to run the model. The larger 70B version of Llama 3 naturally has better capabilities due to the increased parameter number, but the hardware requirement is an order of magnitude greater and would require a significant injection of funds to build a system capable of running it effectively. Luckily, the Llama 3 8B offers enough capability that users can get excellent value without breaking the bank at the same time. If you find that you need the added capability of the larger model, the open-source nature of the model means you can easily rent an external VPS or dedicated server to support your needs, though costs will vary depending on the provider. If you decide that you\u2019d like the increased capability of the larger model, but the investment needed for the required hardware, or the cost to rent an external host, is outside your budget, AWS offers API access to the model via a pay as you go plan which charges you by the token instead. AWS currently charges $3.50 per million output tokens, which is a considerable quantity for a very small price. For comparison, OpenAI\u2019s GPT-4o costs $15.00 for the same quantity of tokens. If this type of solution appeals to you, make sure to shop around for the best provider for your location, budget, and needs.\\nLlama 3 performs well in code generation tasks and adheres well to the prompts given. It will sometimes simplify the code based on the prompt, but it's reasonably receptive to being given instruction to provide a complete solution and will segment if it reaches the token limit for a single response if requested. During testing, we asked for Llama 3 to write a complete solution in Python for a chess game that would immediately compile and could be played via text prompts, and it dutifully provided the requested code. Although the code initially failed to compile, providing Llama 3 with the error messages from the compiler allowed it to identify where the mistakes were and provided a correction. Llama 3 can effectively debug code segments to identify issues and provide new code to fix the error. As a bonus, it can also explain where the error was located and why it needs to be fixed to help the user understand what the mistake was. However, like with all models generating code-related solutions, it's important to check the output and not trust it implicitly. Although the models are becoming increasingly intelligent and accurate, they also hallucinate at times and provide incorrect or insecure responses.\\nLike with other open-source models, any data you submit to train Llama 3 from your own code repositories remains within your control. This helps to alleviate some of the concerns and risks associated with submitting proprietary and personal data to third parties, though keep in mind that also means that you should consider what that means for your information security policies where required. It doesn\u2019t cost anything extra to train a model you have hosted within your own infrastructure, but some hosts providing API access do have an additional cost associated with further training.\\nYou can download Llama 3 today directly from\\nMeta\\n.\\nBest for code generation\\n(Image credit: Claude AI)\\nClaude 3 Opus\\nThe best LLM for generating code\\nReasons to buy\\n+\\nOutperforms most models for code generation tasks\\n+\\nCan provide detailed explanations of the generated code to assist developer understanding\\n+\\nProvides more human responses to prompts than other models\\nReasons to avoid\\n-\\nClosed source and can\u2019t be hosted locally\\n-\\nExpensive per token cost\\n-\\nCan\u2019t be connected to existing knowledgebases\\nReleased in April 2024, Claude 3 Opus is the latest and most capable LLM from Anthropic that they claim is the most intelligent LLM on the market today and is designed to tackle a variety of different tasks. Although most LLMs can generate code, the accuracy and correctness of the generated outputs can vary, and may have mistakes or be flat out incorrect due to not being specifically designed with code generation in mind. Claude 3 Opus bridges that gap by being trained to handle coding related tasks alongside the regular tasks LLMs are often used for, making for a very powerful multi-faceted solution.\\nWhile Anthropic doesn\u2019t mention how many programming languages it supports, Claude 3 Opus can generate code across a large range of programming languages, ranging from incredibly popular languages such as C++, C#, Python and Java, to older or more niche languages such as FORTRAN, COBOL, and Haskell. Claude 3 Opus relies on the patterns, syntaxes, coding conventions and algorithms identified within the code related training data provided to generate new code snippets from scratch to help avoid direct reproduction of code used to train it. The large 200k token context window offered by Claude 3 Opus is incredibly useful when working with large code blocks as you iterate through suggestions and changes. Like all LLMs, Claude 3 Opus also has an output token limit, and tends to either summarise or truncate the response to fit within a single reply. While summarisation of a purely text response isn\u2019t too problematic as you can ask for additional context, not being provided with a large chunk of required code, such as when generating a test case, is quite a problem. Fortunately, Claude 3 Opus can segment its responses if you request it to do so in your initial prompt. You\u2019ll still need to ask it to continue after each reply, but this does allow you to obtain more long form responses where needed. As well as generating functional code, Claude 3 Opus also adds comments to the code and provides explanations as to what the generated code does to help developers understand what is happening. In cases where you are using Claude 3 to debug code and generate fixes, this is extremely valuable as it not only helps solve the problem, but also provides context as to why changes were made, or why the code was generated in this specific way.\\nFor those concerned about privacy and data security, Anthropic states that they don\u2019t use any of the data submitted to Claude 3 for the purposes of training the model further, a welcome feature that many will appreciate when working with proprietary code. They also include copyright indemnity protections with their paid subscriptions.\\nClaude 3 Opus does come with some limitations when it comes to improving the context of responses as it doesn\u2019t currently offer a way to connect your own knowledge bases or codebases for additional training. This probably isn\u2019t a deal breaker for most but could be something worth thinking about when choosing the right LLM for your code generation solution.\\nThis does all come with a hefty price tag compared to other LLMs that offer code generation functionality. API access is one of the more expensive ones on the market at an eye watering $75 per 1 million output tokens, which is considerably more than GPT-4o\u2019s $15 price tag. Anthropic do offer 2 additional models based on Claude 3, Haiku and Sonnet, which are much cheaper at $15 and $1.25 respectively for the same quantity of tokens, though they have reduced capability compared to Opus. In addition to API access, Anthropic offers 3 subscription tiers that grant access to Claude 3. The free tier has a lower daily limit and only grants access to the Sonnet model but should give those looking to test it\u2019s capabilities a good idea of what to expect. To access Opus, you\u2019ll need to subscribe to Pro or Team at $20 and $30 per person per month respectively. The Team subscription does need a minimum of 5 users for a total of $150 per month, but increases the usage limits for each user compared to the Pro tier.\\nHead over to create a free account to access\\nClaude 3\\n.\\nBest for debugging\\n(Image credit: Open AI)\\nGPT-4\\nThe best LLM for debugging\\nReasons to buy\\n+\\nIdentifies issues within blocks of code and suggests corrections\\n+\\nCan explain what the problem was and how the corrections solve it\\n+\\nLarge context window\\nReasons to avoid\\n-\\nPer token cost can be expensive compared to coding-focused offerings with similar capability\\n-\\nRequires a subscription to gain access\\n-\\nManual opt-out needed to prevent data from being used to train the model\\nSince the release of\\nChatGPT\\nin November 2022, OpenAI has taken the world by storm and offers some of the most intelligent and capable LLMs on the market today. GPT-4 was released in March 2023 as an update to GPT-3.5\\nWhile GPT-4 isn\u2019t an LLM designed specifically as a coding assistant, it performs well across a broad range of code related tasks, including real time code suggestions, generating blocks of code, writing test cases, and debugging errors in code. GitHub Copilot has also been using a version of GPT-4 with additional training data since November 2023, leveraging the human response capabilities of GPT-4 for code generation and within their chat assistant, which should give you an idea of the value it can provide.\\nGPT-4 has been trained with code related data that covers many different programming languages and coding practices to help it understand the vast array of logic flows, syntax rules and programming paradigms used by developers. This allows GPT-4 to excel when debugging code by helping to solve a variety of issues commonly encountered by developers. Syntax errors can be incredibly frustrating when working with some languages - I\u2019m looking at you and your indentations, Python \u2013 so using GPT-4 to review your code can massively speed up the process when code won\u2019t compile due to errors that are difficult to find. Logical errors are one of the toughest errors to debug as code usually compiles correctly, but it doesn\u2019t provide the correct output or operate as desired. By giving GPT-4 your code and an explanation of what it should be doing, GPT-4 can analyse and identify where the problem lies, offer suggestions or rewrites to solve the problem, and even provide an explanation as to what the problem is and how the suggested changes solve it. This can help developers quickly understand the cause of the problem and offers an opportunity to learn how to avoid it again in the future.\\nAlthough the training data cutoff for GPT-4 is September 2021, which is quite a long time ago considering the advancements in LLMs over the last year, GPT-4 is continuously trained using new data from user interactions. This allows GPT-4\u2019s debugging to become more accurate over time, though this does present some potential risk when it comes to the code you submit for analysis, especially when using it to write or debug proprietary code. Users do have the option to opt out of their data being used to train GPT-4 further, but it's not something that happens by default so keep this in mind when using GPT-4 for code related tasks.\\nYou might be wondering why the recommendation here is to use GPT-4 when it is 4 times more expensive than the newer, cheaper, and more intelligent GPT-4o model released in May 2024. In general, GPT-4o has proven to be a more capable model, but for code related tasks GPT-4 tends to provide better responses that are more correct, adheres to the prompt better, and offers better error detection than GPT-4o. However, the gap is small and it's likely that GPT-4o will become more capable and overtake GPT-4 in the future as the model matures further through additional training from user interactions. If cost is a major factor in your decision, GPT-4o is a good alternative that covers the majority of what GPT-4 can provide at a much lower cost.\\nBest LLM for Coding Assistants FAQs\\nHow does a coding assistant work?\\nCoding assistants use Large Language Models (LLMs) that are trained with code related data to provide developers with tools that help increase productivity and efficiency when performing code related tasks. The training data often leverages public code repositories, documentation and other licenced work to enable the LLM to recognise syntax, coding styles, programming practices and paradigms to provide code generation, debugging, code analysis, and problem-solving capabilities across many different programming languages.\\nCoding assistants can be integrated into your development environments to provide inline code suggestions, and some can be trained further using an organization's knowledge bases and codebases to improve the context of suggestions.\\nWhy shouldn\u2019t I implicitly trust the code generated by a coding assistant?\\nLLMs are becoming increasingly intelligent, but they aren\u2019t immune to making mistakes known as \u201challucinations\u201d. Most coding assistants generate code that works well, but sometimes the code can be incomplete, inaccurate, or completely wrong. This can vary from model to model and has a high dependency on the training data used and the overall intelligence capability of the model itself.\\nWhat is a context window?\\nA context window is another way of describing how far back the LLM\u2019s memory can go for a conversation, usually measured in tokens. LLMs with a large context window allow for responses that offer better context based on the conversation history which can be valuable for developers working on code related tasks when brainstorming ideas, debugging large sections of code, or iterating on a design.\\nAre you a pro? Subscribe to our newsletter\\nSign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed!\\nContact me with news and offers from other Future brands\\nReceive email from us on behalf of our trusted partners or sponsors\\nGrant Hickey\\nFascinated by computers from a young age, Grant is on an endless quest to leverage existing and emerging technologies to augment and enhance the productivity of individuals and enterprises, and to improve the velocity at which teams can analyze data and identify trends within their customer base or organization. Grant has previously worked as a software engineer building cloud based CRMs, before moving into the games industry to work for Krafton on PUBG:Battlegrounds and later Creative Assembly. Always looking to improve his working practices he often builds his own tools to streamline tasks and become more efficient.\\nLATEST ARTICLES\\n1\\nScuf Nomad review: a solid mobile controller that struggles to stand out\\n2\\nLG says next-gen \\\"dream OLED\\\" panel is finally real \u2013 but it might not come to TVs first\\n3\\nHP 325 FHD webcam: A budget-friendly choice with solid image quality\\n4\\nHow to watch After Baywatch: Moment in the Sun online from anywhere\\n5\\nAt last! Garmin Fenix 8 revealed, with an Apple Watch Ultra-beating dive mode \u2013 alongside the Garmin Enduro 3\",\n        \"tool_call_id\": \"call_S0OnNQqxtPH5HtDb1buzjedV\",\n        \"name\": \"extract_content\",\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": \"&lt;class 'requests.exceptions.ConnectionError'&gt;: Failed to extract content from URL https://www.blog.aipo.rt.tech/p/top-9-libraries-to-accelerate-llm\",\n        \"tool_call_id\": \"call_aCyaCFXUWMWloDkETTrxHyoJ\",\n        \"name\": \"extract_content\",\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": \"Member-only story\\nEssential Python Libraries for LLMs and Application Development in 2024\\nAhmad Waleed\\n\u00b7\\nFollow\\nPublished in\\nPythonForAll\\n\u00b7\\n3 min read\\n\u00b7\\nNov 30, 2023\\n--\\nShare\\nIn the ever-evolving landscape of Language Models (LLMs), Natural Language Processing (NLP), and Machine Learning (ML), the arsenal of Python libraries continues to expand, bringing forth innovation and efficiency in diverse applications. For those venturing into this realm or seeking to enrich their projects, here\u2019s a categorized guide to the essential Python libraries that are reshaping the field in 2024.\\nKey Insights:\\nNLP &amp; ML Model Integration: Libraries such as Transformers and spaCy play pivotal roles in seamlessly integrating pre-trained NLP models, marking a paradigm shift towards context-aware language models across applications.\\nDeep Learning &amp; Neural Networks: TensorFlow and PyTorch stand tall as robust frameworks, showcasing Python\u2019s prowess in handling complex neural network architectures and computations at scale.\\nData Preprocessing &amp; Validation: Libraries like Unstructured and Pydantic underscore the burgeoning emphasis on data quality and integrity, crucial in an era dominated by vast datasets and intricate ML models.\\nApplication Development: Streamlit and Gradio emerge as transformative tools, converting data scripts into interactive web applications, emphasizing the significance of accessibility\u2026\",\n        \"tool_call_id\": \"call_71e9AkvKjIuEp3QceqO4DCUK\",\n        \"name\": \"extract_content\",\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": \"tensorchord\\n/\\nAwesome-LLMOps\\nPublic\\nNotifications\\nYou must be signed in to change notification settings\\nFork\\n353\\nStar\\n3.7k\\nAn awesome &amp; curated list of best LLMOps tools for developers\\nLicense\\nCC0-1.0 license\\n3.7k\\nstars\\n353\\nforks\\nBranches\\nTags\\nActivity\\nStar\\nNotifications\\nYou must be signed in to change notification settings\\ntensorchord/Awesome-LLMOps\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\nmain\\nBranches\\nTags\\nGo to file\\nCode\\nFolders and files\\nName\\nName\\nLast commit message\\nLast commit date\\nLatest commit\\nHistory\\n150 Commits\\nscripts\\nscripts\\n.gitignore\\n.gitignore\\nLICENSE\\nLICENSE\\nREADME.md\\nREADME.md\\ncontributing.md\\ncontributing.md\\nView all files\\nRepository files navigation\\nAwesome LLMOps\\nAn awesome &amp; curated list of the best LLMOps tools for developers.\\nContribute\\nContributions are most welcome, please adhere to the\\ncontribution guidelines\\n.\\nTable of Contents\\nTable of Contents\\nModel\\nLarge Language Model\\nCV Foundation Model\\nAudio Foundation Model\\nServing\\nLarge Model Serving\\nFrameworks/Servers for Serving\\nObservability\\nSecurity\\nLLMOps\\nSearch\\nVector search\\nCode AI\\nTraining\\nIDEs and Workspaces\\nFoundation Model Fine Tuning\\nFrameworks for Training\\nExperiment Tracking\\nVisualization\\nModel Editing\\nData\\nData Management\\nData Storage\\nData Tracking\\nFeature Engineering\\nData/Feature enrichment\\nLarge Scale Deployment\\nML Platforms\\nWorkflow\\nScheduling\\nModel Management\\nPerformance\\nML Compiler\\nProfiling\\nAutoML\\nOptimizations\\nFederated ML\\nAwesome Lists\\nModel\\nLarge Language Model\\nProject\\nDetails\\nRepository\\nAlpaca\\nCode and documentation to train Stanford's Alpaca models, and generate the data.\\nBELLE\\nA 7B Large Language Model fine-tune by 34B Chinese Character Corpus, based on LLaMA and Alpaca.\\nBloom\\nBigScience Large Open-science Open-access Multilingual Language Model\\ndolly\\nDatabricks\u2019 Dolly, a large language model trained on the Databricks Machine Learning Platform\\nFalcon 40B\\nFalcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.\\nFastChat (Vicuna)\\nAn open platform for training, serving, and evaluating large language models. Release repo for Vicuna and FastChat-T5.\\nGemma\\nGemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models.\\nGLM-6B (ChatGLM)\\nAn Open Bilingual Pre-Trained Model, quantization of ChatGLM-130B, can run on consumer-level GPUs.\\nChatGLM2-6B\\nChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model\\nChatGLM-6B\\n.\\nGLM-130B (ChatGLM)\\nAn Open Bilingual Pre-Trained Model (ICLR 2023)\\nGPT-NeoX\\nAn implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library.\\nLuotuo\\nA Chinese LLM, Based on LLaMA and fine tune by Stanford Alpaca, Alpaca LoRA, Japanese-Alpaca-LoRA.\\nMixtral-8x7B-v0.1\\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\\nStableLM\\nStableLM: Stability AI Language Models\\n\u2b06 back to ToC\\nCV Foundation Model\\nProject\\nDetails\\nRepository\\ndisco-diffusion\\nA frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.\\nmidjourney\\nMidjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.\\nsegment-anything (SAM)\\nproduces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image.\\nstable-diffusion\\nA latent text-to-image diffusion model\\nstable-diffusion v2\\nHigh-Resolution Image Synthesis with Latent Diffusion Models\\n\u2b06 back to ToC\\nAudio Foundation Model\\nProject\\nDetails\\nRepository\\nbark\\nBark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects.\\nwhisper\\nRobust Speech Recognition via Large-Scale Weak Supervision\\nServing\\nLarge Model Serving\\nProject\\nDetails\\nRepository\\nAlpaca-LoRA-Serve\\nAlpaca-LoRA as Chatbot service\\nCTranslate2\\nfast inference engine for Transformer models in C++\\nClip-as-a-service\\nserving the OpenAI CLIP model\\nDeepSpeed-MII\\nMII makes low-latency and high-throughput inference possible, powered by DeepSpeed.\\nFaster Whisper\\nfast inference engine for whisper in C++ using CTranslate2.\\nFlexGen\\nRunning large language models on a single GPU for throughput-oriented scenarios.\\nFlowise\\nDrag &amp; drop UI to build your customized LLM flow using LangchainJS.\\nllama.cpp\\nPort of Facebook's LLaMA model in C/C++\\nInfinity\\nRest API server for serving text-embeddings\\nModelz-LLM\\nOpenAI compatible API for LLMs and embeddings (LLaMA, Vicuna, ChatGLM and many others)\\nOllama\\nServe Llama 2 and other large language models locally from command line or through a browser interface.\\nTensorRT-LLM\\nInference engine for TensorRT on Nvidia GPUs\\ntext-generation-inference\\nLarge Language Model Text Generation Inference\\ntext-embeddings-inference\\nInference for text-embedding models\\nvllm\\nA high-throughput and memory-efficient inference and serving engine for LLMs.\\nwhisper.cpp\\nPort of OpenAI's Whisper model in C/C++\\nx-stable-diffusion\\nReal-time inference for Stable Diffusion - 0.88s latency. Covers AITemplate, nvFuser, TensorRT, FlashAttention.\\n\u2b06 back to ToC\\nFrameworks/Servers for Serving\\nProject\\nDetails\\nRepository\\nBentoML\\nThe Unified Model Serving Framework\\nJina\\nBuild multimodal AI services via cloud native technologies \u00b7 Model Serving \u00b7 Generative AI \u00b7 Neural Search \u00b7 Cloud Native\\nMosec\\nA machine learning model serving framework with dynamic batching and pipelined stages, provides an easy-to-use Python interface.\\nTFServing\\nA flexible, high-performance serving system for machine learning models.\\nTorchserve\\nServe, optimize and scale PyTorch models in production\\nTriton Server (TRTIS)\\nThe Triton Inference Server provides an optimized cloud and edge inferencing solution.\\nlangchain-serve\\nServerless LLM apps on Production with Jina AI Cloud\\nlanarky\\nFastAPI framework to build production-grade LLM applications\\nray-llm\\nLLMs on Ray - RayLLM\\nXinference\\nReplace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.\\n\u2b06 back to ToC\\nSecurity\\nFrameworks for LLM security\\nProject\\nDetails\\nRepository\\nPlexiglass\\nA Python Machine Learning Pentesting Toolbox for Adversarial Attacks. Works with LLMs, DNNs, and other machine learning algorithms.\\n\u2b06 back to ToC\\nObservability\\nProject\\nDetails\\nRepository\\nAzure OpenAI Logger\\n\\\"Batteries included\\\" logging solution for your Azure OpenAI instance.\\nDeepchecks\\nTests for Continuous Validation of ML Models &amp; Data. Deepchecks is a Python package for comprehensively validating your machine learning models and data with minimal effort.\\nEvidently\\nEvaluate and monitor ML models from validation to production.\\nFiddler AI\\nEvaluate, monitor, analyze, and improve machine learning and generative models from pre-production to production. Ship more ML and LLMs into production, and monitor ML and LLM metrics like hallucination, PII, and toxicity.\\nGiskard\\nTesting framework dedicated to ML models, from tabular to LLMs. Detect risks of biases, performance issues and errors in 4 lines of code.\\nGreat Expectations\\nAlways know what to expect from your data.\\nwhylogs\\nThe open standard for data logging\\n\u2b06 back to ToC\\nLLMOps\\nProject\\nDetails\\nRepository\\nagenta\\nThe LLMOps platform to build robust LLM apps. Easily experiment and evaluate different prompts, models, and workflows to build robust apps.\\nAI studio\\nA Reliable Open Source AI studio to build core infrastructure stack for your LLM Applications. It allows you to gain visibility, make your application reliable, and prepare it for production with features such as caching, rate limiting, exponential retry, model fallback, and more.\\nArize-Phoenix\\nML observability for LLMs, vision, language, and tabular models.\\nBudgetML\\nDeploy a ML inference service on a budget in less than 10 lines of code.\\nCometLLM\\nThe 100% opensource LLMOps platform to log, manage, and visualize your LLM prompts and chains. Track prompt templates, prompt variables, prompt duration, token usage, and other metadata. Score prompt outputs and visualize chat history all within a single UI.\\ndeeplake\\nStream large multimodal datasets to achieve near 100% GPU utilization. Query, visualize, &amp; version control data. Access data w/o the need to recompute the embeddings for the model finetuning.\\nDify\\nOpen-source framework aims to enable developers (and even non-developers) to quickly build useful applications based on large language models, ensuring they are visual, operable, and improvable.\\nDstack\\nCost-effective LLM development in any cloud (AWS, GCP, Azure, Lambda, etc).\\nEmbedchain\\nFramework to create ChatGPT like bots over your dataset.\\nEvidently\\nAn open-source framework to evaluate, test and monitor ML and LLM-powered systems.\\nFiddler AI\\nEvaluate, monitor, analyze, and improve MLOps and LLMOps from pre-production to production.\\nGlide\\nCloud-Native LLM Routing Engine. Improve LLM app resilience and speed.\\nGPTCache\\nCreating semantic cache to store responses from LLM queries.\\nHaystack\\nQuickly compose applications with LLM Agents, semantic search, question-answering and more.\\nHelicone\\nOpen-source LLM observability platform for logging, monitoring, and debugging AI applications. Simple 1-line integration to get started.\\nIzlo\\nPrompt management tools for teams. Store, improve, test, and deploy your prompts in one unified workspace.\\nKeywords AI\\nA unified DevOps platform for AI software. Keywords AI makes it easy for developers to build LLM applications.\\nlangchain\\nBuilding applications with LLMs through composability\\nLangFlow\\nAn effortless way to experiment and prototype LangChain flows with drag-and-drop components and a chat interface.\\nLangfuse\\nOpen Source LLM Engineering Platform: Traces, evals, prompt management and metrics to debug and improve your LLM application.\\nLangKit\\nOut-of-the-box LLM telemetry collection library that extracts features and profiles prompts, responses and metadata about how your LLM is performing over time to find problems at scale.\\nLiteLLM \ud83d\ude85\\nA simple &amp; light 100 line package to\\nstandardize LLM API calls\\nacross OpenAI, Azure, Cohere, Anthropic, Replicate API Endpoints\\nLiteral AI\\nMulti-modal LLM observability and evaluation platform. Create prompt templates, deploy prompts versions, debug LLM runs, create datasets, run evaluations, monitor LLM metrics and collect human feedback.\\nLlamaIndex\\nProvides a central interface to connect your LLMs with external data.\\nLLMApp\\nLLM App is a Python library that helps you build real-time LLM-enabled data pipelines with few lines of code.\\nLLMFlows\\nLLMFlows is a framework for building simple, explicit, and transparent LLM applications such as chatbots, question-answering systems, and agents.\\nLLMonitor\\nObservability and monitoring for AI apps and agents. Debug agents with powerful tracing and logging. Usage analytics and dive deep into the history of your requests. Developer friendly modules with plug-and-play integration into LangChain.\\nmagentic\\nSeamlessly integrate LLMs as Python functions. Use type annotations to specify structured output. Mix LLM queries and function calling with regular Python code to create complex LLM-powered functionality.\\nManag.ai\\nYour all-in-one prompt management and observability platform. Craft, track, and perfect your LLM prompts with ease.\\nMirascope\\nIntuitive convenience tooling for lightning-fast, efficient development and ensuring quality in LLM-based applications\\nOpenLIT\\nOpenLIT is an OpenTelemetry-native GenAI and LLM Application Observability tool and provides OpenTelmetry Auto-instrumentation for monitoring LLMs, VectorDBs and Frameworks. It provides valuable insights into token &amp; cost usage, user interaction, and performance related metrics.\\nParea AI\\nPlatform and SDK for AI Engineers providing tools for LLM evaluation, observability, and a version-controlled enhanced prompt playground.\\nPezzo \ud83d\udd79\ufe0f\\nPezzo is the open-source LLMOps platform built for developers and teams. In just two lines of code, you can seamlessly troubleshoot your AI operations, collaborate and manage your prompts in one place, and instantly deploy changes to any environment.\\nPromptHub\\nFull stack prompt management tool designed to be usable by technical and non-technical team members. Test, version, collaborate, deploy, and monitor, all from one place.\\npromptfoo\\nOpen-source tool for testing &amp; evaluating prompt quality. Create test cases, automatically check output quality and catch regressions, and reduce evaluation cost.\\nPromptFoundry\\nThe simple prompt engineering and evaluation tool designed for developers building AI applications.\\nPromptLayer \ud83c\udf70\\nPrompt Engineering platform. Collaborate, test, evaluate, and monitor your LLM applications\\nPromptMage\\nOpen-source tool to simplify the process of creating and managing LLM workflows and prompts as a self-hosted solution.\\nPrompteams\\nPrompt management system. Version, test, collaborate, and retrieve prompts through real-time APIs. Have GitHub style with repos, branches, and commits (and commit history).\\nprompttools\\nOpen-source tools for testing and experimenting with prompts. The core idea is to enable developers to evaluate prompts using familiar interfaces like code and notebooks. In just a few lines of codes, you can test your prompts and parameters across different models (whether you are using OpenAI, Anthropic, or LLaMA models). You can even evaluate the retrieval accuracy of vector databases.\\nTreeScale\\nAll In One Dev Platform For LLM Apps. Deploy LLM-enhanced APIs seamlessly using tools for prompt optimization, semantic querying, version management, statistical evaluation, and performance tracking. As a part of the developer friendly API implementation TreeScale offers Elastic LLM product, which makes a unified API Endpoint for all major LLM providers and open source models.\\nTrueFoundry\\nDeploy LLMOps tools like Vector DBs, Embedding server etc on your own Kubernetes (EKS,AKS,GKE,On-prem) Infra including deploying, Fine-tuning, tracking Prompts and serving Open Source LLM Models with full Data Security and Optimal GPU Management. Train and Launch your LLM Application at Production scale with best Software Engineering practices.\\nReliableGPT \ud83d\udcaa\\nHandle OpenAI Errors (overloaded OpenAI servers, rotated keys, or context window errors) for your production LLM Applications.\\nPortkey\\nControl Panel with an observability suite &amp; an AI gateway \u2014 to ship fast, reliable, and cost-efficient apps.\\nVellum\\nAn AI product development platform to experiment with, evaluate, and deploy advanced LLM apps.\\nWeights &amp; Biases (Prompts)\\nA suite of LLMOps tools within the developer-first W&amp;B MLOps platform. Utilize W&amp;B Prompts for visualizing and inspecting LLM execution flow, tracking inputs and outputs, viewing intermediate results, securely managing prompts and LLM chain configurations.\\nWordware\\nA web-hosted IDE where non-technical domain experts work with AI Engineers to build task-specific AI agents. It approaches prompting as a new programming language rather than low/no-code blocks.\\nxTuring\\nBuild and control your personal LLMs with fast and efficient fine-tuning.\\nZenML\\nOpen-source framework for orchestrating, experimenting and deploying production-grade ML solutions, with built-in\\nlangchain\\n&amp;\\nllama_index\\nintegrations.\\n\u2b06 back to ToC\\nSearch\\nVector search\\nProject\\nDetails\\nRepository\\nAquilaDB\\nAn easy to use Neural Search Engine. Index latent vectors along with JSON metadata and do efficient k-NN search.\\nAwadb\\nAI Native database for embedding vectors\\nChroma\\nthe open source embedding database\\nInfinity\\nThe AI-native database built for LLM applications, providing incredibly fast vector and full-text search\\nLancedb\\nDeveloper-friendly, serverless vector database for AI applications. Easily add long-term memory to your LLM apps!\\nMarqo\\nTensor search for humans.\\nMilvus\\nVector database for scalable similarity search and AI applications.\\nPinecone\\nThe Pinecone vector database makes it easy to build high-performance vector search applications. Developer-friendly, fully managed, and easily scalable without infrastructure hassles.\\npgvector\\nOpen-source vector similarity search for Postgres.\\npgvecto.rs\\nVector database plugin for Postgres, written in Rust, specifically designed for LLM.\\nQdrant\\nVector Search Engine and Database for the next generation of AI applications. Also available in the cloud\\ntxtai\\nBuild AI-powered semantic search applications\\nVald\\nA Highly Scalable Distributed Vector Search Engine\\nVearch\\nA distributed system for embedding-based vector retrieval\\nVectorDB\\nA Python vector database you just need - no more, no less.\\nVellum\\nA managed service for ingesting documents and performing hybrid semantic/keyword search across them. Comes with out-of-box support for OCR, text chunking, embedding model experimentation, metadata filtering, and production-grade APIs.\\nWeaviate\\nWeaviate is an open source vector search engine that stores both objects and vectors, allowing for combining vector search with structured filtering with the fault-tolerance and scalability of a cloud-native database, all accessible through GraphQL, REST, and various language clients.\\n\u2b06 back to ToC\\nCode AI\\nProject\\nDetails\\nRepository\\nCodeGeeX\\nCodeGeeX: An Open Multilingual Code Generation Model (KDD 2023)\\nCodeGen\\nCodeGen is an open-source model for program synthesis. Trained on TPU-v4. Competitive with OpenAI Codex.\\nCodeT5\\nOpen Code LLMs for Code Understanding and Generation.\\nContinue\\n\u23e9 the open-source autopilot for software development\u2014bring the power of ChatGPT to VS Code\\nfauxpilot\\nAn open-source alternative to GitHub Copilot server\\ntabby\\nSelf-hosted AI coding assistant. An opensource / on-prem alternative to GitHub Copilot.\\nTraining\\nIDEs and Workspaces\\nProject\\nDetails\\nRepository\\ncode server\\nRun VS Code on any machine anywhere and access it in the browser.\\nconda\\nOS-agnostic, system-level binary package manager and ecosystem.\\nDocker\\nMoby is an open-source project created by Docker to enable and accelerate software containerization.\\nenvd\\n\ud83c\udfd5\ufe0f Reproducible development environment for AI/ML.\\nJupyter Notebooks\\nThe Jupyter notebook is a web-based notebook environment for interactive computing.\\nKurtosis\\nA build, packaging, and run system for ephemeral multi-container environments.\\nWordware\\nA web-hosted IDE where non-technical domain experts work with AI Engineers to build task-specific AI agents. It approaches prompting as a new programming language rather than low/no-code blocks.\\n\u2b06 back to ToC\\nFoundation Model Fine Tuning\\nProject\\nDetails\\nRepository\\nalpaca-lora\\nInstruct-tune LLaMA on consumer hardware\\nfinetuning-scheduler\\nA PyTorch Lightning extension that accelerates and enhances foundation model experimentation with flexible fine-tuning schedules.\\nFlyflow\\nOpen source, high performance fine tuning as a service for GPT4 quality models with 5x lower latency and 3x lower cost\\nLMFlow\\nAn Extensible Toolkit for Finetuning and Inference of Large Foundation Models\\nLora\\nUsing Low-rank adaptation to quickly fine-tune diffusion models.\\npeft\\nState-of-the-art Parameter-Efficient Fine-Tuning.\\np-tuning-v2\\nAn optimized prompt tuning strategy achieving comparable performance to fine-tuning on small/medium-sized models and sequence tagging challenges.\\n(ACL 2022)\\nQLoRA\\nEfficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.\\nTRL\\nTrain transformer language models with reinforcement learning.\\n\u2b06 back to ToC\\nFrameworks for Training\\nProject\\nDetails\\nRepository\\nAccelerate\\n\ud83d\ude80 A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision.\\nApache MXNet\\nLightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler.\\naxolotl\\nA tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.\\nCaffe\\nA fast open framework for deep learning.\\nColossalAI\\nAn integrated large-scale model training system with efficient parallelization techniques.\\nDeepSpeed\\nDeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.\\nHorovod\\nDistributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\\nJax\\nAutograd and XLA for high-performance machine learning research.\\nKedro\\nKedro is an open-source Python framework for creating reproducible, maintainable and modular data science code.\\nKeras\\nKeras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.\\nLightGBM\\nA fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\\nMegEngine\\nMegEngine is a fast, scalable and easy-to-use deep learning framework, with auto-differentiation.\\nmetric-learn\\nMetric Learning Algorithms in Python.\\nMindSpore\\nMindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\\nOneflow\\nOneFlow is a performance-centered and open-source deep learning framework.\\nPaddlePaddle\\nMachine Learning Framework from Industrial Practice.\\nPyTorch\\nTensors and Dynamic neural networks in Python with strong GPU acceleration.\\nPyTorch Lightning\\nDeep learning framework to train, deploy, and ship AI products Lightning fast.\\nXGBoost\\nScalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library.\\nscikit-learn\\nMachine Learning in Python.\\nTensorFlow\\nAn Open Source Machine Learning Framework for Everyone.\\nVectorFlow\\nA minimalist neural network library optimized for sparse data and single machine environments.\\n\u2b06 back to ToC\\nExperiment Tracking\\nProject\\nDetails\\nRepository\\nAim\\nan easy-to-use and performant open-source experiment tracker.\\nClearML\\nAuto-Magical CI/CD to streamline your ML workflow. Experiment Manager, MLOps and Data-Management\\nComet\\nComet is an MLOps platform that offers experiment tracking, model production management, a model registry, and full data lineage from training straight through to production. Comet plays nicely with all your favorite tools, so you don't have to change your existing workflow. Check out CometLLM for all your prompt engineering needs!\\nGuild AI\\nExperiment tracking, ML developer tools.\\nMLRun\\nMachine Learning automation and tracking.\\nKedro-Viz\\nKedro-Viz is an interactive development tool for building data science pipelines with Kedro. Kedro-Viz also allows users to view and compare different runs in the Kedro project.\\nLabNotebook\\nLabNotebook is a tool that allows you to flexibly monitor, record, save, and query all your machine learning experiments.\\nSacred\\nSacred is a tool to help you configure, organize, log and reproduce experiments.\\nWeights &amp; Biases\\nA developer first, lightweight, user-friendly experiment tracking and visualization tool for machine learning projects, streamlining collaboration and simplifying MLOps. W&amp;B excels at tracking LLM-powered applications, featuring W&amp;B Prompts for LLM execution flow visualization, input and output monitoring, and secure management of prompts and LLM chain configurations.\\n\u2b06 back to ToC\\nVisualization\\nProject\\nDetails\\nRepository\\nFiddler AI\\nRich dashboards, reports, and UMAP to perform root cause analysis, pinpoint problem areas, like correctness, safety, and privacy issues, and improve LLM outcomes.\\nManiford\\nA model-agnostic visual debugging tool for machine learning.\\nnetron\\nVisualizer for neural network, deep learning, and machine learning models.\\nOpenOps\\nBring multiple data streams into one dashboard.\\nTensorBoard\\nTensorFlow's Visualization Toolkit.\\nTensorSpace\\nNeural network 3D visualization framework, build interactive and intuitive model in browsers, support pre-trained deep learning models from TensorFlow, Keras, TensorFlow.js.\\ndtreeviz\\nA python library for decision tree visualization and model interpretation.\\nZetane Viewer\\nML models and internal tensors 3D visualizer.\\nZeno\\nAI evaluation platform for interactively exploring data and model outputs.\\nModel Editing\\nProject\\nDetails\\nRepository\\nFastEdit\\nFastEdit aims to assist developers with injecting fresh and customized knowledge into large language models efficiently using one single command.\\n\u2b06 back to ToC\\nData\\nData Management\\nProject\\nDetails\\nRepository\\nArtiVC\\nA version control system to manage large files. Lake is a dataset format with a simple API for creating, storing, and collaborating on AI datasets of any size.\\nDolt\\nGit for Data.\\nDVC\\nData Version Control - Git for Data &amp; Models - ML Experiments Management.\\nDelta-Lake\\nStorage layer that brings scalable, ACID transactions to Apache Spark and other engines.\\nPachyderm\\nPachyderm is a version control system for data.\\nQuilt\\nA self-organizing data hub for S3.\\n\u2b06 back to ToC\\nData Storage\\nProject\\nDetails\\nRepository\\nJuiceFS\\nA distributed POSIX file system built on top of Redis and S3.\\nLakeFS\\nGit-like capabilities for your object storage.\\nLance\\nModern columnar data format for ML implemented in Rust.\\n\u2b06 back to ToC\\nData Tracking\\nProject\\nDetails\\nRepository\\nPiperider\\nA CLI tool that allows you to build data profiles and write assertion tests for easily evaluating and tracking your data's reliability over time.\\nLUX\\nA Python library that facilitates fast and easy data exploration by automating the visualization and data analysis process.\\n\u2b06 back to ToC\\nFeature Engineering\\nProject\\nDetails\\nRepository\\nFeatureform\\nThe Virtual Feature Store. Turn your existing data infrastructure into a feature store.\\nFeatureTools\\nAn open source python framework for automated feature engineering\\n\u2b06 back to ToC\\nData/Feature enrichment\\nProject\\nDetails\\nRepository\\nUpgini\\nFree automated data &amp; feature enrichment library for machine learning: automatically searches through thousands of ready-to-use features from public and community shared data sources and enriches your training dataset with only the accuracy improving features\\nFeast\\nAn open source feature store for machine learning.\\n\u2b06 back to ToC\\nLarge Scale Deployment\\nML Platforms\\nProject\\nDetails\\nRepository\\nComet\\nComet is an MLOps platform that offers experiment tracking, model production management, a model registry, and full data lineage from training straight through to production. Comet plays nicely with all your favorite tools, so you don't have to change your existing workflow. Check out CometLLM for all your prompt engineering needs!\\nClearML\\nAuto-Magical CI/CD to streamline your ML workflow. Experiment Manager, MLOps and Data-Management.\\nHopsworks\\nHopsworks is a MLOps platform for training and operating large and small ML systems, including fine-tuning and serving LLMs. Hopsworks includes both a feature store and vector database for RAG.\\nOpenLLM\\nAn open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.\\nMLflow\\nOpen source platform for the machine learning lifecycle.\\nMLRun\\nAn open MLOps platform for quickly building and managing continuous ML applications across their lifecycle.\\nModelFox\\nModelFox is a platform for managing and deploying machine learning models.\\nKserve\\nStandardized Serverless ML Inference Platform on Kubernetes\\nKubeflow\\nMachine Learning Toolkit for Kubernetes.\\nPAI\\nResource scheduling and cluster management for AI.\\nPolyaxon\\nMachine Learning Management &amp; Orchestration Platform.\\nPrimehub\\nAn effortless infrastructure for machine learning built on the top of Kubernetes.\\nOpenModelZ\\nOne-click machine learning deployment (LLM, text-to-image and so on) at scale on any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine).\\nSeldon-core\\nAn MLOps framework to package, deploy, monitor and manage thousands of production machine learning models\\nStarwhale\\nAn MLOps/LLMOps platform for model building, evaluation, and fine-tuning.\\nTrueFoundry\\nA PaaS to deploy, Fine-tune and serve LLM Models on a company\u2019s own Infrastructure with Data Security and Optimal GPU and Cost Management. Launch your LLM Application at Production scale with best DevSecOps practices.\\nWeights &amp; Biases\\nA lightweight and flexible platform for machine learning experiment tracking, dataset versioning, and model management, enhancing collaboration and streamlining MLOps workflows. W&amp;B excels at tracking LLM-powered applications, featuring W&amp;B Prompts for LLM execution flow visualization, input and output monitoring, and secure management of prompts and LLM chain configurations.\\n\u2b06 back to ToC\\nWorkflow\\nProject\\nDetails\\nRepository\\nAirflow\\nA platform to programmatically author, schedule and monitor workflows.\\naqueduct\\nAn Open-Source Platform for Production Data Science\\nArgo Workflows\\nWorkflow engine for Kubernetes.\\nFlyte\\nKubernetes-native workflow automation platform for complex, mission-critical data and ML processes at scale.\\nHamilton\\nA lightweight framework to represent ML/language model pipelines as a series of python functions.\\nKubeflow Pipelines\\nMachine Learning Pipelines for Kubeflow.\\nLangFlow\\nAn effortless way to experiment and prototype LangChain flows with drag-and-drop components and a chat interface.\\nMetaflow\\nBuild and manage real-life data science projects with ease!\\nPloomber\\nThe fastest way to build data pipelines. Develop iteratively, deploy anywhere.\\nPrefect\\nThe easiest way to automate your data.\\nVDP\\nAn open-source unstructured data ETL tool to streamline the end-to-end unstructured data processing pipeline.\\nZenML\\nMLOps framework to create reproducible pipelines.\\n\u2b06 back to ToC\\nScheduling\\nProject\\nDetails\\nRepository\\nKueue\\nKubernetes-native Job Queueing.\\nPAI\\nResource scheduling and cluster management for AI (Open-sourced by Microsoft).\\nSlurm\\nA Highly Scalable Workload Manager.\\nVolcano\\nA Cloud Native Batch System (Project under CNCF).\\nYunikorn\\nLight-weight, universal resource scheduler for container orchestrator systems.\\n\u2b06 back to ToC\\nModel Management\\nProject\\nDetails\\nRepository\\nComet\\nComet is an MLOps platform that offers Model Production Management, a Model Registry, and full model lineage from training straight through to production. Use Comet for model reproducibility, model debugging, model versioning, model visibility, model auditing, model governance, and model monitoring.\\ndvc\\nML Experiments Management - Data Version Control - Git for Data &amp; Models\\nModelDB\\nOpen Source ML Model Versioning, Metadata, and Experiment Management\\nMLEM\\nA tool to package, serve, and deploy any ML model on any platform.\\normb\\nDocker for Your ML/DL Models Based on OCI Artifacts\\n\u2b06 back to ToC\\nPerformance\\nML Compiler\\nProject\\nDetails\\nRepository\\nONNX-MLIR\\nCompiler technology to transform a valid Open Neural Network Exchange (ONNX) graph into code that implements the graph with minimum runtime support.\\nTVM\\nOpen deep learning compiler stack for cpu, gpu and specialized accelerators\\n\u2b06 back to ToC\\nProfiling\\nProject\\nDetails\\nRepository\\noctoml-profile\\noctoml-profile is a python library and cloud service designed to provide the simplest experience for assessing and optimizing the performance of PyTorch models on cloud hardware with state-of-the-art ML acceleration technology.\\nscalene\\na high-performance, high-precision CPU, GPU, and memory profiler for Python\\n\u2b06 back to ToC\\nAutoML\\nProject\\nDetails\\nRepository\\nArchai\\na platform for Neural Network Search (NAS) that allows you to generate efficient deep networks for your applications.\\nautoai\\nA framework to find the best performing AI/ML model for any AI problem.\\nAutoGL\\nAn autoML framework &amp; toolkit for machine learning on graphs\\nAutoGluon\\nAutoML for Image, Text, and Tabular Data.\\nautoml-gs\\nProvide an input CSV and a target field to predict, generate a model + code to run it.\\nautokeras\\nAutoML library for deep learning.\\nAuto-PyTorch\\nAutomatic architecture search and hyperparameter optimization for PyTorch.\\nauto-sklearn\\nan automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator.\\nDragonfly\\nAn open source python library for scalable Bayesian optimisation.\\nDetermined\\nscalable deep learning training platform with integrated hyperparameter tuning support; includes Hyperband, PBT, and other search methods.\\nDEvol (DeepEvolution)\\na basic proof of concept for genetic architecture search in Keras.\\nEvalML\\nAn open source python library for AutoML.\\nFEDOT\\nAutoML framework for the design of composite pipelines.\\nFLAML\\nFast and lightweight AutoML (\\npaper\\n).\\nGoptuna\\nA hyperparameter optimization framework, inspired by Optuna.\\nHpBandSter\\na framework for distributed hyperparameter optimization.\\nHPOlib2\\na library for hyperparameter optimization and black box optimization benchmarks.\\nHyperband\\nopen source code for tuning hyperparams with Hyperband.\\nHypernets\\nA General Automated Machine Learning Framework.\\nHyperopt\\nDistributed Asynchronous Hyperparameter Optimization in Python.\\nhyperunity\\nA toolset for black-box hyperparameter optimisation.\\nIntelli\\nA framework to connect a flow of ML models by applying graph theory.\\nKatib\\nKatib is a Kubernetes-native project for automated machine learning (AutoML).\\nKeras Tuner\\nHyperparameter tuning for humans.\\nlearn2learn\\nPyTorch Meta-learning Framework for Researchers.\\nLudwig\\na toolbox built on top of TensorFlow that allows to train and test deep learning models without the need to write code.\\nMOE\\na global, black box optimization engine for real world metric optimization by Yelp.\\nModel Search\\na framework that implements AutoML algorithms for model architecture search at scale.\\nNASGym\\na proof-of-concept OpenAI Gym environment for Neural Architecture Search (NAS).\\nNNI\\nAn open source AutoML toolkit for automate machine learning lifecycle, including feature engineering, neural architecture search, model compression and hyper-parameter tuning.\\nOptuna\\nA hyperparameter optimization framework.\\nPycaret\\nAn open-source, low-code machine learning library in Python that automates machine learning workflows.\\nRay Tune\\nScalable Hyperparameter Tuning.\\nREMBO\\nBayesian optimization in high-dimensions via random embedding.\\nRoBO\\na Robust Bayesian Optimization framework.\\nscikit-optimize(skopt)\\nSequential model-based optimization with a\\nscipy.optimize\\ninterface.\\nSpearmint\\na software package to perform Bayesian optimization.\\nTPOT\\none of the very first AutoML methods and open-source software packages.\\nTorchmeta\\nA Meta-Learning library for PyTorch.\\nVegas\\nan AutoML algorithm tool chain by Huawei Noah's Arb Lab.\\n\u2b06 back to ToC\\nOptimizations\\nProject\\nDetails\\nRepository\\nFeatherCNN\\nFeatherCNN is a high performance inference engine for convolutional neural networks.\\nForward\\nA library for high performance deep learning inference on NVIDIA GPUs.\\nNCNN\\nncnn is a high-performance neural network inference framework optimized for the mobile platform.\\nPocketFlow\\nuse AutoML to do model compression.\\nTensorFlow Model Optimization\\nA suite of tools that users, both novice and advanced, can use to optimize machine learning models for deployment and execution.\\nTNN\\nA uniform deep learning inference framework for mobile, desktop and server.\\n\u2b06 back to ToC\\nFederated ML\\nProject\\nDetails\\nRepository\\nEasyFL\\nAn Easy-to-use Federated Learning Platform\\nFATE\\nAn Industrial Grade Federated Learning Framework\\nFedML\\nThe federated learning and analytics library enabling secure and collaborative machine learning on decentralized data anywhere at any scale. Supporting large-scale cross-silo federated learning, cross-device federated learning on smartphones/IoTs, and research simulation.\\nFlower\\nA Friendly Federated Learning Framework\\nHarmonia\\nHarmonia is an open-source project aiming at developing systems/infrastructures and libraries to ease the adoption of federated learning (abbreviated to FL) for researches and production usage.\\nTensorFlow Federated\\nA framework for implementing federated learning\\n\u2b06 back to ToC\\nAwesome Lists\\nProject\\nDetails\\nRepository\\nAwesome Argo\\nA curated list of awesome projects and resources related to Argo\\nAwesome AutoDL\\nAutomated Deep Learning: Neural Architecture Search Is Not the End (a curated list of AutoDL resources and an in-depth analysis)\\nAwesome AutoML\\nCurating a list of AutoML-related research, tools, projects and other resources\\nAwesome AutoML Papers\\nA curated list of automated machine learning papers, articles, tutorials, slides and projects\\nAwesome-Code-LLM\\n\ud83d\udc68\\u200d\ud83d\udcbb An awesome and curated list of best code-LLM for research.\\nAwesome Federated Learning Systems\\nA curated list of Federated Learning Systems related academic papers, articles, tutorials, slides and projects.\\nAwesome Federated Learning\\nA curated list of federated learning publications, re-organized from Arxiv (mostly)\\nawesome-federated-learning\\nacc\\nAll materials you need for Federated Learning: blogs, videos, papers, and software, etc.\\nAwesome Open MLOps\\nThis is the Fuzzy Labs guide to the universe of free and open source MLOps tools.\\nAwesome Production Machine Learning\\nA curated list of awesome open source libraries to deploy, monitor, version and scale your machine learning\\nAwesome Tensor Compilers\\nA list of awesome compiler projects and papers for tensor computation and deep learning.\\nkelvins/awesome-mlops\\nA curated list of awesome MLOps tools.\\nvisenger/awesome-mlops\\nMachine Learning Operations - An awesome list of references for MLOps\\ncurrentslab/awesome-vector-search\\nA curated list of awesome vector search framework/engine, library, cloud service and research papers to vector similarity search.\\npleisto/flappy\\nProduction-Ready LLM Agent SDK for Every Developer\\n\u2b06 back to ToC\\nAbout\\nAn awesome &amp; curated list of best LLMOps tools for developers\\nTopics\\nawesome-list\\nmlops\\nai-development-tools\\nllmops\\nResources\\nReadme\\nLicense\\nCC0-1.0 license\\nActivity\\nCustom properties\\nStars\\n3.7k\\nstars\\nWatchers\\n64\\nwatching\\nForks\\n353\\nforks\\nReport repository\\nReleases\\nNo releases published\\nPackages\\n0\\nNo packages published\\nContributors\\n81\\n+ 67 contributors\\nLanguages\\nShell\\n86.2%\\nPython\\n13.8%\",\n        \"tool_call_id\": \"call_YeaR70E6l7iM7UHEtp709iVc\",\n        \"name\": \"extract_content\",\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": \"Medium\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nPAGE NOT FOUND\\n404\\nOut of nothing, something.\\nYou can find (just about) anything on\\nMedium\\n\u2014 apparently even a page that doesn\u2019t exist. Maybe these stories will take you somewhere new?\\nHome\\n\u201cYou Can\u2019t Eat Technology.\u201d\\nAdam DeMartino\\nAug 25, 2024\\n\u00b7\\n11 min read\\n\u201cYou Can\u2019t Eat Technology.\u201d\\nAdam DeMartino\\nAug 25, 2024\\n\u00b7\\n11 min read\\nReflections from the 2024 DNC\\nIsaac Saul\\nin\\nThe Political Prism\\nAug 25, 2024\\n\u00b7\\n15 min read\\nReflections from the 2024 DNC\\nIsaac Saul\\nin\\nThe Political Prism\\nAug 25, 2024\\n\u00b7\\n15 min read\\nIt\u2019s cognitive bias week, because thinking is hard\\nThe Medium Newsletter\\nin\\nThe Medium Blog\\nAug 27, 2024\\n\u00b7\\n3 min read\\nIt\u2019s cognitive bias week, because thinking is hard\\nThe Medium Newsletter\\nin\\nThe Medium Blog\\nAug 27, 2024\\n\u00b7\\n3 min read\\nThe Fun and Games of College Tours\\njen murphy parker\\nAug 17, 2024\\n\u00b7\\n12 min read\\nMember-only\\nThe Fun and Games of College Tours\\njen murphy parker\\nAug 17, 2024\\n\u00b7\\n12 min read\\nMember-only\",\n        \"tool_call_id\": \"call_UWuyM3dy71Js7fspwSKnMlGC\",\n        \"name\": \"extract_content\",\n    },\n]\n\n\n@pytest.mark.asyncio\nasync def test_conversation():\n    \"\"\"Tests that in a conversation setting, the llm generated query is context-relevant.\"\"\"\n    web_assistant = WebAssistant(\n        search_history=[\n            \"best LLM development tools\",\n            \"top libraries for LLM development\",\n            \"LLM libraries for software engineers\",\n            \"LLM dev tools for machine learning\",\n            \"most popular libraries for LLM development\",\n        ],\n        messages=test_conversation_messages,\n    )\n    response = await web_assistant._stream(\"What is mirascope library?\")\n    async for _, tool in response:\n        queries = tool.args.get(\"queries\", \"\") if tool else \"\"\n        is_context_relevant = False\n        for query in queries:\n            context_relevant = await check_context_relevance(\n                web_assistant.search_history, \"What is mirascope library?\", query\n            )\n            is_context_relevant = context_relevant.is_context_relevant\n            if is_context_relevant:\n                break\n        assert is_context_relevant\n\n\nipytest.run()\n</pre> test_conversation_messages = [     {\"role\": \"user\", \"content\": \"I am a SWE looking for a LLM dev tool library\"},     {         \"role\": \"assistant\",         \"content\": \"\",         \"tool_calls\": [             {                 \"type\": \"function\",                 \"function\": {                     \"arguments\": '{\"queries\":[\"best LLM development tools\",\"top libraries for LLM development\",\"LLM libraries for software engineers\",\"LLM dev tools for machine learning\",\"most popular libraries for LLM development\"]}',                     \"name\": \"_web_search\",                 },                 \"id\": \"call_1\",             }         ],     },     {         \"role\": \"tool\",         \"content\": \"https://www.blog.aiport.tech/p/top-9-libraries-to-accelerate-llm\\n\\nhttps://github.com/tensorchord/awesome-llmops\\n\\nhttps://www.blog.aiport.tech/p/top-9-libraries-to-accelerate-llm\\n\\nhttps://medium.com/pythonforall/essential-python-libraries-for-llms-and-application-development-in-2024-17c64b672421\\n\\nhttps://www.datacamp.com/blog/top-open-source-llms\\n\\nhttps://machinelearningmastery.com/5-essential-free-tools-getting-started-llms/\\n\\nhttps://github.com/princeton-nlp/swe-agent\\n\\nhttps://arxiv.org/html/2407.01489v1\\n\\nhttps://www.datacamp.com/blog/top-open-source-llms\\n\\nhttps://llmmodels.org/blog/top-10-open-source-llm-frameworks-2024/\",         \"tool_call_id\": \"call_1\",         \"name\": \"_web_search\",     },     {         \"role\": \"assistant\",         \"content\": \"\",         \"tool_calls\": [             {                 \"type\": \"function\",                 \"function\": {                     \"arguments\": '{\"url\": \"https://research.aimultiple.com/llmops-tools/\"}',                     \"name\": \"extract_content\",                 },                 \"id\": \"call_UXnnadcCnki8qvCxrzRI1fXA\",             },             {                 \"type\": \"function\",                 \"function\": {                     \"arguments\": '{\"url\": \"https://www.techradar.com/computing/artificial-intelligence/best-large-language-models-llms-for-coding\"}',                     \"name\": \"extract_content\",                 },                 \"id\": \"call_S0OnNQqxtPH5HtDb1buzjedV\",             },             {                 \"type\": \"function\",                 \"function\": {                     \"arguments\": '{\"url\": \"https://www.blog.aipo.rt.tech/p/top-9-libraries-to-accelerate-llm\"}',                     \"name\": \"extract_content\",                 },                 \"id\": \"call_aCyaCFXUWMWloDkETTrxHyoJ\",             },             {                 \"type\": \"function\",                 \"function\": {                     \"arguments\": '{\"url\": \"https://medium.com/pythonforall/essential-python-libraries-for-llms-and-application-development-in-2024-17c64b672421\"}',                     \"name\": \"extract_content\",                 },                 \"id\": \"call_71e9AkvKjIuEp3QceqO4DCUK\",             },             {                 \"type\": \"function\",                 \"function\": {                     \"arguments\": '{\"url\": \"https://github.com/tensorchord/awesome-llmops\"}',                     \"name\": \"extract_content\",                 },                 \"id\": \"call_YeaR70E6l7iM7UHEtp709iVc\",             },             {                 \"type\": \"function\",                 \"function\": {                     \"arguments\": '{\"url\": \"https://medium.com/@bijit2111987/top-llm-dev-tool-and-when-to-use-them-in-your-ai-stack-622a651ec0e6\"}',                     \"name\": \"extract_content\",                 },                 \"id\": \"call_UWuyM3dy71Js7fspwSKnMlGC\",             },         ],     },     {         \"role\": \"tool\",         \"content\": \"We follow\\nethical norms\\n&amp;\\nour process\\nfor objectivity. Brands with links to their websites\\nfund\\nour research.\\nTable of contents\\nLLMOps Landscape\\nCompare the top LLMOps platforms in 2023\\nTools for secure and complaint LLMs\\nDisclaimer about current categorization approach\\nWhich LLMOps tool is the best choice for your business?\\nFurther reading\\nMLOps\\nLLM\\nUpdated on\\nAug 7\\nComparing 10+ LLMOps Tools: A Comprehensive Vendor Benchmark\\nWritten by\\nCem Dilmegani\\nCem Dilmegani\\nCem Dilmegani\\nCem Dilmegani\\nCem has been the principal analyst at AIMultiple since 2017. AIMultiple informs hundreds of thousands of businesses (as per similarWeb) including 55% of Fortune 500 every month.\\nCem's work has been cited by leading global publications including Business Insider,  Forbes, Washington Post, global firms like Deloitte, HPE and NGOs like World Economic Forum and supranational organizations like European Commission. You can see more reputable companies and resources that referenced AIMultiple.\\nThroughout his career, Cem served as a tech consultant, tech buyer and tech entrepreneur. He advised enterprises on their technology decisions at McKinsey &amp; Company and Altman Solon for more than a decade. He also published a McKinsey report on digitalization.\\nHe led technology strategy and procurement of a telco while reporting to the CEO. He has also led commercial growth of deep tech company Hypatos that reached a 7 digit annual recurring revenue and a 9 digit valuation from 0 within 2 years. Cem's work in Hypatos was covered by leading technology publications like TechCrunch and Business Insider.\\nCem regularly speaks at international technology conferences. He graduated from Bogazici University as a computer engineer and holds an MBA from Columbia Business School.\\nView Full Profile\\nFollow on\\nWe follow\\nethical norms\\n&amp;\\nour process\\nfor objectivity. Brands with links to their websites\\nfund\\nour research.\\nThe number of\\nlarge language models (LLMs)\\nhas been increasing since 2019 due to the models\u2019 extensive application areas and capabilities (See Figure 1).\\nYet, the estimates show that designing a new foundation model can cost up to $90 million while fine-tuning or enhancing existing large language models can cost $1 million to $100 thousand.\\n1\\nThese costs result from:\\nComputational costs like hardware usage for training runs\\nData gathering and labelling costs\\nEngineering and R&amp;D costs\\nFigure 1: The increasing number of LLMs since 2019\\n2\\nLLMOps\\ntools can reduce these costs by facilitating LLM management. However, LLMOps is a relatively recent solution and most business leaders are not aware of the leading players in this market. This article explains the LLMOps market and compares available tools.\\nLLMOps Landscape\\nThere are 20+ tools that claim to be LLMOps solutions, which can be evaluated under 6 main categories:\\n1. LLMOps Platforms\\nThese are either designed specifically for LLMOps or are MLOps platforms that started offering LLMOps capabilities. They include features that allow carrying out these operations on LLMs:\\nFinetuning\\nVersioning\\nDeploying\\nThese LLM platforms can offer different levels of flexibility and ease of use:\\nNo-code LLM platforms:\\nSome of these platforms are no-code and low-code, which facilitate LLM adoption. However, these tools typically have limited flexibility.\\nCode-first platforms:\\nThese platforms target machine learning engineers and data scientists. They tend to offer a higher level of flexibility.\\n2. Integration frameworks\\nThese tools are built to facilitate developing\\nLLM applications\\nsuch as document analyzers, code analyzers, chatbots etc.\\n3.) Vector databases (VD)\\nVDs store high-dimensional data vectors, such as patient data covering symptoms, blood test results, behaviors, and general health. Some VD software like deep lake can facilitate LLM operations.\\n4.) Fine-tuning tools\\nFine-tuning tools are frameworks, or platforms for fine-tuning pre-trained models. These tools provide a streamlined workflow to modify, retrain, and optimize pre-trained models for natural language processing, computer vision, and more tasks. Some libraries are also designed for fine-tuning, such as Hugging Face Transformers, PyTorch, and TensorFlow.\\n5.) RLHF tools\\nReinforcement learning from human feedback\\n, or RLHF, is a way for AI to learn the best actions by listening to human input. Typically, Reinforcement learning includes an RL algorithm to learn by interacting with the environment and receiving rewards or penalties based on its actions.\\nIn contrast, RLHF tools (e.g. Clickworker or Appen) include human feedback in the learning loop. RLHF can be useful to:\\nEnhance LLM fine-tuning by large data labeling\\nImplement AI governance by reducing biases in LLM responses and moderating content\\nCustomize model\\nImprove contextual understanding.\\n6.) LLM testing tools\\nLLM testing tools evaluate and assess LLMs by testing model performance, capabilities, and potential biases in various language-related tasks and applications, such as natural language understanding and generation. Testing tools may include:\\nTesting frameworks\\nBenchmark datasets\\nEvaluation metrics.\\n7.) LLM monitoring and observability\\nLLM monitoring and observability tools ensure their proper functioning, user safety, and brand protection. LLM monitoring includes activities like:\\nFunctional monitoring\\n: Keeping track of factors like response time, token usage, number of requests, costs and error rates.\\nPrompt monitoring\\n: Checking user inputs and prompts to evaluate toxic content in responses, measure embedding distances, and identify malicious prompt injections.\\nResponse monitoring:\\nAnalyzing to discover\\nhallucinatory\\nbehavior, topic divergence, tone and sentiment in the responses.\\nCompare the top LLMOps platforms in 2023\\nIn this section, we focus on LLMOps platforms and excluded integration frameworks and other tools. LLMOps platforms can be examined in these categories:\\n1. MLOps platforms\\nSome\\nMLOps platforms\\noffer LLMOps toolkits.\\nMachine Learning Operations (MLOps)\\nmanages and optimizes the end-to-end machine learning lifecycle. Since LLMs are also machine learning models, MLOps vendors are naturally expanding into this domain.\\n2. LLM platforms\\nSome\\nLLM providers\\n, especially OpenAI, are also providing LLMOps capabilities to fine-tune, integrate and deploy their models.\\n3. Data and cloud platforms\\nData or cloud platforms are starting to offer LLMOps capabilities that allow their users to leverage their own data to build and finetune LLMs. For example, Databricks acquired MosaicML for $1.3 billion.\\n3\\nCloud platforms\\nCloud leaders Amazon, Azure and Google have all launched their LLMOps offering which allows users to deploy models from different providers with ease\\n4. LLMOPs frameworks\\nThis category includes tools that exclusively focus on optimizing and managing LLM operations. The table below shows the Github stars, B2B reviews and average B2B score from B2B review pages (Trustradius, Gartner &amp; G2) for some of these LLMOps tools:\\nLLMOps Tools\\nGithub Stars\\nNumber of B2B Reviews*\\nAverage Review Score**\\nNemo by Nvidia\\n7,900\\nNA\\nNA\\nDeep Lake\\n6,600\\nNA\\nNA\\nFine-Tuner AI\\n6,000\\nNA\\nNA\\nSnorkel AI\\n5,500\\nNA\\nNA\\nZen ML\\n3,000\\nNA\\nNA\\nLamini AI\\n2100\\nNA\\nNA\\nComet\\n54\\nNA\\nNA\\nTitan ML\\n47\\nNA\\nNA\\nDeepset AI\\n6\\nNA\\nNA\\nValohai\\nNot open source\\n20\\n4.9\\nHere is a brief explanation for each tool in alphabetical order:\\nComet:\\nComet streamlines the ML lifecycle, tracking experiments and production models. Suited for large enterprise teams, it offers various deployment strategies. It supports private cloud, hybrid, and on-premise setups.\\nFigure 2:\\nComet LLMops platform\\n4\\nDeep Lake:\\nDeep Lake combines the capabilities of Data Lakes and Vector Databases to create, refine, and implement high-quality LLMs and MLOps solutions for businesses. Deep Lake allows users to visualize and manipulate datasets in their browser or Jupyter notebook, swiftly accessing different versions and generating new ones through queries, all compatible with PyTorch and TensorFlow.\\nDeepset AI:\\nDeepset AI is a comprehensive platform that allows users to integrate their data with LLMs to build and deploy customized LLM features in their applications. Deepset supports Retrieval-augmented generation (RAG) and Enterprise knowledge search, as well.\\nLamini AI:\\nLamini AI provides an easy method for training LLMs through both prompt-tuning and base model training. Lamini AI users can write custom code, integrate their own data, and host the resulting LLM on their infrastructure.\\nNemo by Nvidia:\\nNvidia offers an end-to-end, cloud-native enterprise framework to develop, customize, and employ generative AI models and LLM applications. The framework can execute various tasks required to train LLMs, such as token classification, prompt learning and question answering.\\nSnorkel AI:\\nSnorkel AI empowers enterprises to construct or customize foundation models (FMs) and large language models (LLMs) to achieve remarkable precision on domain-specific datasets and use cases. Snorkel AI introduces programmatic labelling, enabling data-centric AI development with automated processes.\\nFigure 3:\\nSnorkel AI LLMOps platform\\n5\\n6.\\nTitan ML:\\nTitanML is an NLP development platform that aims to allow businesses to swiftly build and implement smaller, more economical deployments of large language models. It offers proprietary, automated, efficient fine-tuning and inference optimization techniques. This way, it allows businesses to create and roll out large language models in-house.\\n7.\\nValohai:\\nValohai streamlines MLOps and LLMs, automating data extraction to model deployment. It can store models, experiments, and artefacts, making monitoring and deployment easier. Valohai creates an efficient workflow from code to deployment, supporting notebooks, scripts, and Git projects.\\n8.\\nZen ML:\\nZenML primarily focuses on machine learning operations (MLOps) and the management of the machine learning workflow, including data preparation, experimentation, and model deployment.\\nTools for secure and complaint LLMs\\nSome LLMOps integrate with AI governance and\\nLLM security\\ntechnologies to ensure safe, unbiased, and ethical LLM deployment and operation. Check out more on these:\\nCompare Top 25 AI Governance Tools: A Vendor Benchmark\\nCompare 20 LLM Security Tools &amp; Open-Source Frameworks\\nDisclaimer about current categorization approach\\nWe are aware that there are different approaches to categorize these tools. For instance, some vendors include other technologies that can help large language model development in this landscape, such as containerization or edge computing. However, such technologies are not built for designing or monitoring models, even though they can be paired with LLMOps tools to improve model performance. Therefore, we excluded these tools.\\nA more classical approach categorizes tools based on licence type (e.g. open source or not) or whether the tool provides pre-trained models or not. While these are relevant categorizations, we think they are less critical than other functionality provided by the tool. For example, it is quite important whether an LLM is open source or not since it impacts how the end user can finetune the model. However, an LLMOps platform, like most other software, will be used by most end users without modifications to the software code and therefore it is less impactful for an LLMOps tool to be open source.\\nWhich LLMOps tool is the best choice for your business?\\nWe now provide relatively generic recommendations on choosing these tools. We will make these more specific as we explore LLMOps platforms in more detail and as the market matures.\\nHere are a few steps you must complete in your selection process:\\nDefine goals:\\nClearly outline your business goals to establish a solid foundation for your LLMOps tool selection process. For example, if your goal requires training a model from scratch vs fine-tuning an existing model, this will have important implications to your LLMOps stack.\\nDefine requirements:\\nBased on your goal, certain requirements will become more important. For example, if you aim to enable business users to use LLMs, you may want to include no code in your list of requirements.\\nPrepare a shortlist\\n: Consider user reviews and feedback to gain insights into real-world experiences with different LLMOps tools. Rely on this market data to prepare a shortlist.\\nCompare functionality:\\nUtilize free trials and demos provided by various LLMOps tools to compare their features and functionalities firsthand.\\nWhat is LLMOps?\\nLarge Language Models (LLMs) are advanced machine learning models designed to understand and generate human-like text based on the patterns and information they\u2019ve learned from training data. These models are built using deep learning models to capture intricate linguistic nuances and context.\\nLLMOps refer to techniques and tools used for the operational model management of LLMs in production environments.\\nKey components of LLMOps tools\\nLarge Language Model Operations (LLMOps) tools encompass crucial components for efficient management and deployment of large language models (LLMs). These tools typically include features such as:\\n\u2013\\nPrompt Engineering:\\nCreating effective prompt templates for improved model performance.\\n\u2013\\nData Management:\\nHandling vast datasets, ensuring proper data versioning, and facilitating exploratory data analysis.\\n\u2013\\nModel Fine Tuning:\\nFine-tuning LLMs to specific tasks and refining models for optimal performance.\\n\u2013\\nModel Monitoring:\\nContinuous tracking of model outcomes, detection of accuracy degradation, and addressing model drift.\\nWhat are LLMOps benefits?\\nLLMOps delivers significant advantages to machine learning projects leveraging large language models:\\n1.) Increased Accuracy:\\nEnsuring high-quality data for training and reliable deployment enhances model accuracy.\\n2.)\\nReduced Latency:\\nEfficient deployment strategies lead to reduced latency in LLMs, enabling faster data retrieval.\\n3.) Fairness Promotion:\\nStriving to eliminate bias ensures more impartial outputs, preventing discrimination.\\nLLMOps challenges &amp; solutions\\nChallenges in large language model operations require robust solutions to maintain optimal performance:\\n1.) Data Management Challenges:\\nHandling vast datasets and sensitive data necessitates efficient data collection and versioning.\\n2.)\\nModel Monitoring Solutions:\\nImplementing model monitoring tools to track model outcomes, detect accuracy degradation, and address model drift.\\n3.)\\nScalable Deployment:\\nDeploying scalable infrastructure and utilizing cloud-native technologies to meet computational power requirements.\\n4.)\\nOptimizing Models:\\nEmploying model compression techniques and refining models to enhance overall efficiency.\\nLLMOps tools are pivotal in overcoming challenges and delivering higher quality models in the dynamic landscape of large language models.\\nFurther reading\\nExplore more on LLMs, MLOps and AIOps by checking out our articles:\\nMLOps Tools &amp; Platforms Landscape: In-Depth Guide\\n15 Best AiOps Platforms: Streamline IT Ops with AI\\nChatGPT AIOps in IT Automation: 8 Powerful Examples\\nIf you still have questions about LLMOps tools and landscape, we would like to help:\\nFind the Right Vendors\\nExternal sources\\n1.  \u201c\\nThe CEO\u2019s\\xa0Roadmap on Generative AI\\n\u201d BCG. March 2023. Revisited August 11, 2023.\\n2.  \u201c\\nA Survey of Large Language Models.\\n\u201d\\nGithub\\n. March 2023. Revisited August 11, 2023.\\n3. \u201c\\nDatabricks Signs Definitive Agreement to Acquire MosaicML, a Leading Generative AI Platform\\n\u201c.\\nDatabricks\\n. June 26, 2023. Retrieved August 24, 2023.\\n4.  \u201c\\nDebugging Large Language Models with Comet LLMOps Tools\\n.\u201d\\nComet\\n. Revisited August 16, 2023.\\n5.  Harvey, N(March 20, 2023). \u201c\\nSnorkel Flow Spring 2023: warm starts and foundation models.\\n\u201d\\nSnorkelAI\\n. Revisited August 16, 2023.\\nShare This Article\\nCem Dilmegani\\nCem has been the principal analyst at AIMultiple since 2017. AIMultiple informs hundreds of thousands of businesses (as per similarWeb) including 55% of Fortune 500 every month.\\nCem's work has been cited by leading global publications including Business Insider,  Forbes, Washington Post, global firms like Deloitte, HPE and NGOs like World Economic Forum and supranational organizations like European Commission. You can see more reputable companies and resources that referenced AIMultiple.\\nThroughout his career, Cem served as a tech consultant, tech buyer and tech entrepreneur. He advised enterprises on their technology decisions at McKinsey &amp; Company and Altman Solon for more than a decade. He also published a McKinsey report on digitalization.\\nHe led technology strategy and procurement of a telco while reporting to the CEO. He has also led commercial growth of deep tech company Hypatos that reached a 7 digit annual recurring revenue and a 9 digit valuation from 0 within 2 years. Cem's work in Hypatos was covered by leading technology publications like TechCrunch and Business Insider.\\nCem regularly speaks at international technology conferences. He graduated from Bogazici University as a computer engineer and holds an MBA from Columbia Business School.\\nNext to Read\\nGuide to RLHF LLMs in 2024: Benefits &amp; Top Vendors\\nJan 3\\n5 min read\\nLLMOPs vs MLOPs in 2024: Discover the Best Choice for You\\nAug 7\\n6 min read\\nUsing Vector Databases for LLMs: Why is it Crucial in 2024?\\nAug 14\\n5 min read\\nComments\\nYour email address will not be published. All fields are required.\\n0\\nComments\\nPost Comment\\nRelated research\\nCompare 20 LLM Security Tools &amp; Open-Source Frameworks in '24\\nAug 7\\n12 min read\\n12 Retrieval Augmented Generation (RAG) Tools / Software in '24\\nAug 7\\n6 min read\",         \"tool_call_id\": \"call_UXnnadcCnki8qvCxrzRI1fXA\",         \"name\": \"extract_content\",     },     {         \"role\": \"tool\",         \"content\": \"PRICE\\nVERDICT\\nREASONS TO BUY\\nREASONS TO AVOID\\nVERDICT\\nREASONS TO BUY\\nREASONS TO AVOID\\n(Image credit: Future)\\nJump to:\\nBest for Enterprises\\nBest free\\nBest Value\\nBest for code generation\\nBest for debugging\\nFAQs\\nThe best Large Language Models (LLMs) for coding have been trained with code related data and are a new approach that developers are using to augment workflows to improve efficiency and productivity. These coding assistants can be used for a wide range of code related tasks, such as code generation, code analysis to help with debugging, refactoring, and writing test cases, as well offering chat capabilities to discuss problems and inspire developers with solutions. For this guide we tested several different LLMs that can be used for coding assistants to work out which ones present the best results for their given category.\\nThe\\nbest large language models\\nare area of technology that is moving very quickly so while we do our best to keep this guide as up to date as possible, you may want to check if a newer model has been released and whether it fits your specific use case better.\\nThe best large language models (LLMs) for coding\\nWhy you can trust TechRadar\\nWe spend hours testing every product or service we review, so you can be sure you\u2019re buying the best.\\nFind out more about how we test.\\nBest for Enterprises\\n(Image credit: Copilot)\\nGitHub Copilot\\nThe best LLM for business\\nReasons to buy\\n+\\nOffers a first-party extension for direct integration into several popular development environments\\n+\\nMultiple subscription tiers with varying feature levels\\n+\\nBuilt upon OpenAI\u2019s GPT-4 model\\n+\\nUnlimited messages and interactions for all subscription tiers\\nReasons to avoid\\n-\\nRequires a subscription to use\\n-\\nCan\u2019t be self-hosted\\n-\\nNot immune to providing inaccurate or incorrect prompts\\nOriginally released in October 2021, GitHub Copilot is a version of\\nMicrosoft\\n\u2019s Copilot LLM that is specifically trained with data to assist coders and developers with their work with the aim to improve efficiency and productivity. While the original release used\\nOpenAI\\n\u2019s Codex model, a modified version of GPT-3 which was also trained as a coding assistant, GitHub Copilot was updated to use the more advanced GPT-4 model in November 2023.\\nA core feature of GitHub Copilot is the extension provided that allows direct integration of the LLM into commonly used Integrated Development Environments (IDEs) popular among developers today, including Visual Studio Code, Visual Studio, Vim, Neovim, the JetBrains suite of IDEs, and Azure Data Studio. This direct integration allows GitHub Copilot to access your existing project to improve the suggestions made when given a prompt, while also providing users hassle free installation and access to the features provided. For enterprise users, the model can also be granted access to existing repositories and knowledge bases from your organization to further enhance the quality of outputs and suggestions.\\nWhen writing code, GitHub Copilot can offer suggestions in a few different ways. Firstly, you can write a prompt using an inline comment that can be converted into a block of code. This works in a similar way to how you might use other LLMs to generate code blocks from a prompt, but with the added advantage of GitHub Copilot being able to access existing project files to use as context and produce a better output. Secondly, GitHub Copilot can provide real-time suggestions as you are writing your code. For example, if you are writing a regex function to validate an email address, simply starting to write the function can offer an autocomplete suggestion that provides the required syntax. Additionally, you can also use the GitHub Copilot Chat extension to ask questions, request suggestions, and help you to debug code in a more context aware fashion than you might get from LLMs trained on more broad datasets. Users can enjoy unlimited messages and interactions with GitHub Copilot\u2019s chat feature across all subscription tiers.\\nGitHub Copilot is trained using data from publicly available code repositories, including GitHub itself. GitHub Copilot claims it can provide code assistance in any language where a public repository exists, however the quality of the suggestions will depend on the volume of data available. All subscription tiers include a public code filter to reduce the risk of suggestions directly copying code from a public repository. By default, GitHub Copilot excludes submitted data from being used to train the model further for business and enterprise tier customers and offers the ability to exclude files or repositories from being used to inform suggestions offered. Administrators can configure both features as needed based on your business use cases.\\nWhile these features aim to keep your data private, it\u2019s worth keeping in mind that prompts aren\u2019t processed locally and rely on external infrastructure to provide code suggestions and you should factor this into whether this is the right product for you. Users should also be cautious about trusting any outputs implicitly \u2013 while the model is generally very good at providing suggestions, like all LLMs it is still prone to hallucinations and can make poor or incorrect suggestions. Always make sure to review any code generated by the model to make sure it does what you intend it to do.\\nIn the future it\u2019s possible that GitHub will upgrade GitHub Copilot to use the recently released GPT-4o model. GPT-4 was originally released in March 2023, with GitHub Copilot being updated to use the new model roughly 7 months later. It makes sense to update the model further given the improved intelligence, reduced latency, and reduced cost to operate GPT-4o, though at this time there has been no official announcement.\\nIf you want to try before you buy, GitHub Copilot offers a free 30 day trial of their cheapest package which should be sufficient to test out its capabilities, with a $10 per month fee thereafter. Copilot Business costs $19 per user per month, while Enterprise costs $39 per user per month\\nBest for individuals\\n(Image credit: Qwen)\\nCodeQwen1.5\\nBest coding assistant for individuals\\nReasons to buy\\n+\\nOpen source\\n+\\nHas options for local hosting\\n+\\nCan be trained further using your own code repositories\\n+\\nOffers a range of model sizes to fit your requirements\\nReasons to avoid\\n-\\nNo first-party extensions for popular IDEs\\n-\\nUp front hardware, cost needs to be considered when hosted locally\\nCodeQwen1.5 is a version of Alibaba\u2019s open-source Qwen1.5 LLM specifically trained using public code repositories to assist developers in coding related tasks. This specialized version was released in April 2024, a few months after the release of Qwen1.5 to the public in February 2024.\\nThere are 2 different versions of CodeQwen1.5 available today. The base model of CodeQwen1.5 is designed for code generation and suggestions but has limited chat functionality, while the second version can also be used as a chat interface that can answer questions in a more human-like way. Both models have been trained with 3 trillion tokens of code related data and support a very respectable 92 languages, which include some of the most common languages in use today such as Python, C++, Java, PHP, C# and JavaScript.\\nUnlike the base version of Qwen1.5, which has several different sizes available for download, CodeQwen1.5 is only available in a single size of 7B. While this is quite small when compared to other models on the market that can also be used as coding assistants, there are a few advantages that developers can take advantage of. Despite its small size, CodeQwen1.5 performs incredibly well compared to some of the larger models that offer coding assistance, both open and closed source. CodeQwen1.5 comfortably beats GPT3.5 in most benchmarks and provides a competitive alternative to GPT-4, though this can sometimes depend on the specific programming language. While GPT-4 may perform better overall by comparison, it\u2019s important to remember that GPT-4 requires a subscription and has per token costs that could make using it very expensive compared to CodeQwen1.5 and GPT-4 cannot be hosted locally. Like with all LLMs, its risky to implicitly trust any suggestions or responses provided by the model. While steps have been taken to reduce hallucinations, always check the output to make sure it is correct.\\nAs CodeQwen1.5 is open source, you can download a copy of the LLM to use at no additional cost beyond the hardware needed to run it. You\u2019ll still need to make sure your system has enough resources to ensure the model can run well, but the bonus of the smaller model size means a modern system with GPU that has at least 16GB of VRAM and at least 32GB of system RAM should be sufficient. CodeQwen1.5 can also be trained using code from existing projects or other code repositories to further improve the context of the generated responses and suggestions. The ability to host CodeQwen1.5 within your own local or remote infrastructure, such as a Virtual Private Server (VPS) or dedicated server, should also help to alleviate some of the concerns related to data privacy or security often connected to submitting information to third party providers.\\nAlibaba surprised us by releasing their new Qwen2 LLM at the start of June that they claim offers significant gains over the base model of Qwen1.5. Alibaba also mentioned that the training data used for CodeQwen1.5 is included in Qwen2-72B, so has the potential to offer improved results, but it\u2019s currently unclear if there is a plan to upgrade CodeQwen to use the new model.\\nBest Value\\n(Image credit: Meta)\\nLLama 3\\nBest value LLM\\nReasons to buy\\n+\\nOpen source\\n+\\nSmaller models can be hosted locally\\n+\\nCan be fine tuned with your own dataset\\n+\\nExternal hosting provided by AWS and Azure have low per token costs\\nReasons to avoid\\n-\\nHardware requirements for the larger models could require significant upfront investment\\n-\\nNot specifically trained as a coding LLM\\nWhen it comes to the best bang for buck, Meta\u2019s open-source Llama 3 model released in April 2024 is one of the best low-cost models available on the market today. Unlike many other models specifically trained with code related data to assist developers with coding tasks, Llama 3 is a more general LLM capable of assisting in many ways \u2013 one of which also happens to be as a coding assistant \u2013 and outperforms CodeLlama, a coding model released by Meta in August 2023 based on Llama 2.\\nIn like for like testing with models of the same size, Llama 3 outperforms CodeLlama by a considerable margin when it comes to code generation, interpretation, and understanding. This is impressive considering Llama 3 wasn\u2019t trained specifically for code related tasks but can still outperform those that have. This means that not only can you use Llama 3 to improve efficiency and productivity when performing coding tasks, but it can also be used for other tasks as well. Llama 3 has a training data cutoff of December 2023, which isn\u2019t always of critical importance for code related tasks, but some languages can develop quickly and having the most recent data available can be incredibly valuable.\\nLlama 3 is an open-source model that allows developers to download and deploy the model to their own local system or infrastructure. Like CodeQwen1.5, Llama 3 8B is small enough that a modern system with at least 16GB of VRAM and 32GB of system RAM is sufficient to run the model. The larger 70B version of Llama 3 naturally has better capabilities due to the increased parameter number, but the hardware requirement is an order of magnitude greater and would require a significant injection of funds to build a system capable of running it effectively. Luckily, the Llama 3 8B offers enough capability that users can get excellent value without breaking the bank at the same time. If you find that you need the added capability of the larger model, the open-source nature of the model means you can easily rent an external VPS or dedicated server to support your needs, though costs will vary depending on the provider. If you decide that you\u2019d like the increased capability of the larger model, but the investment needed for the required hardware, or the cost to rent an external host, is outside your budget, AWS offers API access to the model via a pay as you go plan which charges you by the token instead. AWS currently charges $3.50 per million output tokens, which is a considerable quantity for a very small price. For comparison, OpenAI\u2019s GPT-4o costs $15.00 for the same quantity of tokens. If this type of solution appeals to you, make sure to shop around for the best provider for your location, budget, and needs.\\nLlama 3 performs well in code generation tasks and adheres well to the prompts given. It will sometimes simplify the code based on the prompt, but it's reasonably receptive to being given instruction to provide a complete solution and will segment if it reaches the token limit for a single response if requested. During testing, we asked for Llama 3 to write a complete solution in Python for a chess game that would immediately compile and could be played via text prompts, and it dutifully provided the requested code. Although the code initially failed to compile, providing Llama 3 with the error messages from the compiler allowed it to identify where the mistakes were and provided a correction. Llama 3 can effectively debug code segments to identify issues and provide new code to fix the error. As a bonus, it can also explain where the error was located and why it needs to be fixed to help the user understand what the mistake was. However, like with all models generating code-related solutions, it's important to check the output and not trust it implicitly. Although the models are becoming increasingly intelligent and accurate, they also hallucinate at times and provide incorrect or insecure responses.\\nLike with other open-source models, any data you submit to train Llama 3 from your own code repositories remains within your control. This helps to alleviate some of the concerns and risks associated with submitting proprietary and personal data to third parties, though keep in mind that also means that you should consider what that means for your information security policies where required. It doesn\u2019t cost anything extra to train a model you have hosted within your own infrastructure, but some hosts providing API access do have an additional cost associated with further training.\\nYou can download Llama 3 today directly from\\nMeta\\n.\\nBest for code generation\\n(Image credit: Claude AI)\\nClaude 3 Opus\\nThe best LLM for generating code\\nReasons to buy\\n+\\nOutperforms most models for code generation tasks\\n+\\nCan provide detailed explanations of the generated code to assist developer understanding\\n+\\nProvides more human responses to prompts than other models\\nReasons to avoid\\n-\\nClosed source and can\u2019t be hosted locally\\n-\\nExpensive per token cost\\n-\\nCan\u2019t be connected to existing knowledgebases\\nReleased in April 2024, Claude 3 Opus is the latest and most capable LLM from Anthropic that they claim is the most intelligent LLM on the market today and is designed to tackle a variety of different tasks. Although most LLMs can generate code, the accuracy and correctness of the generated outputs can vary, and may have mistakes or be flat out incorrect due to not being specifically designed with code generation in mind. Claude 3 Opus bridges that gap by being trained to handle coding related tasks alongside the regular tasks LLMs are often used for, making for a very powerful multi-faceted solution.\\nWhile Anthropic doesn\u2019t mention how many programming languages it supports, Claude 3 Opus can generate code across a large range of programming languages, ranging from incredibly popular languages such as C++, C#, Python and Java, to older or more niche languages such as FORTRAN, COBOL, and Haskell. Claude 3 Opus relies on the patterns, syntaxes, coding conventions and algorithms identified within the code related training data provided to generate new code snippets from scratch to help avoid direct reproduction of code used to train it. The large 200k token context window offered by Claude 3 Opus is incredibly useful when working with large code blocks as you iterate through suggestions and changes. Like all LLMs, Claude 3 Opus also has an output token limit, and tends to either summarise or truncate the response to fit within a single reply. While summarisation of a purely text response isn\u2019t too problematic as you can ask for additional context, not being provided with a large chunk of required code, such as when generating a test case, is quite a problem. Fortunately, Claude 3 Opus can segment its responses if you request it to do so in your initial prompt. You\u2019ll still need to ask it to continue after each reply, but this does allow you to obtain more long form responses where needed. As well as generating functional code, Claude 3 Opus also adds comments to the code and provides explanations as to what the generated code does to help developers understand what is happening. In cases where you are using Claude 3 to debug code and generate fixes, this is extremely valuable as it not only helps solve the problem, but also provides context as to why changes were made, or why the code was generated in this specific way.\\nFor those concerned about privacy and data security, Anthropic states that they don\u2019t use any of the data submitted to Claude 3 for the purposes of training the model further, a welcome feature that many will appreciate when working with proprietary code. They also include copyright indemnity protections with their paid subscriptions.\\nClaude 3 Opus does come with some limitations when it comes to improving the context of responses as it doesn\u2019t currently offer a way to connect your own knowledge bases or codebases for additional training. This probably isn\u2019t a deal breaker for most but could be something worth thinking about when choosing the right LLM for your code generation solution.\\nThis does all come with a hefty price tag compared to other LLMs that offer code generation functionality. API access is one of the more expensive ones on the market at an eye watering $75 per 1 million output tokens, which is considerably more than GPT-4o\u2019s $15 price tag. Anthropic do offer 2 additional models based on Claude 3, Haiku and Sonnet, which are much cheaper at $15 and $1.25 respectively for the same quantity of tokens, though they have reduced capability compared to Opus. In addition to API access, Anthropic offers 3 subscription tiers that grant access to Claude 3. The free tier has a lower daily limit and only grants access to the Sonnet model but should give those looking to test it\u2019s capabilities a good idea of what to expect. To access Opus, you\u2019ll need to subscribe to Pro or Team at $20 and $30 per person per month respectively. The Team subscription does need a minimum of 5 users for a total of $150 per month, but increases the usage limits for each user compared to the Pro tier.\\nHead over to create a free account to access\\nClaude 3\\n.\\nBest for debugging\\n(Image credit: Open AI)\\nGPT-4\\nThe best LLM for debugging\\nReasons to buy\\n+\\nIdentifies issues within blocks of code and suggests corrections\\n+\\nCan explain what the problem was and how the corrections solve it\\n+\\nLarge context window\\nReasons to avoid\\n-\\nPer token cost can be expensive compared to coding-focused offerings with similar capability\\n-\\nRequires a subscription to gain access\\n-\\nManual opt-out needed to prevent data from being used to train the model\\nSince the release of\\nChatGPT\\nin November 2022, OpenAI has taken the world by storm and offers some of the most intelligent and capable LLMs on the market today. GPT-4 was released in March 2023 as an update to GPT-3.5\\nWhile GPT-4 isn\u2019t an LLM designed specifically as a coding assistant, it performs well across a broad range of code related tasks, including real time code suggestions, generating blocks of code, writing test cases, and debugging errors in code. GitHub Copilot has also been using a version of GPT-4 with additional training data since November 2023, leveraging the human response capabilities of GPT-4 for code generation and within their chat assistant, which should give you an idea of the value it can provide.\\nGPT-4 has been trained with code related data that covers many different programming languages and coding practices to help it understand the vast array of logic flows, syntax rules and programming paradigms used by developers. This allows GPT-4 to excel when debugging code by helping to solve a variety of issues commonly encountered by developers. Syntax errors can be incredibly frustrating when working with some languages - I\u2019m looking at you and your indentations, Python \u2013 so using GPT-4 to review your code can massively speed up the process when code won\u2019t compile due to errors that are difficult to find. Logical errors are one of the toughest errors to debug as code usually compiles correctly, but it doesn\u2019t provide the correct output or operate as desired. By giving GPT-4 your code and an explanation of what it should be doing, GPT-4 can analyse and identify where the problem lies, offer suggestions or rewrites to solve the problem, and even provide an explanation as to what the problem is and how the suggested changes solve it. This can help developers quickly understand the cause of the problem and offers an opportunity to learn how to avoid it again in the future.\\nAlthough the training data cutoff for GPT-4 is September 2021, which is quite a long time ago considering the advancements in LLMs over the last year, GPT-4 is continuously trained using new data from user interactions. This allows GPT-4\u2019s debugging to become more accurate over time, though this does present some potential risk when it comes to the code you submit for analysis, especially when using it to write or debug proprietary code. Users do have the option to opt out of their data being used to train GPT-4 further, but it's not something that happens by default so keep this in mind when using GPT-4 for code related tasks.\\nYou might be wondering why the recommendation here is to use GPT-4 when it is 4 times more expensive than the newer, cheaper, and more intelligent GPT-4o model released in May 2024. In general, GPT-4o has proven to be a more capable model, but for code related tasks GPT-4 tends to provide better responses that are more correct, adheres to the prompt better, and offers better error detection than GPT-4o. However, the gap is small and it's likely that GPT-4o will become more capable and overtake GPT-4 in the future as the model matures further through additional training from user interactions. If cost is a major factor in your decision, GPT-4o is a good alternative that covers the majority of what GPT-4 can provide at a much lower cost.\\nBest LLM for Coding Assistants FAQs\\nHow does a coding assistant work?\\nCoding assistants use Large Language Models (LLMs) that are trained with code related data to provide developers with tools that help increase productivity and efficiency when performing code related tasks. The training data often leverages public code repositories, documentation and other licenced work to enable the LLM to recognise syntax, coding styles, programming practices and paradigms to provide code generation, debugging, code analysis, and problem-solving capabilities across many different programming languages.\\nCoding assistants can be integrated into your development environments to provide inline code suggestions, and some can be trained further using an organization's knowledge bases and codebases to improve the context of suggestions.\\nWhy shouldn\u2019t I implicitly trust the code generated by a coding assistant?\\nLLMs are becoming increasingly intelligent, but they aren\u2019t immune to making mistakes known as \u201challucinations\u201d. Most coding assistants generate code that works well, but sometimes the code can be incomplete, inaccurate, or completely wrong. This can vary from model to model and has a high dependency on the training data used and the overall intelligence capability of the model itself.\\nWhat is a context window?\\nA context window is another way of describing how far back the LLM\u2019s memory can go for a conversation, usually measured in tokens. LLMs with a large context window allow for responses that offer better context based on the conversation history which can be valuable for developers working on code related tasks when brainstorming ideas, debugging large sections of code, or iterating on a design.\\nAre you a pro? Subscribe to our newsletter\\nSign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed!\\nContact me with news and offers from other Future brands\\nReceive email from us on behalf of our trusted partners or sponsors\\nGrant Hickey\\nFascinated by computers from a young age, Grant is on an endless quest to leverage existing and emerging technologies to augment and enhance the productivity of individuals and enterprises, and to improve the velocity at which teams can analyze data and identify trends within their customer base or organization. Grant has previously worked as a software engineer building cloud based CRMs, before moving into the games industry to work for Krafton on PUBG:Battlegrounds and later Creative Assembly. Always looking to improve his working practices he often builds his own tools to streamline tasks and become more efficient.\\nLATEST ARTICLES\\n1\\nScuf Nomad review: a solid mobile controller that struggles to stand out\\n2\\nLG says next-gen \\\"dream OLED\\\" panel is finally real \u2013 but it might not come to TVs first\\n3\\nHP 325 FHD webcam: A budget-friendly choice with solid image quality\\n4\\nHow to watch After Baywatch: Moment in the Sun online from anywhere\\n5\\nAt last! Garmin Fenix 8 revealed, with an Apple Watch Ultra-beating dive mode \u2013 alongside the Garmin Enduro 3\",         \"tool_call_id\": \"call_S0OnNQqxtPH5HtDb1buzjedV\",         \"name\": \"extract_content\",     },     {         \"role\": \"tool\",         \"content\": \": Failed to extract content from URL https://www.blog.aipo.rt.tech/p/top-9-libraries-to-accelerate-llm\",         \"tool_call_id\": \"call_aCyaCFXUWMWloDkETTrxHyoJ\",         \"name\": \"extract_content\",     },     {         \"role\": \"tool\",         \"content\": \"Member-only story\\nEssential Python Libraries for LLMs and Application Development in 2024\\nAhmad Waleed\\n\u00b7\\nFollow\\nPublished in\\nPythonForAll\\n\u00b7\\n3 min read\\n\u00b7\\nNov 30, 2023\\n--\\nShare\\nIn the ever-evolving landscape of Language Models (LLMs), Natural Language Processing (NLP), and Machine Learning (ML), the arsenal of Python libraries continues to expand, bringing forth innovation and efficiency in diverse applications. For those venturing into this realm or seeking to enrich their projects, here\u2019s a categorized guide to the essential Python libraries that are reshaping the field in 2024.\\nKey Insights:\\nNLP &amp; ML Model Integration: Libraries such as Transformers and spaCy play pivotal roles in seamlessly integrating pre-trained NLP models, marking a paradigm shift towards context-aware language models across applications.\\nDeep Learning &amp; Neural Networks: TensorFlow and PyTorch stand tall as robust frameworks, showcasing Python\u2019s prowess in handling complex neural network architectures and computations at scale.\\nData Preprocessing &amp; Validation: Libraries like Unstructured and Pydantic underscore the burgeoning emphasis on data quality and integrity, crucial in an era dominated by vast datasets and intricate ML models.\\nApplication Development: Streamlit and Gradio emerge as transformative tools, converting data scripts into interactive web applications, emphasizing the significance of accessibility\u2026\",         \"tool_call_id\": \"call_71e9AkvKjIuEp3QceqO4DCUK\",         \"name\": \"extract_content\",     },     {         \"role\": \"tool\",         \"content\": \"tensorchord\\n/\\nAwesome-LLMOps\\nPublic\\nNotifications\\nYou must be signed in to change notification settings\\nFork\\n353\\nStar\\n3.7k\\nAn awesome &amp; curated list of best LLMOps tools for developers\\nLicense\\nCC0-1.0 license\\n3.7k\\nstars\\n353\\nforks\\nBranches\\nTags\\nActivity\\nStar\\nNotifications\\nYou must be signed in to change notification settings\\ntensorchord/Awesome-LLMOps\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\nmain\\nBranches\\nTags\\nGo to file\\nCode\\nFolders and files\\nName\\nName\\nLast commit message\\nLast commit date\\nLatest commit\\nHistory\\n150 Commits\\nscripts\\nscripts\\n.gitignore\\n.gitignore\\nLICENSE\\nLICENSE\\nREADME.md\\nREADME.md\\ncontributing.md\\ncontributing.md\\nView all files\\nRepository files navigation\\nAwesome LLMOps\\nAn awesome &amp; curated list of the best LLMOps tools for developers.\\nContribute\\nContributions are most welcome, please adhere to the\\ncontribution guidelines\\n.\\nTable of Contents\\nTable of Contents\\nModel\\nLarge Language Model\\nCV Foundation Model\\nAudio Foundation Model\\nServing\\nLarge Model Serving\\nFrameworks/Servers for Serving\\nObservability\\nSecurity\\nLLMOps\\nSearch\\nVector search\\nCode AI\\nTraining\\nIDEs and Workspaces\\nFoundation Model Fine Tuning\\nFrameworks for Training\\nExperiment Tracking\\nVisualization\\nModel Editing\\nData\\nData Management\\nData Storage\\nData Tracking\\nFeature Engineering\\nData/Feature enrichment\\nLarge Scale Deployment\\nML Platforms\\nWorkflow\\nScheduling\\nModel Management\\nPerformance\\nML Compiler\\nProfiling\\nAutoML\\nOptimizations\\nFederated ML\\nAwesome Lists\\nModel\\nLarge Language Model\\nProject\\nDetails\\nRepository\\nAlpaca\\nCode and documentation to train Stanford's Alpaca models, and generate the data.\\nBELLE\\nA 7B Large Language Model fine-tune by 34B Chinese Character Corpus, based on LLaMA and Alpaca.\\nBloom\\nBigScience Large Open-science Open-access Multilingual Language Model\\ndolly\\nDatabricks\u2019 Dolly, a large language model trained on the Databricks Machine Learning Platform\\nFalcon 40B\\nFalcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.\\nFastChat (Vicuna)\\nAn open platform for training, serving, and evaluating large language models. Release repo for Vicuna and FastChat-T5.\\nGemma\\nGemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models.\\nGLM-6B (ChatGLM)\\nAn Open Bilingual Pre-Trained Model, quantization of ChatGLM-130B, can run on consumer-level GPUs.\\nChatGLM2-6B\\nChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model\\nChatGLM-6B\\n.\\nGLM-130B (ChatGLM)\\nAn Open Bilingual Pre-Trained Model (ICLR 2023)\\nGPT-NeoX\\nAn implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library.\\nLuotuo\\nA Chinese LLM, Based on LLaMA and fine tune by Stanford Alpaca, Alpaca LoRA, Japanese-Alpaca-LoRA.\\nMixtral-8x7B-v0.1\\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\\nStableLM\\nStableLM: Stability AI Language Models\\n\u2b06 back to ToC\\nCV Foundation Model\\nProject\\nDetails\\nRepository\\ndisco-diffusion\\nA frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.\\nmidjourney\\nMidjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.\\nsegment-anything (SAM)\\nproduces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image.\\nstable-diffusion\\nA latent text-to-image diffusion model\\nstable-diffusion v2\\nHigh-Resolution Image Synthesis with Latent Diffusion Models\\n\u2b06 back to ToC\\nAudio Foundation Model\\nProject\\nDetails\\nRepository\\nbark\\nBark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects.\\nwhisper\\nRobust Speech Recognition via Large-Scale Weak Supervision\\nServing\\nLarge Model Serving\\nProject\\nDetails\\nRepository\\nAlpaca-LoRA-Serve\\nAlpaca-LoRA as Chatbot service\\nCTranslate2\\nfast inference engine for Transformer models in C++\\nClip-as-a-service\\nserving the OpenAI CLIP model\\nDeepSpeed-MII\\nMII makes low-latency and high-throughput inference possible, powered by DeepSpeed.\\nFaster Whisper\\nfast inference engine for whisper in C++ using CTranslate2.\\nFlexGen\\nRunning large language models on a single GPU for throughput-oriented scenarios.\\nFlowise\\nDrag &amp; drop UI to build your customized LLM flow using LangchainJS.\\nllama.cpp\\nPort of Facebook's LLaMA model in C/C++\\nInfinity\\nRest API server for serving text-embeddings\\nModelz-LLM\\nOpenAI compatible API for LLMs and embeddings (LLaMA, Vicuna, ChatGLM and many others)\\nOllama\\nServe Llama 2 and other large language models locally from command line or through a browser interface.\\nTensorRT-LLM\\nInference engine for TensorRT on Nvidia GPUs\\ntext-generation-inference\\nLarge Language Model Text Generation Inference\\ntext-embeddings-inference\\nInference for text-embedding models\\nvllm\\nA high-throughput and memory-efficient inference and serving engine for LLMs.\\nwhisper.cpp\\nPort of OpenAI's Whisper model in C/C++\\nx-stable-diffusion\\nReal-time inference for Stable Diffusion - 0.88s latency. Covers AITemplate, nvFuser, TensorRT, FlashAttention.\\n\u2b06 back to ToC\\nFrameworks/Servers for Serving\\nProject\\nDetails\\nRepository\\nBentoML\\nThe Unified Model Serving Framework\\nJina\\nBuild multimodal AI services via cloud native technologies \u00b7 Model Serving \u00b7 Generative AI \u00b7 Neural Search \u00b7 Cloud Native\\nMosec\\nA machine learning model serving framework with dynamic batching and pipelined stages, provides an easy-to-use Python interface.\\nTFServing\\nA flexible, high-performance serving system for machine learning models.\\nTorchserve\\nServe, optimize and scale PyTorch models in production\\nTriton Server (TRTIS)\\nThe Triton Inference Server provides an optimized cloud and edge inferencing solution.\\nlangchain-serve\\nServerless LLM apps on Production with Jina AI Cloud\\nlanarky\\nFastAPI framework to build production-grade LLM applications\\nray-llm\\nLLMs on Ray - RayLLM\\nXinference\\nReplace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.\\n\u2b06 back to ToC\\nSecurity\\nFrameworks for LLM security\\nProject\\nDetails\\nRepository\\nPlexiglass\\nA Python Machine Learning Pentesting Toolbox for Adversarial Attacks. Works with LLMs, DNNs, and other machine learning algorithms.\\n\u2b06 back to ToC\\nObservability\\nProject\\nDetails\\nRepository\\nAzure OpenAI Logger\\n\\\"Batteries included\\\" logging solution for your Azure OpenAI instance.\\nDeepchecks\\nTests for Continuous Validation of ML Models &amp; Data. Deepchecks is a Python package for comprehensively validating your machine learning models and data with minimal effort.\\nEvidently\\nEvaluate and monitor ML models from validation to production.\\nFiddler AI\\nEvaluate, monitor, analyze, and improve machine learning and generative models from pre-production to production. Ship more ML and LLMs into production, and monitor ML and LLM metrics like hallucination, PII, and toxicity.\\nGiskard\\nTesting framework dedicated to ML models, from tabular to LLMs. Detect risks of biases, performance issues and errors in 4 lines of code.\\nGreat Expectations\\nAlways know what to expect from your data.\\nwhylogs\\nThe open standard for data logging\\n\u2b06 back to ToC\\nLLMOps\\nProject\\nDetails\\nRepository\\nagenta\\nThe LLMOps platform to build robust LLM apps. Easily experiment and evaluate different prompts, models, and workflows to build robust apps.\\nAI studio\\nA Reliable Open Source AI studio to build core infrastructure stack for your LLM Applications. It allows you to gain visibility, make your application reliable, and prepare it for production with features such as caching, rate limiting, exponential retry, model fallback, and more.\\nArize-Phoenix\\nML observability for LLMs, vision, language, and tabular models.\\nBudgetML\\nDeploy a ML inference service on a budget in less than 10 lines of code.\\nCometLLM\\nThe 100% opensource LLMOps platform to log, manage, and visualize your LLM prompts and chains. Track prompt templates, prompt variables, prompt duration, token usage, and other metadata. Score prompt outputs and visualize chat history all within a single UI.\\ndeeplake\\nStream large multimodal datasets to achieve near 100% GPU utilization. Query, visualize, &amp; version control data. Access data w/o the need to recompute the embeddings for the model finetuning.\\nDify\\nOpen-source framework aims to enable developers (and even non-developers) to quickly build useful applications based on large language models, ensuring they are visual, operable, and improvable.\\nDstack\\nCost-effective LLM development in any cloud (AWS, GCP, Azure, Lambda, etc).\\nEmbedchain\\nFramework to create ChatGPT like bots over your dataset.\\nEvidently\\nAn open-source framework to evaluate, test and monitor ML and LLM-powered systems.\\nFiddler AI\\nEvaluate, monitor, analyze, and improve MLOps and LLMOps from pre-production to production.\\nGlide\\nCloud-Native LLM Routing Engine. Improve LLM app resilience and speed.\\nGPTCache\\nCreating semantic cache to store responses from LLM queries.\\nHaystack\\nQuickly compose applications with LLM Agents, semantic search, question-answering and more.\\nHelicone\\nOpen-source LLM observability platform for logging, monitoring, and debugging AI applications. Simple 1-line integration to get started.\\nIzlo\\nPrompt management tools for teams. Store, improve, test, and deploy your prompts in one unified workspace.\\nKeywords AI\\nA unified DevOps platform for AI software. Keywords AI makes it easy for developers to build LLM applications.\\nlangchain\\nBuilding applications with LLMs through composability\\nLangFlow\\nAn effortless way to experiment and prototype LangChain flows with drag-and-drop components and a chat interface.\\nLangfuse\\nOpen Source LLM Engineering Platform: Traces, evals, prompt management and metrics to debug and improve your LLM application.\\nLangKit\\nOut-of-the-box LLM telemetry collection library that extracts features and profiles prompts, responses and metadata about how your LLM is performing over time to find problems at scale.\\nLiteLLM \ud83d\ude85\\nA simple &amp; light 100 line package to\\nstandardize LLM API calls\\nacross OpenAI, Azure, Cohere, Anthropic, Replicate API Endpoints\\nLiteral AI\\nMulti-modal LLM observability and evaluation platform. Create prompt templates, deploy prompts versions, debug LLM runs, create datasets, run evaluations, monitor LLM metrics and collect human feedback.\\nLlamaIndex\\nProvides a central interface to connect your LLMs with external data.\\nLLMApp\\nLLM App is a Python library that helps you build real-time LLM-enabled data pipelines with few lines of code.\\nLLMFlows\\nLLMFlows is a framework for building simple, explicit, and transparent LLM applications such as chatbots, question-answering systems, and agents.\\nLLMonitor\\nObservability and monitoring for AI apps and agents. Debug agents with powerful tracing and logging. Usage analytics and dive deep into the history of your requests. Developer friendly modules with plug-and-play integration into LangChain.\\nmagentic\\nSeamlessly integrate LLMs as Python functions. Use type annotations to specify structured output. Mix LLM queries and function calling with regular Python code to create complex LLM-powered functionality.\\nManag.ai\\nYour all-in-one prompt management and observability platform. Craft, track, and perfect your LLM prompts with ease.\\nMirascope\\nIntuitive convenience tooling for lightning-fast, efficient development and ensuring quality in LLM-based applications\\nOpenLIT\\nOpenLIT is an OpenTelemetry-native GenAI and LLM Application Observability tool and provides OpenTelmetry Auto-instrumentation for monitoring LLMs, VectorDBs and Frameworks. It provides valuable insights into token &amp; cost usage, user interaction, and performance related metrics.\\nParea AI\\nPlatform and SDK for AI Engineers providing tools for LLM evaluation, observability, and a version-controlled enhanced prompt playground.\\nPezzo \ud83d\udd79\ufe0f\\nPezzo is the open-source LLMOps platform built for developers and teams. In just two lines of code, you can seamlessly troubleshoot your AI operations, collaborate and manage your prompts in one place, and instantly deploy changes to any environment.\\nPromptHub\\nFull stack prompt management tool designed to be usable by technical and non-technical team members. Test, version, collaborate, deploy, and monitor, all from one place.\\npromptfoo\\nOpen-source tool for testing &amp; evaluating prompt quality. Create test cases, automatically check output quality and catch regressions, and reduce evaluation cost.\\nPromptFoundry\\nThe simple prompt engineering and evaluation tool designed for developers building AI applications.\\nPromptLayer \ud83c\udf70\\nPrompt Engineering platform. Collaborate, test, evaluate, and monitor your LLM applications\\nPromptMage\\nOpen-source tool to simplify the process of creating and managing LLM workflows and prompts as a self-hosted solution.\\nPrompteams\\nPrompt management system. Version, test, collaborate, and retrieve prompts through real-time APIs. Have GitHub style with repos, branches, and commits (and commit history).\\nprompttools\\nOpen-source tools for testing and experimenting with prompts. The core idea is to enable developers to evaluate prompts using familiar interfaces like code and notebooks. In just a few lines of codes, you can test your prompts and parameters across different models (whether you are using OpenAI, Anthropic, or LLaMA models). You can even evaluate the retrieval accuracy of vector databases.\\nTreeScale\\nAll In One Dev Platform For LLM Apps. Deploy LLM-enhanced APIs seamlessly using tools for prompt optimization, semantic querying, version management, statistical evaluation, and performance tracking. As a part of the developer friendly API implementation TreeScale offers Elastic LLM product, which makes a unified API Endpoint for all major LLM providers and open source models.\\nTrueFoundry\\nDeploy LLMOps tools like Vector DBs, Embedding server etc on your own Kubernetes (EKS,AKS,GKE,On-prem) Infra including deploying, Fine-tuning, tracking Prompts and serving Open Source LLM Models with full Data Security and Optimal GPU Management. Train and Launch your LLM Application at Production scale with best Software Engineering practices.\\nReliableGPT \ud83d\udcaa\\nHandle OpenAI Errors (overloaded OpenAI servers, rotated keys, or context window errors) for your production LLM Applications.\\nPortkey\\nControl Panel with an observability suite &amp; an AI gateway \u2014 to ship fast, reliable, and cost-efficient apps.\\nVellum\\nAn AI product development platform to experiment with, evaluate, and deploy advanced LLM apps.\\nWeights &amp; Biases (Prompts)\\nA suite of LLMOps tools within the developer-first W&amp;B MLOps platform. Utilize W&amp;B Prompts for visualizing and inspecting LLM execution flow, tracking inputs and outputs, viewing intermediate results, securely managing prompts and LLM chain configurations.\\nWordware\\nA web-hosted IDE where non-technical domain experts work with AI Engineers to build task-specific AI agents. It approaches prompting as a new programming language rather than low/no-code blocks.\\nxTuring\\nBuild and control your personal LLMs with fast and efficient fine-tuning.\\nZenML\\nOpen-source framework for orchestrating, experimenting and deploying production-grade ML solutions, with built-in\\nlangchain\\n&amp;\\nllama_index\\nintegrations.\\n\u2b06 back to ToC\\nSearch\\nVector search\\nProject\\nDetails\\nRepository\\nAquilaDB\\nAn easy to use Neural Search Engine. Index latent vectors along with JSON metadata and do efficient k-NN search.\\nAwadb\\nAI Native database for embedding vectors\\nChroma\\nthe open source embedding database\\nInfinity\\nThe AI-native database built for LLM applications, providing incredibly fast vector and full-text search\\nLancedb\\nDeveloper-friendly, serverless vector database for AI applications. Easily add long-term memory to your LLM apps!\\nMarqo\\nTensor search for humans.\\nMilvus\\nVector database for scalable similarity search and AI applications.\\nPinecone\\nThe Pinecone vector database makes it easy to build high-performance vector search applications. Developer-friendly, fully managed, and easily scalable without infrastructure hassles.\\npgvector\\nOpen-source vector similarity search for Postgres.\\npgvecto.rs\\nVector database plugin for Postgres, written in Rust, specifically designed for LLM.\\nQdrant\\nVector Search Engine and Database for the next generation of AI applications. Also available in the cloud\\ntxtai\\nBuild AI-powered semantic search applications\\nVald\\nA Highly Scalable Distributed Vector Search Engine\\nVearch\\nA distributed system for embedding-based vector retrieval\\nVectorDB\\nA Python vector database you just need - no more, no less.\\nVellum\\nA managed service for ingesting documents and performing hybrid semantic/keyword search across them. Comes with out-of-box support for OCR, text chunking, embedding model experimentation, metadata filtering, and production-grade APIs.\\nWeaviate\\nWeaviate is an open source vector search engine that stores both objects and vectors, allowing for combining vector search with structured filtering with the fault-tolerance and scalability of a cloud-native database, all accessible through GraphQL, REST, and various language clients.\\n\u2b06 back to ToC\\nCode AI\\nProject\\nDetails\\nRepository\\nCodeGeeX\\nCodeGeeX: An Open Multilingual Code Generation Model (KDD 2023)\\nCodeGen\\nCodeGen is an open-source model for program synthesis. Trained on TPU-v4. Competitive with OpenAI Codex.\\nCodeT5\\nOpen Code LLMs for Code Understanding and Generation.\\nContinue\\n\u23e9 the open-source autopilot for software development\u2014bring the power of ChatGPT to VS Code\\nfauxpilot\\nAn open-source alternative to GitHub Copilot server\\ntabby\\nSelf-hosted AI coding assistant. An opensource / on-prem alternative to GitHub Copilot.\\nTraining\\nIDEs and Workspaces\\nProject\\nDetails\\nRepository\\ncode server\\nRun VS Code on any machine anywhere and access it in the browser.\\nconda\\nOS-agnostic, system-level binary package manager and ecosystem.\\nDocker\\nMoby is an open-source project created by Docker to enable and accelerate software containerization.\\nenvd\\n\ud83c\udfd5\ufe0f Reproducible development environment for AI/ML.\\nJupyter Notebooks\\nThe Jupyter notebook is a web-based notebook environment for interactive computing.\\nKurtosis\\nA build, packaging, and run system for ephemeral multi-container environments.\\nWordware\\nA web-hosted IDE where non-technical domain experts work with AI Engineers to build task-specific AI agents. It approaches prompting as a new programming language rather than low/no-code blocks.\\n\u2b06 back to ToC\\nFoundation Model Fine Tuning\\nProject\\nDetails\\nRepository\\nalpaca-lora\\nInstruct-tune LLaMA on consumer hardware\\nfinetuning-scheduler\\nA PyTorch Lightning extension that accelerates and enhances foundation model experimentation with flexible fine-tuning schedules.\\nFlyflow\\nOpen source, high performance fine tuning as a service for GPT4 quality models with 5x lower latency and 3x lower cost\\nLMFlow\\nAn Extensible Toolkit for Finetuning and Inference of Large Foundation Models\\nLora\\nUsing Low-rank adaptation to quickly fine-tune diffusion models.\\npeft\\nState-of-the-art Parameter-Efficient Fine-Tuning.\\np-tuning-v2\\nAn optimized prompt tuning strategy achieving comparable performance to fine-tuning on small/medium-sized models and sequence tagging challenges.\\n(ACL 2022)\\nQLoRA\\nEfficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.\\nTRL\\nTrain transformer language models with reinforcement learning.\\n\u2b06 back to ToC\\nFrameworks for Training\\nProject\\nDetails\\nRepository\\nAccelerate\\n\ud83d\ude80 A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision.\\nApache MXNet\\nLightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler.\\naxolotl\\nA tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.\\nCaffe\\nA fast open framework for deep learning.\\nColossalAI\\nAn integrated large-scale model training system with efficient parallelization techniques.\\nDeepSpeed\\nDeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.\\nHorovod\\nDistributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\\nJax\\nAutograd and XLA for high-performance machine learning research.\\nKedro\\nKedro is an open-source Python framework for creating reproducible, maintainable and modular data science code.\\nKeras\\nKeras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.\\nLightGBM\\nA fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\\nMegEngine\\nMegEngine is a fast, scalable and easy-to-use deep learning framework, with auto-differentiation.\\nmetric-learn\\nMetric Learning Algorithms in Python.\\nMindSpore\\nMindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\\nOneflow\\nOneFlow is a performance-centered and open-source deep learning framework.\\nPaddlePaddle\\nMachine Learning Framework from Industrial Practice.\\nPyTorch\\nTensors and Dynamic neural networks in Python with strong GPU acceleration.\\nPyTorch Lightning\\nDeep learning framework to train, deploy, and ship AI products Lightning fast.\\nXGBoost\\nScalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library.\\nscikit-learn\\nMachine Learning in Python.\\nTensorFlow\\nAn Open Source Machine Learning Framework for Everyone.\\nVectorFlow\\nA minimalist neural network library optimized for sparse data and single machine environments.\\n\u2b06 back to ToC\\nExperiment Tracking\\nProject\\nDetails\\nRepository\\nAim\\nan easy-to-use and performant open-source experiment tracker.\\nClearML\\nAuto-Magical CI/CD to streamline your ML workflow. Experiment Manager, MLOps and Data-Management\\nComet\\nComet is an MLOps platform that offers experiment tracking, model production management, a model registry, and full data lineage from training straight through to production. Comet plays nicely with all your favorite tools, so you don't have to change your existing workflow. Check out CometLLM for all your prompt engineering needs!\\nGuild AI\\nExperiment tracking, ML developer tools.\\nMLRun\\nMachine Learning automation and tracking.\\nKedro-Viz\\nKedro-Viz is an interactive development tool for building data science pipelines with Kedro. Kedro-Viz also allows users to view and compare different runs in the Kedro project.\\nLabNotebook\\nLabNotebook is a tool that allows you to flexibly monitor, record, save, and query all your machine learning experiments.\\nSacred\\nSacred is a tool to help you configure, organize, log and reproduce experiments.\\nWeights &amp; Biases\\nA developer first, lightweight, user-friendly experiment tracking and visualization tool for machine learning projects, streamlining collaboration and simplifying MLOps. W&amp;B excels at tracking LLM-powered applications, featuring W&amp;B Prompts for LLM execution flow visualization, input and output monitoring, and secure management of prompts and LLM chain configurations.\\n\u2b06 back to ToC\\nVisualization\\nProject\\nDetails\\nRepository\\nFiddler AI\\nRich dashboards, reports, and UMAP to perform root cause analysis, pinpoint problem areas, like correctness, safety, and privacy issues, and improve LLM outcomes.\\nManiford\\nA model-agnostic visual debugging tool for machine learning.\\nnetron\\nVisualizer for neural network, deep learning, and machine learning models.\\nOpenOps\\nBring multiple data streams into one dashboard.\\nTensorBoard\\nTensorFlow's Visualization Toolkit.\\nTensorSpace\\nNeural network 3D visualization framework, build interactive and intuitive model in browsers, support pre-trained deep learning models from TensorFlow, Keras, TensorFlow.js.\\ndtreeviz\\nA python library for decision tree visualization and model interpretation.\\nZetane Viewer\\nML models and internal tensors 3D visualizer.\\nZeno\\nAI evaluation platform for interactively exploring data and model outputs.\\nModel Editing\\nProject\\nDetails\\nRepository\\nFastEdit\\nFastEdit aims to assist developers with injecting fresh and customized knowledge into large language models efficiently using one single command.\\n\u2b06 back to ToC\\nData\\nData Management\\nProject\\nDetails\\nRepository\\nArtiVC\\nA version control system to manage large files. Lake is a dataset format with a simple API for creating, storing, and collaborating on AI datasets of any size.\\nDolt\\nGit for Data.\\nDVC\\nData Version Control - Git for Data &amp; Models - ML Experiments Management.\\nDelta-Lake\\nStorage layer that brings scalable, ACID transactions to Apache Spark and other engines.\\nPachyderm\\nPachyderm is a version control system for data.\\nQuilt\\nA self-organizing data hub for S3.\\n\u2b06 back to ToC\\nData Storage\\nProject\\nDetails\\nRepository\\nJuiceFS\\nA distributed POSIX file system built on top of Redis and S3.\\nLakeFS\\nGit-like capabilities for your object storage.\\nLance\\nModern columnar data format for ML implemented in Rust.\\n\u2b06 back to ToC\\nData Tracking\\nProject\\nDetails\\nRepository\\nPiperider\\nA CLI tool that allows you to build data profiles and write assertion tests for easily evaluating and tracking your data's reliability over time.\\nLUX\\nA Python library that facilitates fast and easy data exploration by automating the visualization and data analysis process.\\n\u2b06 back to ToC\\nFeature Engineering\\nProject\\nDetails\\nRepository\\nFeatureform\\nThe Virtual Feature Store. Turn your existing data infrastructure into a feature store.\\nFeatureTools\\nAn open source python framework for automated feature engineering\\n\u2b06 back to ToC\\nData/Feature enrichment\\nProject\\nDetails\\nRepository\\nUpgini\\nFree automated data &amp; feature enrichment library for machine learning: automatically searches through thousands of ready-to-use features from public and community shared data sources and enriches your training dataset with only the accuracy improving features\\nFeast\\nAn open source feature store for machine learning.\\n\u2b06 back to ToC\\nLarge Scale Deployment\\nML Platforms\\nProject\\nDetails\\nRepository\\nComet\\nComet is an MLOps platform that offers experiment tracking, model production management, a model registry, and full data lineage from training straight through to production. Comet plays nicely with all your favorite tools, so you don't have to change your existing workflow. Check out CometLLM for all your prompt engineering needs!\\nClearML\\nAuto-Magical CI/CD to streamline your ML workflow. Experiment Manager, MLOps and Data-Management.\\nHopsworks\\nHopsworks is a MLOps platform for training and operating large and small ML systems, including fine-tuning and serving LLMs. Hopsworks includes both a feature store and vector database for RAG.\\nOpenLLM\\nAn open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.\\nMLflow\\nOpen source platform for the machine learning lifecycle.\\nMLRun\\nAn open MLOps platform for quickly building and managing continuous ML applications across their lifecycle.\\nModelFox\\nModelFox is a platform for managing and deploying machine learning models.\\nKserve\\nStandardized Serverless ML Inference Platform on Kubernetes\\nKubeflow\\nMachine Learning Toolkit for Kubernetes.\\nPAI\\nResource scheduling and cluster management for AI.\\nPolyaxon\\nMachine Learning Management &amp; Orchestration Platform.\\nPrimehub\\nAn effortless infrastructure for machine learning built on the top of Kubernetes.\\nOpenModelZ\\nOne-click machine learning deployment (LLM, text-to-image and so on) at scale on any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine).\\nSeldon-core\\nAn MLOps framework to package, deploy, monitor and manage thousands of production machine learning models\\nStarwhale\\nAn MLOps/LLMOps platform for model building, evaluation, and fine-tuning.\\nTrueFoundry\\nA PaaS to deploy, Fine-tune and serve LLM Models on a company\u2019s own Infrastructure with Data Security and Optimal GPU and Cost Management. Launch your LLM Application at Production scale with best DevSecOps practices.\\nWeights &amp; Biases\\nA lightweight and flexible platform for machine learning experiment tracking, dataset versioning, and model management, enhancing collaboration and streamlining MLOps workflows. W&amp;B excels at tracking LLM-powered applications, featuring W&amp;B Prompts for LLM execution flow visualization, input and output monitoring, and secure management of prompts and LLM chain configurations.\\n\u2b06 back to ToC\\nWorkflow\\nProject\\nDetails\\nRepository\\nAirflow\\nA platform to programmatically author, schedule and monitor workflows.\\naqueduct\\nAn Open-Source Platform for Production Data Science\\nArgo Workflows\\nWorkflow engine for Kubernetes.\\nFlyte\\nKubernetes-native workflow automation platform for complex, mission-critical data and ML processes at scale.\\nHamilton\\nA lightweight framework to represent ML/language model pipelines as a series of python functions.\\nKubeflow Pipelines\\nMachine Learning Pipelines for Kubeflow.\\nLangFlow\\nAn effortless way to experiment and prototype LangChain flows with drag-and-drop components and a chat interface.\\nMetaflow\\nBuild and manage real-life data science projects with ease!\\nPloomber\\nThe fastest way to build data pipelines. Develop iteratively, deploy anywhere.\\nPrefect\\nThe easiest way to automate your data.\\nVDP\\nAn open-source unstructured data ETL tool to streamline the end-to-end unstructured data processing pipeline.\\nZenML\\nMLOps framework to create reproducible pipelines.\\n\u2b06 back to ToC\\nScheduling\\nProject\\nDetails\\nRepository\\nKueue\\nKubernetes-native Job Queueing.\\nPAI\\nResource scheduling and cluster management for AI (Open-sourced by Microsoft).\\nSlurm\\nA Highly Scalable Workload Manager.\\nVolcano\\nA Cloud Native Batch System (Project under CNCF).\\nYunikorn\\nLight-weight, universal resource scheduler for container orchestrator systems.\\n\u2b06 back to ToC\\nModel Management\\nProject\\nDetails\\nRepository\\nComet\\nComet is an MLOps platform that offers Model Production Management, a Model Registry, and full model lineage from training straight through to production. Use Comet for model reproducibility, model debugging, model versioning, model visibility, model auditing, model governance, and model monitoring.\\ndvc\\nML Experiments Management - Data Version Control - Git for Data &amp; Models\\nModelDB\\nOpen Source ML Model Versioning, Metadata, and Experiment Management\\nMLEM\\nA tool to package, serve, and deploy any ML model on any platform.\\normb\\nDocker for Your ML/DL Models Based on OCI Artifacts\\n\u2b06 back to ToC\\nPerformance\\nML Compiler\\nProject\\nDetails\\nRepository\\nONNX-MLIR\\nCompiler technology to transform a valid Open Neural Network Exchange (ONNX) graph into code that implements the graph with minimum runtime support.\\nTVM\\nOpen deep learning compiler stack for cpu, gpu and specialized accelerators\\n\u2b06 back to ToC\\nProfiling\\nProject\\nDetails\\nRepository\\noctoml-profile\\noctoml-profile is a python library and cloud service designed to provide the simplest experience for assessing and optimizing the performance of PyTorch models on cloud hardware with state-of-the-art ML acceleration technology.\\nscalene\\na high-performance, high-precision CPU, GPU, and memory profiler for Python\\n\u2b06 back to ToC\\nAutoML\\nProject\\nDetails\\nRepository\\nArchai\\na platform for Neural Network Search (NAS) that allows you to generate efficient deep networks for your applications.\\nautoai\\nA framework to find the best performing AI/ML model for any AI problem.\\nAutoGL\\nAn autoML framework &amp; toolkit for machine learning on graphs\\nAutoGluon\\nAutoML for Image, Text, and Tabular Data.\\nautoml-gs\\nProvide an input CSV and a target field to predict, generate a model + code to run it.\\nautokeras\\nAutoML library for deep learning.\\nAuto-PyTorch\\nAutomatic architecture search and hyperparameter optimization for PyTorch.\\nauto-sklearn\\nan automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator.\\nDragonfly\\nAn open source python library for scalable Bayesian optimisation.\\nDetermined\\nscalable deep learning training platform with integrated hyperparameter tuning support; includes Hyperband, PBT, and other search methods.\\nDEvol (DeepEvolution)\\na basic proof of concept for genetic architecture search in Keras.\\nEvalML\\nAn open source python library for AutoML.\\nFEDOT\\nAutoML framework for the design of composite pipelines.\\nFLAML\\nFast and lightweight AutoML (\\npaper\\n).\\nGoptuna\\nA hyperparameter optimization framework, inspired by Optuna.\\nHpBandSter\\na framework for distributed hyperparameter optimization.\\nHPOlib2\\na library for hyperparameter optimization and black box optimization benchmarks.\\nHyperband\\nopen source code for tuning hyperparams with Hyperband.\\nHypernets\\nA General Automated Machine Learning Framework.\\nHyperopt\\nDistributed Asynchronous Hyperparameter Optimization in Python.\\nhyperunity\\nA toolset for black-box hyperparameter optimisation.\\nIntelli\\nA framework to connect a flow of ML models by applying graph theory.\\nKatib\\nKatib is a Kubernetes-native project for automated machine learning (AutoML).\\nKeras Tuner\\nHyperparameter tuning for humans.\\nlearn2learn\\nPyTorch Meta-learning Framework for Researchers.\\nLudwig\\na toolbox built on top of TensorFlow that allows to train and test deep learning models without the need to write code.\\nMOE\\na global, black box optimization engine for real world metric optimization by Yelp.\\nModel Search\\na framework that implements AutoML algorithms for model architecture search at scale.\\nNASGym\\na proof-of-concept OpenAI Gym environment for Neural Architecture Search (NAS).\\nNNI\\nAn open source AutoML toolkit for automate machine learning lifecycle, including feature engineering, neural architecture search, model compression and hyper-parameter tuning.\\nOptuna\\nA hyperparameter optimization framework.\\nPycaret\\nAn open-source, low-code machine learning library in Python that automates machine learning workflows.\\nRay Tune\\nScalable Hyperparameter Tuning.\\nREMBO\\nBayesian optimization in high-dimensions via random embedding.\\nRoBO\\na Robust Bayesian Optimization framework.\\nscikit-optimize(skopt)\\nSequential model-based optimization with a\\nscipy.optimize\\ninterface.\\nSpearmint\\na software package to perform Bayesian optimization.\\nTPOT\\none of the very first AutoML methods and open-source software packages.\\nTorchmeta\\nA Meta-Learning library for PyTorch.\\nVegas\\nan AutoML algorithm tool chain by Huawei Noah's Arb Lab.\\n\u2b06 back to ToC\\nOptimizations\\nProject\\nDetails\\nRepository\\nFeatherCNN\\nFeatherCNN is a high performance inference engine for convolutional neural networks.\\nForward\\nA library for high performance deep learning inference on NVIDIA GPUs.\\nNCNN\\nncnn is a high-performance neural network inference framework optimized for the mobile platform.\\nPocketFlow\\nuse AutoML to do model compression.\\nTensorFlow Model Optimization\\nA suite of tools that users, both novice and advanced, can use to optimize machine learning models for deployment and execution.\\nTNN\\nA uniform deep learning inference framework for mobile, desktop and server.\\n\u2b06 back to ToC\\nFederated ML\\nProject\\nDetails\\nRepository\\nEasyFL\\nAn Easy-to-use Federated Learning Platform\\nFATE\\nAn Industrial Grade Federated Learning Framework\\nFedML\\nThe federated learning and analytics library enabling secure and collaborative machine learning on decentralized data anywhere at any scale. Supporting large-scale cross-silo federated learning, cross-device federated learning on smartphones/IoTs, and research simulation.\\nFlower\\nA Friendly Federated Learning Framework\\nHarmonia\\nHarmonia is an open-source project aiming at developing systems/infrastructures and libraries to ease the adoption of federated learning (abbreviated to FL) for researches and production usage.\\nTensorFlow Federated\\nA framework for implementing federated learning\\n\u2b06 back to ToC\\nAwesome Lists\\nProject\\nDetails\\nRepository\\nAwesome Argo\\nA curated list of awesome projects and resources related to Argo\\nAwesome AutoDL\\nAutomated Deep Learning: Neural Architecture Search Is Not the End (a curated list of AutoDL resources and an in-depth analysis)\\nAwesome AutoML\\nCurating a list of AutoML-related research, tools, projects and other resources\\nAwesome AutoML Papers\\nA curated list of automated machine learning papers, articles, tutorials, slides and projects\\nAwesome-Code-LLM\\n\ud83d\udc68\\u200d\ud83d\udcbb An awesome and curated list of best code-LLM for research.\\nAwesome Federated Learning Systems\\nA curated list of Federated Learning Systems related academic papers, articles, tutorials, slides and projects.\\nAwesome Federated Learning\\nA curated list of federated learning publications, re-organized from Arxiv (mostly)\\nawesome-federated-learning\\nacc\\nAll materials you need for Federated Learning: blogs, videos, papers, and software, etc.\\nAwesome Open MLOps\\nThis is the Fuzzy Labs guide to the universe of free and open source MLOps tools.\\nAwesome Production Machine Learning\\nA curated list of awesome open source libraries to deploy, monitor, version and scale your machine learning\\nAwesome Tensor Compilers\\nA list of awesome compiler projects and papers for tensor computation and deep learning.\\nkelvins/awesome-mlops\\nA curated list of awesome MLOps tools.\\nvisenger/awesome-mlops\\nMachine Learning Operations - An awesome list of references for MLOps\\ncurrentslab/awesome-vector-search\\nA curated list of awesome vector search framework/engine, library, cloud service and research papers to vector similarity search.\\npleisto/flappy\\nProduction-Ready LLM Agent SDK for Every Developer\\n\u2b06 back to ToC\\nAbout\\nAn awesome &amp; curated list of best LLMOps tools for developers\\nTopics\\nawesome-list\\nmlops\\nai-development-tools\\nllmops\\nResources\\nReadme\\nLicense\\nCC0-1.0 license\\nActivity\\nCustom properties\\nStars\\n3.7k\\nstars\\nWatchers\\n64\\nwatching\\nForks\\n353\\nforks\\nReport repository\\nReleases\\nNo releases published\\nPackages\\n0\\nNo packages published\\nContributors\\n81\\n+ 67 contributors\\nLanguages\\nShell\\n86.2%\\nPython\\n13.8%\",         \"tool_call_id\": \"call_YeaR70E6l7iM7UHEtp709iVc\",         \"name\": \"extract_content\",     },     {         \"role\": \"tool\",         \"content\": \"Medium\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nPAGE NOT FOUND\\n404\\nOut of nothing, something.\\nYou can find (just about) anything on\\nMedium\\n\u2014 apparently even a page that doesn\u2019t exist. Maybe these stories will take you somewhere new?\\nHome\\n\u201cYou Can\u2019t Eat Technology.\u201d\\nAdam DeMartino\\nAug 25, 2024\\n\u00b7\\n11 min read\\n\u201cYou Can\u2019t Eat Technology.\u201d\\nAdam DeMartino\\nAug 25, 2024\\n\u00b7\\n11 min read\\nReflections from the 2024 DNC\\nIsaac Saul\\nin\\nThe Political Prism\\nAug 25, 2024\\n\u00b7\\n15 min read\\nReflections from the 2024 DNC\\nIsaac Saul\\nin\\nThe Political Prism\\nAug 25, 2024\\n\u00b7\\n15 min read\\nIt\u2019s cognitive bias week, because thinking is hard\\nThe Medium Newsletter\\nin\\nThe Medium Blog\\nAug 27, 2024\\n\u00b7\\n3 min read\\nIt\u2019s cognitive bias week, because thinking is hard\\nThe Medium Newsletter\\nin\\nThe Medium Blog\\nAug 27, 2024\\n\u00b7\\n3 min read\\nThe Fun and Games of College Tours\\njen murphy parker\\nAug 17, 2024\\n\u00b7\\n12 min read\\nMember-only\\nThe Fun and Games of College Tours\\njen murphy parker\\nAug 17, 2024\\n\u00b7\\n12 min read\\nMember-only\",         \"tool_call_id\": \"call_UWuyM3dy71Js7fspwSKnMlGC\",         \"name\": \"extract_content\",     }, ]   @pytest.mark.asyncio async def test_conversation():     \"\"\"Tests that in a conversation setting, the llm generated query is context-relevant.\"\"\"     web_assistant = WebAssistant(         search_history=[             \"best LLM development tools\",             \"top libraries for LLM development\",             \"LLM libraries for software engineers\",             \"LLM dev tools for machine learning\",             \"most popular libraries for LLM development\",         ],         messages=test_conversation_messages,     )     response = await web_assistant._stream(\"What is mirascope library?\")     async for _, tool in response:         queries = tool.args.get(\"queries\", \"\") if tool else \"\"         is_context_relevant = False         for query in queries:             context_relevant = await check_context_relevance(                 web_assistant.search_history, \"What is mirascope library?\", query             )             is_context_relevant = context_relevant.is_context_relevant             if is_context_relevant:                 break         assert is_context_relevant   ipytest.run() <p>A few things to note:</p> <ul> <li>Messages are appended to reduce testing time and token usage. Check messages.py for the full history used.</li> <li>Our test asserts at least one of the llm queries generated must be context-relevant.</li> </ul> <p>Evaluating context relevance is just of the crucial steps towards enhancing LLM-powered search systems, enabling them to provide more coherent, personalized, and valuable results across diverse user interactions.</p> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Use <code>pytest.mark.parametrize</code> and add more examples from user queries to further identify areas of improvement.</li> <li>Implement context relevance call in to the agent to generate a feedback loop for the agent to ask a better search query.</li> <li>Context-relevance is not always what the user is asking for. The challenge lies in distinguishing between unintentional context loss and intentional topic shifts. This can be addressed by implementing a like/dislike answer feature and using that feedback to refine its search.</li> </ul>"},{"location":"tutorials/evals/evaluating_web_search_agent/#evaluating-web-search-agent-with-llm","title":"Evaluating Web Search Agent with LLM\u00b6","text":"<p>In this recipe, we will be using taking our Web Search Agent Cookbook and running evaluations on the LLM call. We will be exploring writing a context relevance test since that is one of the most important aspects of web search.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Tools</li> <li>Async</li> <li>Evals</li> </ul> <p>Check out the Web Search Agent Cookbook</p> <p> We will be using our <code>WebAssistantAgent</code> for our evaluations. For a detailed explanation regarding this code snippet, refer to the Web Search Agent Cookbook. </p>"},{"location":"tutorials/evals/evaluating_web_search_agent/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/evals/evaluating_web_search_agent/#basic-evaluations","title":"Basic Evaluations\u00b6","text":"<p>Let's start off with some basic evaluations to know whether our agent is working in general. Given that updates to prompts can significantly influence LLM behavior, it's crucial to test each component of our agent individually.</p>"},{"location":"tutorials/evals/evaluating_web_search_agent/#evaluating-_web_search-tool","title":"Evaluating <code>_web_search</code> tool\u00b6","text":"<p>Our goal is to ensure that the LLM consistently utilizes the web search tool, rather than relying on its inherent knowledge base to generate responses. We've intentionally refrained from explicitly instructing the agent to always utilize the web search tool, as some user queries may be more conversational in nature and not necessitate web searches. However, for user queries that are more information-seeking, the agent should always leverage the web search tool.</p>"},{"location":"tutorials/evals/evaluating_web_search_agent/#evaluating-extract_content-tool","title":"Evaluating <code>extract_content</code> tool\u00b6","text":"<p>Our agent has been prompt engineered to utilize the extract_content tool at its discretion. Given the non-deterministic nature of this test, we'll implement a basic verification to ensure that the <code>extract_content</code> tool is invoked at least once per user query. We'll employ the same golden dataset used in the <code>test_web_search</code>, allowing us to assume that <code>test_extract_content</code> will always have a functional <code>_web_search</code>.</p>"},{"location":"tutorials/evals/evaluating_web_search_agent/#evaluating-context-relevance-of-llm-generated-queries","title":"Evaluating context relevance of LLM-generated queries\u00b6","text":"<p>LLMs can easily answer detailed queries, but real-world scenarios often involve vague questions from users who may not fully understand what they're seeking. Just as many people struggle to master advanced search techniques despite years of using search engines, becoming proficient at formulating effective queries for LLMs is equally challenging.</p>"},{"location":"tutorials/evals/evaluating_web_search_agent/#examples","title":"Examples\u00b6","text":"<p>We can write some simple examples to verify if the evaluation is working properly, like so:</p>"},{"location":"tutorials/evals/evaluating_web_search_agent/#implementing-the-test","title":"Implementing the test\u00b6","text":"<p>Now that we have our evaluation, we can write our test.</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/","title":"Dynamic Configuration &amp; Chaining","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\" In\u00a0[2]: Copied! <pre>from mirascope.core import BaseDynamicConfig, Messages, openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book(genre: str, creativity: float) -&gt; BaseDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"call_params\": {\"temperature\": creativity},\n    }\n\n\n# Low creativity recommendation\nresponse = recommend_book(\"mystery\", 0.2)\nprint(\"Low creativity:\", response.content)\n\n# High creativity recommendation\nresponse = recommend_book(\"mystery\", 0.8)\nprint(\"High creativity:\", response.content)\n</pre> from mirascope.core import BaseDynamicConfig, Messages, openai   @openai.call(\"gpt-4o-mini\") def recommend_book(genre: str, creativity: float) -&gt; BaseDynamicConfig:     return {         \"messages\": [Messages.User(f\"Recommend a {genre} book\")],         \"call_params\": {\"temperature\": creativity},     }   # Low creativity recommendation response = recommend_book(\"mystery\", 0.2) print(\"Low creativity:\", response.content)  # High creativity recommendation response = recommend_book(\"mystery\", 0.8) print(\"High creativity:\", response.content) <pre>Low creativity: I recommend \"The Guest List\" by Lucy Foley. This gripping mystery unfolds during a glamorous wedding on a remote Irish island, where tensions among the guests rise and secrets are revealed. When a murder occurs, everyone becomes a suspect, and the story alternates between different perspectives, keeping you guessing until the very end. It's a compelling read with well-developed characters and a twisty plot that will keep you on the edge of your seat!\nHigh creativity: I recommend \"The Girl with the Dragon Tattoo\" by Stieg Larsson. This gripping mystery combines elements of crime, family saga, and social commentary. It follows journalist Mikael Blomkvist and hacker Lisbeth Salander as they investigate a decades-old disappearance amidst a backdrop of dark family secrets and corporate corruption. The complex characters and intricate plot make it a compelling read for mystery enthusiasts. Enjoy!\n</pre> In\u00a0[3]: Copied! <pre>@openai.call(\"gpt-4o-mini\")\ndef recommend_book_by_age(genre: str, age: int) -&gt; Messages.Type:\n    reading_level = \"adult\"\n    if age &lt; 12:\n        reading_level = \"elementary\"\n    elif age &lt; 18:\n        reading_level = \"young adult\"\n    return f\"Recommend a {genre} book with a reading level of {reading_level}\"\n\n\nresponse = recommend_book_by_age(\"fantasy\", 15)\nprint(response.content)\n</pre> @openai.call(\"gpt-4o-mini\") def recommend_book_by_age(genre: str, age: int) -&gt; Messages.Type:     reading_level = \"adult\"     if age &lt; 12:         reading_level = \"elementary\"     elif age &lt; 18:         reading_level = \"young adult\"     return f\"Recommend a {genre} book with a reading level of {reading_level}\"   response = recommend_book_by_age(\"fantasy\", 15) print(response.content) <pre>I recommend **\"An Ember in the Ashes\"** by Sabaa Tahir. This young adult fantasy novel is set in a brutal, ancient Rome-inspired world where the oppressed must fight against a tyrannical regime. The story follows Laia, a young woman who becomes a spy for the resistance to save her brother, and Elias, a soldier who wants to escape the oppressive society. The book is rich in world-building, features complex characters, and explores themes of freedom, loyalty, and sacrifice. It's an engaging read for young adults looking for an exciting fantasy adventure!\n</pre> <p>When using string templates, you can use computed fields to dynamically generate or modify template variables used in your prompt. For more information on prompt templates, see the Prompts documentation.</p> In\u00a0[4]: Copied! <pre>from mirascope.core import prompt_template\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Recommend a {genre} book with a reading level of {reading_level}\")\ndef recommend_book_by_age(genre: str, age: int) -&gt; openai.OpenAIDynamicConfig:\n    reading_level = \"adult\"\n    if age &lt; 12:\n        reading_level = \"elementary\"\n    elif age &lt; 18:\n        reading_level = \"young adult\"\n    return {\"computed_fields\": {\"reading_level\": reading_level}}\n\n\nresponse = recommend_book_by_age(\"fantasy\", 15)\nprint(response.content)\n</pre> from mirascope.core import prompt_template   @openai.call(\"gpt-4o-mini\") @prompt_template(\"Recommend a {genre} book with a reading level of {reading_level}\") def recommend_book_by_age(genre: str, age: int) -&gt; openai.OpenAIDynamicConfig:     reading_level = \"adult\"     if age &lt; 12:         reading_level = \"elementary\"     elif age &lt; 18:         reading_level = \"young adult\"     return {\"computed_fields\": {\"reading_level\": reading_level}}   response = recommend_book_by_age(\"fantasy\", 15) print(response.content) <pre>I recommend \"An Ember in the Ashes\" by Sabaa Tahir. This gripping fantasy novel is set in a world inspired by ancient Rome, where a fierce soldier and a rebellious scholar find their destinies intertwined. With themes of bravery, sacrifice, and love, it's an engaging read suitable for young adult audiences. The rich world-building and complex characters make it a standout in the young adult fantasy genre. Enjoy your reading!\n</pre> In\u00a0[5]: Copied! <pre>def format_book(title: str, author: str, genre: str) -&gt; str:\n    \"\"\"Format a book recommendation.\"\"\"\n    return f\"{title} by {author} ({genre})\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef recommend_book_with_tool(genre: str) -&gt; openai.OpenAIDynamicConfig:\n    return {\n        \"messages\": [Messages.User(f\"Recommend a {genre} book\")],\n        \"tools\": [format_book],\n    }\n\n\nresponse = recommend_book_with_tool(\"mystery\")\nif response.tool:\n    print(response.tool.call())\nelse:\n    print(response.content)\n</pre> def format_book(title: str, author: str, genre: str) -&gt; str:     \"\"\"Format a book recommendation.\"\"\"     return f\"{title} by {author} ({genre})\"   @openai.call(\"gpt-4o-mini\") def recommend_book_with_tool(genre: str) -&gt; openai.OpenAIDynamicConfig:     return {         \"messages\": [Messages.User(f\"Recommend a {genre} book\")],         \"tools\": [format_book],     }   response = recommend_book_with_tool(\"mystery\") if response.tool:     print(response.tool.call()) else:     print(response.content) <pre>The Girl with the Dragon Tattoo by Stieg Larsson (Mystery)\n</pre> <p>For more advanced usage of tools, including the <code>BaseToolKit</code> class, please refer to the Tools documentation and the Tools and Agents Tutorial.</p> In\u00a0[6]: Copied! <pre>@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\noriginal_text = \"Long English text here...\"\nsummary = summarize(original_text)\ntranslation = translate(summary.content, \"french\")\nprint(translation.content)\n</pre> @openai.call(\"gpt-4o-mini\") def summarize(text: str) -&gt; str:     return f\"Summarize this text: {text}\"   @openai.call(\"gpt-4o-mini\") def translate(text: str, language: str) -&gt; str:     return f\"Translate this text to {language}: {text}\"   original_text = \"Long English text here...\" summary = summarize(original_text) translation = translate(summary.content, \"french\") print(translation.content) <pre>Bien s\u00fbr ! Veuillez fournir le long texte en anglais que vous souhaitez que je r\u00e9sume.\n</pre> In\u00a0[7]: Copied! <pre>@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; str:\n    summary = summarize(original_text)\n    return f\"Translate this text to {language}: {summary.content}\"\n\n\noriginal_text = \"Long English text here...\"\ntranslation = summarize_and_translate(original_text, \"french\")\nprint(translation.content)\n</pre> @openai.call(\"gpt-4o-mini\") def summarize(text: str) -&gt; str:     return f\"Summarize this text: {text}\"   @openai.call(\"gpt-4o-mini\") def summarize_and_translate(text: str, language: str) -&gt; str:     summary = summarize(original_text)     return f\"Translate this text to {language}: {summary.content}\"   original_text = \"Long English text here...\" translation = summarize_and_translate(original_text, \"french\") print(translation.content) <pre>Bien s\u00fbr ! Veuillez fournir le texte que vous souhaitez que je r\u00e9sume, et je ferai de mon mieux pour vous aider.\n</pre> In\u00a0[8]: Copied! <pre>@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\n@prompt_template(\"Translate this text to {language}: {summary}\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    return {\"computed_fields\": {\"summary\": summarize(text)}}\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(\"Translation:\", response.content)\nprint(\n    \"\\nComputed fields (including summary):\", response.dynamic_config[\"computed_fields\"]\n)\n</pre> @openai.call(\"gpt-4o-mini\") def summarize(text: str) -&gt; str:     return f\"Summarize this text: {text}\"   @openai.call(\"gpt-4o-mini\") @prompt_template(\"Translate this text to {language}: {summary}\") def summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:     return {\"computed_fields\": {\"summary\": summarize(text)}}   response = summarize_and_translate(\"Long English text here...\", \"french\") print(\"Translation:\", response.content) print(     \"\\nComputed fields (including summary):\", response.dynamic_config[\"computed_fields\"] ) <pre>Translation: Bien s\u00fbr ! Veuillez fournir le texte que vous aimeriez que je r\u00e9sume, et je serai heureux de vous aider.\n\nComputed fields (including summary): {'summary': OpenAICallResponse(metadata={}, response=ChatCompletion(id='chatcmpl-ABSRkdlz6phtmOBWqwVigidqET2tr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Of course! Please provide the text you'd like me to summarize, and I'll be happy to help.\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727294320, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_1bb46167f9', usage=CompletionUsage(completion_tokens=20, prompt_tokens=18, total_tokens=38, completion_tokens_details={'reasoning_tokens': 0})), tool_types=None, prompt_template=None, fn_args={'text': 'Long English text here...'}, dynamic_config={'messages': [BaseMessageParam(role='user', content='Summarize this text: Long English text here...')]}, messages=[{'role': 'user', 'content': 'Summarize this text: Long English text here...'}], call_params={}, call_kwargs={'model': 'gpt-4o-mini', 'messages': [{'role': 'user', 'content': 'Summarize this text: Long English text here...'}]}, user_message_param={'content': 'Summarize this text: Long English text here...', 'role': 'user'}, start_time=1727294320238.506, end_time=1727294320929.094, message_param={'content': \"Of course! Please provide the text you'd like me to summarize, and I'll be happy to help.\", 'refusal': None, 'role': 'assistant', 'tool_calls': None}, tools=None, tool=None)}\n</pre> <p>As you can see, with computed fields, you get access to both the final translation and the intermediate summary in a single response. This approach provides better traceability and can be particularly useful for debugging and understanding the entire chain of operations without the need to manage multiple separate function calls.</p> <p>Of course, you can always put the computed fields in the dynamic configuration without using string templates for the same effect:</p> In\u00a0[9]: Copied! <pre>@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:\n    summary = summarize(text)\n    return {\n        \"messages\": [\n            Messages.User(f\"Translate this text to {language}: {summary.content}\"),\n        ],\n        \"computed_fields\": {\"summary\": summary},\n    }\n\n\nresponse = summarize_and_translate(\"Long English text here...\", \"french\")\nprint(\"Translation:\", response.content)\nprint(\n    \"\\nComputed fields (including summary):\", response.dynamic_config[\"computed_fields\"]\n)\n</pre> @openai.call(\"gpt-4o-mini\") def summarize(text: str) -&gt; str:     return f\"Summarize this text: {text}\"   @openai.call(\"gpt-4o-mini\") def summarize_and_translate(text: str, language: str) -&gt; BaseDynamicConfig:     summary = summarize(text)     return {         \"messages\": [             Messages.User(f\"Translate this text to {language}: {summary.content}\"),         ],         \"computed_fields\": {\"summary\": summary},     }   response = summarize_and_translate(\"Long English text here...\", \"french\") print(\"Translation:\", response.content) print(     \"\\nComputed fields (including summary):\", response.dynamic_config[\"computed_fields\"] ) <pre>Translation: Bien s\u00fbr ! Veuillez fournir le texte que vous aimeriez que je r\u00e9sume.\n\nComputed fields (including summary): {'summary': OpenAICallResponse(metadata={}, response=ChatCompletion(id='chatcmpl-ABSRz6D82ecWKlAU7VUmWU4wbqiaY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure! Please provide the text you'd like me to summarize.\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1727294335, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_1bb46167f9', usage=CompletionUsage(completion_tokens=12, prompt_tokens=18, total_tokens=30, completion_tokens_details={'reasoning_tokens': 0})), tool_types=None, prompt_template=None, fn_args={'text': 'Long English text here...'}, dynamic_config={'messages': [BaseMessageParam(role='user', content='Summarize this text: Long English text here...')]}, messages=[{'role': 'user', 'content': 'Summarize this text: Long English text here...'}], call_params={}, call_kwargs={'model': 'gpt-4o-mini', 'messages': [{'role': 'user', 'content': 'Summarize this text: Long English text here...'}]}, user_message_param={'content': 'Summarize this text: Long English text here...', 'role': 'user'}, start_time=1727294335206.361, end_time=1727294335879.617, message_param={'content': \"Sure! Please provide the text you'd like me to summarize.\", 'refusal': None, 'role': 'assistant', 'tool_calls': None}, tools=None, tool=None)}\n</pre> In\u00a0[10]: Copied! <pre>from openai import OpenAIError\n\n\n@openai.call(\"gpt-4o-mini\")\ndef summarize(text: str) -&gt; str:\n    return f\"Summarize this text: {text}\"\n\n\n@openai.call(\"gpt-4o-mini\")\ndef translate(text: str, language: str) -&gt; str:\n    return f\"Translate this text to {language}: {text}\"\n\n\ndef process_text_with_error_handling(text: str, target_language: str):\n    try:\n        summary = summarize(text).content\n    except OpenAIError as e:\n        print(f\"Error during summarization: {e}\")\n        summary = text  # Fallback to original text if summarization fails\n\n    try:\n        translation = translate(summary, target_language).content\n        return translation\n    except OpenAIError as e:\n        print(f\"Error during translation: {e}\")\n        return summary  # Fallback to summary if translation fails\n\n\nresult = process_text_with_error_handling(\"Long text here...\", \"French\")\nprint(\"Processed Result:\", result)\n</pre> from openai import OpenAIError   @openai.call(\"gpt-4o-mini\") def summarize(text: str) -&gt; str:     return f\"Summarize this text: {text}\"   @openai.call(\"gpt-4o-mini\") def translate(text: str, language: str) -&gt; str:     return f\"Translate this text to {language}: {text}\"   def process_text_with_error_handling(text: str, target_language: str):     try:         summary = summarize(text).content     except OpenAIError as e:         print(f\"Error during summarization: {e}\")         summary = text  # Fallback to original text if summarization fails      try:         translation = translate(summary, target_language).content         return translation     except OpenAIError as e:         print(f\"Error during translation: {e}\")         return summary  # Fallback to summary if translation fails   result = process_text_with_error_handling(\"Long text here...\", \"French\") print(\"Processed Result:\", result) <pre>Processed Result: Bien s\u00fbr ! Veuillez fournir le texte que vous souhaiteriez que je r\u00e9sume.\n</pre>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#dynamic-configuration-chaining","title":"Dynamic Configuration &amp; Chaining\u00b6","text":"<p>This notebook provides a detailed introduction to using Dynamic Configuration and Chaining in Mirascope. We'll cover various examples ranging from basic usage to more complex chaining techniques.</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#setup","title":"Setup\u00b6","text":"<p>First, let's install Mirascope and set up our environment. We'll use OpenAI for our examples, but you can adapt these to other providers supported by Mirascope. For more information on supported providers, see the Calls documentation.</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#dynamic-configuration","title":"Dynamic Configuration\u00b6","text":"<p>Dynamic Configuration in Mirascope allows you to modify the behavior of LLM calls at runtime based on input arguments or other conditions.</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#basic-usage","title":"Basic Usage\u00b6","text":""},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#computed-fields","title":"Computed Fields\u00b6","text":"<p>When using the <code>Messages.Type</code> return for writing prompt templates, you can inject computed fields directly into the formatted strings:</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#dynamic-tools","title":"Dynamic Tools\u00b6","text":"<p>You can dynamically configure which tools are available to the LLM based on runtime conditions. Here's a simple example using a basic tool function:</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#chaining","title":"Chaining\u00b6","text":"<p>Chaining in Mirascope allows you to combine multiple LLM calls or operations in a sequence to solve complex tasks. Let's explore two main approaches to chaining: function-based chaining and chaining with computed fields.</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#function-based-chaining","title":"Function-based Chaining\u00b6","text":"<p>In function-based chaining, you call multiple functions in sequence, passing the output of one function as input to the next. This approach requires you to manage the sequence of calls manually.</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#nested-chaining","title":"Nested Chaining\u00b6","text":"<p>You can easily create a single function that calls the entire chain simply by calling each part of the chain in the function body of the corresponding parent:</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#chaining-with-computed-fields","title":"Chaining with Computed Fields\u00b6","text":"<p>Chaining with computed fields allows you to better trace your nested chains since the full chain of operations will exist in the response of the single function (rather than having to call and track each part of the chain separately).</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#error-handling-in-chains","title":"Error Handling in Chains\u00b6","text":"<p>Implementing robust error handling is crucial in complex chains:</p>"},{"location":"tutorials/getting_started/dynamic_configuration_and_chaining/#conclusion","title":"Conclusion\u00b6","text":"<p>This notebook has demonstrated various techniques for using Dynamic Configuration and Chaining in Mirascope. These powerful features allow you to create flexible, efficient, and complex LLM-powered applications. By combining these techniques, you can build sophisticated AI systems that can adapt to different inputs and requirements while maintaining robustness and traceability.</p> <p>Remember to always consider error handling, especially in complex chains, to ensure your applications are resilient to potential issues that may arise during LLM calls or processing steps.</p> <p>If you like what you've seen so far, give us a star and join our community.</p>"},{"location":"tutorials/getting_started/quickstart/","title":"Quickstart","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" <p>This command installs Mirascope along with the necessary packages for the OpenAI integration.</p> In\u00a0[5]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[2]: Copied! <pre>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\")\ndef get_capital(country: str) -&gt; str:\n    return f\"What is the capital of {country}?\"\n\n\nresponse = get_capital(\"Japan\")\nprint(response.content)\n</pre> from mirascope.core import openai   @openai.call(\"gpt-4o-mini\") def get_capital(country: str) -&gt; str:     return f\"What is the capital of {country}?\"   response = get_capital(\"Japan\") print(response.content) <pre>The capital of Japan is Tokyo.\n</pre> <p>In this example:</p> <ol> <li>We import the <code>openai</code> module from Mirascope, which provides the <code>call</code> decorator.</li> <li>The <code>@openai.call(\"gpt-4o-mini\")</code> decorator specifies which OpenAI model to use.</li> <li>We return the content of a single user message in the function body.</li> <li>When we call <code>get_capital(\"Japan\")</code>, it templates the prompt, sends a request to the OpenAI API, and returns the response.</li> <li>We print the <code>content</code> of the response, which contains the LLM's answer.</li> </ol> <p>This approach allows you to use LLMs as if they were regular Python functions, making it easy to integrate AI capabilities into your applications. For more advanced usage, including controlling model parameters and handling errors, see our documentation on Calls.</p> In\u00a0[3]: Copied! <pre>@openai.call(\"gpt-4o-mini\", stream=True)\ndef stream_city_info(city: str) -&gt; str:\n    return f\"Provide a brief description of {city}.\"\n\n\nfor chunk, _ in stream_city_info(\"Tokyo\"):\n    print(chunk.content, end=\"\", flush=True)\n</pre> @openai.call(\"gpt-4o-mini\", stream=True) def stream_city_info(city: str) -&gt; str:     return f\"Provide a brief description of {city}.\"   for chunk, _ in stream_city_info(\"Tokyo\"):     print(chunk.content, end=\"\", flush=True) <pre>Tokyo, the capital of Japan, is a vibrant metropolis known for its unique blend of tradition and modernity. As one of the world's most populous cities, it features a bustling urban landscape filled with skyscrapers, renowned shopping districts like Shibuya and Ginza, and cultural landmarks such as the historic Senso-ji Temple. Tokyo is also famous for its diverse culinary scene, ranging from street food to Michelin-starred restaurants. The city's efficient public transportation system makes it easy to explore its many neighborhoods, each offering distinct experiences, whether it\u2019s the tranquil gardens of Ueno, the electronic town of Akihabara, or the fashion-forward streets of Harajuku. With its rich cultural heritage, cutting-edge technology, and constant innovation, Tokyo embodies the essence of contemporary urban life.</pre> <p>Here's what's happening in this streaming example:</p> <ol> <li>We use the <code>stream=True</code> parameter in the <code>@openai.call</code> decorator to enable streaming.</li> <li>The function returns an iterator that yields chunks of the response as they become available.</li> <li>We iterate over the chunks, printing each one immediately.</li> <li>The <code>end=\"\"</code> and <code>flush=True</code> parameters in the print function ensure that the output is displayed in real-time without line breaks.</li> </ol> <p>Streaming is beneficial for:</p> <ul> <li>Providing immediate feedback to users</li> <li>Processing very long responses efficiently</li> <li>Implementing typewriter-like effects in user interfaces</li> </ul> <p>For more advanced streaming techniques, including error handling and processing streamed content, refer to our documentation on Streams.</p> In\u00a0[4]: Copied! <pre>from pydantic import BaseModel\n\n\nclass Capital(BaseModel):\n    city: str\n    country: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Capital)\ndef extract_capital(query: str) -&gt; str:\n    return f\"{query}\"\n\n\ncapital = extract_capital(\"The capital of France is Paris\")\nprint(capital)\n</pre> from pydantic import BaseModel   class Capital(BaseModel):     city: str     country: str   @openai.call(\"gpt-4o-mini\", response_model=Capital) def extract_capital(query: str) -&gt; str:     return f\"{query}\"   capital = extract_capital(\"The capital of France is Paris\") print(capital) <pre>city='Paris' country='France'\n</pre> <p>For more details on response models, including advanced validation techniques, check our documentation on Response Models.</p> In\u00a0[5]: Copied! <pre>import asyncio\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Capital)\nasync def get_capital_async(country: str) -&gt; str:\n    return f\"What is the capital of {country}?\"\n\n\nasync def main():\n    countries = [\"France\", \"Japan\", \"Brazil\"]\n    tasks = [get_capital_async(country) for country in countries]\n    capitals = await asyncio.gather(*tasks)\n    for capital in capitals:\n        print(f\"The capital of {capital.country} is {capital.city}\")\n\n\n# await main() when running in a Jupyter notebook\nawait main()\n\n# asyncio.run(main()) when running in a Python script\n</pre> import asyncio   @openai.call(\"gpt-4o-mini\", response_model=Capital) async def get_capital_async(country: str) -&gt; str:     return f\"What is the capital of {country}?\"   async def main():     countries = [\"France\", \"Japan\", \"Brazil\"]     tasks = [get_capital_async(country) for country in countries]     capitals = await asyncio.gather(*tasks)     for capital in capitals:         print(f\"The capital of {capital.country} is {capital.city}\")   # await main() when running in a Jupyter notebook await main()  # asyncio.run(main()) when running in a Python script <pre>The capital of France is Paris\nThe capital of Japan is Tokyo\nThe capital of Brazil is Bras\u00edlia\n</pre> <p>This asynchronous example demonstrates:</p> <ol> <li>An async version of our <code>get_capital</code> function, defined with <code>async def</code>.</li> <li>Use of <code>asyncio.gather()</code> to run multiple async tasks concurrently.</li> <li>Processing of results as they become available.</li> </ol> <p>Asynchronous processing offers several advantages:</p> <ul> <li>Improved performance when making multiple LLM calls</li> <li>Better resource utilization</li> <li>Compatibility with async web frameworks like FastAPI or aiohttp</li> </ul> <p>For more advanced asynchronous techniques, including error handling and async streaming, refer to our documentation on Async.</p> In\u00a0[6]: Copied! <pre>@openai.call(\"gpt-4o-mini\", json_mode=True)\ndef city_info(city: str) -&gt; str:\n    return f\"Provide information about {city} in JSON format\"\n\n\nresponse = city_info(\"Tokyo\")\nprint(response.content)  # This will be a JSON-formatted string\n</pre> @openai.call(\"gpt-4o-mini\", json_mode=True) def city_info(city: str) -&gt; str:     return f\"Provide information about {city} in JSON format\"   response = city_info(\"Tokyo\") print(response.content)  # This will be a JSON-formatted string <pre>{\n  \"city\": \"Tokyo\",\n  \"country\": \"Japan\",\n  \"population\": 13929286,\n  \"area_km2\": 2191,\n  \"language\": [\"Japanese\"],\n  \"currency\": {\n    \"name\": \"Yen\",\n    \"symbol\": \"\u00a5\"\n  },\n  \"landmarks\": [\n    {\n      \"name\": \"Tokyo Tower\",\n      \"type\": \"Observation Tower\"\n    },\n    {\n      \"name\": \"Shibuya Crossing\",\n      \"type\": \"Famous Intersection\"\n    },\n    {\n      \"name\": \"Senso-ji Temple\",\n      \"type\": \"Historic Site\"\n    },\n    {\n      \"name\": \"Meiji Shrine\",\n      \"type\": \"Shinto Shrine\"\n    }\n  ],\n  \"transportation\": {\n    \"rail\": {\n      \"types\": [\"Subway\", \"Light Rail\", \"High-Speed Rail\"],\n      \"notable_lines\": [\"Yamanote Line\", \"Chuo Line\", \"Tozai Line\"]\n    },\n    \"airport\": [\"Narita International Airport\", \"Haneda Airport\"]\n  },\n  \"cuisine\": [\n    \"Sushi\",\n    \"Ramen\",\n    \"Tempura\",\n    \"Yakitori\"\n  ],\n  \"climate\": {\n    \"type\": \"Humid subtropical\",\n    \"average_temperature\": {\n      \"summer\": \"26\u00b0C\",\n      \"winter\": \"5\u00b0C\"\n    },\n    \"average_precipitation_mm\": 1650\n  }\n}\n</pre> <p>JSON mode is beneficial for:</p> <ul> <li>Ensuring structured outputs from LLMs</li> <li>Easy integration with data processing pipelines</li> <li>Creating APIs that return JSON data</li> </ul> <p>Note that not all providers have an explicit JSON mode. For those providers, we attempt to instruct the model to provide JSON; however, there is no guarantee that it will output only JSON (it may start with some text like \"Here is the JSON: ...\"). This is where Output Parsers can be useful.</p> <p>It's also worth noting that you can combine <code>json_mode=True</code> with <code>response_model</code> to automatically parse the JSON output into a Pydantic model. This approach combines the benefits of JSON mode with the type safety and validation of response models. Here's an example:</p> In\u00a0[7]: Copied! <pre>from pydantic import BaseModel\n\n\nclass CityInfo(BaseModel):\n    name: str\n    population: int\n    country: str\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True, response_model=CityInfo)\ndef city_info(city: str) -&gt; str:\n    return f\"Provide information about {city} in JSON format\"\n\n\nresponse = city_info(\"Tokyo\")\nprint(\n    f\"Name: {response.name}, Population: {response.population}, Country: {response.country}\"\n)\n</pre> from pydantic import BaseModel   class CityInfo(BaseModel):     name: str     population: int     country: str   @openai.call(\"gpt-4o-mini\", json_mode=True, response_model=CityInfo) def city_info(city: str) -&gt; str:     return f\"Provide information about {city} in JSON format\"   response = city_info(\"Tokyo\") print(     f\"Name: {response.name}, Population: {response.population}, Country: {response.country}\" ) <pre>Name: Tokyo, Population: 13929286, Country: Japan\n</pre> <p>For more information on JSON mode and its limitations with different providers, refer to our documentation on JSON Mode.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[anthropic]\"\n</pre> !pip install \"mirascope[anthropic]\" In\u00a0[9]: Copied! <pre>import json\n\nfrom mirascope.core import anthropic\n\n\ndef only_json(response: anthropic.AnthropicCallResponse) -&gt; str:\n    json_start = response.content.index(\"{\")\n    json_end = response.content.rfind(\"}\")\n    return response.content[json_start : json_end + 1]\n\n\n@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True, output_parser=only_json)\ndef json_extraction(text: str, fields: list[str]) -&gt; str:\n    return f\"Extract {fields} from the following text: {text}\"\n\n\njson_response = json_extraction(\n    text=\"The capital of France is Paris\",\n    fields=[\"capital\", \"country\"],\n)\nprint(json.loads(json_response))\n</pre> import json  from mirascope.core import anthropic   def only_json(response: anthropic.AnthropicCallResponse) -&gt; str:     json_start = response.content.index(\"{\")     json_end = response.content.rfind(\"}\")     return response.content[json_start : json_end + 1]   @anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True, output_parser=only_json) def json_extraction(text: str, fields: list[str]) -&gt; str:     return f\"Extract {fields} from the following text: {text}\"   json_response = json_extraction(     text=\"The capital of France is Paris\",     fields=[\"capital\", \"country\"], ) print(json.loads(json_response)) <pre>{'capital': 'Paris', 'country': 'France'}\n</pre> <p>In this example:</p> <ol> <li>We define a custom <code>only_json</code> parser that extracts the JSON portion from the response.</li> <li>We use both <code>json_mode=True</code> and the custom output parser to ensure we get clean JSON output.</li> <li>The <code>json_extraction</code> function demonstrates how to combine JSON mode with a custom parser.</li> </ol> <p>Output parsers are useful for:</p> <ul> <li>Extracting specific formats or data structures from LLM responses</li> <li>Cleaning and standardizing LLM outputs</li> <li>Implementing custom post-processing logic</li> </ul> <p>For more information on output parsers and advanced usage scenarios, see our documentation on Output Parsers.</p>"},{"location":"tutorials/getting_started/quickstart/#quickstart","title":"Quickstart\u00b6","text":"<p>Mirascope supports various LLM providers, including OpenAI, Anthropic, Mistral, Gemini, Groq, Cohere, LiteLLM, Azure AI, and Vertex AI. For the purposes of this guide, we will be using OpenAI.</p>"},{"location":"tutorials/getting_started/quickstart/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/getting_started/quickstart/#basic-llm-call","title":"Basic LLM Call\u00b6","text":"<p>The <code>call</code> decorator in Mirascope transforms Python functions into LLM API calls. This allows you to seamlessly integrate LLM interactions into your Python code.</p>"},{"location":"tutorials/getting_started/quickstart/#streaming-responses","title":"Streaming Responses\u00b6","text":"<p>Streaming allows you to process LLM responses in real-time, which is particularly useful for long-form content generation or when you want to provide immediate feedback to users.</p>"},{"location":"tutorials/getting_started/quickstart/#response-models","title":"Response Models\u00b6","text":"<p>Response models in Mirascope allow you to structure and validate the output from LLMs. This feature is particularly useful when you need to ensure that the LLM's response adheres to a specific format or contains certain fields.</p>"},{"location":"tutorials/getting_started/quickstart/#asynchronous-processing","title":"Asynchronous Processing\u00b6","text":"<p>Mirascope supports asynchronous processing, allowing for efficient parallel execution of multiple LLM calls. This is particularly useful when you need to make many LLM calls concurrently or when working with asynchronous web frameworks.</p>"},{"location":"tutorials/getting_started/quickstart/#json-mode","title":"JSON Mode\u00b6","text":"<p>JSON mode allows you to directly parse LLM outputs as JSON. This is particularly useful when you need structured data from your LLM calls.</p>"},{"location":"tutorials/getting_started/quickstart/#output-parsers","title":"Output Parsers\u00b6","text":"<p>Output parsers allow you to process LLM responses in custom formats. They are particularly useful when working with JSON outputs, especially for providers like Anthropic that don't have a strict JSON mode.</p>"},{"location":"tutorials/getting_started/quickstart/#next-steps","title":"Next Steps\u00b6","text":"<p>This concludes our Quickstart Guide to Mirascope. We've covered the main features of the library, including prompt templates, basic calls, streaming, response models, asynchronous processing, JSON mode, and output parsers. Each of these features can be combined and customized to create powerful, flexible AI applications.</p> <p>If you like what you've seen so far, give us a star and join our community.</p> <p>We recommend checking out our other getting started notebooks for more advanced usage:</p> <ul> <li>Structured Outputs</li> <li>Dynamic Configuration &amp; Chaining</li> <li>Tools &amp; Agents</li> </ul> <p>Check out our more comprehensive Learn documentation for more detailed information on Mirascope's features.</p>"},{"location":"tutorials/getting_started/structured_outputs/","title":"Structured Outputs","text":"In\u00a0[1]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\n# Set your API keys\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n</pre> import os  # Set your API keys os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\" In\u00a0[2]: Copied! <pre>from mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Book)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract the book from this text: {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nassert isinstance(book, Book)\nprint(book)\n</pre> from mirascope.core import openai from pydantic import BaseModel   class Book(BaseModel):     title: str     author: str   @openai.call(\"gpt-4o-mini\", response_model=Book) def extract_book(text: str) -&gt; str:     return f\"Extract the book from this text: {text}\"   book = extract_book(\"The Name of the Wind by Patrick Rothfuss\") assert isinstance(book, Book) print(book) <pre>title='The Name of the Wind' author='Patrick Rothfuss'\n</pre> <p>In this example we are:</p> <ol> <li>Defining a response model <code>Book</code> as a Pydantic <code>BaseModel</code> subclass</li> <li>Setting <code>response_model</code> equal to our <code>Book</code> type.</li> <li>Running our LLM API call function as normal, except the output is now a <code>Book</code> instance.</li> </ol> In\u00a0[3]: Copied! <pre>@openai.call(\"gpt-4o-mini\", response_model=list[Book])\ndef recommend_books(genre: str, num: int) -&gt; str:\n    return f\"Recommend a list of {num} {genre} books\"\n\n\nbooks = recommend_books(\"fantasy\", 3)\nfor book in books:\n    print(book)\n</pre> @openai.call(\"gpt-4o-mini\", response_model=list[Book]) def recommend_books(genre: str, num: int) -&gt; str:     return f\"Recommend a list of {num} {genre} books\"   books = recommend_books(\"fantasy\", 3) for book in books:     print(book) <pre>title='The Name of the Wind' author='Patrick Rothfuss'\ntitle='Mistborn: The Final Empire' author='Brandon Sanderson'\ntitle='A Darker Shade of Magic' author='V.E. Schwab'\n</pre> <p>In this example we:</p> <ol> <li>Updated our prompt to instruct to model to \"recommend\" (i.e. generate) books.</li> <li>We set <code>response_model</code> equal to <code>list[Book]</code> to output multiple books instead of just one.</li> <li>We further updated our prompt to enable the user to specify how many books to generate.</li> </ol> In\u00a0[4]: Copied! <pre>import json\n\n\n@openai.call(\"gpt-4o-mini\", json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract the book title and author from this text: {text}\"\n\n\nresponse = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(json.loads(response.content))\n</pre> import json   @openai.call(\"gpt-4o-mini\", json_mode=True) def extract_book(text: str) -&gt; str:     return f\"Extract the book title and author from this text: {text}\"   response = extract_book(\"The Name of the Wind by Patrick Rothfuss\") print(json.loads(response.content)) <pre>{'title': 'The Name of the Wind', 'author': 'Patrick Rothfuss'}\n</pre> <p>In this example we:</p> <ol> <li>Set <code>json_mode=True</code> to signal we want to use JSON Mode</li> <li>Specify the fields that we want in the prompt</li> <li>Parse the json string output into a Python dictionary</li> </ol> <p>If you want additional validation on the output structure and types, you can use <code>json_mode</code> in conjunction with <code>response_model</code> to validate your structured outputs:</p> In\u00a0[5]: Copied! <pre>@openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True)\ndef extract_book(text: str) -&gt; str:\n    return f\"Extract the book from this text: {text}\"\n\n\nbook = extract_book(\"The Name of the Wind by Patrick Rothfuss\")\nassert isinstance(book, Book)\nprint(book)\n</pre> @openai.call(\"gpt-4o-mini\", response_model=Book, json_mode=True) def extract_book(text: str) -&gt; str:     return f\"Extract the book from this text: {text}\"   book = extract_book(\"The Name of the Wind by Patrick Rothfuss\") assert isinstance(book, Book) print(book) <pre>title='The Name of the Wind' author='Patrick Rothfuss'\n</pre> In\u00a0[6]: Copied! <pre>from pydantic import ConfigDict, Field\n\n\nclass FewShotBook(BaseModel):\n    title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"},\n            ]\n        }\n    )\n\n\n@openai.call(\"gpt-4o-mini\", response_model=list[FewShotBook], json_mode=True)\ndef recommend_few_shot_books(genre: str, num: int) -&gt; str:\n    return f\"Recommend a list of {num} {genre} books. Match example format.\"\n\n\nbooks = recommend_few_shot_books(\"fantasy\", 3)\nfor book in books:\n    print(book)\n</pre> from pydantic import ConfigDict, Field   class FewShotBook(BaseModel):     title: str = Field(..., examples=[\"THE NAME OF THE WIND\"])     author: str = Field(..., examples=[\"Rothfuss, Patrick\"])      model_config = ConfigDict(         json_schema_extra={             \"examples\": [                 {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Rothfuss, Patrick\"},             ]         }     )   @openai.call(\"gpt-4o-mini\", response_model=list[FewShotBook], json_mode=True) def recommend_few_shot_books(genre: str, num: int) -&gt; str:     return f\"Recommend a list of {num} {genre} books. Match example format.\"   books = recommend_few_shot_books(\"fantasy\", 3) for book in books:     print(book) <pre>title='THE HOBBIT' author='Tolkien, J.R.R.'\ntitle='A WIZARD OF EARTHSEA' author='Le Guin, Ursula K.'\ntitle='A GAME OF THRONES' author='Martin, George R.R.'\n</pre> <p>In this example we:</p> <ol> <li>Added a few-shot example to each field in our response model.</li> <li>Added a few-shot example for the entire response model.</li> <li>Set <code>json_mode=True</code> because we have found that examples are more effective with this setting.</li> <li>Updated the prompt to instruct the LLM to match the format of the examples.</li> </ol> In\u00a0[7]: Copied! <pre>from pydantic import field_validator\n\n\nclass ValidatedBook(BaseModel):\n    title: str\n    author: str\n\n    @field_validator(\"title\", \"author\")\n    @classmethod\n    def must_be_uppercase(cls, v: str) -&gt; str:\n        assert v.isupper(), \"All fields must be uppercase\"\n        return v\n\n\n@openai.call(\"gpt-4o-mini\", response_model=ValidatedBook)\ndef extract_all_caps_book(text: str) -&gt; str:\n    return f\"Extract the book from this text: {text}\"\n\n\nbook = extract_all_caps_book(\"The Name of the Wind by Patrick Rothfuss\")\nprint(book)\n</pre> from pydantic import field_validator   class ValidatedBook(BaseModel):     title: str     author: str      @field_validator(\"title\", \"author\")     @classmethod     def must_be_uppercase(cls, v: str) -&gt; str:         assert v.isupper(), \"All fields must be uppercase\"         return v   @openai.call(\"gpt-4o-mini\", response_model=ValidatedBook) def extract_all_caps_book(text: str) -&gt; str:     return f\"Extract the book from this text: {text}\"   book = extract_all_caps_book(\"The Name of the Wind by Patrick Rothfuss\") print(book) <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[7], line 20\n     15 @openai.call(\"gpt-4o-mini\", response_model=ValidatedBook)\n     16 def extract_all_caps_book(text: str) -&gt; str:\n     17     return f\"Extract the book from this text: {text}\"\n---&gt; 20 book = extract_all_caps_book(\"The Name of the Wind by Patrick Rothfuss\")\n     21 print(book)\n\nFile ~/Mirascope/GitHub/mirascope/mirascope/core/base/_extract.py:132, in extract_factory.&lt;locals&gt;.decorator.&lt;locals&gt;.inner(*args, **kwargs)\n    130 except ValidationError as e:\n    131     e._response = call_response  # pyright: ignore [reportAttributeAccessIssue]\n--&gt; 132     raise e\n    133 if isinstance(output, BaseModel):\n    134     output._response = call_response  # pyright: ignore [reportAttributeAccessIssue]\n\nFile ~/Mirascope/GitHub/mirascope/mirascope/core/base/_extract.py:129, in extract_factory.&lt;locals&gt;.decorator.&lt;locals&gt;.inner(*args, **kwargs)\n    127 json_output = get_json_output(call_response, json_mode)\n    128 try:\n--&gt; 129     output = extract_tool_return(response_model, json_output, False)\n    130 except ValidationError as e:\n    131     e._response = call_response  # pyright: ignore [reportAttributeAccessIssue]\n\nFile ~/Mirascope/GitHub/mirascope/mirascope/core/base/_utils/_extract_tool_return.py:38, in extract_tool_return(response_model, json_output, allow_partial)\n     36 elif allow_partial:\n     37     return partial(response_model).model_validate(json_obj)\n---&gt; 38 return response_model.model_validate(json_obj)\n\nFile ~/Mirascope/GitHub/mirascope/.venv/lib/python3.10/site-packages/pydantic/main.py:551, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)\n    549 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    550 __tracebackhide__ = True\n--&gt; 551 return cls.__pydantic_validator__.validate_python(\n    552     obj, strict=strict, from_attributes=from_attributes, context=context\n    553 )\n\nValidationError: 2 validation errors for ValidatedBook\ntitle\n  Assertion failed, All fields must be uppercase [type=assertion_error, input_value='The Name of the Wind', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\nauthor\n  Assertion failed, All fields must be uppercase [type=assertion_error, input_value='Patrick Rothfuss', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/assertion_error</pre> <p>We can see in this example that the model threw a <code>ValidationError</code> on construction because the extracted <code>title</code> and <code>author</code> fields were not all caps.</p> <p>This example is of course for demonstration purposes. In a real-world example we would ensure we catch such errors and handle them gracefully as well as further engineer the prompt to ensure such errors occur rarely or not at all.</p> In\u00a0[8]: Copied! <pre>import re\n\nfrom mirascope.core import prompt_template\n\n\ndef parse_cot(response: openai.OpenAICallResponse) -&gt; tuple[str, str]:\n    pattern = r\"&lt;thinking&gt;(.*?)&lt;/thinking&gt;.*?&lt;output&gt;(.*?)&lt;/output&gt;\"\n    match = re.search(pattern, response.content, re.DOTALL)\n    if not match:\n        return \"\", response.content\n    else:\n        return match.group(1).strip(), match.group(2).strip()\n\n\n@openai.call(\"gpt-4o-mini\", output_parser=parse_cot)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    First, output your thought process in &lt;thinking&gt; tags.\n    Then, provide your final output in &lt;output&gt; tags.\n\n    USER: {query}\n    \"\"\"\n)\ndef cot(query: str): ...\n\n\noutput = cot(\n    \"How many tennis balls does Roger have if he started with 2 and bought 3 tubes?\"\n)\nprint(f\"Thinking: {output[0]}\")\nprint(f\"Output: {output[1]}\")\n</pre> import re  from mirascope.core import prompt_template   def parse_cot(response: openai.OpenAICallResponse) -&gt; tuple[str, str]:     pattern = r\"(.*?).*?(.*?)\"     match = re.search(pattern, response.content, re.DOTALL)     if not match:         return \"\", response.content     else:         return match.group(1).strip(), match.group(2).strip()   @openai.call(\"gpt-4o-mini\", output_parser=parse_cot) @prompt_template(     \"\"\"     SYSTEM:     First, output your thought process in  tags.     Then, provide your final output in  tags.      USER: {query}     \"\"\" ) def cot(query: str): ...   output = cot(     \"How many tennis balls does Roger have if he started with 2 and bought 3 tubes?\" ) print(f\"Thinking: {output[0]}\") print(f\"Output: {output[1]}\") <pre>Thinking: To find out how many tennis balls Roger has, we first need to know how many balls are in a tube. A standard tube of tennis balls typically contains 3 balls. Given that Roger bought 3 tubes, we can calculate the total number of balls from the tubes by multiplying the number of tubes by the number of balls per tube. After that, we can add the 2 tennis balls he started with to find the total. So, the steps are:\n\n1. Calculate the total number of balls from the tubes: 3 tubes x 3 balls/tube = 9 balls.\n2. Add the 2 balls he started with to the total from the tubes: 2 + 9 = 11 balls.\n\nThus, I will output the final answer that Roger has a total of 11 tennis balls.\nOutput: Roger has 11 tennis balls.\n</pre>"},{"location":"tutorials/getting_started/structured_outputs/#structured-outputs","title":"Structured Outputs\u00b6","text":"<p>Large Language Models (LLMs) generate unstructured text data by default. Structured outputs are essential for building reliable and efficient AI applications, and this notebook demonstrates various techniques for structuring LLM outputs using Mirascope.</p> <p>These methods help ensure consistency, type safety, and easier integration of LLM responses into your application. For more detailed information on structured outputs in Mirascope, refer to the Response Models documentation, JSON Mode documentation, and Output Parser documentation.</p>"},{"location":"tutorials/getting_started/structured_outputs/#setup","title":"Setup\u00b6","text":"<p>First, let's set up our environment by installing Mirascope and importing the necessary modules.</p>"},{"location":"tutorials/getting_started/structured_outputs/#extracting-structured-outputs","title":"Extracting Structured Outputs\u00b6","text":"<p>The simplest way to extract structured outputs with Mirascope is using <code>response_model</code> to define the output type of the call:</p>"},{"location":"tutorials/getting_started/structured_outputs/#generating-structured-outputs","title":"Generating Structured Outputs\u00b6","text":"<p>Another common use-case for structured outputs is to generate synthetic data. The interface is the same, requiring only an update to the prompt:</p>"},{"location":"tutorials/getting_started/structured_outputs/#json-mode","title":"JSON Mode\u00b6","text":"<p>Many LLM providers have JSON Mode to instruct the model to output JSON. Although not all providers offer JSON Mode support officially, Mirascope offers support for all providers. For providers with official support, we simply use the native API feature. For providers without official support, we prompt engineer the model to give us the JSON:</p>"},{"location":"tutorials/getting_started/structured_outputs/#few-shot-examples","title":"Few-Shot Examples\u00b6","text":"<p>Often when guiding an LLM's response, providing few-shot examples can greatly help steer the output in the right direction:</p>"},{"location":"tutorials/getting_started/structured_outputs/#validating-outputs","title":"Validating Outputs\u00b6","text":"<p>Since <code>response_model</code> relies on Pydantic <code>BaseModel</code> types, you can easily add additional validation criteria to your model. This ensures the validation will run on every call:</p>"},{"location":"tutorials/getting_started/structured_outputs/#output-parsers","title":"Output Parsers\u00b6","text":"<p>Mirascope also provides an <code>output_parser</code> argument that will run on the call response for every call. This enables writing prompts that have a more specific structure (such as Chain of Thought) while still ensuring the output has the desired structure:</p>"},{"location":"tutorials/getting_started/structured_outputs/#conclusion","title":"Conclusion\u00b6","text":"<p>These techniques provide a solid foundation for structuring outputs in your LLM applications. As you continue to work with LLMs and develop more complex applications, robust prompt engineering and validation will be crucial for ensuring the quality and reliability of your models and outputs.</p> <p>If you like what you've seen so far, give us a star and join our community.</p> <p>For more advanced topics and best practices, refer to the Mirascope Response Models documentation, JSON Mode documentation, and Output Parsers documentation.</p> <p>We also recommend taking a look at our Tenacity Integration to learn how to build more robust pipelines that automatically re-insert validation errors into a subsequent call, enabling the LLM to learn from its mistakes and (hopefully) output the correct answer on the next attempt.</p>"},{"location":"tutorials/getting_started/tools_and_agents/","title":"Tools &amp; Agents","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\" duckduckgo_search beautifulsoup4 requests\n</pre> !pip install \"mirascope[openai]\" duckduckgo_search beautifulsoup4 requests In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\" <p>For more information on setting up Mirascope and its dependencies, see the Mirascope getting started guide.</p> In\u00a0[2]: Copied! <pre>from mirascope.core import Messages, openai\nfrom pydantic import BaseModel\n\n\nclass BasicChatbot(BaseModel):\n    messages: list[openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\")\n    async def chat(self, user_input: str) -&gt; openai.OpenAIDynamicConfig:\n        messages = [\n            Messages.System(\n                \"You are a friendly chatbot assistant. Engage in a conversation with the user.\"\n            ),\n            *self.messages,\n            Messages.User(user_input),\n        ]\n        return {\"messages\": messages}\n\n    async def run(self):\n        while True:\n            user_input = input(\"User: \")\n            if user_input.lower() == \"exit\":\n                break\n            response = await self.chat(user_input)\n            print(f\"User: {user_input}\")\n            print(f\"Chatbot: {response.content}\")\n            self.messages.append(response.user_message_param)\n            self.messages.append(response.message_param)\n\n\n# Usage\nchatbot = BasicChatbot()\n# Run the chatbot in a Jupyter notebook\nawait chatbot.run()\n\n# Run the chatbot in a Python script\n# import asyncio\n# asyncio.run(chatbot.run())\n</pre> from mirascope.core import Messages, openai from pydantic import BaseModel   class BasicChatbot(BaseModel):     messages: list[openai.OpenAIMessageParam] = []      @openai.call(model=\"gpt-4o-mini\")     async def chat(self, user_input: str) -&gt; openai.OpenAIDynamicConfig:         messages = [             Messages.System(                 \"You are a friendly chatbot assistant. Engage in a conversation with the user.\"             ),             *self.messages,             Messages.User(user_input),         ]         return {\"messages\": messages}      async def run(self):         while True:             user_input = input(\"User: \")             if user_input.lower() == \"exit\":                 break             response = await self.chat(user_input)             print(f\"User: {user_input}\")             print(f\"Chatbot: {response.content}\")             self.messages.append(response.user_message_param)             self.messages.append(response.message_param)   # Usage chatbot = BasicChatbot() # Run the chatbot in a Jupyter notebook await chatbot.run()  # Run the chatbot in a Python script # import asyncio # asyncio.run(chatbot.run()) <pre>User: Hi!\nChatbot: Hello! How can I assist you today?\nUser: What can you help me with?\nChatbot: I can help with a variety of things! Whether you have questions about general knowledge, need information on a specific topic, want advice, or even just want to chat, I\u2019m here for you. What\u2019s on your mind?\n</pre> <p>In this basic chatbot implementation, we've created a <code>BasicChatbot</code> class that</p> <ol> <li>Maintains a conversation history (<code>messages</code>)</li> <li>Uses Mirascope's <code>@openai.call</code> decorators to interact with the LLM.</li> </ol> <p>Note that the <code>chat</code> method can directly access <code>self.messages</code> in the template, allowing for easy integration of the conversation history into the prompt.</p> <p>The <code>chat</code> method is where the LLM interaction occurs. It uses the conversation history and the current user input to generate a response. The <code>run</code> method manages the main conversation loop, updating the conversation history after each interaction. We give it the <code>openai.OpenAIDynamicConfig</code> return type since we will eventually want to insert tool call message parameters into the history, which will be specific to OpenAI.</p> <p>This sets the foundation for more complex agents that we'll build upon in the following sections.</p> In\u00a0[5]: Copied! <pre>import re\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef extract_content(url: str) -&gt; str:\n    \"\"\"Extract the main content from a webpage.\n\n    Args:\n        url: The URL of the webpage to extract the content from.\n\n    Returns:\n        The extracted content as a string.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        unwanted_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]\n        for tag in unwanted_tags:\n            for element in soup.find_all(tag):\n                element.decompose()\n        main_content = (\n            soup.find(\"main\")\n            or soup.find(\"article\")\n            or soup.find(\"div\", class_=re.compile(\"content|main\"))\n        )\n        if main_content:\n            text = main_content.get_text(separator=\"\\n\", strip=True)\n        else:\n            text = soup.get_text(separator=\"\\n\", strip=True)\n        lines = (line.strip() for line in text.splitlines())\n        return \"\\n\".join(line for line in lines if line)\n    except Exception as e:\n        return f\"{type(e)}: Failed to extract content from URL {url}\"\n</pre> import re  import requests from bs4 import BeautifulSoup   def extract_content(url: str) -&gt; str:     \"\"\"Extract the main content from a webpage.      Args:         url: The URL of the webpage to extract the content from.      Returns:         The extracted content as a string.     \"\"\"     try:         response = requests.get(url, timeout=5)         soup = BeautifulSoup(response.content, \"html.parser\")         unwanted_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]         for tag in unwanted_tags:             for element in soup.find_all(tag):                 element.decompose()         main_content = (             soup.find(\"main\")             or soup.find(\"article\")             or soup.find(\"div\", class_=re.compile(\"content|main\"))         )         if main_content:             text = main_content.get_text(separator=\"\\n\", strip=True)         else:             text = soup.get_text(separator=\"\\n\", strip=True)         lines = (line.strip() for line in text.splitlines())         return \"\\n\".join(line for line in lines if line)     except Exception as e:         return f\"{type(e)}: Failed to extract content from URL {url}\" <p>This <code>extract_content</code> function is a Tool that takes a URL as input and returns the main content of the webpage as a string. It uses BeautifulSoup to parse the HTML and extract the relevant text content.</p> <p>When this function is passed to a Mirascope <code>call</code> decorator, it is automatically converted into a <code>BaseTool</code> class. This conversion process generates a schema that the LLM API uses to understand and interact with the tool.</p> <p>Let's take a look at what this schema looks like:</p> In\u00a0[3]: Copied! <pre>from mirascope.core.openai import OpenAITool\n\ntool_type = OpenAITool.type_from_fn(extract_content)\nprint(tool_type.tool_schema())\n</pre> from mirascope.core.openai import OpenAITool  tool_type = OpenAITool.type_from_fn(extract_content) print(tool_type.tool_schema()) <pre>{'function': {'name': 'extract_content', 'description': 'Extract the main content from a webpage.\\n\\nArgs:\\n    url: The URL of the webpage to extract the content from.\\n\\nReturns:\\n    The extracted content as a string.', 'parameters': {'properties': {'url': {'description': 'The URL of the webpage to extract the content from.', 'type': 'string'}}, 'required': ['url'], 'type': 'object'}}, 'type': 'function'}\n</pre> <p>This schema provides the LLM with information about the tool's name, description, and expected input parameters. The LLM uses this information to determine when and how to use the tool during its reasoning process.</p> <p>For more details on implementing and using Tools in Mirascope, see the Tools documentation.</p> In\u00a0[6]: Copied! <pre>from duckduckgo_search import DDGS\n\n\nclass WebSearchAgentBase(BaseModel):\n    messages: list[openai.OpenAIMessageParam] = []\n    search_history: list[str] = []\n    max_results_per_query: int = 2\n\n    def web_search(self, queries: list[str]) -&gt; str:\n        \"\"\"Performs web searches for given queries and returns URLs.\n\n        Args:\n            queries: List of search queries.\n\n        Returns:\n            str: Newline-separated URLs from search results or error messages.\n\n        Raises:\n            Exception: If web search fails entirely.\n        \"\"\"\n        try:\n            urls = []\n            for query in queries:\n                results = DDGS(proxies=None).text(\n                    query, max_results=self.max_results_per_query\n                )\n                for result in results:\n                    link = result[\"href\"]\n                    try:\n                        urls.append(link)\n                    except Exception as e:\n                        urls.append(\n                            f\"{type(e)}: Failed to parse content from URL {link}\"\n                        )\n            self.search_history.extend(queries)\n            return \"\\n\\n\".join(urls)\n        except Exception as e:\n            return f\"{type(e)}: Failed to search the web for text\"\n</pre> from duckduckgo_search import DDGS   class WebSearchAgentBase(BaseModel):     messages: list[openai.OpenAIMessageParam] = []     search_history: list[str] = []     max_results_per_query: int = 2      def web_search(self, queries: list[str]) -&gt; str:         \"\"\"Performs web searches for given queries and returns URLs.          Args:             queries: List of search queries.          Returns:             str: Newline-separated URLs from search results or error messages.          Raises:             Exception: If web search fails entirely.         \"\"\"         try:             urls = []             for query in queries:                 results = DDGS(proxies=None).text(                     query, max_results=self.max_results_per_query                 )                 for result in results:                     link = result[\"href\"]                     try:                         urls.append(link)                     except Exception as e:                         urls.append(                             f\"{type(e)}: Failed to parse content from URL {link}\"                         )             self.search_history.extend(queries)             return \"\\n\\n\".join(urls)         except Exception as e:             return f\"{type(e)}: Failed to search the web for text\" <p>This <code>web_search</code> method is a more complex Tool that takes a list of search queries and returns a string of newline-separated URLs from the search results. It uses the DuckDuckGo search API to perform the searches and updates the agent's <code>search_history</code>. Luckily, the DuckDuckGo search API does not require an API key, making it easy to use in this example.</p> <p>By implementing this tool as a method within our <code>WebSearchAgent</code> class, we can access and update the agent's state (like <code>search_history</code>) directly. This approach allows for more integrated and stateful tool usage within our agent.</p> In\u00a0[11]: Copied! <pre>from datetime import datetime\n\nfrom mirascope.core import prompt_template\n\n\nclass WebSearchAgent(WebSearchAgentBase):\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. Your task is to answer the user's question using the provided tools.\n        The current date is {current_date}.\n\n        You have access to the following tools:\n        - `web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:\n            1. There is a previous search context: {self.search_history}\n            2. There is the current user query: {question}\n            3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query. \n                Even if the connection isn't immediately clear, consider how they might be related.\n        - `extract_content`: Parse the content of a webpage.\n\n        When calling the `web_search` tool, the `body` is simply the body of the search\n        result. You MUST then call the `extract_content` tool to get the actual content\n        of the webpage. It is up to you to determine which search results to parse.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness based on the context of the user's query.\n\n        MESSAGES: {self.messages}\n        USER: {question}\n        \"\"\"\n    )\n    async def _stream(self, question: str) -&gt; openai.OpenAIDynamicConfig:\n        return {\n            \"tools\": [self.web_search, extract_content],\n            \"computed_fields\": {\n                \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            },\n        }\n\n    async def _step(self, question: str):\n        response = await self._stream(question)\n        tools_and_outputs = []\n        async for chunk, tool in response:\n            if tool:\n                print(f\"using {tool._name()} tool with args: {tool.args}\")\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages.append(response.message_param)\n        if tools_and_outputs:\n            self.messages += response.tool_message_params(tools_and_outputs)\n            await self._step(\"\")\n\n    async def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(f\"(User): {question}\")\n            print(\"(Assistant): \", end=\"\", flush=True)\n            await self._step(question)\n            print()\n</pre> from datetime import datetime  from mirascope.core import prompt_template   class WebSearchAgent(WebSearchAgentBase):     @openai.call(model=\"gpt-4o-mini\", stream=True)     @prompt_template(         \"\"\"         SYSTEM:         You are an expert web searcher. Your task is to answer the user's question using the provided tools.         The current date is {current_date}.          You have access to the following tools:         - `web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:             1. There is a previous search context: {self.search_history}             2. There is the current user query: {question}             3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query.                  Even if the connection isn't immediately clear, consider how they might be related.         - `extract_content`: Parse the content of a webpage.          When calling the `web_search` tool, the `body` is simply the body of the search         result. You MUST then call the `extract_content` tool to get the actual content         of the webpage. It is up to you to determine which search results to parse.          Once you have gathered all of the information you need, generate a writeup that         strikes the right balance between brevity and completeness based on the context of the user's query.          MESSAGES: {self.messages}         USER: {question}         \"\"\"     )     async def _stream(self, question: str) -&gt; openai.OpenAIDynamicConfig:         return {             \"tools\": [self.web_search, extract_content],             \"computed_fields\": {                 \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")             },         }      async def _step(self, question: str):         response = await self._stream(question)         tools_and_outputs = []         async for chunk, tool in response:             if tool:                 print(f\"using {tool._name()} tool with args: {tool.args}\")                 tools_and_outputs.append((tool, tool.call()))             else:                 print(chunk.content, end=\"\", flush=True)         if response.user_message_param:             self.messages.append(response.user_message_param)         self.messages.append(response.message_param)         if tools_and_outputs:             self.messages += response.tool_message_params(tools_and_outputs)             await self._step(\"\")      async def run(self):         while True:             question = input(\"(User): \")             if question == \"exit\":                 break             print(f\"(User): {question}\")             print(\"(Assistant): \", end=\"\", flush=True)             await self._step(question)             print() <p>This implementation includes:</p> <ol> <li>The <code>_stream</code> method, which sets up the LLM call with the necessary tools and computed fields.</li> <li>The <code>_step</code> method, which processes the LLM response, handles tool calls, and updates the conversation history.</li> <li>The <code>run</code> method, which implements the main interaction loop for the agent.</li> </ol> <p>The <code>@openai.call</code> and <code>@prompt_template</code> decorators are used to set up the LLM interaction and define the prompt for the agent. Note that we've explicitly specified the available tools and their usage instructions in the system prompt. We have found that this improves the performance and reliability of tool usage by the LLM.</p> <p>Note how we're passing <code>self.web_search</code> as a tool, which allows the LLM to use our custom web search method that has access to the agent's state.</p> In\u00a0[13]: Copied! <pre>async def main():\n    web_assistant = WebSearchAgent()\n    await web_assistant.run()\n\n\n# Run main in a jupyter notebook\nawait main()\n\n# Run main in a python script\n# import asyncio\n# asyncio.run(main())\n</pre> async def main():     web_assistant = WebSearchAgent()     await web_assistant.run()   # Run main in a jupyter notebook await main()  # Run main in a python script # import asyncio # asyncio.run(main()) <pre>(User): large language models\n(Assistant): using web_search tool with args: {'queries': ['large language models overview', 'applications of large language models', 'latest advancements in large language models', 'large language models ethical concerns', 'large language models future developments']}\nusing extract_content tool with args: {'url': 'https://en.wikipedia.org/wiki/Large_language_model'}\nusing extract_content tool with args: {'url': 'https://pixelplex.io/blog/llm-applications/'}\nusing extract_content tool with args: {'url': 'https://www.ox.ac.uk/news/2023-05-05-tackling-ethical-dilemma-responsibility-large-language-models'}\nusing extract_content tool with args: {'url': 'https://research.aimultiple.com/future-of-large-language-models/'}\nLarge Language Models (LLMs) like OpenAI's GPT and Google's Gemini represent a significant advancement in artificial intelligence, particularly in natural language processing (NLP). These models excel at generating human-like text, understanding context, and performing various tasks across different domains. Below is an overview of LLMs, their applications, advancements, ethical considerations, and future developments.\n\n### Overview of Large Language Models (LLMs)\nAn LLM is a type of AI model designed to understand and generate text. They are built using deep learning techniques, particularly neural networks with a vast number of parameters. These models learn from extensive datasets, capturing complex patterns of human language. Notable examples include:\n- **GPT-4 (OpenAI)**: A multimodal model capable of processing both text and images, with up to 1.8 trillion parameters.\n- **Claude (Anthropic)**: Known for its impressive context handling, Claude 3 can process up to 100,000 tokens.\n- **BLOOM (BigScience)**: An open-access multilingual model with 176 billion parameters.\n\n### Applications\nLLMs are used in a variety of fields, including:\n1. **Content Generation**: Automating the creation of written material for blogs, marketing, and social media.\n2. **Translation and Localization**: Providing accurate translations while maintaining cultural context and nuances.\n3. **Search and Recommendation**: Enhancing user search experiences and personalizing recommendations based on individual preferences.\n4. **Virtual Assistants**: Powering functionalities in services like Google Assistant and Amazon Alexa for real-time user interactions.\n5. **Code Development**: Assisting programmers by generating and debugging code, leading to more efficient workflows.\n6. **Sentiment Analysis**: Analyzing customer feedback to gauge public sentiment towards products or services.\n7. **Question Answering**: Responding accurately to user queries across various domains.\n8. **Market Research**: Gathering insights from consumer data and trends for strategic business decisions.\n9. **Education and Tutoring**: Personalizing learning experiences and providing real-time educational support.\n10. **Classification**: Categorizing text for various applications including moderation and customer service.\n\n### Latest Advancements\nThe field is rapidly evolving, with trends including:\n- **Multimodal Learning**: Combining text with images and audio to enhance model capability (as seen in tools like GPT-4 and Gemini).\n- **Self-improving Models**: Some LLMs can generate their own training data, improving their performance without external input.\n- **Sparse Expert Models**: New models are being designed to activate only relevant parts for specific tasks, increasing efficiency.\n\n### Ethical Concerns\nDespite their efficacy, LLMs face significant ethical challenges:\n- **Bias and Fairness**: These models can reflect and amplify biases present in their training data, which raises concerns about fairness and equality in their outputs.\n- **Misinformation and Hallucination**: LLMs can generate convincing but false information, leading to a phenomenon known as \"hallucination.\"\n- **Data Privacy**: The data used can sometimes include sensitive information, making privacy a critical aspect to address.\n\n### Future Developments\nLooking forward, several promising directions include:\n- Enhanced **fact-checking** capabilities, integrating real-time data to ensure accuracy.\n- Development of **domain-specific models** tailored to particular industries like healthcare and finance for more relevant outputs.\n- Greater focus on **ethical AI**, with ongoing research into bias mitigation and the development of responsible AI frameworks.\n\n### Conclusion\nLLMs are at the forefront of AI innovation, driving significant changes across multiple sectors. Their ability to interact in a human-like manner provides vast opportunities for automation and personalization, transforming industries from education to entertainment. However, addressing ethical concerns and ensuring their responsible use will be pivotal as these technologies continue to advance.\n</pre> <p>To use the WebSearchAgent, run the code above and start interacting with it by typing your questions. The agent will use web searches and content extraction to provide answers. Type 'exit' to end the interaction.</p>"},{"location":"tutorials/getting_started/tools_and_agents/#tools-agents","title":"Tools &amp; Agents\u00b6","text":"<p>Tools and Agents are two key concepts in building advanced AI systems, particularly those involving Large Language Models (LLMs):</p> <ul> <li>Tools: Functions that extend the capabilities of LLMs, allowing them to perform specific tasks or access external data.</li> <li>Agents: Autonomous or semi-autonomous entities that use LLMs and Tools to perform complex tasks or interact with users.</li> </ul> <p>In this notebook we'll implement a <code>WebSearchAgent</code> to demonstrate how to built Agents in Mirascope using Tools.</p> <p>For more detailed information on these concepts, refer to the following Mirascope documentation:</p> <ul> <li>Tools documentation</li> <li>Agents documentation</li> </ul>"},{"location":"tutorials/getting_started/tools_and_agents/#setting-up-the-environment","title":"Setting Up the Environment\u00b6","text":"<p>First, let's set up our environment by installing Mirascope and other necessary libraries.</p>"},{"location":"tutorials/getting_started/tools_and_agents/#building-a-basic-chatbot","title":"Building a Basic Chatbot\u00b6","text":"<p>Before we dive into Tools and Agents, let's start by building a basic chatbot. This will help us understand the fundamental concepts of state management and conversation flow in Mirascope.</p>"},{"location":"tutorials/getting_started/tools_and_agents/#understanding-tools-in-mirascope","title":"Understanding Tools in Mirascope\u00b6","text":"<p>Tools in Mirascope are functions that extend the capabilities of LLMs. They allow the LLM to perform specific tasks or access external data. Tools are typically used to:</p> <ol> <li>Retrieve information from external sources</li> <li>Perform calculations or data processing</li> <li>Interact with APIs or databases</li> <li>Execute specific actions based on the LLM's decisions</li> </ol> <p>Let's start by creating a Tool that extracts content from a webpage:</p>"},{"location":"tutorials/getting_started/tools_and_agents/#tools-with-access-to-agent-state","title":"Tools With Access To Agent State\u00b6","text":"<p>Now, let's create a more complex Tool that performs web searches using the DuckDuckGo search engine. We'll implement this tool as a method within our agent class to demonstrate how tools can access the agent's state:</p>"},{"location":"tutorials/getting_started/tools_and_agents/#implementing-the-websearchagent","title":"Implementing the WebSearchAgent\u00b6","text":"<p>Now that we have our custom tools, let's implement the full WebSearchAgent by adding the LLM interaction and main execution flow. Since the prompt is now rather long, we opt to use a string template for enhanced readability:</p>"},{"location":"tutorials/getting_started/tools_and_agents/#running-and-testing-the-agent","title":"Running and Testing the Agent\u00b6","text":"<p>Now that we have implemented our WebSearchAgent, let's run and test it:</p>"},{"location":"tutorials/getting_started/tools_and_agents/#advanced-concepts-and-best-practices","title":"Advanced Concepts and Best Practices\u00b6","text":"<p>When working with Tools and Agents in Mirascope, consider the following best practices:</p> <ol> <li>Error Handling: Implement robust error handling in your Tools and Agent implementation.</li> <li>Rate Limiting: Be mindful of rate limits when using external APIs in your Tools.</li> <li>Caching: Implement caching mechanisms to improve performance and reduce API calls.</li> <li>Testing: Write unit tests for your Tools and integration tests for your Agents.</li> <li>Modularity: Design your Tools and Agents to be modular and reusable.</li> <li>Security: Be cautious about the information you expose through your Tools and Agents.</li> <li>Logging: Implement logging to track the behavior and performance of your Agents.</li> </ol> <p>For more advanced usage, you can explore concepts like:</p> <ul> <li>Multi-agent systems</li> <li>Tool chaining and composition</li> <li>Dynamic tool selection</li> <li>Memory and state management for long-running agents</li> </ul> <p>For more techniques and best practices in using Mirascope, refer to the following documentation:</p> <ul> <li>Tools</li> <li>Agents</li> <li>Streams</li> <li>Chaining</li> </ul>"},{"location":"tutorials/getting_started/tools_and_agents/#9-conclusion","title":"9. Conclusion\u00b6","text":"<p>In this notebook, we've explored the basics of creating Tools and implementing Agents in Mirascope. We've built a WebSearchAgent that can perform web searches, extract content from webpages, and use an LLM to generate responses based on the gathered information.</p> <p>This example demonstrates the power and flexibility of Mirascope in building AI applications that combine LLMs with custom tools and logic. As you continue to work with Mirascope, you'll discover more advanced features and patterns that can help you build even more sophisticated AI agents and applications.</p> <p>If you like what you've seen so far, give us a star and join our community.</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/","title":"LangGraph Quickstart using Mirascope","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[4]: Copied! <pre>from mirascope.core import BaseMessageParam, openai, prompt_template\nfrom pydantic import BaseModel\n\n\nclass Chatbot(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True)\n    @prompt_template(\n        \"\"\"\n        SYSTEM: You are a helpful assistant.\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    def _call(self, question: str): ...\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question in [\"quit\", \"exit\"]:\n                print(\"(Assistant): Have a great day!\")\n                break\n            stream = self._call(question)\n            print(f\"(User): {question}\", flush=True)\n            print(\"(Assistant): \", end=\"\", flush=True)\n            for chunk, _ in stream:\n                print(chunk.content, end=\"\", flush=True)\n            print(\"\")\n            if stream.user_message_param:\n                self.history.append(stream.user_message_param)\n            self.history.append(stream.message_param)\n\n\nChatbot().run()\n</pre> from mirascope.core import BaseMessageParam, openai, prompt_template from pydantic import BaseModel   class Chatbot(BaseModel):     history: list[BaseMessageParam | openai.OpenAIMessageParam] = []      @openai.call(model=\"gpt-4o-mini\", stream=True)     @prompt_template(         \"\"\"         SYSTEM: You are a helpful assistant.         MESSAGES: {self.history}         USER: {question}         \"\"\"     )     def _call(self, question: str): ...      def run(self):         while True:             question = input(\"(User): \")             if question in [\"quit\", \"exit\"]:                 print(\"(Assistant): Have a great day!\")                 break             stream = self._call(question)             print(f\"(User): {question}\", flush=True)             print(\"(Assistant): \", end=\"\", flush=True)             for chunk, _ in stream:                 print(chunk.content, end=\"\", flush=True)             print(\"\")             if stream.user_message_param:                 self.history.append(stream.user_message_param)             self.history.append(stream.message_param)   Chatbot().run() <pre>(User): Hi there! My name is Will\n(Assistant): Hi Will! How can I assist you today?\n(User): Remember my name?\n(Assistant): Yes, your name is Will! What would you like to talk about today?\n(Assistant): Have a great day!\n</pre> <p>The <code>run</code> method serves as the entry point for our Chatbot. It implements a continuous loop that:</p> <ol> <li>Prompts the user for input</li> <li>Processes the user's message</li> <li>Generates and streams the assistant's response for a real time feel</li> <li>Updates the conversation history</li> </ol> <p>This loop persists until the user chooses to exit. After each interaction, both the user's input and the assistant's response are appended to the history list. This accumulation of context allows the language model to maintain continuity and relevance in future conversations.</p> <p>While this basic chatbot demonstrates core functionality, we can enhance its capabilities by incorporating tools.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install duckduckgo-search beautifulsoup4 requests\n</pre> !pip install duckduckgo-search beautifulsoup4 requests <p>No API key is required for DuckDuckGo.</p> In\u00a0[7]: Copied! <pre>import requests\nfrom bs4 import BeautifulSoup\nfrom duckduckgo_search import DDGS\nfrom pydantic import BaseModel, Field\n\n\nclass WebSearch(openai.OpenAITool):\n    \"\"\"Search the web for the given text and parse the paragraphs of the results.\"\"\"\n\n    query: str = Field(..., description=\"The text to search for.\")\n\n    def call(self) -&gt; str:\n        \"\"\"Search the web for the given text and parse the paragraphs of the results.\n\n        Returns:\n            Parsed paragraphs of each of the webpages, separated by newlines.\n        \"\"\"\n        try:\n            # Search the web for the given text\n            results = DDGS(proxy=None).text(self.query, max_results=2)\n\n            # Parse the paragraphs of each resulting webpage\n            parsed_results = []\n            for result in results:\n                link = result[\"href\"]\n                try:\n                    response = requests.get(link)\n                    soup = BeautifulSoup(response.content, \"html.parser\")\n                    parsed_results.append(\n                        \"\\n\".join([p.text for p in soup.find_all(\"p\")])\n                    )\n                except Exception as e:\n                    parsed_results.append(\n                        f\"{type(e)}: Failed to parse content from URL {link}\"\n                    )\n\n            return \"\\n\\n\".join(parsed_results)\n\n        except Exception as e:\n            return f\"{type(e)}: Failed to search the web for text\"\n</pre> import requests from bs4 import BeautifulSoup from duckduckgo_search import DDGS from pydantic import BaseModel, Field   class WebSearch(openai.OpenAITool):     \"\"\"Search the web for the given text and parse the paragraphs of the results.\"\"\"      query: str = Field(..., description=\"The text to search for.\")      def call(self) -&gt; str:         \"\"\"Search the web for the given text and parse the paragraphs of the results.          Returns:             Parsed paragraphs of each of the webpages, separated by newlines.         \"\"\"         try:             # Search the web for the given text             results = DDGS(proxy=None).text(self.query, max_results=2)              # Parse the paragraphs of each resulting webpage             parsed_results = []             for result in results:                 link = result[\"href\"]                 try:                     response = requests.get(link)                     soup = BeautifulSoup(response.content, \"html.parser\")                     parsed_results.append(                         \"\\n\".join([p.text for p in soup.find_all(\"p\")])                     )                 except Exception as e:                     parsed_results.append(                         f\"{type(e)}: Failed to parse content from URL {link}\"                     )              return \"\\n\\n\".join(parsed_results)          except Exception as e:             return f\"{type(e)}: Failed to search the web for text\" In\u00a0[\u00a0]: Copied! <pre>!pip install tavily-python\n</pre> !pip install tavily-python <p>Then get a free API key to use for the <code>WebSearch</code> tool.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nfrom typing import ClassVar\n\nfrom pydantic import Field\nfrom tavily import TavilyClient\n\n\nclass WebSearch(openai.OpenAITool):  # noqa: F811\n    \"\"\"Search the web for the given text using the TavilyClient.\"\"\"\n\n    tavily_client: ClassVar[TavilyClient] = TavilyClient(\n        api_key=os.environ[\"TAVILY_API_KEY\"]\n    )\n    query: str = Field(..., description=\"The text to search for.\")\n\n    def call(self) -&gt; str:\n        \"\"\"A web search tool that takes in a query and returns the top 2 search results.\"\"\"\n        return self.tavily_client.get_search_context(query=self.query, max_results=2)\n</pre> import os from typing import ClassVar  from pydantic import Field from tavily import TavilyClient   class WebSearch(openai.OpenAITool):  # noqa: F811     \"\"\"Search the web for the given text using the TavilyClient.\"\"\"      tavily_client: ClassVar[TavilyClient] = TavilyClient(         api_key=os.environ[\"TAVILY_API_KEY\"]     )     query: str = Field(..., description=\"The text to search for.\")      def call(self) -&gt; str:         \"\"\"A web search tool that takes in a query and returns the top 2 search results.\"\"\"         return self.tavily_client.get_search_context(query=self.query, max_results=2) In\u00a0[\u00a0]: Copied! <pre>class Chatbot(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch])\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. \n        Your task is to answer the user's question using the provided tools.\n        You have access to the following tools:\n            - `WebSearch`: Search the web for information.\n            - `RequestAssistance`: Request assistance from a human expert if you do not\n                know how to answer the question.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness. The goal is to\n        provide as much information to the writer as possible without overwhelming them.\n\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    def _call(self, question: str | None = None): ...\n\n    def _step(self, question: str | None = None):\n        response = self._call(question)\n        tools_and_outputs = []\n        for chunk, tool in response:\n            if tool:\n                tools_and_outputs.append((tool, tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.history.append(response.user_message_param)\n        self.history.append(response.message_param)\n        if tools_and_outputs:\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step()\n        return response.content\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question in [\"quit\", \"exit\"]:\n                print(\"(Assistant): Have a great day!\")\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(question)\n            print(\"\")\n\n\nChatbot().run()\n# Prompt:\n\"\"\"\n(User): Can you tell me about the Mirascope python library?\n(Assistant): The **Mirascope** library is a Python toolkit designed for creating applications using language model (LLM) APIs. Developed by William Bakst and released on August 18, 2024, Mirascope emphasizes simplicity, elegance, and developer experience. Here are some key features and details about the library:\n\n### Key Features\n1. **Simplicity and Ease of Use**: Mirascope aims to provide straightforward abstractions that enhance the developer experience without overwhelming complexity. It is designed for ease of onboarding and development.\n\n2. **Type Safety**: One of its strengths is the provision of proper type hints throughout the library. It actively manages Python typings, allowing developers to write their code intuitively while still benefiting from type safety.\n\n3. **Modular Design**: Mirascope is modular and extensible, enabling developers to tailor the library to their specific needs. Most dependencies are optional and provider-specific, so you can include only the components you require.\n\n4. **Core Primitives**: The library offers two main components:\n   - **Call and BasePrompt**: These primitives facilitate interactions with LLMs. Developers can create functions that integrate seamlessly with multiple LLM providers through decorators.\n\n5. **Advanced Functionality**: Mirascope supports features like asynchronous function calls, streaming responses, structured data extraction, custom output parsers, and dynamic variable injection.\n\n6. **Integration with FastAPI**: Mirascope includes decorators for wrapping functions into FastAPI routes, making it easier to deploy applications as web services.\n\n7. **Documentation and Examples**: The project comes with extensive usage documentation and example code to help users quickly understand how to utilize its features effectively.\n\n### Installation\nTo install Mirascope, you can use the following command:\n```bash\npip install mirascope\n```\n### Compatibility\nMirascope is compatible with Python versions 3.10 to 3.11 (not supporting Python 4.0 and above) and is licensed under the MIT License.\n\n### Summary\nMirascope positions itself as a simpler, less cumbersome alternative to other LLM frameworks like LangChain. It focuses on providing essential functionalities without unnecessary complexity, making development enjoyable and productive for software engineers looking to integrate LLMs into their applications.\n\"\"\"\n</pre> class Chatbot(BaseModel):     history: list[BaseMessageParam | openai.OpenAIMessageParam] = []      @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch])     @prompt_template(         \"\"\"         SYSTEM:         You are an expert web searcher.          Your task is to answer the user's question using the provided tools.         You have access to the following tools:             - `WebSearch`: Search the web for information.             - `RequestAssistance`: Request assistance from a human expert if you do not                 know how to answer the question.          Once you have gathered all of the information you need, generate a writeup that         strikes the right balance between brevity and completeness. The goal is to         provide as much information to the writer as possible without overwhelming them.          MESSAGES: {self.history}         USER: {question}         \"\"\"     )     def _call(self, question: str | None = None): ...      def _step(self, question: str | None = None):         response = self._call(question)         tools_and_outputs = []         for chunk, tool in response:             if tool:                 tools_and_outputs.append((tool, tool.call()))             else:                 print(chunk.content, end=\"\", flush=True)         if response.user_message_param:             self.history.append(response.user_message_param)         self.history.append(response.message_param)         if tools_and_outputs:             self.history += response.tool_message_params(tools_and_outputs)             return self._step()         return response.content      def run(self):         while True:             question = input(\"(User): \")             if question in [\"quit\", \"exit\"]:                 print(\"(Assistant): Have a great day!\")                 break             print(\"(Assistant): \", end=\"\", flush=True)             self._step(question)             print(\"\")   Chatbot().run() # Prompt: \"\"\" (User): Can you tell me about the Mirascope python library? (Assistant): The **Mirascope** library is a Python toolkit designed for creating applications using language model (LLM) APIs. Developed by William Bakst and released on August 18, 2024, Mirascope emphasizes simplicity, elegance, and developer experience. Here are some key features and details about the library:  ### Key Features 1. **Simplicity and Ease of Use**: Mirascope aims to provide straightforward abstractions that enhance the developer experience without overwhelming complexity. It is designed for ease of onboarding and development.  2. **Type Safety**: One of its strengths is the provision of proper type hints throughout the library. It actively manages Python typings, allowing developers to write their code intuitively while still benefiting from type safety.  3. **Modular Design**: Mirascope is modular and extensible, enabling developers to tailor the library to their specific needs. Most dependencies are optional and provider-specific, so you can include only the components you require.  4. **Core Primitives**: The library offers two main components:    - **Call and BasePrompt**: These primitives facilitate interactions with LLMs. Developers can create functions that integrate seamlessly with multiple LLM providers through decorators.  5. **Advanced Functionality**: Mirascope supports features like asynchronous function calls, streaming responses, structured data extraction, custom output parsers, and dynamic variable injection.  6. **Integration with FastAPI**: Mirascope includes decorators for wrapping functions into FastAPI routes, making it easier to deploy applications as web services.  7. **Documentation and Examples**: The project comes with extensive usage documentation and example code to help users quickly understand how to utilize its features effectively.  ### Installation To install Mirascope, you can use the following command: ```bash pip install mirascope ``` ### Compatibility Mirascope is compatible with Python versions 3.10 to 3.11 (not supporting Python 4.0 and above) and is licensed under the MIT License.  ### Summary Mirascope positions itself as a simpler, less cumbersome alternative to other LLM frameworks like LangChain. It focuses on providing essential functionalities without unnecessary complexity, making development enjoyable and productive for software engineers looking to integrate LLMs into their applications. \"\"\" <p>We have enhanced our chatbot's functionality with several key modifications:</p> <ul> <li>Added a <code>WebSearch</code> tool to the tools parameter in the <code>@openai.call</code> decorator.</li> <li>Refactored the streaming logic and history management into a new <code>_step</code> method.<ul> <li>This enables calling <code>_step</code> iteratively until the agent is done calling tools and is ready to respond.</li> <li>For each <code>_step</code> we update the history to include tool usage and outputs.</li> </ul> </li> <li>Revised the prompt template to instruct the chatbot on how to utilize the new <code>WebSearch</code> tool.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>class Chatbot(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch])\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. \n        Your task is to answer the user's question using the provided tools.\n        You have access to the following tools:\n            - `WebSearch`: Search the web for information.\n            - `RequestAssistance`: Request assistance from a human expert if you do not\n                know how to answer the question.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness. The goal is to\n        provide as much information to the writer as possible without overwhelming them.\n\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    def _call(self, question: str | None = None): ...\n\n    def _interrupt_before(self, tool: openai.OpenAITool) -&gt; openai.OpenAITool | None:\n        \"\"\"Interrupt before the tool is called. Return the modified tool or None.\"\"\"\n        if not isinstance(tool, WebSearch):\n            return tool\n        response = input(f\"Do you want to use the {tool._name()} tool? (y/n): \")\n        if response.lower() in [\"n\", \"no\"]:\n            response = input(\n                f\"Do you want to modify the {tool._name()} tool's query? (y/n): \"\n            )\n            if response.lower() in [\"n\", \"no\"]:\n                return None\n            else:\n                tool.query = input(\"(Assistant): Enter a new query: \")\n                return tool\n        else:\n            return tool\n\n    def _step(self, question: str | None = None):\n        response = self._call(question)\n        tools_and_outputs = []\n        for chunk, tool in response:\n            if tool:\n                new_tool = self._interrupt_before(tool)\n                if new_tool:\n                    tools_and_outputs.append((new_tool, new_tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.history.append(response.user_message_param)\n        self.history.append(response.message_param)\n        if tools_and_outputs:\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step()\n        return response.content\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question in [\"quit\", \"exit\"]:\n                print(\"(Assistant): Have a great day!\")\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(question)\n            print(\"\")\n</pre> class Chatbot(BaseModel):     history: list[BaseMessageParam | openai.OpenAIMessageParam] = []      @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch])     @prompt_template(         \"\"\"         SYSTEM:         You are an expert web searcher.          Your task is to answer the user's question using the provided tools.         You have access to the following tools:             - `WebSearch`: Search the web for information.             - `RequestAssistance`: Request assistance from a human expert if you do not                 know how to answer the question.          Once you have gathered all of the information you need, generate a writeup that         strikes the right balance between brevity and completeness. The goal is to         provide as much information to the writer as possible without overwhelming them.          MESSAGES: {self.history}         USER: {question}         \"\"\"     )     def _call(self, question: str | None = None): ...      def _interrupt_before(self, tool: openai.OpenAITool) -&gt; openai.OpenAITool | None:         \"\"\"Interrupt before the tool is called. Return the modified tool or None.\"\"\"         if not isinstance(tool, WebSearch):             return tool         response = input(f\"Do you want to use the {tool._name()} tool? (y/n): \")         if response.lower() in [\"n\", \"no\"]:             response = input(                 f\"Do you want to modify the {tool._name()} tool's query? (y/n): \"             )             if response.lower() in [\"n\", \"no\"]:                 return None             else:                 tool.query = input(\"(Assistant): Enter a new query: \")                 return tool         else:             return tool      def _step(self, question: str | None = None):         response = self._call(question)         tools_and_outputs = []         for chunk, tool in response:             if tool:                 new_tool = self._interrupt_before(tool)                 if new_tool:                     tools_and_outputs.append((new_tool, new_tool.call()))             else:                 print(chunk.content, end=\"\", flush=True)         if response.user_message_param:             self.history.append(response.user_message_param)         self.history.append(response.message_param)         if tools_and_outputs:             self.history += response.tool_message_params(tools_and_outputs)             return self._step()         return response.content      def run(self):         while True:             question = input(\"(User): \")             if question in [\"quit\", \"exit\"]:                 print(\"(Assistant): Have a great day!\")                 break             print(\"(Assistant): \", end=\"\", flush=True)             self._step(question)             print(\"\") <p>Now, before the LLM calls the tool, it will prompt the user requesting permission:</p> In\u00a0[\u00a0]: Copied! <pre>Chatbot().run()\n# &gt; (User): Can you tell me about the Mirascope python library?\n# &gt; (Assistant): Do you want to use the WebSearch tool? (y/n): y\n</pre> Chatbot().run() # &gt; (User): Can you tell me about the Mirascope python library? # &gt; (Assistant): Do you want to use the WebSearch tool? (y/n): y In\u00a0[\u00a0]: Copied! <pre>class RequestAssistance(openai.OpenAITool):\n    \"\"\"A tool that requests assistance from a human expert.\"\"\"\n\n    query: str = Field(\n        ...,\n        description=\"The request for assistance needed to properly respond to the user\",\n    )\n\n    def call(self) -&gt; str:\n        \"\"\"Prompts a human to enter a response.\"\"\"\n        print(f\"I am in need of assistance. {self.query}\")\n        response = input(\"\\t(Human): \")\n        return f\"Human response: {response}\"\n\n\nclass Chatbot(BaseModel):\n    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch, RequestAssistance])\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert web searcher. \n        Your task is to answer the user's question using the provided tools.\n        You have access to the following tools:\n            - `WebSearch`: Search the web for information.\n            - `RequestAssistance`: Request assistance from a human expert if you do not\n                know how to answer the question.\n\n        Once you have gathered all of the information you need, generate a writeup that\n        strikes the right balance between brevity and completeness. The goal is to\n        provide as much information to the writer as possible without overwhelming them.\n\n        MESSAGES: {self.history}\n        USER: {question}\n        \"\"\"\n    )\n    def _call(self, question: str | None = None): ...\n\n    def _interrupt_before(self, tool: openai.OpenAITool) -&gt; openai.OpenAITool | None:\n        \"\"\"Interrupt before the tool is called. Return the modified tool or None.\"\"\"\n        if not isinstance(tool, WebSearch):\n            return tool\n        response = input(f\"Do you want to use the {tool._name()} tool? (y/n): \")\n        if response.lower() in [\"n\", \"no\"]:\n            response = input(\n                f\"Do you want to modify the {tool._name()} tool's query? (y/n): \"\n            )\n            if response.lower() in [\"n\", \"no\"]:\n                return None\n            else:\n                tool.query = input(\"(Assistant): Enter a new query: \")\n                return tool\n        else:\n            return tool\n\n    def _step(self, question: str | None = None):\n        response = self._call(question)\n        tools_and_outputs = []\n        for chunk, tool in response:\n            if tool:\n                new_tool = self._interrupt_before(tool)\n                if new_tool:\n                    tools_and_outputs.append((new_tool, new_tool.call()))\n            else:\n                print(chunk.content, end=\"\", flush=True)\n        if response.user_message_param:\n            self.history.append(response.user_message_param)\n        self.history.append(response.message_param)\n        if tools_and_outputs:\n            self.history += response.tool_message_params(tools_and_outputs)\n            return self._step()\n        return response.content\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question in [\"quit\", \"exit\"]:\n                print(\"(Assistant): Have a great day!\")\n                break\n            print(\"(Assistant): \", end=\"\", flush=True)\n            self._step(question)\n            print(\"\")\n\n\nChatbot().run()\n# &gt; (User): What is my name?\n# &gt; (Assistant): I am in need of assistance. How to identify a user's name in a conversation without them explicitly stating it?\n#       (Human): Tell them you don't know their name and would love for them to share it with you\n#   I\u2019m not sure what your name is. I would love to know, so please feel free to share it!\n# &gt; (User):\n</pre> class RequestAssistance(openai.OpenAITool):     \"\"\"A tool that requests assistance from a human expert.\"\"\"      query: str = Field(         ...,         description=\"The request for assistance needed to properly respond to the user\",     )      def call(self) -&gt; str:         \"\"\"Prompts a human to enter a response.\"\"\"         print(f\"I am in need of assistance. {self.query}\")         response = input(\"\\t(Human): \")         return f\"Human response: {response}\"   class Chatbot(BaseModel):     history: list[BaseMessageParam | openai.OpenAIMessageParam] = []      @openai.call(model=\"gpt-4o-mini\", stream=True, tools=[WebSearch, RequestAssistance])     @prompt_template(         \"\"\"         SYSTEM:         You are an expert web searcher.          Your task is to answer the user's question using the provided tools.         You have access to the following tools:             - `WebSearch`: Search the web for information.             - `RequestAssistance`: Request assistance from a human expert if you do not                 know how to answer the question.          Once you have gathered all of the information you need, generate a writeup that         strikes the right balance between brevity and completeness. The goal is to         provide as much information to the writer as possible without overwhelming them.          MESSAGES: {self.history}         USER: {question}         \"\"\"     )     def _call(self, question: str | None = None): ...      def _interrupt_before(self, tool: openai.OpenAITool) -&gt; openai.OpenAITool | None:         \"\"\"Interrupt before the tool is called. Return the modified tool or None.\"\"\"         if not isinstance(tool, WebSearch):             return tool         response = input(f\"Do you want to use the {tool._name()} tool? (y/n): \")         if response.lower() in [\"n\", \"no\"]:             response = input(                 f\"Do you want to modify the {tool._name()} tool's query? (y/n): \"             )             if response.lower() in [\"n\", \"no\"]:                 return None             else:                 tool.query = input(\"(Assistant): Enter a new query: \")                 return tool         else:             return tool      def _step(self, question: str | None = None):         response = self._call(question)         tools_and_outputs = []         for chunk, tool in response:             if tool:                 new_tool = self._interrupt_before(tool)                 if new_tool:                     tools_and_outputs.append((new_tool, new_tool.call()))             else:                 print(chunk.content, end=\"\", flush=True)         if response.user_message_param:             self.history.append(response.user_message_param)         self.history.append(response.message_param)         if tools_and_outputs:             self.history += response.tool_message_params(tools_and_outputs)             return self._step()         return response.content      def run(self):         while True:             question = input(\"(User): \")             if question in [\"quit\", \"exit\"]:                 print(\"(Assistant): Have a great day!\")                 break             print(\"(Assistant): \", end=\"\", flush=True)             self._step(question)             print(\"\")   Chatbot().run() # &gt; (User): What is my name? # &gt; (Assistant): I am in need of assistance. How to identify a user's name in a conversation without them explicitly stating it? #       (Human): Tell them you don't know their name and would love for them to share it with you #   I\u2019m not sure what your name is. I would love to know, so please feel free to share it! # &gt; (User): <p>Real-World Difference</p> <p> In the above example, we are getting the \"Human\" input in the console for demonstration purposes. In a real-world use-case, this would be hidden from the user and simply mention that the AI is requesting assistance while actually requesting assistance in the background (where the user would not see this interaction). </p> In\u00a0[\u00a0]: Copied! <pre>chatbot = Chatbot()\nchatbot.run()\n# (User): Hi there! My name is Will.\n# (Assistant): It's nice to meet you, Will! I'm an AI assistant created by Anthropic. I'm here to help you with any questions or tasks you may have. Please let me know how I can assist you today.\n\n\nchatbot.run()\n# (User): Remember my name?\n# (Assistant): Of course, your name is Will. It's nice to meet you again!\n\nchatbot.history = chatbot.history[:-4]\n# (User): Remember my name?\n# (Assistant): I'm afraid I don't actually have the capability to remember your name. As an AI assistant, I don't have a persistent memory of our previous conversations or interactions. I respond based on the current context provided to me. Could you please restate your name or provide more information so I can try to assist you?\n</pre> chatbot = Chatbot() chatbot.run() # (User): Hi there! My name is Will. # (Assistant): It's nice to meet you, Will! I'm an AI assistant created by Anthropic. I'm here to help you with any questions or tasks you may have. Please let me know how I can assist you today.   chatbot.run() # (User): Remember my name? # (Assistant): Of course, your name is Will. It's nice to meet you again!  chatbot.history = chatbot.history[:-4] # (User): Remember my name? # (Assistant): I'm afraid I don't actually have the capability to remember your name. As an AI assistant, I don't have a persistent memory of our previous conversations or interactions. I respond based on the current context provided to me. Could you please restate your name or provide more information so I can try to assist you? <p>While in this example, we are calling the run function multiple times, there is nothing stopping you from updating the history inside the Chatbot. Note that in real-world applications, conversation history is typically stored in a cache, database, or other persistent storage systems, so add <code>computed_fields</code> as necessary to retrieve from storage.</p> <p>Dynamic, Relevant History</p> <p> We have seen cases where the history is retrieved dynamically from e.g. a vector store. This enables injecting only relevant history along with each call, which can help reduce token usage and keep the assistant's responses more relevant. </p> <p>Congratulations! You've now learned how to create a sophisticated chatbot using Mirascope and simple Python code. This approach demonstrates that powerful AI applications can be built with minimal reliance on complex abstractions or pre-built tools, giving you greater flexibility and control over your project's architecture.</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#langgraph-quickstart-using-mirascope","title":"LangGraph Quickstart using Mirascope\u00b6","text":"<p>We'll implement the LangGraph Quickstart using Mirascope. We'll build a chatbot with a web search tool, conversation history, and human-in-the-loop functionality. Throughout the process, we'll apply Mirascope best practices, which align with general Python best practices. This approach will demonstrate how straightforward it is to create a sophisticated conversational AI system using Mirascope.</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#part-1-building-a-basic-chatbot","title":"Part 1: Building a Basic Chatbot\u00b6","text":"<p>A chatbot must possess at least two key capabilities to be considered as such:</p> <ul> <li>The ability to engage in conversation with a user</li> <li>The capacity to retain and reference information from the ongoing dialogue</li> </ul> <p>Let's take a look at how that looks using Mirascope:</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#part-2-enhancing-the-chatbot-with-tools","title":"Part 2: Enhancing the Chatbot with Tools\u00b6","text":"<p>Tools enable language models to extend beyond their training data and access real-time information. Let's implement a <code>WebSearch</code> tool that allows the LLM to query the internet for current and relevant data.</p> <p>Here are a few search tools you can use.</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#duckduckgo","title":"DuckDuckGo\u00b6","text":""},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#duckduckgo-setup","title":"DuckDuckGo Setup\u00b6","text":"<p>Install the DuckDuckGo python library:</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#define-the-duckduckgo-tool","title":"Define the DuckDuckGo tool\u00b6","text":""},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#tavily","title":"Tavily\u00b6","text":""},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#tavily-setup","title":"Tavily Setup\u00b6","text":"<p>Install the Tavily python library:</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#define-the-tavily-tool","title":"Define the Tavily tool\u00b6","text":""},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#update-mirascope-call","title":"Update Mirascope call\u00b6","text":"<p>Now that we have our tool defined, we can easily add the tool to our Mirascope call, like so:</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#part-3-human-in-the-loop","title":"Part 3: Human-in-the-loop\u00b6","text":"<p>Let us take a look at how we can slot in human input or approval using Mirascope.</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#permission-before-using-tool","title":"Permission before using tool\u00b6","text":"<p>Since we are just writing python code, we don't need to setup an <code>interrupt_before</code>. We can simply add a function <code>_interrupt_before</code> that we call before calling our tool, like so:</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#human-as-a-tool","title":"Human-as-a-tool\u00b6","text":"<p>We can use prompt engineering techniques to help the LLM make a decision on whether it needs human intervention or not. Let's add a <code>RequestAssistance</code> tool and update our prompt so the LLM knows how to use our new tool.</p>"},{"location":"tutorials/langgraph_vs_mirascope/quickstart/#part-4-time-travel-also-known-as-list-splicing","title":"Part 4: Time Travel also known as list splicing\u00b6","text":"<p>In order to revisit previous states, you can take the history list that stores all the messages and manipulate it however you want by basic list splicing. For example, if you want to \"rewind\" your state, you can simply remove user and assistant messages and then run your chatbot.</p>"},{"location":"tutorials/more_advanced/code_generation_and_execution/","title":"Code Generation and Execution","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[\u00a0]: Copied! <pre>from mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=bool)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are a software engineer who is an expert at reviewing whether code is safe to execute.\n    Determine if the given string is safe to execute as Python code.\n\n    USER:\n    {code}\n    \"\"\"\n)\ndef evaluate_code_safety(code: str): ...\n\n\ndef execute_code(code: str):\n    \"\"\"Execute Python code and return the output.\"\"\"\n    is_code_safe = evaluate_code_safety(code)\n    if not is_code_safe:\n        return f\"Error: The code: {code} is not safe to execute.\"\n    try:\n        local_vars = {}\n        exec(code, globals(), local_vars)\n        if \"result\" in local_vars:\n            return local_vars[\"result\"]\n    except Exception as e:\n        print(e)\n        return f\"Error: {str(e)}\"\n</pre> from mirascope.core import openai, prompt_template   @openai.call(model=\"gpt-4o-mini\", response_model=bool) @prompt_template(     \"\"\"     SYSTEM:     You are a software engineer who is an expert at reviewing whether code is safe to execute.     Determine if the given string is safe to execute as Python code.      USER:     {code}     \"\"\" ) def evaluate_code_safety(code: str): ...   def execute_code(code: str):     \"\"\"Execute Python code and return the output.\"\"\"     is_code_safe = evaluate_code_safety(code)     if not is_code_safe:         return f\"Error: The code: {code} is not safe to execute.\"     try:         local_vars = {}         exec(code, globals(), local_vars)         if \"result\" in local_vars:             return local_vars[\"result\"]     except Exception as e:         print(e)         return f\"Error: {str(e)}\" <p>This safety check uses Mirascope's response_model to return a boolean indicating whether the code is safe to execute.</p> In\u00a0[7]: Copied! <pre>from mirascope.core import BaseMessageParam\nfrom pydantic import BaseModel\n\n\nclass SoftwareEngineer(BaseModel):\n    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n\n    @openai.call(model=\"gpt-4o-mini\", tools=[execute_code])\n    @prompt_template(\n        \"\"\"\n        SYSTEM:\n        You are an expert software engineer who can write good clean code and solve\n        complex problems.\n\n        Write Python code to solve the following problem with variable 'result' as the answer.\n        If the code does not run or has an error, return the error message and try again.\n\n        Example: What is the sqrt of 2?\n        import math\n        result = None\n        try:\n            result = math.sqrt(2)\n        except Exception as e:\n            result = str(e)\n\n        MESSAGES: {self.messages}\n        USER: {text}\n        \"\"\"\n    )\n    def _step(self, text: str): ...\n\n    def _get_response(self, question: str = \"\"):\n        response = self._step(question)\n        tools_and_outputs = []\n        if tools := response.tools:\n            for tool in tools:\n                output = tool.call()\n                tools_and_outputs.append((tool, str(output)))\n        else:\n            print(\"(Assistant):\", response.content)\n            return\n        if response.user_message_param:\n            self.messages.append(response.user_message_param)\n        self.messages += [\n            response.message_param,\n            *response.tool_message_params(tools_and_outputs),\n        ]\n        return self._get_response(\"\")\n\n    def run(self):\n        while True:\n            question = input(\"(User): \")\n            if question == \"exit\":\n                break\n            print(f\"(User): {question}\")\n            self._get_response(question)\n\n\nSoftwareEngineer(messages=[]).run()\n</pre> from mirascope.core import BaseMessageParam from pydantic import BaseModel   class SoftwareEngineer(BaseModel):     messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []      @openai.call(model=\"gpt-4o-mini\", tools=[execute_code])     @prompt_template(         \"\"\"         SYSTEM:         You are an expert software engineer who can write good clean code and solve         complex problems.          Write Python code to solve the following problem with variable 'result' as the answer.         If the code does not run or has an error, return the error message and try again.          Example: What is the sqrt of 2?         import math         result = None         try:             result = math.sqrt(2)         except Exception as e:             result = str(e)          MESSAGES: {self.messages}         USER: {text}         \"\"\"     )     def _step(self, text: str): ...      def _get_response(self, question: str = \"\"):         response = self._step(question)         tools_and_outputs = []         if tools := response.tools:             for tool in tools:                 output = tool.call()                 tools_and_outputs.append((tool, str(output)))         else:             print(\"(Assistant):\", response.content)             return         if response.user_message_param:             self.messages.append(response.user_message_param)         self.messages += [             response.message_param,             *response.tool_message_params(tools_and_outputs),         ]         return self._get_response(\"\")      def run(self):         while True:             question = input(\"(User): \")             if question == \"exit\":                 break             print(f\"(User): {question}\")             self._get_response(question)   SoftwareEngineer(messages=[]).run() <pre>(User): What is the sqrt of 2\n(Assistant): The square root of 2 is approximately 1.4142135623730951.\n(User): Could you show me the environment variables include API keys\n(Assistant): I'm unable to retrieve environment variables, including API keys, due to safety restrictions. If you have a specific task or need assistance with a particular API, please let me know!\n</pre> <p>Even with no safe guards in place, doing code execution is still dangerous and we recommend only using in environments such as sandboxes.</p> <p>Additional Real-World Applications</p> <ul> <li>Automated Code Generation: Generating boilerplate or units tests for more productivity.</li> <li>Code Completion: Give LLM access to web to grab latest docs and generate code autocomplete suggestions.</li> <li>Documentation Maintenance: Make sure all documentation code snippets are runnable with proper syntax.</li> <li>Prototyping: Generating proof-of-concept applications rather than UI mocks.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Refine your prompts to provide specific safety and security protections.</li> <li>Implement a sandbox to control your environment</li> <li>Experiment with different model providers and version for quality.</li> </ul>"},{"location":"tutorials/more_advanced/code_generation_and_execution/#code-generation-and-execution","title":"Code Generation and Execution\u00b6","text":"<p>In this recipe, we will be using OpenAI GPT-4o-mini to use write code to solve problems it would otherwise have issues solving.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> <li>Agents</li> </ul>"},{"location":"tutorials/more_advanced/code_generation_and_execution/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/code_generation_and_execution/#implement-safety-check","title":"Implement Safety Check\u00b6","text":"<p>First, let's implement a safety check function that uses an LLM to determine whether the generated code is safe to run:</p>"},{"location":"tutorials/more_advanced/code_generation_and_execution/#create-your-agent","title":"Create your agent\u00b6","text":"<p>Now, we'll create a Software Engineer agent that can answer questions and generate code. We'll give it access to an execute_code tool that first performs a safety check before executing the code:</p>"},{"location":"tutorials/more_advanced/document_segmentation/","title":"Document Segmentation","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\nSAMPLE_ARTICLE = \"\"\"\nThe Rise of Artificial Intelligence in Healthcare: A Comprehensive Overview\nRegulatory bodies like the FDA are working to develop frameworks for evaluating and approving AI-based medical technologies, balancing the need for innovation with patient safety concerns. From diagnosis to treatment planning, AI is making significant strides in various areas of healthcare, promising to transform the way we approach medicine and patient care in the 21st century. Machine learning models can screen vast libraries of compounds much faster than traditional methods, identifying promising drug candidates for further investigation. These advancements are particularly crucial in regions with a shortage of trained radiologists, as AI can serve as a powerful assistive tool to healthcare providers. As AI continues to evolve, it promises to augment human capabilities in healthcare, allowing for more precise, efficient, and personalized medical care. AI algorithms can identify patterns in patient data that may not be apparent to human clinicians, leading to more precise treatment recommendations. Beyond diagnosis and treatment planning, AI is proving valuable in providing clinical decision support to healthcare providers. Artificial Intelligence (AI) is revolutionizing the healthcare industry, offering unprecedented opportunities to improve patient care, streamline operations, and advance medical research. In patient monitoring, AI algorithms can continuously analyze data from ICU equipment or wearable devices, alerting healthcare providers to subtle changes in a patient's condition before they become critical. For instance, AI-powered systems have shown impressive results in detecting early signs of breast cancer in mammograms, identifying lung nodules in chest X-rays, and spotting signs of diabetic retinopathy in eye scans. At its core, AI in healthcare relies on machine learning algorithms and neural networks that can process vast amounts of medical data. Issues such as data privacy, algorithmic bias, and the need for regulatory frameworks are ongoing concerns that need to be addressed. Companies like Atomwise and Exscientia are already using AI to discover novel drug candidates for various diseases, including COVID-19. This tailored approach has the potential to significantly improve treatment efficacy and reduce adverse effects. One of the most promising applications of AI in healthcare is in medical imaging. Ensuring that AI systems are trained on diverse, representative data and regularly audited for bias is crucial for their equitable implementation. Traditional drug development is a time-consuming and expensive process, often taking over a decade and costing billions of dollars to bring a new drug to market. By analyzing vast amounts of patient data, including genetic information, lifestyle factors, and treatment outcomes, AI systems can help predict which treatments are likely to be most effective for individual patients. These systems are trained on diverse datasets, including electronic health records, medical imaging, genetic information, and even data from wearable devices. AI can also predict potential side effects and drug interactions, helping to prioritize safer compounds earlier in the development process. Additionally, there's a need for healthcare professionals to adapt and acquire new skills to work effectively alongside AI systems. Machine learning algorithms can now analyze X-rays, MRIs, and CT scans with remarkable accuracy, often outperforming human radiologists in detecting certain conditions. While AI will not replace human healthcare providers, it will undoubtedly become an indispensable tool in the medical toolkit, helping to address global healthcare challenges and improve patient outcomes on a massive scale. The sensitive nature of health data requires robust security measures and clear guidelines on data usage and sharing. Emerging areas of research include Natural Language Processing (NLP) for analyzing clinical notes and medical literature, AI-powered robotic surgery assistants for enhanced precision in complex procedures, predictive analytics for population health management and disease prevention, and virtual nursing assistants to provide basic patient care and monitoring. This proactive approach to patient care has the potential to prevent complications and improve outcomes, particularly for chronic disease management. However, the integration of AI in healthcare is not without challenges. As these AI systems learn from more data, they become increasingly accurate and capable of handling complex medical tasks. Algorithmic bias is a particularly pressing issue, as AI systems trained on non-diverse datasets may perform poorly for underrepresented populations. Despite these challenges, the potential benefits of AI in healthcare are immense. For example, in oncology, AI systems are being used to analyze tumor genetics and patient characteristics to recommend personalized cancer treatments. AI-powered systems can analyze molecular structures, predict drug-target interactions, and simulate clinical trials, potentially reducing the time and cost of bringing new drugs to market. This includes understanding the capabilities and limitations of AI tools and interpreting their outputs in the context of patient care. Similarly, in psychiatry, AI is helping to predict patient responses to different antidepressants, potentially reducing the trial-and-error approach often used in mental health treatment. As technology continues to advance, we can expect to see even more innovative applications of AI that will shape the future of medicine and improve patient outcomes worldwide.\n\"\"\"\n\n\nclass Segment(BaseModel):\n    topic: str = Field(..., description=\"The topic of the section.\")\n    content: str = Field(..., description=\"The content that relates to the topic.\")\n\n\n@openai.call(\"gpt-4o-mini\", response_model=list[Segment])\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are an expert in document semantic segmentation.\n    Can you segment the following article into coherent secttions based on topic?\n\n    USER:\n    {article}\n    \"\"\"\n)\ndef semantic_segmentation(article: str): ...\n</pre> from mirascope.core import openai, prompt_template from pydantic import BaseModel, Field  SAMPLE_ARTICLE = \"\"\" The Rise of Artificial Intelligence in Healthcare: A Comprehensive Overview Regulatory bodies like the FDA are working to develop frameworks for evaluating and approving AI-based medical technologies, balancing the need for innovation with patient safety concerns. From diagnosis to treatment planning, AI is making significant strides in various areas of healthcare, promising to transform the way we approach medicine and patient care in the 21st century. Machine learning models can screen vast libraries of compounds much faster than traditional methods, identifying promising drug candidates for further investigation. These advancements are particularly crucial in regions with a shortage of trained radiologists, as AI can serve as a powerful assistive tool to healthcare providers. As AI continues to evolve, it promises to augment human capabilities in healthcare, allowing for more precise, efficient, and personalized medical care. AI algorithms can identify patterns in patient data that may not be apparent to human clinicians, leading to more precise treatment recommendations. Beyond diagnosis and treatment planning, AI is proving valuable in providing clinical decision support to healthcare providers. Artificial Intelligence (AI) is revolutionizing the healthcare industry, offering unprecedented opportunities to improve patient care, streamline operations, and advance medical research. In patient monitoring, AI algorithms can continuously analyze data from ICU equipment or wearable devices, alerting healthcare providers to subtle changes in a patient's condition before they become critical. For instance, AI-powered systems have shown impressive results in detecting early signs of breast cancer in mammograms, identifying lung nodules in chest X-rays, and spotting signs of diabetic retinopathy in eye scans. At its core, AI in healthcare relies on machine learning algorithms and neural networks that can process vast amounts of medical data. Issues such as data privacy, algorithmic bias, and the need for regulatory frameworks are ongoing concerns that need to be addressed. Companies like Atomwise and Exscientia are already using AI to discover novel drug candidates for various diseases, including COVID-19. This tailored approach has the potential to significantly improve treatment efficacy and reduce adverse effects. One of the most promising applications of AI in healthcare is in medical imaging. Ensuring that AI systems are trained on diverse, representative data and regularly audited for bias is crucial for their equitable implementation. Traditional drug development is a time-consuming and expensive process, often taking over a decade and costing billions of dollars to bring a new drug to market. By analyzing vast amounts of patient data, including genetic information, lifestyle factors, and treatment outcomes, AI systems can help predict which treatments are likely to be most effective for individual patients. These systems are trained on diverse datasets, including electronic health records, medical imaging, genetic information, and even data from wearable devices. AI can also predict potential side effects and drug interactions, helping to prioritize safer compounds earlier in the development process. Additionally, there's a need for healthcare professionals to adapt and acquire new skills to work effectively alongside AI systems. Machine learning algorithms can now analyze X-rays, MRIs, and CT scans with remarkable accuracy, often outperforming human radiologists in detecting certain conditions. While AI will not replace human healthcare providers, it will undoubtedly become an indispensable tool in the medical toolkit, helping to address global healthcare challenges and improve patient outcomes on a massive scale. The sensitive nature of health data requires robust security measures and clear guidelines on data usage and sharing. Emerging areas of research include Natural Language Processing (NLP) for analyzing clinical notes and medical literature, AI-powered robotic surgery assistants for enhanced precision in complex procedures, predictive analytics for population health management and disease prevention, and virtual nursing assistants to provide basic patient care and monitoring. This proactive approach to patient care has the potential to prevent complications and improve outcomes, particularly for chronic disease management. However, the integration of AI in healthcare is not without challenges. As these AI systems learn from more data, they become increasingly accurate and capable of handling complex medical tasks. Algorithmic bias is a particularly pressing issue, as AI systems trained on non-diverse datasets may perform poorly for underrepresented populations. Despite these challenges, the potential benefits of AI in healthcare are immense. For example, in oncology, AI systems are being used to analyze tumor genetics and patient characteristics to recommend personalized cancer treatments. AI-powered systems can analyze molecular structures, predict drug-target interactions, and simulate clinical trials, potentially reducing the time and cost of bringing new drugs to market. This includes understanding the capabilities and limitations of AI tools and interpreting their outputs in the context of patient care. Similarly, in psychiatry, AI is helping to predict patient responses to different antidepressants, potentially reducing the trial-and-error approach often used in mental health treatment. As technology continues to advance, we can expect to see even more innovative applications of AI that will shape the future of medicine and improve patient outcomes worldwide. \"\"\"   class Segment(BaseModel):     topic: str = Field(..., description=\"The topic of the section.\")     content: str = Field(..., description=\"The content that relates to the topic.\")   @openai.call(\"gpt-4o-mini\", response_model=list[Segment]) @prompt_template(     \"\"\"     SYSTEM:     You are an expert in document semantic segmentation.     Can you segment the following article into coherent secttions based on topic?      USER:     {article}     \"\"\" ) def semantic_segmentation(article: str): ... <p>We use Mirascope <code>response_model</code> to tell the LLM to output a list of <code>Segment</code> that will break the article into multiple different topics.</p> In\u00a0[2]: Copied! <pre>segments = semantic_segmentation(SAMPLE_ARTICLE)\nprint(segments)\n</pre> segments = semantic_segmentation(SAMPLE_ARTICLE) print(segments) <pre>[Segment(topic='Introduction to AI in Healthcare', content='Artificial Intelligence (AI) is revolutionizing the healthcare industry, offering unprecedented opportunities to improve patient care, streamline operations, and advance medical research.'), Segment(topic='Role of Regulatory Bodies', content='Regulatory bodies like the FDA are working to develop frameworks for evaluating and approving AI-based medical technologies, balancing the need for innovation with patient safety concerns.'), Segment(topic='AI in Diagnosis and Treatment Planning', content='From diagnosis to treatment planning, AI is making significant strides in various areas of healthcare, promising to transform the way we approach medicine and patient care in the 21st century.'), Segment(topic='AI in Drug Discovery', content='Machine learning models can screen vast libraries of compounds much faster than traditional methods, identifying promising drug candidates for further investigation. Companies like Atomwise and Exscientia are already using AI to discover novel drug candidates for various diseases, including COVID-19.'), Segment(topic='AI in Medical Imaging', content='One of the most promising applications of AI in healthcare is in medical imaging. AI algorithms can analyze X-rays, MRIs, and CT scans with remarkable accuracy, often outperforming human radiologists in detecting certain conditions.'), Segment(topic='Clinical Decision Support', content='Beyond diagnosis and treatment planning, AI is proving valuable in providing clinical decision support to healthcare providers. AI algorithms can identify patterns in patient data that may not be apparent to human clinicians, leading to more precise treatment recommendations.'), Segment(topic='Patient Monitoring and Risk Alerts', content=\"In patient monitoring, AI algorithms can continuously analyze data from ICU equipment or wearable devices, alerting healthcare providers to subtle changes in a patient's condition before they become critical.\"), Segment(topic='Data Privacy and Bias Concerns', content='Issues such as data privacy, algorithmic bias, and the need for regulatory frameworks are ongoing concerns that need to be addressed. Ensuring that AI systems are trained on diverse, representative data and regularly audited for bias is crucial for their equitable implementation.'), Segment(topic='The Need for Healthcare Professional Training', content=\"Additionally, there's a need for healthcare professionals to adapt and acquire new skills to work effectively alongside AI systems.\"), Segment(topic='Innovative AI Applications', content='Emerging areas of research include Natural Language Processing (NLP) for analyzing clinical notes and medical literature, AI-powered robotic surgery assistants for enhanced precision in complex procedures, predictive analytics for population health management and disease prevention, and virtual nursing assistants to provide basic patient care and monitoring.'), Segment(topic='Challenges and Future Directions', content='However, the integration of AI in healthcare is not without challenges. Algorithmic bias is a particularly pressing issue, as AI systems trained on non-diverse datasets may perform poorly for underrepresented populations. Despite these challenges, the potential benefits of AI in healthcare are immense.')]\n</pre> <p>The impact of segmenting a single article is not as obvious compared to segmenting multiple articles or pages. However, by compiling multiple articles, you can gain a deeper understanding of a particular topic or theme without reading each article in full.</p> <p>Additional Real-World Applications</p> <ul> <li>General Information Retrieval: Segmentation helps in organizing large documents into smaller, searchable units, improving the efficiency of information retrieval systems.</li> <li>Education: Create study guides or slides from textbook material using summaries.</li> <li>Productivity: Summarize email chains, slack threads, word documents for your day-to-day.</li> <li>Summarization: Segmentation is a crucial step in creating summaries of long documents or articles.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following: - Refine your prompts to provide clear instructions and relevant context for text summarization. - Experiment with different model providers and version to balance quality and speed. - Provide a feedback loop, use an LLM to evaluate the quality of the summary based on a criteria and feed that back into the prompt for refinement.</p>"},{"location":"tutorials/more_advanced/document_segmentation/#document-segmentation","title":"Document Segmentation\u00b6","text":"<p>In this recipe, we go over how to do semantic document segmentation. Topics and themes of articles can frequently be dispersed across multiple sections or even separate files. We will be using OpenAI GPT-4o-mini to break down an article into topics and themes.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p> Traditional machine learning techniques often relied on handcrafted features, such as detecting paragraph breaks, identifying section headers, or using statistical measures of text coherence. While effective for well-structured documents, these approaches often struggled with more complex or inconsistently formatted texts. LLMs have revolutionized document segmentation by enabling more flexible and context-aware parsing of text, regardless of formatting or structure. </p>"},{"location":"tutorials/more_advanced/document_segmentation/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/document_segmentation/#create-your-prompt","title":"Create your prompt\u00b6","text":"<p>We will create a simple prompt that instructs the LLM to semantically segment an article:</p>"},{"location":"tutorials/more_advanced/document_segmentation/#make-a-call","title":"Make a call\u00b6","text":"<p>We can see that there are sub-sections that the LLM created for us and also the content related to those sub-sections:</p>"},{"location":"tutorials/more_advanced/extract_from_pdf/","title":"Extracting from PDF","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\" PyMuPDF\n</pre> !pip install \"mirascope[openai]\" PyMuPDF In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[\u00a0]: Copied! <pre>import asyncio\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass ResumeInfo(BaseModel):\n    name: str = Field(..., description=\"The name of the person\")\n    email: str = Field(..., description=\"The email of the person\")\n    phone: str = Field(..., description=\"The phone number of the person\")\n    skills: list[str] = Field(..., description=\"The skills of the person\")\n    experience: list[str] = Field(..., description=\"The experience of the person\")\n    education: list[str] = Field(..., description=\"The education of the person\")\n\n\n@openai.call(model=\"gpt-4o\", response_model=ResumeInfo)\n@prompt_template(\n    \"\"\"\n    Extract the resume information from the pdf file.\n    {pdf_text}\n    \"\"\"\n)\nasync def extract_resume_info(pdf_text: str): ...\n\n\nasync def convert_pdf_to_text(pdf_file_path: str) -&gt; str:\n    doc = fitz.open(pdf_file_path, filetype=\"pdf\")\n    text = []\n    for page in doc:\n        text.append(page.get_text())  # type: ignore\n    return \"\\n\".join(text)\n\n\nasync def process_pdf(pdf_file_path: str) -&gt; ResumeInfo:\n    pdf_text = await convert_pdf_to_text(pdf_file_path)\n    return await extract_resume_info(pdf_text)\n\n\nasync def run(pdf_file_paths: list[str]):\n    tasks = [process_pdf(pdf_file_path) for pdf_file_path in pdf_file_paths]\n    await asyncio.gather(*tasks)\n\n\n# In jupyter notebook to run an async function\nawait run([\"resume.pdf\"])\n\n# In Python scripts to run an async function\n# asyncio.run(run([\"resume.pdf\"]))\n</pre> import asyncio  from mirascope.core import openai, prompt_template from pydantic import BaseModel, Field   class ResumeInfo(BaseModel):     name: str = Field(..., description=\"The name of the person\")     email: str = Field(..., description=\"The email of the person\")     phone: str = Field(..., description=\"The phone number of the person\")     skills: list[str] = Field(..., description=\"The skills of the person\")     experience: list[str] = Field(..., description=\"The experience of the person\")     education: list[str] = Field(..., description=\"The education of the person\")   @openai.call(model=\"gpt-4o\", response_model=ResumeInfo) @prompt_template(     \"\"\"     Extract the resume information from the pdf file.     {pdf_text}     \"\"\" ) async def extract_resume_info(pdf_text: str): ...   async def convert_pdf_to_text(pdf_file_path: str) -&gt; str:     doc = fitz.open(pdf_file_path, filetype=\"pdf\")     text = []     for page in doc:         text.append(page.get_text())  # type: ignore     return \"\\n\".join(text)   async def process_pdf(pdf_file_path: str) -&gt; ResumeInfo:     pdf_text = await convert_pdf_to_text(pdf_file_path)     return await extract_resume_info(pdf_text)   async def run(pdf_file_paths: list[str]):     tasks = [process_pdf(pdf_file_path) for pdf_file_path in pdf_file_paths]     await asyncio.gather(*tasks)   # In jupyter notebook to run an async function await run([\"resume.pdf\"])  # In Python scripts to run an async function # asyncio.run(run([\"resume.pdf\"])) <p>This script takes in multiple text PDF documents and extracts them into a <code>ResumeInfo</code> Pydantic model. However, PDF documents contain not only text but images, tables, and more. In order to handle these more complex documents, we use Optical Character Recognition (OCR).</p> In\u00a0[\u00a0]: Copied! <pre>import fitz\n\n\nasync def convert_pdf_to_png(pdf_file_path: str) -&gt; list[bytes]:\n    doc = fitz.open(pdf_file_path, filetype=\"pdf\")\n    images = []\n    for page_num in range(len(doc)):\n        page = doc.load_page(page_num)\n        pix = page.get_pixmap()\n        images.append(pix.tobytes(output=\"png\"))\n    return images\n</pre> import fitz   async def convert_pdf_to_png(pdf_file_path: str) -&gt; list[bytes]:     doc = fitz.open(pdf_file_path, filetype=\"pdf\")     images = []     for page_num in range(len(doc)):         page = doc.load_page(page_num)         pix = page.get_pixmap()         images.append(pix.tobytes(output=\"png\"))     return images <p>Afterwards, we update our previously defined call function to take in image bytes and run:</p> In\u00a0[\u00a0]: Copied! <pre>from mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=ResumeInfo)\n@prompt_template(\n    \"\"\"\n    Extract the resume information from the pdf file.\n    {pdf_imgs:images}\n    \"\"\"\n)\nasync def extract_resume_info_as_image(pdf_imgs: list[bytes]): ...\n\n\nasync def process_pdf_as_image(pdf_file_path: str) -&gt; ResumeInfo:\n    pdf_pngs = await convert_pdf_to_png(pdf_file_path)\n    return await extract_resume_info_as_image(pdf_pngs)\n\n\n# In jupyter notebook to run an async function\nawait process_pdf_as_image(\"resume.pdf\")\n\n# In Python scripts to run an async function\n# asyncio.run(process_pdf_as_image(\"resume.pdf\"))\n</pre> from mirascope.core import openai, prompt_template   @openai.call(model=\"gpt-4o-mini\", response_model=ResumeInfo) @prompt_template(     \"\"\"     Extract the resume information from the pdf file.     {pdf_imgs:images}     \"\"\" ) async def extract_resume_info_as_image(pdf_imgs: list[bytes]): ...   async def process_pdf_as_image(pdf_file_path: str) -&gt; ResumeInfo:     pdf_pngs = await convert_pdf_to_png(pdf_file_path)     return await extract_resume_info_as_image(pdf_pngs)   # In jupyter notebook to run an async function await process_pdf_as_image(\"resume.pdf\")  # In Python scripts to run an async function # asyncio.run(process_pdf_as_image(\"resume.pdf\")) <p>This approach converts the PDF to images, allowing the AI model to analyze the visual layout and content. It's particularly useful for PDFs with non-standard layouts or those containing charts and diagrams.</p> <p>Not all providers support vision, so be sure to check provider documentation.</p> <p>Additional Real-World Examples</p> <ul> <li>Automating document processing: Take invoices, statements, tax-forms, etc and other manual data entry tasks and automate them.</li> <li>Medical Document Integrations: Extract patient records, often a PDF document into Electronic Health Record (EHR) systems.</li> <li>Resume Parsing Advanced: Update the <code>ResumeInfo</code> schema to a format for a Customer Relationship Management (CRM) tool.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <pre><code>- OCR accuracy: For better OCR results when using Vision-based extraction, use high-quality scans.\n- Multiple providers: Use multiple LLM providers to verify whether the extracted information is accurate and not hallucination.\n- Implement Pydantic `ValidationError` and Tenacity `retry` to improve reliability and accuracy.</code></pre>"},{"location":"tutorials/more_advanced/extract_from_pdf/#extracting-from-pdf","title":"Extracting from PDF\u00b6","text":"<p>This recipe demonstrates how to leverage Large Language Models (LLMs) -- specifically the OpenAI API -- to extract pages and content from PDF files. We'll cover single PDF document as well as multiple PDF files and also use OCR to extract text from scanned documents.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p> Prior to LLMs, extracting pages from pdf files has been a time-consuming and expensive task. Natural Language Processing (NLP) would be used to identify and categorize information in text specific to the PDF document, or worse yet manually by humans. This would need to happen on every new PDF with new categories, which is not scalable. LLMs possess the ability to understand context, and the versatility to handle diverse data beyond PDFs such as word documents, powerpoints, email, and more. </p>"},{"location":"tutorials/more_advanced/extract_from_pdf/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/more_advanced/extract_from_pdf/#text-based-extraction","title":"Text-Based Extraction\u00b6","text":""},{"location":"tutorials/more_advanced/extract_from_pdf/#vision-based-extraction","title":"Vision-Based Extraction\u00b6","text":"<p>For PDFs with complex layouts or embedded images, vision-based extraction can be more effective. We first write a helper function that uses <code>PyMuPDF</code> to convert the PDF document into a PNG image:</p>"},{"location":"tutorials/more_advanced/extraction_using_vision/","title":"Extraction using Vision","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass Item(BaseModel):\n    name: str = Field(..., description=\"The name of the item\")\n    quantity: int = Field(..., description=\"The quantity of the item\")\n    price: float = Field(..., description=\"The price of the item\")\n</pre> from pydantic import BaseModel, Field   class Item(BaseModel):     name: str = Field(..., description=\"The name of the item\")     quantity: int = Field(..., description=\"The quantity of the item\")     price: float = Field(..., description=\"The price of the item\") In\u00a0[2]: Copied! <pre>from mirascope.core import anthropic, openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o\", response_model=list[Item])\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Extract the receipt information from the image.\n    \n    USER:\n    {url:image}\n    \"\"\"\n)\ndef extract_receipt_info_openai(url: str): ...\n\n\n@anthropic.call(\n    model=\"claude-3-5-sonnet-20240620\", response_model=list[Item], json_mode=True\n)\n@prompt_template(\n    \"\"\"\n    Extract the receipt information from the image.\n    \n    {url:image}\n    \"\"\"\n)\ndef extract_receipt_info_anthropic(url: str): ...\n</pre> from mirascope.core import anthropic, openai, prompt_template   @openai.call(model=\"gpt-4o\", response_model=list[Item]) @prompt_template(     \"\"\"     SYSTEM:     Extract the receipt information from the image.          USER:     {url:image}     \"\"\" ) def extract_receipt_info_openai(url: str): ...   @anthropic.call(     model=\"claude-3-5-sonnet-20240620\", response_model=list[Item], json_mode=True ) @prompt_template(     \"\"\"     Extract the receipt information from the image.          {url:image}     \"\"\" ) def extract_receipt_info_anthropic(url: str): ... <p>let's get the results:</p> In\u00a0[3]: Copied! <pre>import base64\n\nimport httpx\n\nimage_url = \"https://www.receiptfont.com/wp-content/uploads/template-mcdonalds-1-screenshot-fit.png\"\n\nimage_media_type = \"image/png\"\nimage_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n\nprint(extract_receipt_info_openai(image_url))\n\nprint(extract_receipt_info_anthropic(image_url))\n</pre> import base64  import httpx  image_url = \"https://www.receiptfont.com/wp-content/uploads/template-mcdonalds-1-screenshot-fit.png\"  image_media_type = \"image/png\" image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")  print(extract_receipt_info_openai(image_url))  print(extract_receipt_info_anthropic(image_url)) <pre>[Item(name='Happy Meal 6 Pc', quantity=1, price=4.89), Item(name='Snack Oreo McFlurry', quantity=1, price=2.69)]\n[Item(name='Happy Meal 6 Pc', quantity=1, price=4.89), Item(name='Snack Oreo McFlurry', quantity=1, price=2.69)]\n</pre> <p>We see that both LLMs return the same response which gives us more confidence that the image was extracted accurately, but it is not guaranteed.</p> <p>Additional Real-World Examples</p> <ul> <li>Split your Bill: Building off our example, we can upload our receipt along with a query stating who ordered what dish and have the LLM split the bill for you.</li> <li>Content Moderation: Classify user-generated images as appropriate, inappropriate, or requiring manual review.</li> <li>Ecommerce Product Classification: Create descriptions and features from product images.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Refine your prompts to provide clear instructions and relevant context for your image extraction. In our example, there were sub-items that were not extracted, which depending on your situation may need to be extracted as well.</li> <li>Experiment with different model providers and version to balance accuracy and speed.</li> <li>Use multiple model providers to verify if results are correct.</li> <li>Use <code>async</code> for multiple images and run the calls in parallel.</li> </ul>"},{"location":"tutorials/more_advanced/extraction_using_vision/#extraction-using-vision","title":"Extraction using Vision\u00b6","text":"<p>This recipe shows how to use LLMs \u2014 in this case, OpenAI GPT-4o and Anthropic Claude 3.5 Sonnet \u2014 to extract an image into a structured output using Mirascope\u2019s <code>response_model</code>.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p> Traditionally, extracting text from images was done using Optical Character Recognition (OCR). LLMs have greatly enhanced the ability to automatically extract and analyze complex information from documents, significantly improving efficiency and accuracy in data processing tasks, since they have the ability to comprehend context, semantics, and implications within that text. </p>"},{"location":"tutorials/more_advanced/extraction_using_vision/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/more_advanced/extraction_using_vision/#extracting-receipt-items","title":"Extracting Receipt Items\u00b6","text":"<p>We define an <code>Item</code> model that has some information we care to extract.</p>"},{"location":"tutorials/more_advanced/extraction_using_vision/#creating-the-prompt","title":"Creating the prompt\u00b6","text":"<p>There are differences between how providers handle their multimodal inputs. For example, OpenAI supports passing in images directly, whereas Anthropic requires the image to be base64 encoded. Mirascope eliminates the need to handle these providers differently and unifies the interface for you. For all providers that support multimodal, we can take advantage of Mirascope parsing and pass in the image directly via <code>{&lt;variable_name&gt;:image}</code>. Also, We set the <code>response_model</code> to <code>list[Item]</code> so that our LLM knows to extract each item.</p>"},{"location":"tutorials/more_advanced/generating_captions/","title":"Generate Captions for an Image","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\n\nurl = \"https://c02.purpledshub.com/uploads/sites/41/2023/01/How-to-see-the-Wolf-Moon-in-2023--4bb6bb7.jpg?w=940&amp;webp=1\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"Generate a short, descriptive caption for this image: {url:image}\")\ndef generate_caption(url: str): ...\n\n\nresponse = generate_caption(url)\nprint(response)\n</pre> from mirascope.core import openai, prompt_template  url = \"https://c02.purpledshub.com/uploads/sites/41/2023/01/How-to-see-the-Wolf-Moon-in-2023--4bb6bb7.jpg?w=940&amp;webp=1\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(\"Generate a short, descriptive caption for this image: {url:image}\") def generate_caption(url: str): ...   response = generate_caption(url) print(response) <pre>A lone wolf howls into the night, silhouetted against a glowing full moon, creating a hauntingly beautiful scene that captures the wild spirit of nature.\n</pre> <p>Additional Real-World Examples</p> <ul> <li>Content Moderation: Classify user-generated images as appropriate, inappropriate, or requiring manual review.</li> <li>Ecommerce Product Classification: Create descriptions and features from product images.</li> <li>AI Assistant for People with Vision Impairments: Convert images to text, then text-to-speech so people with vision impairments can be more independent.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Refine your prompts to provide clear instructions and relevant context for your caption generation task.</li> <li>Experiment with different model providers and version to balance accuracy and speed.</li> <li>Use multiple model providers to evaluate results for accuracy.</li> <li>Use <code>async</code> for multiple images and run the calls in parallel.</li> </ul>"},{"location":"tutorials/more_advanced/generating_captions/#generate-captions-for-an-image","title":"Generate Captions for an Image\u00b6","text":"<p>In this recipe, we go over how to use LLMs to generate a descriptive caption set of tags for an image with OpenAI\u2019s <code>gpt-4o-mini</code>.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p> Caption generation evolved from manual human effort to machine learning techniques like Conditional Random Fields (CRFs) and Support Vector Machines (SVMs), which were time-consuming and resource-intensive. Large Language Models (LLMs) have revolutionized this field, enabling efficient multi-modal tasks through API calls and prompt engineering, dramatically improving caption generation speed and accuracy. </p>"},{"location":"tutorials/more_advanced/generating_captions/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/generating_captions/#generate-captions","title":"Generate Captions\u00b6","text":"<p>Warning</p> <p> This recipe will only work for those which support images (OpenAI, Gemini, Anthropic) as of 8/13/2024. Be sure to check if your provider has multimodal support. </p> <p>With OpenAI\u2019s multimodal capabilities, image inputs are treated just like text inputs, which means we can use it as context to ask questions or make requests. For the sake of reproducibility, we will get our image from a URL to save you the hassle of having to find and download an image. The image is a public image from BBC Science of a wolf in front of the moon.</p> <p>Since we can treat the image like any other text context, we can simply ask the model to caption the image:</p>"},{"location":"tutorials/more_advanced/generating_synthetic_data/","title":"Generate Synthetic Data","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\" pandas\n</pre> !pip install \"mirascope[openai]\" pandas In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[9]: Copied! <pre>from mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Generate {num_datapoints} random but realistic datapoints of items which could\n    be in a home appliances warehouse. Output the datapoints as a csv, and your\n    response should only be the CSV.\n\n    Format:\n    Name, Price, Inventory\n\n    Name - the name of the home appliance product\n    Price - the price of an individual product, in dollars (include cents)\n    Inventory - how many are left in stock\n    \"\"\"\n)\ndef generate_csv_data(num_datapoints: int): ...\n\n\nprint(generate_csv_data(5))\n</pre> from mirascope.core import openai, prompt_template   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Generate {num_datapoints} random but realistic datapoints of items which could     be in a home appliances warehouse. Output the datapoints as a csv, and your     response should only be the CSV.      Format:     Name, Price, Inventory      Name - the name of the home appliance product     Price - the price of an individual product, in dollars (include cents)     Inventory - how many are left in stock     \"\"\" ) def generate_csv_data(num_datapoints: int): ...   print(generate_csv_data(5)) <pre>Name, Price, Inventory  \n\"4-Slice Toaster\", 29.99, 150  \n\"Stainless Steel Blender\", 49.99, 75  \n\"Robot Vacuum Cleaner\", 199.99, 30  \n\"Microwave Oven 1000W\", 89.99, 50  \n\"Electric Kettle\", 24.99, 200\n</pre> <p>Note that the prices and inventory of each item are somewhat realistic for their corresponding item, something which would be otherwise difficult to accomplish.</p> In\u00a0[10]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass HomeAppliance(BaseModel):\n    name: str = Field(description=\"The name of the home appliance product\")\n    price: float = Field(\n        description=\"The price of an individual product, in dollars (include cents)\"\n    )\n    inventory: int = Field(description=\"How many of the items are left in stock\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=list[HomeAppliance])\n@prompt_template(\n    \"\"\"\n    Generate {num_datapoints} random but realistic datapoints of items which could\n    be in a home appliances warehouse. Output the datapoints as a list of instances\n    of HomeAppliance.\n    \"\"\"\n)\ndef generate_home_appliance_data(num_datapoints: int): ...\n\n\nprint(generate_home_appliance_data(5))\n</pre> from pydantic import BaseModel, Field   class HomeAppliance(BaseModel):     name: str = Field(description=\"The name of the home appliance product\")     price: float = Field(         description=\"The price of an individual product, in dollars (include cents)\"     )     inventory: int = Field(description=\"How many of the items are left in stock\")   @openai.call(model=\"gpt-4o-mini\", response_model=list[HomeAppliance]) @prompt_template(     \"\"\"     Generate {num_datapoints} random but realistic datapoints of items which could     be in a home appliances warehouse. Output the datapoints as a list of instances     of HomeAppliance.     \"\"\" ) def generate_home_appliance_data(num_datapoints: int): ...   print(generate_home_appliance_data(5)) <pre>[HomeAppliance(name='Refrigerator', price=899.99, inventory=25), HomeAppliance(name='Microwave', price=129.99, inventory=50), HomeAppliance(name='Washing Machine', price=499.99, inventory=15), HomeAppliance(name='Dishwasher', price=749.99, inventory=10), HomeAppliance(name='Air Conditioner', price=349.99, inventory=30)]\n</pre> In\u00a0[11]: Copied! <pre>from typing import Any, Literal\n\nimport pandas as pd\n\n\nclass DataFrameGenerator(BaseModel):\n    data: list[list[Any]] = Field(\n        description=\"the data to be inserted into the dataframe\"\n    )\n    column_names: list[str] = Field(description=\"The names of the columns in data\")\n\n    def append_dataframe(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.concat([df, self.generate_dataframe()], ignore_index=True)\n\n    def generate_dataframe(self) -&gt; pd.DataFrame:\n        return pd.DataFrame(dict(zip(self.column_names, self.data, strict=False)))\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=DataFrameGenerator)\n@prompt_template(\n    \"\"\"\n    Generate {num_datapoints} random but realistic datapoints of items which could\n    be in a home appliances warehouse. Generate your response as `data` and\n    `column_names`, so that a pandas DataFrame may be generated with:\n    `pd.DataFrame(data, columns=column_names)`.\n\n    Format:\n    Name, Price, Inventory\n\n    Name - the name of the home appliance product\n    Price - the price of an individual product, in dollars (include cents)\n    Inventory - how many are left in stock\n    \"\"\"\n)\ndef generate_df_data(num_datapoints: int): ...\n\n\ndf_data = generate_df_data(5)\ndf = df_data.generate_dataframe()\nprint(df)\n</pre> from typing import Any, Literal  import pandas as pd   class DataFrameGenerator(BaseModel):     data: list[list[Any]] = Field(         description=\"the data to be inserted into the dataframe\"     )     column_names: list[str] = Field(description=\"The names of the columns in data\")      def append_dataframe(self, df: pd.DataFrame) -&gt; pd.DataFrame:         return pd.concat([df, self.generate_dataframe()], ignore_index=True)      def generate_dataframe(self) -&gt; pd.DataFrame:         return pd.DataFrame(dict(zip(self.column_names, self.data, strict=False)))   @openai.call(model=\"gpt-4o-mini\", response_model=DataFrameGenerator) @prompt_template(     \"\"\"     Generate {num_datapoints} random but realistic datapoints of items which could     be in a home appliances warehouse. Generate your response as `data` and     `column_names`, so that a pandas DataFrame may be generated with:     `pd.DataFrame(data, columns=column_names)`.      Format:     Name, Price, Inventory      Name - the name of the home appliance product     Price - the price of an individual product, in dollars (include cents)     Inventory - how many are left in stock     \"\"\" ) def generate_df_data(num_datapoints: int): ...   df_data = generate_df_data(5) df = df_data.generate_dataframe() print(df) <pre>             Name         Price Inventory\n0  Microwave Oven  Refrigerator   Blender\n1           79.99        899.99     49.99\n2              25            10        40\n</pre> In\u00a0[12]: Copied! <pre>@openai.call(model=\"gpt-4o-mini\", response_model=DataFrameGenerator)\n@prompt_template(\n    \"\"\"\n    Generate {num_datapoints} random but realistic datapoints of items which would\n    make sense to the following dataset:\n    {df}\n    Generate your response as `data` and\n    `column_names`, so that a pandas DataFrame may be generated with:\n    `pd.DataFrame(data, columns=column_names)` then appended to the existing data.\n    \"\"\"\n)\ndef generate_additional_df_data(num_datapoints: int, df: pd.DataFrame): ...\n\n\ndf_data = generate_additional_df_data(5, df)\ndf = df_data.append_dataframe(df)\nprint(df)\n</pre> @openai.call(model=\"gpt-4o-mini\", response_model=DataFrameGenerator) @prompt_template(     \"\"\"     Generate {num_datapoints} random but realistic datapoints of items which would     make sense to the following dataset:     {df}     Generate your response as `data` and     `column_names`, so that a pandas DataFrame may be generated with:     `pd.DataFrame(data, columns=column_names)` then appended to the existing data.     \"\"\" ) def generate_additional_df_data(num_datapoints: int, df: pd.DataFrame): ...   df_data = generate_additional_df_data(5, df) df = df_data.append_dataframe(df) print(df) <pre>             Name         Price     Inventory\n0  Microwave Oven  Refrigerator       Blender\n1           79.99        899.99         49.99\n2              25            10            40\n3         Toaster   Slow Cooker  Coffee Maker\n4           29.99         49.99         39.99\n5             150            80           200\n</pre> In\u00a0[13]: Copied! <pre>class TV(BaseModel):\n    size: int = Field(description=\"The size of the TV\")\n    price: float = Field(description=\"The price of the TV in dollars (include cents)\")\n    tv_type: Literal[\"OLED\", \"QLED\"]\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=list[TV])\n@prompt_template(\n    \"\"\"\n    Generate {num_datapoints} random but realistic datapoints of TVs.\n    Output the datapoints as a list of instances of TV.\n\n    Make sure to abide by the following constraints:\n    QLEDS should be roughly (not exactly) 2x the price of an OLED of the same size\n    for both OLEDs and QLEDS, price should increase roughly proportionately to size\n    \"\"\"\n)\ndef generate_tv_data(num_datapoints: int): ...\n\n\nfor tv in generate_tv_data(10):\n    print(tv)\n</pre> class TV(BaseModel):     size: int = Field(description=\"The size of the TV\")     price: float = Field(description=\"The price of the TV in dollars (include cents)\")     tv_type: Literal[\"OLED\", \"QLED\"]   @openai.call(model=\"gpt-4o-mini\", response_model=list[TV]) @prompt_template(     \"\"\"     Generate {num_datapoints} random but realistic datapoints of TVs.     Output the datapoints as a list of instances of TV.      Make sure to abide by the following constraints:     QLEDS should be roughly (not exactly) 2x the price of an OLED of the same size     for both OLEDs and QLEDS, price should increase roughly proportionately to size     \"\"\" ) def generate_tv_data(num_datapoints: int): ...   for tv in generate_tv_data(10):     print(tv) <pre>size=32 price=299.99 tv_type='OLED'\nsize=32 price=549.99 tv_type='QLED'\nsize=43 price=399.99 tv_type='OLED'\nsize=43 price=749.99 tv_type='QLED'\nsize=55 price=699.99 tv_type='OLED'\nsize=55 price=1399.99 tv_type='QLED'\nsize=65 price=999.99 tv_type='OLED'\nsize=65 price=1999.99 tv_type='QLED'\nsize=75 price=1299.99 tv_type='OLED'\nsize=75 price=2499.99 tv_type='QLED'\n</pre> <p>To demonstrate the constraints\u2019 being followed, you can graph the data using matplotlib, which shows the linear relationships between size and price, and QLEDs costing roughly twice as much as OLED:</p> <p>Additional Real-World Examples</p> <ul> <li>Healthcare and Medical Research: Generating synthetic patient records for training machine learning models without compromising patient privacy</li> <li>Environmental Science: Generating synthetic climate data for modeling long-term environmental changes</li> <li>Fraud Detection Systems: Generating synthetic data of fraudulent and legitimate transactions for training fraud detection models.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following: - Add Pydantic <code>AfterValidators</code> to constrain your synthetic data generation - Verify that the synthetic data generated actually matches real-world data. - Make sure no biases are present in the generated data, this can be prompt engineered. - Experiment with different model providers and versions for quality.</p>"},{"location":"tutorials/more_advanced/generating_synthetic_data/#generate-synthetic-data","title":"Generate Synthetic Data\u00b6","text":"<p>In this cookbook recipe, we go over how to generate synthetic data for LLMs, in this case, OpenAI\u2019s <code>gpt-4o-mini</code>. When using LLMs to synthetically generate data, it is most useful to generate non-numerical data which isn\u2019t strictly dependent on a defined probability distribution - in those cases, it will be far easier to define a distribution and generate these points directly from the distribution.</p> <p>However, for:</p> <ul> <li>data that needs general intelligence to be realistic</li> <li>data that lists many items within a broad category</li> <li>data which is language related</li> </ul> <p>and more, LLMs are far easier to use and yield better (or the only feasible) results.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p> Large Language Models (LLMs) have emerged as powerful tools for generating synthetic data, particularly for text-based applications. Compared to traditional synthetic data generation methods, LLMs can produce more diverse, contextually rich, and human-like textual data, often with less need for domain-specific rules or statistical modeling. </p>"},{"location":"tutorials/more_advanced/generating_synthetic_data/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/more_advanced/generating_synthetic_data/#generate-data-as-csv","title":"Generate Data as CSV\u00b6","text":"<p>To generate realistic, synthetic data as a csv, you can accomplish this in a single call by requesting a csv format in the prompt and describing the kind of data you would like generated.</p>"},{"location":"tutorials/more_advanced/generating_synthetic_data/#generate-data-with-response_model","title":"Generate Data with <code>response_model</code>\u00b6","text":"<p>Sometimes, it will be easier to integrate your datapoints into your code if they are defined as some schema, namely a Pydantic <code>BaseModel</code>. In this case, describe each column as the <code>description</code> of a <code>Field</code> in the <code>BaseModel</code> instead of the prompt, and set <code>response_model</code> to your defined schema:</p>"},{"location":"tutorials/more_advanced/generating_synthetic_data/#generate-data-into-a-pandas-dataframe","title":"Generate Data into a pandas <code>DataFrame</code>\u00b6","text":"<p>Since pandas is a common library for working with data, it\u2019s also worth knowing how to directly create and append to a dataframe with LLMs.</p>"},{"location":"tutorials/more_advanced/generating_synthetic_data/#create-a-new-dataframe","title":"Create a New <code>DataFrame</code>\u00b6","text":"<p>To create a new <code>DataFrame</code>, we define a <code>BaseModel</code> schema with a simple function to generate  <code>DataFrame</code> via a list of list of data and the column names:</p>"},{"location":"tutorials/more_advanced/generating_synthetic_data/#appending-to-a-dataframe","title":"Appending to a <code>DataFrame</code>\u00b6","text":"<p>To append to a <code>DataFrame</code>, we can modify the prompt so that instead of describing the data we want to generate, we ask the LLM to match the type of data it already sees. Furthermore, we add a <code>append_dataframe()</code> function to append to an existing <code>DataFrame</code>. Finally, note that we use the generated <code>df</code> from above as the <code>DataFrame</code> to append to in the following example:</p>"},{"location":"tutorials/more_advanced/generating_synthetic_data/#adding-constraints","title":"Adding Constraints\u00b6","text":"<p>While you cannot successfully add complex mathematical constraints to generated data (think statistics, such as distributions and covariances), asking LLMs to abide by basic constraints will (generally) prove successful, especially with newer models. Let\u2019s look at an example where we generate TVs where the TV price should roughly linearly correlate with TV size, and QLEDs are 2-3x more expensive than OLEDs of the same size:</p>"},{"location":"tutorials/more_advanced/knowledge_graph/","title":"Knowledge Graph","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n# (Optional) For visualization\n!pip install matplotlib networkx\n# (Optional) For parsing HTML\n!pip install beautifulsoup4\n</pre> !pip install \"mirascope[openai]\" # (Optional) For visualization !pip install matplotlib networkx # (Optional) For parsing HTML !pip install beautifulsoup4 In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass Edge(BaseModel):\n    source: str = Field(..., description=\"The source node of the edge\")\n    target: str = Field(..., description=\"The target node of the edge\")\n    relationship: str = Field(\n        ..., description=\"The relationship between the source and target nodes\"\n    )\n\n\nclass Node(BaseModel):\n    id: str = Field(..., description=\"The unique identifier of the node\")\n    type: str = Field(..., description=\"The type or label of the node\")\n    properties: dict | None = Field(\n        ..., description=\"Additional properties and metadata associated with the node\"\n    )\n\n\nclass KnowledgeGraph(BaseModel):\n    nodes: list[Node] = Field(..., description=\"List of nodes in the knowledge graph\")\n    edges: list[Edge] = Field(..., description=\"List of edges in the knowledge graph\")\n</pre> from pydantic import BaseModel, Field   class Edge(BaseModel):     source: str = Field(..., description=\"The source node of the edge\")     target: str = Field(..., description=\"The target node of the edge\")     relationship: str = Field(         ..., description=\"The relationship between the source and target nodes\"     )   class Node(BaseModel):     id: str = Field(..., description=\"The unique identifier of the node\")     type: str = Field(..., description=\"The type or label of the node\")     properties: dict | None = Field(         ..., description=\"Additional properties and metadata associated with the node\"     )   class KnowledgeGraph(BaseModel):     nodes: list[Node] = Field(..., description=\"List of nodes in the knowledge graph\")     edges: list[Edge] = Field(..., description=\"List of edges in the knowledge graph\") <p>Our <code>Edge</code> represents connections between nodes, with attributes for the source node, target node, and the relationship between them. While our <code>Node</code> defines nodes with an ID, type, and optional properties. Our <code>KnowledgeGraph</code> then aggregates these nodes and edges into a comprehensive knowledge graph.</p> <p>Now that we have our schema defined, it's time to create our knowledge graph.</p> In\u00a0[\u00a0]: Copied! <pre>!curl https://en.wikipedia.org/wiki/Large_language_model -o wikipedia.html\n</pre> !curl https://en.wikipedia.org/wiki/Large_language_model -o wikipedia.html <p>{% raw %}</p> In\u00a0[5]: Copied! <pre>from bs4 import BeautifulSoup\nfrom mirascope.core import openai, prompt_template\n\n\ndef get_text_from_html(file_path: str) -&gt; str:\n    with open(file_path) as file:\n        html_text = file.read()\n    return BeautifulSoup(html_text, \"html.parser\").get_text()\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=KnowledgeGraph)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Your job is to create a knowledge graph based on the text and user question.\n    \n    The article:\n    {text}\n\n    Example:\n    John and Jane Doe are siblings. Jane is 25 and 5 years younger than John.\n    Node(id=\"John Doe\", type=\"Person\", properties={{\"age\": 30}})\n    Node(id=\"Jane Doe\", type=\"Person\", properties={{\"age\": 25}})\n    Edge(source=\"John Doe\", target=\"Jane Doe\", relationship=\"Siblings\")\n\n    USER:\n    {question}\n    \"\"\"\n)\ndef generate_knowledge_graph(\n    question: str, file_name: str\n) -&gt; openai.OpenAIDynamicConfig:\n    text = get_text_from_html(file_name)\n    return {\"computed_fields\": {\"text\": text}}\n\n\nquestion = \"What are the pitfalls of using LLMs?\"\n\nkg = generate_knowledge_graph(question, \"wikipedia.html\")\nprint(kg)\n</pre> from bs4 import BeautifulSoup from mirascope.core import openai, prompt_template   def get_text_from_html(file_path: str) -&gt; str:     with open(file_path) as file:         html_text = file.read()     return BeautifulSoup(html_text, \"html.parser\").get_text()   @openai.call(model=\"gpt-4o-mini\", response_model=KnowledgeGraph) @prompt_template(     \"\"\"     SYSTEM:     Your job is to create a knowledge graph based on the text and user question.          The article:     {text}      Example:     John and Jane Doe are siblings. Jane is 25 and 5 years younger than John.     Node(id=\"John Doe\", type=\"Person\", properties={{\"age\": 30}})     Node(id=\"Jane Doe\", type=\"Person\", properties={{\"age\": 25}})     Edge(source=\"John Doe\", target=\"Jane Doe\", relationship=\"Siblings\")      USER:     {question}     \"\"\" ) def generate_knowledge_graph(     question: str, file_name: str ) -&gt; openai.OpenAIDynamicConfig:     text = get_text_from_html(file_name)     return {\"computed_fields\": {\"text\": text}}   question = \"What are the pitfalls of using LLMs?\"  kg = generate_knowledge_graph(question, \"wikipedia.html\") print(kg) <pre>nodes=[Node(id='Large Language Models', type='Large Language Model', properties=None), Node(id='Data Cleaning Issues', type='Pitfall', properties=None), Node(id='Bias Inheritance', type='Pitfall', properties=None), Node(id='Hallucinations', type='Pitfall', properties=None), Node(id='Limited Understanding', type='Pitfall', properties=None), Node(id='Dependence on Training Data', type='Pitfall', properties=None), Node(id='Security Risks', type='Pitfall', properties=None), Node(id='Stereotyping', type='Pitfall', properties=None), Node(id='Political Bias', type='Pitfall', properties=None)] edges=[Edge(source='Large Language Models', target='Data Cleaning Issues', relationship='has pitfall'), Edge(source='Large Language Models', target='Bias Inheritance', relationship='has pitfall'), Edge(source='Large Language Models', target='Hallucinations', relationship='has pitfall'), Edge(source='Large Language Models', target='Limited Understanding', relationship='has pitfall'), Edge(source='Large Language Models', target='Dependence on Training Data', relationship='has pitfall'), Edge(source='Large Language Models', target='Security Risks', relationship='has pitfall'), Edge(source='Large Language Models', target='Stereotyping', relationship='has pitfall'), Edge(source='Large Language Models', target='Political Bias', relationship='has pitfall')]\n</pre> <p>{% endraw %}</p> <p>We engineer our prompt by giving examples of how the properties should be filled out and use Mirascope's <code>DynamicConfig</code> to pass in the article. While it seems silly in this context, there may be multiple documents that you may want to conditionally pass in depending on the query. This can include text chunks from a Vector Store or data from a Database.</p> <p>After we generated our knowledge graph, it is time to create our <code>run</code> function</p> In\u00a0[6]: Copied! <pre>@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Answer the following question based on the knowledge graph.\n\n    Knowledge Graph:\n    {knowledge_graph}\n    \n    USER:\n    {question}\n    \"\"\"\n)\ndef run(question: str, knowledge_graph: KnowledgeGraph): ...\n</pre> @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     SYSTEM:     Answer the following question based on the knowledge graph.      Knowledge Graph:     {knowledge_graph}          USER:     {question}     \"\"\" ) def run(question: str, knowledge_graph: KnowledgeGraph): ... <p>We define a simple <code>run</code> function that answers the users query based on the knowledge graph. Combining knowledge graphs with semantic search will lead to the LLM having better context to address complex questions.</p> In\u00a0[7]: Copied! <pre>print(run(question, kg))\n</pre> print(run(question, kg)) <pre>The pitfalls of using Large Language Models (LLMs) include:\n\n1. Data Cleaning Issues\n2. Bias Inheritance\n3. Hallucinations\n4. Limited Understanding\n5. Dependence on Training Data\n6. Security Risks\n7. Stereotyping\n8. Political Bias\n</pre> In\u00a0[8]: Copied! <pre>import matplotlib.pyplot as plt\nimport networkx as nx\n\n\ndef render_graph(kg: KnowledgeGraph):\n    G = nx.DiGraph()\n\n    for node in kg.nodes:\n        G.add_node(node.id, label=node.type, **(node.properties or {}))\n\n    for edge in kg.edges:\n        G.add_edge(edge.source, edge.target, label=edge.relationship)\n\n    plt.figure(figsize=(15, 10))\n    pos = nx.spring_layout(G)\n\n    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color=\"lightblue\")\n    nx.draw_networkx_edges(G, pos, arrowstyle=\"-&gt;\", arrowsize=20)\n    nx.draw_networkx_labels(G, pos, font_size=12, font_weight=\"bold\")\n\n    edge_labels = nx.get_edge_attributes(G, \"label\")\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color=\"red\")\n\n    plt.title(\"Knowledge Graph Visualization\", fontsize=15)\n    plt.show()\n\n\nquestion = \"What are the pitfalls of using LLMs?\"\nrender_graph(kg)\n</pre> import matplotlib.pyplot as plt import networkx as nx   def render_graph(kg: KnowledgeGraph):     G = nx.DiGraph()      for node in kg.nodes:         G.add_node(node.id, label=node.type, **(node.properties or {}))      for edge in kg.edges:         G.add_edge(edge.source, edge.target, label=edge.relationship)      plt.figure(figsize=(15, 10))     pos = nx.spring_layout(G)      nx.draw_networkx_nodes(G, pos, node_size=2000, node_color=\"lightblue\")     nx.draw_networkx_edges(G, pos, arrowstyle=\"-&gt;\", arrowsize=20)     nx.draw_networkx_labels(G, pos, font_size=12, font_weight=\"bold\")      edge_labels = nx.get_edge_attributes(G, \"label\")     nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color=\"red\")      plt.title(\"Knowledge Graph Visualization\", fontsize=15)     plt.show()   question = \"What are the pitfalls of using LLMs?\" render_graph(kg) <pre>Matplotlib is building the font cache; this may take a moment.\n</pre> <p>Additional Real-World Applications</p> <ol> <li><p>Enhance your Q&amp;A</p> <ul> <li>Customer support system uses knowledge graph containing information about products to answer questions.</li> <li>Example: \"Does the Mirascope phone support fast charging?\" The knowledge graph has a node \"Mirascope smartphone\" and searches \"support\" edge to find fast charging and returns results for the LLM to use.</li> </ul> </li> <li><p>Supply Chain Optimization</p> <ul> <li>A knowledge graph could represent complex relationships between suppliers, manufacturing plants, distribution centers, products, and transportation routes.</li> <li>Example: How would a 20% increase in demand for a mirascope affect our inventory needs and shipping costs? Use knowledge graph to trace the mirascope toy, calculate inventory, and then estimate shipping costs and return results for the LLM to give a report.</li> </ul> </li> <li><p>Healthcare Assistant</p> <ul> <li>Assuming no PII or HIPPA violation, build a knowledge graph from patient remarks.</li> <li>Example: \"Mary said help, I've fallen\". Build up a knowledge graph from comments and use an LLM to scan the node \"Mary\" for any worrying activity. Have the LLM alert Healthcare employees that there may be an emergency.</li> </ul> </li> </ol> <p>When adapting this recipe, consider:</p> <ul> <li>Combining knowledge graph with Text Embeddings for both structured search and semantic search, depending on your requirements.</li> <li>Store your knowledge graph in a database / cache for faster retrieval.</li> <li>Experiment with different LLM models, some may be better than others for generating the knowledge graph.</li> <li>Turn the example into an Agentic workflow, giving it access to tools such as web search so the LLM can call tools to update its own knowledge graph to answer any question.</li> <li>Adding Pydantic <code>AfterValidators</code> to prevent duplicate Node IDs.</li> </ul>"},{"location":"tutorials/more_advanced/knowledge_graph/#knowledge-graph","title":"Knowledge Graph\u00b6","text":"<p>Often times, data is messy and not always stored in a structured manner ready for use by an LLM. In this recipe, we show how to create a knowledge graph from an unstructured document using common python libraries and Mirascope using OpenAI GPT-4o-mini.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p> While traditional Natural Language Processing (NLP) techniques have long been used in knowledge graphs to identify entities and relationships in unstructured text, Large Language Models (LLMs) have significantly improved this process. LLMs enhance the accuracy of entity identification and linking to knowledge graph entries, demonstrating superior ability to handle context and ambiguity compared to conventional NLP methods.  </p>"},{"location":"tutorials/more_advanced/knowledge_graph/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/more_advanced/knowledge_graph/#create-the-knowledgegraph","title":"Create the <code>KnowledgeGraph</code>\u00b6","text":"<p>The first step is to create a <code>KnowledgeGraph</code> with <code>Nodes</code> and <code>Edges</code> that represent our entities and relationships. For our simple recipe, we will use a Pydantic <code>BaseModel</code> to represent our <code>KnowledgeGraph</code>:</p>"},{"location":"tutorials/more_advanced/knowledge_graph/#creating-the-knowledge-graph","title":"Creating the knowledge graph\u00b6","text":"<p>We start off with engineering our prompt, prompting the LLM to create a knowledge graph based on the user query. Then we are taking a Wikipedia article and converting the raw text into a structured knowledge graph. The command below will download the article to your local machine by using the <code>curl</code> command. If you don't have <code>curl</code> installed, you can download the article manually from the link above and save it as <code>wikipedia.html</code>.</p>"},{"location":"tutorials/more_advanced/knowledge_graph/#render-your-graph","title":"Render your graph\u00b6","text":"<p>Optionally, to visualize the knowledge graph, we use networkx and matplotlib to draw the edges and nodes.</p>"},{"location":"tutorials/more_advanced/llm_validation_with_retries/","title":"LLM Validation With Retries","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[anthropic,tenacity]\"\n</pre> !pip install \"mirascope[anthropic,tenacity]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>from mirascope.core import anthropic, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass SpellingAndGrammarCheck(BaseModel):\n    has_errors: bool = Field(\n        description=\"Whether the text has typos or grammatical errors\"\n    )\n\n\n@anthropic.call(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=SpellingAndGrammarCheck,\n    json_mode=True,\n)\n@prompt_template(\n    \"\"\"\n    Does the following text have any typos or grammatical errors? {text}\n    \"\"\"\n)\ndef check_for_errors(text: str): ...\n\n\ntext = \"Yestday I had a gret time!\"\nresponse = check_for_errors(text)\nassert response.has_errors\n</pre> from mirascope.core import anthropic, prompt_template from pydantic import BaseModel, Field   class SpellingAndGrammarCheck(BaseModel):     has_errors: bool = Field(         description=\"Whether the text has typos or grammatical errors\"     )   @anthropic.call(     model=\"claude-3-5-sonnet-20240620\",     response_model=SpellingAndGrammarCheck,     json_mode=True, ) @prompt_template(     \"\"\"     Does the following text have any typos or grammatical errors? {text}     \"\"\" ) def check_for_errors(text: str): ...   text = \"Yestday I had a gret time!\" response = check_for_errors(text) assert response.has_errors In\u00a0[2]: Copied! <pre>from typing import Annotated\n\nfrom mirascope.core import anthropic, prompt_template\nfrom pydantic import AfterValidator, BaseModel, ValidationError\n\n\n@anthropic.call(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=SpellingAndGrammarCheck,\n    json_mode=True,\n)\n@prompt_template(\n    \"\"\"\n    Does the following text have any typos or grammatical errors? {text}\n    \"\"\"\n)\ndef check_for_errors(text: str): ...\n\n\nclass TextWithoutErrors(BaseModel):\n    text: Annotated[\n        str,\n        AfterValidator(\n            lambda t: t\n            if not (check_for_errors(t)).has_errors\n            else (_ for _ in ()).throw(ValueError(\"Text contains errors\"))\n        ),\n    ]\n\n\nvalid = TextWithoutErrors(text=\"This is a perfectly written sentence.\")\n\ntry:\n    invalid = TextWithoutErrors(\n        text=\"I walkd to supermarket and i picked up some fish?\"\n    )\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n</pre> from typing import Annotated  from mirascope.core import anthropic, prompt_template from pydantic import AfterValidator, BaseModel, ValidationError   @anthropic.call(     model=\"claude-3-5-sonnet-20240620\",     response_model=SpellingAndGrammarCheck,     json_mode=True, ) @prompt_template(     \"\"\"     Does the following text have any typos or grammatical errors? {text}     \"\"\" ) def check_for_errors(text: str): ...   class TextWithoutErrors(BaseModel):     text: Annotated[         str,         AfterValidator(             lambda t: t             if not (check_for_errors(t)).has_errors             else (_ for _ in ()).throw(ValueError(\"Text contains errors\"))         ),     ]   valid = TextWithoutErrors(text=\"This is a perfectly written sentence.\")  try:     invalid = TextWithoutErrors(         text=\"I walkd to supermarket and i picked up some fish?\"     ) except ValidationError as e:     print(f\"Validation error: {e}\") <pre>Validation error: 1 validation error for TextWithoutErrors\ntext\n  Value error, Text contains errors [type=value_error, input_value='I walkd to supermarket a... i picked up some fish?', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.8/v/value_error\n</pre> In\u00a0[3]: Copied! <pre>from typing import Annotated\n\nfrom mirascope.core import anthropic, prompt_template\nfrom mirascope.integrations.tenacity import collect_errors\nfrom pydantic import AfterValidator, BaseModel, Field, ValidationError\nfrom tenacity import retry, stop_after_attempt\n\n\nclass SpellingAndGrammarCheck(BaseModel):\n    has_errors: bool = Field(\n        description=\"Whether the text has typos or grammatical errors\"\n    )\n\n\n@anthropic.call(\n    model=\"claude-3-5-sonnet-20240620\",\n    response_model=SpellingAndGrammarCheck,\n    json_mode=True,\n)\n@prompt_template(\n    \"\"\"\n    Does the following text have any typos or grammatical errors? {text}\n    \"\"\"\n)\ndef check_for_errors(text: str): ...\n\n\nclass GrammarCheck(BaseModel):\n    text: Annotated[\n        str,\n        AfterValidator(\n            lambda t: t\n            if not (check_for_errors(t)).has_errors\n            else (_ for _ in ()).throw(ValueError(\"Text still contains errors\"))\n        ),\n    ] = Field(description=\"The corrected text with proper grammar\")\n    explanation: str = Field(description=\"Explanation of the corrections made\")\n\n\n@retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError))\n@anthropic.call(\n    \"claude-3-5-sonnet-20240620\", response_model=GrammarCheck, json_mode=True\n)\n@prompt_template(\n    \"\"\"\n    {previous_errors}\n\n    Correct the grammar in the following text.\n    If no corrections are needed, return the original text.\n    Provide an explanation of any corrections made.\n\n    Text: {text}\n    \"\"\"\n)\ndef correct_grammar(\n    text: str, *, errors: list[ValidationError] | None = None\n) -&gt; anthropic.AnthropicDynamicConfig:\n    previous_errors = f\"Previous Errors: {errors}\" if errors else \"No previous errors.\"\n    return {\"computed_fields\": {\"previous_errors\": previous_errors}}\n\n\ntry:\n    text = \"I has went to the store yesterday and buyed some milk.\"\n    result = correct_grammar(text)\n    print(f\"Corrected text: {result.text}\")\n    print(f\"Explanation: {result.explanation}\")\nexcept ValidationError:\n    print(\"Failed to correct grammar after 3 attempts\")\n</pre> from typing import Annotated  from mirascope.core import anthropic, prompt_template from mirascope.integrations.tenacity import collect_errors from pydantic import AfterValidator, BaseModel, Field, ValidationError from tenacity import retry, stop_after_attempt   class SpellingAndGrammarCheck(BaseModel):     has_errors: bool = Field(         description=\"Whether the text has typos or grammatical errors\"     )   @anthropic.call(     model=\"claude-3-5-sonnet-20240620\",     response_model=SpellingAndGrammarCheck,     json_mode=True, ) @prompt_template(     \"\"\"     Does the following text have any typos or grammatical errors? {text}     \"\"\" ) def check_for_errors(text: str): ...   class GrammarCheck(BaseModel):     text: Annotated[         str,         AfterValidator(             lambda t: t             if not (check_for_errors(t)).has_errors             else (_ for _ in ()).throw(ValueError(\"Text still contains errors\"))         ),     ] = Field(description=\"The corrected text with proper grammar\")     explanation: str = Field(description=\"Explanation of the corrections made\")   @retry(stop=stop_after_attempt(3), after=collect_errors(ValidationError)) @anthropic.call(     \"claude-3-5-sonnet-20240620\", response_model=GrammarCheck, json_mode=True ) @prompt_template(     \"\"\"     {previous_errors}      Correct the grammar in the following text.     If no corrections are needed, return the original text.     Provide an explanation of any corrections made.      Text: {text}     \"\"\" ) def correct_grammar(     text: str, *, errors: list[ValidationError] | None = None ) -&gt; anthropic.AnthropicDynamicConfig:     previous_errors = f\"Previous Errors: {errors}\" if errors else \"No previous errors.\"     return {\"computed_fields\": {\"previous_errors\": previous_errors}}   try:     text = \"I has went to the store yesterday and buyed some milk.\"     result = correct_grammar(text)     print(f\"Corrected text: {result.text}\")     print(f\"Explanation: {result.explanation}\") except ValidationError:     print(\"Failed to correct grammar after 3 attempts\") <pre>Corrected text: I went to the store yesterday and bought some milk.\nExplanation: The sentence has been corrected as follows: 'has went' was changed to 'went' (simple past tense of 'go'), and 'buyed' was changed to 'bought' (past tense of 'buy'). The subject-verb agreement was also fixed by changing 'I has' to 'I'.\n</pre> <p>Additional Real-World Examples</p> <ul> <li>Code Review: Validate code snippets for best practices and potential bugs.</li> <li>Data Quality Checks: Validate complex data structures for consistency and completeness.</li> <li>Legal Document Validation: Check legal documents for compliance with specific regulations.</li> <li>Medical Record Validation: Ensure medical records are complete and consistent.</li> <li>Financial Report Validation: Verify financial reports for accuracy and compliance with accounting standards.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Tailor the prompts to provide clear instructions and relevant context for your specific validation tasks.</li> <li>Balance the trade-off between validation accuracy and performance, especially when implementing retries.</li> <li>Implement proper error handling and logging for production use.</li> <li>Consider caching validation results for frequently validated items to improve performance.</li> </ul>"},{"location":"tutorials/more_advanced/llm_validation_with_retries/#llm-validation-with-retries","title":"LLM Validation With Retries\u00b6","text":"<p>This recipe demonstrates how to leverage Large Language Models (LLMs) -- specifically Anthropic's Claude 3.5 Sonnet -- to perform automated validation on any value. We'll cover how to use LLMs for complex validation tasks, how to integrate this with Pydantic's validation system, and how to leverage Tenacity to automatically reinsert validation errors back into an LLM call to improve results.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Model</li> <li>Tenacity Integration</li> </ul> <p>Background</p> <p> While traditional validation tools like type checkers or Pydantic are limited to hardcoded rules (such as variable types or arithmetic), LLMs allow for much more nuanced and complex validation. This approach can be particularly useful for validating natural language inputs or complex data structures where traditional rule-based validation falls short. </p>"},{"location":"tutorials/more_advanced/llm_validation_with_retries/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/llm_validation_with_retries/#basic-llm-validation","title":"Basic LLM Validation\u00b6","text":"<p>Let's start with a simple example of using an LLM to check for spelling and grammatical errors in a text snippet:</p>"},{"location":"tutorials/more_advanced/llm_validation_with_retries/#pydantics-aftervalidator","title":"Pydantic's AfterValidator\u00b6","text":"<p>We can use Pydantic's <code>AfterValidator</code> to integrate our LLM-based validation directly into a Pydantic model:</p>"},{"location":"tutorials/more_advanced/llm_validation_with_retries/#reinsert-validation-errors-for-improved-performance","title":"Reinsert Validation Errors For Improved Performance\u00b6","text":"<p>One powerful technique for enhancing LLM generations is to automatically reinsert validation errors into subsequent calls. This approach allows the LLM to learn from its previous mistakes as few-shot examples and improve it's output in real-time. We can achieve this using Mirascope's integration with Tenacity, which collects <code>ValidationError</code> messages for easy insertion into the prompt.</p>"},{"location":"tutorials/more_advanced/named_entity_recognition/","title":"Named Entity Recognition","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[groq]\" pytest\n!pip install ipytest # For running pytest in Jupyter Notebooks\n</pre> !pip install \"mirascope[groq]\" pytest !pip install ipytest # For running pytest in Jupyter Notebooks In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"GROQ_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"GROQ_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[3]: Copied! <pre>from __future__ import annotations  # noqa: F404\n\nimport textwrap\n\nfrom mirascope.core import groq, prompt_template\nfrom pydantic import BaseModel, Field\n\nunstructured_text = \"\"\"\nApple Inc., the tech giant founded by Steve Jobs and Steve Wozniak, recently announced a partnership with OpenAI, the artificial intelligence research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. This collaboration aims to enhance Siri, Apple's virtual assistant, which competes with Amazon's Alexa and Google Assistant, a product of Alphabet Inc.'s Google division. The joint project will be led by Apple's AI chief John Giannandrea, a former Google executive, and will take place at Apple Park, the company's headquarters in Cupertino, California.\n\"\"\"\n\n\nclass SimpleEntity(BaseModel):\n    entity: str = Field(description=\"The entity found in the text\")\n    label: str = Field(\n        description=\"The label of the entity (e.g., PERSON, ORGANIZATION, LOCATION)\"\n    )\n\n\n@groq.call(\n    model=\"llama-3.1-8b-instant\",\n    response_model=list[SimpleEntity],\n    json_mode=True,\n    call_params={\"temperature\": 0.0},\n)\ndef simple_ner(text: str) -&gt; str:\n    return f\"Extract the entities from this text: {text}\"\n\n\nprint(\"Simple NER Results:\")\nsimple_result = simple_ner(unstructured_text)\nfor entity in simple_result:\n    print(f\"Entity: {entity.entity}, Label: {entity.label}\")\n</pre> from __future__ import annotations  # noqa: F404  import textwrap  from mirascope.core import groq, prompt_template from pydantic import BaseModel, Field  unstructured_text = \"\"\" Apple Inc., the tech giant founded by Steve Jobs and Steve Wozniak, recently announced a partnership with OpenAI, the artificial intelligence research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. This collaboration aims to enhance Siri, Apple's virtual assistant, which competes with Amazon's Alexa and Google Assistant, a product of Alphabet Inc.'s Google division. The joint project will be led by Apple's AI chief John Giannandrea, a former Google executive, and will take place at Apple Park, the company's headquarters in Cupertino, California. \"\"\"   class SimpleEntity(BaseModel):     entity: str = Field(description=\"The entity found in the text\")     label: str = Field(         description=\"The label of the entity (e.g., PERSON, ORGANIZATION, LOCATION)\"     )   @groq.call(     model=\"llama-3.1-8b-instant\",     response_model=list[SimpleEntity],     json_mode=True,     call_params={\"temperature\": 0.0}, ) def simple_ner(text: str) -&gt; str:     return f\"Extract the entities from this text: {text}\"   print(\"Simple NER Results:\") simple_result = simple_ner(unstructured_text) for entity in simple_result:     print(f\"Entity: {entity.entity}, Label: {entity.label}\") <pre>Simple NER Results:\nEntity: Apple Inc., Label: ORGANIZATION\nEntity: Steve Jobs, Label: PERSON\nEntity: Steve Wozniak, Label: PERSON\nEntity: OpenAI, Label: ORGANIZATION\nEntity: OpenAI LP, Label: ORGANIZATION\nEntity: OpenAI Inc., Label: ORGANIZATION\nEntity: Amazon, Label: ORGANIZATION\nEntity: Google, Label: ORGANIZATION\nEntity: Alphabet Inc., Label: ORGANIZATION\nEntity: John Giannandrea, Label: PERSON\nEntity: Apple Park, Label: LOCATION\nEntity: Cupertino, Label: LOCATION\nEntity: California, Label: LOCATION\n</pre> <p>In this example, we're extracting entities that have just the entity's text and label. However, entities often have relationships that are worth extracting and understanding.</p> In\u00a0[4]: Copied! <pre>class NestedEntity(BaseModel):\n    entity: str = Field(description=\"The entity found in the text\")\n    label: str = Field(\n        description=\"The label of the entity (e.g., PERSON, ORGANIZATION, LOCATION)\"\n    )\n    parent: str | None = Field(\n        description=\"The parent entity if this entity is nested within another entity\",\n        default=None,\n    )\n    children: list[NestedEntity] = Field(\n        default_factory=list, description=\"Nested entities within this entity\"\n    )\n\n\n@groq.call(\n    model=\"llama-3.1-8b-instant\",\n    response_model=list[NestedEntity],\n    json_mode=True,\n    call_params={\"temperature\": 0.0},\n)\n@prompt_template(\n    \"\"\"\n    Identify all named entities in the following text, including deeply nested entities. \n    For each entity, provide its label and any nested entities within it.\n\n    Guidelines:\n    1. Identify entities of types PERSON, ORGANIZATION, LOCATION, and any other relevant types.\n    2. Capture hierarchical relationships between entities.\n    3. Include all relevant information, even if it requires deep nesting.\n    4. Be thorough and consider all possible entities and their relationships.\n\n    Example:\n    Text: \"John Smith, the CEO of Tech Innovations Inc., a subsidiary of Global Corp, announced a new product at their headquarters in Silicon Valley.\"\n    Entities:\n    - Entity: \"John Smith\", Label: \"PERSON\", Parent: None\n      Children:\n        - Entity: \"Tech Innovations Inc.\", Label: \"ORGANIZATION\", Parent: \"John Smith\"\n          Children:\n            - Entity: \"Global Corp\", Label: \"ORGANIZATION\", Parent: \"Tech Innovations Inc.\"\n    - Entity: \"Silicon Valley\", Label: \"LOCATION\", Parent: None\n\n    Now, analyze the following text: {text}\n    \"\"\"\n)\ndef nested_ner(text: str): ...\n\n\nprint(\"\\nNested NER Results:\")\nimproved_result = nested_ner(unstructured_text)\n\n\ndef print_nested_entities(entities, level=0):\n    for entity in entities:\n        indent = \"  \" * level\n        entity_info = (\n            f\"Entity: {entity.entity}, Label: {entity.label}, Parent: {entity.parent}\"\n        )\n        print(textwrap.indent(entity_info, indent))\n        if entity.children:\n            print_nested_entities(entity.children, level + 1)\n\n\nprint_nested_entities(improved_result)\n</pre> class NestedEntity(BaseModel):     entity: str = Field(description=\"The entity found in the text\")     label: str = Field(         description=\"The label of the entity (e.g., PERSON, ORGANIZATION, LOCATION)\"     )     parent: str | None = Field(         description=\"The parent entity if this entity is nested within another entity\",         default=None,     )     children: list[NestedEntity] = Field(         default_factory=list, description=\"Nested entities within this entity\"     )   @groq.call(     model=\"llama-3.1-8b-instant\",     response_model=list[NestedEntity],     json_mode=True,     call_params={\"temperature\": 0.0}, ) @prompt_template(     \"\"\"     Identify all named entities in the following text, including deeply nested entities.      For each entity, provide its label and any nested entities within it.      Guidelines:     1. Identify entities of types PERSON, ORGANIZATION, LOCATION, and any other relevant types.     2. Capture hierarchical relationships between entities.     3. Include all relevant information, even if it requires deep nesting.     4. Be thorough and consider all possible entities and their relationships.      Example:     Text: \"John Smith, the CEO of Tech Innovations Inc., a subsidiary of Global Corp, announced a new product at their headquarters in Silicon Valley.\"     Entities:     - Entity: \"John Smith\", Label: \"PERSON\", Parent: None       Children:         - Entity: \"Tech Innovations Inc.\", Label: \"ORGANIZATION\", Parent: \"John Smith\"           Children:             - Entity: \"Global Corp\", Label: \"ORGANIZATION\", Parent: \"Tech Innovations Inc.\"     - Entity: \"Silicon Valley\", Label: \"LOCATION\", Parent: None      Now, analyze the following text: {text}     \"\"\" ) def nested_ner(text: str): ...   print(\"\\nNested NER Results:\") improved_result = nested_ner(unstructured_text)   def print_nested_entities(entities, level=0):     for entity in entities:         indent = \"  \" * level         entity_info = (             f\"Entity: {entity.entity}, Label: {entity.label}, Parent: {entity.parent}\"         )         print(textwrap.indent(entity_info, indent))         if entity.children:             print_nested_entities(entity.children, level + 1)   print_nested_entities(improved_result) <pre>\nNested NER Results:\nEntity: Steve Jobs, Label: PERSON, Parent: None\n  Entity: Apple Inc., Label: ORGANIZATION, Parent: Steve Jobs\n    Entity: Steve Wozniak, Label: PERSON, Parent: Apple Inc.\n    Entity: Apple Park, Label: LOCATION, Parent: Apple Inc.\n    Entity: Cupertino, Label: LOCATION, Parent: Apple Park\n    Entity: California, Label: LOCATION, Parent: Cupertino\nEntity: Steve Wozniak, Label: PERSON, Parent: None\n  Entity: Apple Inc., Label: ORGANIZATION, Parent: Steve Wozniak\nEntity: Apple Inc., Label: ORGANIZATION, Parent: None\n  Entity: John Giannandrea, Label: PERSON, Parent: Apple Inc.\n  Entity: Apple Park, Label: LOCATION, Parent: Apple Inc.\n  Entity: Cupertino, Label: LOCATION, Parent: Apple Park\n  Entity: California, Label: LOCATION, Parent: Cupertino\n  Entity: OpenAI, Label: ORGANIZATION, Parent: Apple Inc.\n    Entity: OpenAI LP, Label: ORGANIZATION, Parent: OpenAI\n    Entity: OpenAI Inc., Label: ORGANIZATION, Parent: OpenAI\nEntity: John Giannandrea, Label: PERSON, Parent: None\n  Entity: Apple Inc., Label: ORGANIZATION, Parent: John Giannandrea\nEntity: Apple Park, Label: LOCATION, Parent: None\n  Entity: Cupertino, Label: LOCATION, Parent: Apple Park\n  Entity: California, Label: LOCATION, Parent: Cupertino\nEntity: Cupertino, Label: LOCATION, Parent: None\n  Entity: California, Label: LOCATION, Parent: Cupertino\nEntity: California, Label: LOCATION, Parent: None\nEntity: OpenAI, Label: ORGANIZATION, Parent: None\n  Entity: OpenAI LP, Label: ORGANIZATION, Parent: OpenAI\n  Entity: OpenAI Inc., Label: ORGANIZATION, Parent: OpenAI\nEntity: OpenAI LP, Label: ORGANIZATION, Parent: None\nEntity: OpenAI Inc., Label: ORGANIZATION, Parent: None\nEntity: Amazon, Label: ORGANIZATION, Parent: None\n  Entity: Alexa, Label: PRODUCT, Parent: Amazon\nEntity: Alexa, Label: PRODUCT, Parent: None\nEntity: Google, Label: ORGANIZATION, Parent: None\n  Entity: Google Assistant, Label: PRODUCT, Parent: Google\n  Entity: Alphabet Inc., Label: ORGANIZATION, Parent: Google\nEntity: Google Assistant, Label: PRODUCT, Parent: None\nEntity: Alphabet Inc., Label: ORGANIZATION, Parent: None\n</pre> In\u00a0[\u00a0]: Copied! <pre>import ipytest  # noqa: E402\nimport pytest  # noqa: E402\n\nipytest.autoconfig()\n\n\ntest_cases = [\n    (\n        \"\"\"\n    The multinational conglomerate Alphabet Inc., parent company of Google, has acquired \n    DeepMind, a leading AI research laboratory based in London. DeepMind's founder, \n    Demis Hassabis, will join Google Brain, a division of Google AI, as Chief AI Scientist. \n    This move strengthens Alphabet's position in the AI field, challenging competitors like \n    OpenAI, which is backed by Microsoft, and Facebook AI Research, a part of Meta Platforms Inc.\n        \"\"\",\n        [\n            NestedEntity(\n                entity=\"Alphabet Inc.\",\n                label=\"ORGANIZATION\",\n                parent=None,\n                children=[\n                    NestedEntity(\n                        entity=\"Google\",\n                        label=\"ORGANIZATION\",\n                        parent=\"Alphabet Inc.\",\n                        children=[\n                            NestedEntity(\n                                entity=\"Google Brain\",\n                                label=\"ORGANIZATION\",\n                                parent=\"Google\",\n                                children=[],\n                            ),\n                            NestedEntity(\n                                entity=\"Google AI\",\n                                label=\"ORGANIZATION\",\n                                parent=\"Google\",\n                                children=[\n                                    NestedEntity(\n                                        entity=\"Google Brain\",\n                                        label=\"ORGANIZATION\",\n                                        parent=\"Google AI\",\n                                        children=[],\n                                    )\n                                ],\n                            ),\n                        ],\n                    ),\n                    NestedEntity(\n                        entity=\"DeepMind\",\n                        label=\"ORGANIZATION\",\n                        parent=\"Alphabet Inc.\",\n                        children=[\n                            NestedEntity(\n                                entity=\"Demis Hassabis\",\n                                label=\"PERSON\",\n                                parent=\"DeepMind\",\n                                children=[],\n                            )\n                        ],\n                    ),\n                ],\n            ),\n            NestedEntity(entity=\"London\", label=\"LOCATION\", parent=None, children=[]),\n            NestedEntity(\n                entity=\"Demis Hassabis\", label=\"PERSON\", parent=None, children=[]\n            ),\n            NestedEntity(\n                entity=\"OpenAI\",\n                label=\"ORGANIZATION\",\n                parent=None,\n                children=[\n                    NestedEntity(\n                        entity=\"Microsoft\",\n                        label=\"ORGANIZATION\",\n                        parent=\"OpenAI\",\n                        children=[],\n                    )\n                ],\n            ),\n            NestedEntity(\n                entity=\"Facebook AI Research\",\n                label=\"ORGANIZATION\",\n                parent=None,\n                children=[\n                    NestedEntity(\n                        entity=\"Meta Platforms Inc.\",\n                        label=\"ORGANIZATION\",\n                        parent=\"Facebook AI Research\",\n                        children=[],\n                    )\n                ],\n            ),\n            NestedEntity(\n                entity=\"Meta Platforms Inc.\",\n                label=\"ORGANIZATION\",\n                parent=None,\n                children=[],\n            ),\n            NestedEntity(\n                entity=\"Microsoft\", label=\"ORGANIZATION\", parent=None, children=[]\n            ),\n        ],\n    ),\n]\n\n\n@pytest.mark.parametrize(\"text,expected_output\", test_cases)\ndef test_nested_ner(text: str, expected_output: list[NestedEntity]):\n    output = nested_ner(text)\n    assert len(output) == len(expected_output)\n    for entity, expected_entity in zip(output, expected_output, strict=False):\n        assert entity.model_dump() == expected_entity.model_dump()\n\n\nipytest.run()  # Run the tests in Jupyter Notebook\n</pre> import ipytest  # noqa: E402 import pytest  # noqa: E402  ipytest.autoconfig()   test_cases = [     (         \"\"\"     The multinational conglomerate Alphabet Inc., parent company of Google, has acquired      DeepMind, a leading AI research laboratory based in London. DeepMind's founder,      Demis Hassabis, will join Google Brain, a division of Google AI, as Chief AI Scientist.      This move strengthens Alphabet's position in the AI field, challenging competitors like      OpenAI, which is backed by Microsoft, and Facebook AI Research, a part of Meta Platforms Inc.         \"\"\",         [             NestedEntity(                 entity=\"Alphabet Inc.\",                 label=\"ORGANIZATION\",                 parent=None,                 children=[                     NestedEntity(                         entity=\"Google\",                         label=\"ORGANIZATION\",                         parent=\"Alphabet Inc.\",                         children=[                             NestedEntity(                                 entity=\"Google Brain\",                                 label=\"ORGANIZATION\",                                 parent=\"Google\",                                 children=[],                             ),                             NestedEntity(                                 entity=\"Google AI\",                                 label=\"ORGANIZATION\",                                 parent=\"Google\",                                 children=[                                     NestedEntity(                                         entity=\"Google Brain\",                                         label=\"ORGANIZATION\",                                         parent=\"Google AI\",                                         children=[],                                     )                                 ],                             ),                         ],                     ),                     NestedEntity(                         entity=\"DeepMind\",                         label=\"ORGANIZATION\",                         parent=\"Alphabet Inc.\",                         children=[                             NestedEntity(                                 entity=\"Demis Hassabis\",                                 label=\"PERSON\",                                 parent=\"DeepMind\",                                 children=[],                             )                         ],                     ),                 ],             ),             NestedEntity(entity=\"London\", label=\"LOCATION\", parent=None, children=[]),             NestedEntity(                 entity=\"Demis Hassabis\", label=\"PERSON\", parent=None, children=[]             ),             NestedEntity(                 entity=\"OpenAI\",                 label=\"ORGANIZATION\",                 parent=None,                 children=[                     NestedEntity(                         entity=\"Microsoft\",                         label=\"ORGANIZATION\",                         parent=\"OpenAI\",                         children=[],                     )                 ],             ),             NestedEntity(                 entity=\"Facebook AI Research\",                 label=\"ORGANIZATION\",                 parent=None,                 children=[                     NestedEntity(                         entity=\"Meta Platforms Inc.\",                         label=\"ORGANIZATION\",                         parent=\"Facebook AI Research\",                         children=[],                     )                 ],             ),             NestedEntity(                 entity=\"Meta Platforms Inc.\",                 label=\"ORGANIZATION\",                 parent=None,                 children=[],             ),             NestedEntity(                 entity=\"Microsoft\", label=\"ORGANIZATION\", parent=None, children=[]             ),         ],     ), ]   @pytest.mark.parametrize(\"text,expected_output\", test_cases) def test_nested_ner(text: str, expected_output: list[NestedEntity]):     output = nested_ner(text)     assert len(output) == len(expected_output)     for entity, expected_entity in zip(output, expected_output, strict=False):         assert entity.model_dump() == expected_entity.model_dump()   ipytest.run()  # Run the tests in Jupyter Notebook <p>It's important to heavily test any system before you put it in practice. The above example demonstrates how to test such a method (<code>nested_ner</code> in this case), but it only shows a single input/output pair for brevity.</p> <p>We strongly encourage you to write far more robust tests in your applications with many more test cases. This is why our examples uses <code>@pytest.mark.parametrize</code> to easily include additional test cases.</p>"},{"location":"tutorials/more_advanced/named_entity_recognition/#named-entity-recognition","title":"Named Entity Recognition\u00b6","text":"<p>This guide demonstrates techniques to perform Named Entity Recognition (NER) using Large Language Models (LLMs) with various levels of nested entity recognition. We'll use Groq's llama-3.1-8b-instant model, but you can adapt this approach to other models with similar capabilities.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Model</li> </ul> <p>Background</p> <p> Named Entity Recognition is a subtask of information extraction that seeks to locate and classify named entities in text into predefined categories such as person names, organizations, locations, etc. LLMs have revolutionized NER by enabling more context-aware and hierarchical entity recognition, going beyond traditional rule-based or statistical methods. </p> <p>LLMs are not trained specifically for NER</p> <p> It's worth noting that there are models that are trained specifically for NER (such as GLiNER). These models are often much smaller and cheapr and can often get better results for the right tasks. LLMs should generally be reserved for quick and dirty prototyping for NER or for tasks that may require a more nuanced, open-ended language-based approach. For example, an NER system that accepts user input to guide the system by be easier to build using LLMs than a traditionally trained NER-specific model. </p>"},{"location":"tutorials/more_advanced/named_entity_recognition/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/more_advanced/named_entity_recognition/#simple-ner","title":"Simple NER\u00b6","text":"<p>We'll implement NER with different levels of complexity: simple and nested entity recognition. Let's start with the simple version:</p>"},{"location":"tutorials/more_advanced/named_entity_recognition/#nested-ner","title":"Nested NER\u00b6","text":"<p>Now, let's implement a more sophisticated version that can handle nested entities:</p>"},{"location":"tutorials/more_advanced/named_entity_recognition/#testing","title":"Testing\u00b6","text":"<p>To ensure robustness, it's crucial to test the NER system with diverse scenarios. Here's a function to run multiple test cases:</p>"},{"location":"tutorials/more_advanced/named_entity_recognition/#further-improvements","title":"Further Improvements\u00b6","text":"<p>This Named Entity Recognition (NER) system leverages the power of LLMs to perform context-aware, hierarchical entity extraction with various levels of nesting. It can identify complex relationships between entities, making it suitable for a wide range of applications.</p> <p>Additional Real-World Applications</p> <ul> <li>Information Extraction: Extracting structured information from unstructured text data.</li> <li>Question Answering: Identifying entities relevant to a given question.</li> <li>Document Summarization: Summarizing documents by extracting key entities and relationships.</li> <li>Sentiment Analysis: Analyzing sentiment towards specific entities or topics.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Prompt customization to guide the model towards specific entity types or relationships.</li> <li>Fine-tuning the model on domain-specific data for better accuracy in particular fields.</li> <li>Implementing a confidence score for each identified entity.</li> <li>Integrating with a knowledge base to enhance entity disambiguation.</li> <li>Developing a post-processing step to refine and validate the LLM's output.</li> <li>Exploring ways to optimize performance for real-time applications.</li> </ul> <p>By leveraging the power of LLMs and the flexibility of the Mirascope library, you can create sophisticated NER systems that go beyond traditional approaches, enabling more nuanced and context-aware entity recognition for various applications.</p>"},{"location":"tutorials/more_advanced/o1_style_thinking/","title":"o1 Style Thinking","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[groq]\" \n!pip install datetime\n</pre> !pip install \"mirascope[groq]\"  !pip install datetime In\u00a0[\u00a0]: Copied! <pre># Set the appropriate API key for the provider you're using\n# Here we are using GROQ_API_KEY\n\nexport GROQ_API=\"Your API Key\"\n</pre> # Set the appropriate API key for the provider you're using # Here we are using GROQ_API_KEY  export GROQ_API=\"Your API Key\" In\u00a0[2]: Copied! <pre>from datetime import datetime\n\nfrom mirascope.core import groq\n\nhistory: list[dict[str, str]] = []\n\n\n@groq.call(\"llama-3.1-8b-instant\")\ndef generate_answer(question: str) -&gt; str:\n    return f\"Generate an answer to this question: {question}\"\n\n\ndef run() -&gt; None:\n    question: str = \"how many s's in the word mississssippi\"\n    response: str = generate_answer(question)\n    print(f\"(User): {question}\")\n    print(f\"(Assistant): {response}\")\n    history.append({\"role\": \"user\", \"content\": question})\n    history.append({\"role\": \"assistant\", \"content\": response})\n\n\nrun()\n</pre> from datetime import datetime  from mirascope.core import groq  history: list[dict[str, str]] = []   @groq.call(\"llama-3.1-8b-instant\") def generate_answer(question: str) -&gt; str:     return f\"Generate an answer to this question: {question}\"   def run() -&gt; None:     question: str = \"how many s's in the word mississssippi\"     response: str = generate_answer(question)     print(f\"(User): {question}\")     print(f\"(Assistant): {response}\")     history.append({\"role\": \"user\", \"content\": question})     history.append({\"role\": \"assistant\", \"content\": response})   run() <pre>(User): how many s's in the word mississssippi\n(Assistant): There are 5 s's in the word 'mississippi'.\n</pre> <p>In this example, the zero-shot method is used to generate the output. The model is not provided with any additional information or context to help it generate the output. The model is only given the input prompt and asked to generate the output.</p> <p>This is not so effective when there is a logcial task to be performed.</p> <p>Now let's see how the model performs on this task when it can reason using Chain-of-Thought Reasoning.</p> In\u00a0[43]: Copied! <pre>from mirascope.core import groq\n\n\nhistory: list[dict] = []\n\nMODEL = \"llama-3.1-8b-instant\"\n\n\n@groq.call(MODEL)\ndef cot_step(prompt: str, step_number: int, previous_steps: str) -&gt; str:\n    return f\"\"\"\n    You are an expert AI assistant that explains your reasoning step by step. For this step, provide a title that describes what you're doing, along with the content. Decide if you need another step or if you're ready to give the final answer.\n\n    USE AS MANY REASONING STEPS AS POSSIBLE. AT LEAST 3. BE AWARE OF YOUR LIMITATIONS AS AN LLM AND WHAT YOU CAN AND CANNOT DO. IN YOUR REASONING, INCLUDE EXPLORATION OF ALTERNATIVE ANSWERS. CONSIDER YOU MAY BE WRONG, AND IF YOU ARE WRONG IN YOUR REASONING, WHERE IT WOULD BE. FULLY TEST ALL OTHER POSSIBILITIES. YOU CAN BE WRONG. WHEN YOU SAY YOU ARE RE-EXAMINING, ACTUALLY RE-EXAMINE, AND USE ANOTHER APPROACH TO DO SO. DO NOT JUST SAY YOU ARE RE-EXAMINING. USE AT LEAST 3 METHODS TO DERIVE THE ANSWER. USE BEST PRACTICES.\n\n    IMPORTANT: DO NOT USE CODE BLOCKS OR PROGRAMMING EXAMPLES IN YOUR REASONING. EXPLAIN YOUR PROCESS IN PLAIN LANGUAGE.\n\n    This is step number {step_number}.\n\n    Question: {prompt}\n\n    Previous steps:\n    {previous_steps}\n\n    Respond in the following format:\n    Step {step_number}: [Title]\n    [Content]\n    Next action: [continue/final_answer]\n    \"\"\"\n\n\n@groq.call(MODEL)\ndef final_answer(prompt: str, reasoning: str) -&gt; str:\n    return f\"\"\"\n    Based on the following chain of reasoning, provide a final answer to the question. Only provide the text response without any titles or preambles. Retain any formatting as instructed by the original prompt, such as exact formatting for free response or multiple choice.\n\n    Question: {prompt}\n\n    Reasoning:\n    {reasoning}\n\n    Final Answer:\n    \"\"\"\n\n\ndef generate_cot_response(\n    user_query: str,\n) -&gt; tuple[list[tuple[str, str, float]], float]:\n    steps: list[tuple[str, str, float]] = []\n    total_thinking_time: float = 0.0\n    step_count: int = 1\n    reasoning: str = \"\"\n    previous_steps: str = \"\"\n\n    while True:\n        start_time: datetime = datetime.now()\n        step_result: str = cot_step(user_query, step_count, previous_steps).content\n        end_time: datetime = datetime.now()\n        thinking_time: float = (end_time - start_time).total_seconds()\n\n        lines = step_result.strip().split(\"\\n\")\n        title = lines[0].split(\": \", 1)[1]\n        content = \"\\n\".join(lines[1:-1])\n        next_action = lines[-1].split(\": \", 1)[1]\n\n        steps.append((f\"Step {step_count}: {title}\", content, thinking_time))\n        total_thinking_time += thinking_time\n\n        reasoning += f\"\\n{step_result}\\n\"\n        previous_steps += f\"\\n{step_result}\\n\"\n\n        if next_action.lower() == \"final_answer\" or step_count &gt;= 5:\n            break\n\n        step_count += 1\n\n    # Generate final answer\n    start_time = datetime.now()\n    final_result: str = final_answer(user_query, reasoning).content\n    end_time = datetime.now()\n    thinking_time = (end_time - start_time).total_seconds()\n    total_thinking_time += thinking_time\n\n    steps.append((\"Final Answer\", final_result, thinking_time))\n\n    return steps, total_thinking_time\n\n\ndef display_cot_response(\n    steps: list[tuple[str, str, float]], total_thinking_time: float\n) -&gt; None:\n    for title, content, thinking_time in steps:\n        print(f\"{title}:\")\n        print(content.strip())\n        print(f\"**Thinking time: {thinking_time:.2f} seconds**\\n\")\n\n    print(f\"**Total thinking time: {total_thinking_time:.2f} seconds**\")\n\n\ndef run() -&gt; None:\n    question: str = \"How many s's are in the word 'mississssippi'?\"\n    print(\"(User):\", question)\n    # Generate COT response\n    steps, total_thinking_time = generate_cot_response(question)\n    display_cot_response(steps, total_thinking_time)\n\n    # Add the interaction to the history\n    history.append({\"role\": \"user\", \"content\": question})\n    history.append(\n        {\"role\": \"assistant\", \"content\": steps[-1][1]}\n    )  # Add only the final answer to the history\n\n\n# Run the function\n\nrun()\n</pre> from mirascope.core import groq   history: list[dict] = []  MODEL = \"llama-3.1-8b-instant\"   @groq.call(MODEL) def cot_step(prompt: str, step_number: int, previous_steps: str) -&gt; str:     return f\"\"\"     You are an expert AI assistant that explains your reasoning step by step. For this step, provide a title that describes what you're doing, along with the content. Decide if you need another step or if you're ready to give the final answer.      USE AS MANY REASONING STEPS AS POSSIBLE. AT LEAST 3. BE AWARE OF YOUR LIMITATIONS AS AN LLM AND WHAT YOU CAN AND CANNOT DO. IN YOUR REASONING, INCLUDE EXPLORATION OF ALTERNATIVE ANSWERS. CONSIDER YOU MAY BE WRONG, AND IF YOU ARE WRONG IN YOUR REASONING, WHERE IT WOULD BE. FULLY TEST ALL OTHER POSSIBILITIES. YOU CAN BE WRONG. WHEN YOU SAY YOU ARE RE-EXAMINING, ACTUALLY RE-EXAMINE, AND USE ANOTHER APPROACH TO DO SO. DO NOT JUST SAY YOU ARE RE-EXAMINING. USE AT LEAST 3 METHODS TO DERIVE THE ANSWER. USE BEST PRACTICES.      IMPORTANT: DO NOT USE CODE BLOCKS OR PROGRAMMING EXAMPLES IN YOUR REASONING. EXPLAIN YOUR PROCESS IN PLAIN LANGUAGE.      This is step number {step_number}.      Question: {prompt}      Previous steps:     {previous_steps}      Respond in the following format:     Step {step_number}: [Title]     [Content]     Next action: [continue/final_answer]     \"\"\"   @groq.call(MODEL) def final_answer(prompt: str, reasoning: str) -&gt; str:     return f\"\"\"     Based on the following chain of reasoning, provide a final answer to the question. Only provide the text response without any titles or preambles. Retain any formatting as instructed by the original prompt, such as exact formatting for free response or multiple choice.      Question: {prompt}      Reasoning:     {reasoning}      Final Answer:     \"\"\"   def generate_cot_response(     user_query: str, ) -&gt; tuple[list[tuple[str, str, float]], float]:     steps: list[tuple[str, str, float]] = []     total_thinking_time: float = 0.0     step_count: int = 1     reasoning: str = \"\"     previous_steps: str = \"\"      while True:         start_time: datetime = datetime.now()         step_result: str = cot_step(user_query, step_count, previous_steps).content         end_time: datetime = datetime.now()         thinking_time: float = (end_time - start_time).total_seconds()          lines = step_result.strip().split(\"\\n\")         title = lines[0].split(\": \", 1)[1]         content = \"\\n\".join(lines[1:-1])         next_action = lines[-1].split(\": \", 1)[1]          steps.append((f\"Step {step_count}: {title}\", content, thinking_time))         total_thinking_time += thinking_time          reasoning += f\"\\n{step_result}\\n\"         previous_steps += f\"\\n{step_result}\\n\"          if next_action.lower() == \"final_answer\" or step_count &gt;= 5:             break          step_count += 1      # Generate final answer     start_time = datetime.now()     final_result: str = final_answer(user_query, reasoning).content     end_time = datetime.now()     thinking_time = (end_time - start_time).total_seconds()     total_thinking_time += thinking_time      steps.append((\"Final Answer\", final_result, thinking_time))      return steps, total_thinking_time   def display_cot_response(     steps: list[tuple[str, str, float]], total_thinking_time: float ) -&gt; None:     for title, content, thinking_time in steps:         print(f\"{title}:\")         print(content.strip())         print(f\"**Thinking time: {thinking_time:.2f} seconds**\\n\")      print(f\"**Total thinking time: {total_thinking_time:.2f} seconds**\")   def run() -&gt; None:     question: str = \"How many s's are in the word 'mississssippi'?\"     print(\"(User):\", question)     # Generate COT response     steps, total_thinking_time = generate_cot_response(question)     display_cot_response(steps, total_thinking_time)      # Add the interaction to the history     history.append({\"role\": \"user\", \"content\": question})     history.append(         {\"role\": \"assistant\", \"content\": steps[-1][1]}     )  # Add only the final answer to the history   # Run the function  run() <pre>(User): How many s's are in the word 'mississssippi'?\nStep 1: Identifying the Task and Clarifying the Question:\nThe task requires me to determine the number of 's' surnames in the word 'mississssippi'. However, the terminology 'surnames' might be misleading here since we are talking about the word 'mississssippi', a word that consists of multiple letters and not names.\n\nAssuming the correct interpretation of the task is to count the number of 's' occurrences in the word 'mississssippi', it seems we are dealing with a relatively straightforward task. The first step would be:\n\n1.1: Breaking down the word 'mississssippi' into individual letters to visually inspect and manipulate the letters. By doing this, we can then observe the number of times the letter 's' repeats.\n\nUpon visual inspection of the letters in 'mississssippi', the word contains:\n\nm-i-s-s-i-s-s-s-s-s-i-p-p-i\n\n1.2: Observing the consistency of my visual inspection of the letters by reading them together. Analyzing each character, the s in the given is counted when its correct alphabetical order. While in the alphabetical order A to Z, every character remains the same, when specifically choosing one letter, sometimes using them just for individual purposes it might be understood in a personal way of showing with alphabetic order.\n**Thinking time: 1.40 seconds**\n\nStep 2: Identifying the Consistent Pattern of Consecutive 's's:\nUpon breaking down the word 'mississssippi' into individual letters and visually inspecting them, I notice that the sequence of consecutive 's's follows a consistent pattern. To confirm this observation, I will count the consecutive occurrences of 's' in the word, separating the non-'s' characters from the consecutive sequence of 's's.\n\nFrom the original visual inspection of the letters:\nm-i-s-s-i-s-s-s-s-s-i-p-p-i\n\nBreaking down the consecutive 's' sequence:\n- 's' appears as the 3rd letter\n- it appears 6 more times in a row consecutively\n\nThe non-'s' characters in the word 'mississssippi' might be used to temporarily separate the sequences, looking at the initial sequence this occurs at: 'i-s-s-i'. Now using each 's' for counting, we are examining specific sequences, determining when another 's' is encountered. \n\nBy focusing only on the sequence of consecutive 's's in the word, I can see that it occurs 9 times in the given word.\n\nHowever, to avoid undercounting or overcounting, I will re-examine this task using an alternative approach.\n**Thinking time: 1.43 seconds**\n\nStep 3: Re-examining the Task Using a Counting Approach:\nIn an attempt to validate the initial observation of consecutive 's' sequences and their count, I will employ a direct counting method to determine the number of 's' occurrences in the word 'mississssippi'. This approach may offer an alternative perspective, confirming or refuting the initial count based on the visually inspected sequence.\n\nUpon choosing the word \"mississssippi\" and placing all individual  letters in one line we get:\nm - i - s - s - i - s - s - s - s - s - i - p - p - i\n\nI will start at the very beginning and manually count the number of times the letter 's' appears. Keeping track of the count can help eliminate any potential biases or overlooks, also maintaining constant awareness with the objective goal. This straightforward counting process can provide reassurance that the observations from the initial and alternative inspection methods are accurate.\n\n1. Counting the first 's' at the 3rd position,\n2. The next consecutive 's' appears afterward, at the 4th position,\n3. Counting the consecutive 's' sequences, note the 's' at 4 is indeed followed by an additional 's' located at position 5.\n4. The next 's' appears after that one, located at 6, but of course that is obviously a following 's'.\n5. And so does the following 's's appear until it finishes counting.\nThis manual counting might potentially show results different from the previously established ideas based upon where multiple 's's would appear at many places.\n\nAt this point, we have counted 9 's's in the manual approach. However, I will further validate the results using an elimination-comparison method to ensure the reliability and consistency of our initial analysis.\n**Thinking time: 1.64 seconds**\n\nStep 4: Validation Through an Elimination-Comparison Method:\nTo further validate the results and ensure the reliability and consistency of our initial analysis, I will employ an elimination-comparison method. This involves cross-checking the counts obtained from the visually inspected sequences and the direct counting method.\n\nThe manual count from Step 3 manually counting 's's matches the count from the initial observation of consecutive 's' sequences in the word. To confirm this, I will compare each manual count with the expected number of 's's in each sequence.\n\n1. The sequence starts with the character 'm', which is not an 's'.\n2. In the first sequence (m, i, s, s, i, s, ...), we observe 3 's's. Since 'm' and 'i' appear before the first 's', they do not contribute to the count for the consecutive 's' sequence. Therefore, this count aligns with the direct counting method.\n3. For each subsequent sequence, we can confirm the number of 's's in the direct counting method matches the observed count of consecutive 's's.\n\nUpon analyzing each sequence and matching the counts, I confirm that the manual count of 9 's's matches the initial observation and the counts from the other approaches. Using this method, we have exhaustively validated our initial count and ensured that it is consistent across different inspection techniques.\n**Thinking time: 1.45 seconds**\n\nFinal Answer:\n6\n**Thinking time: 1.01 seconds**\n\n**Total thinking time: 6.92 seconds**\n</pre> <p>As demonstrated in the COT Reasoning example, we can guide the model to break down the task into multiple steps and generate a coherent output. This allows the model to solve complex tasks in logical steps. However, this requires multiple calls to the model, which may be expensive in terms of cost and time. Also model may not always identify the correct steps to solve the task, hence is not deterministic.</p>"},{"location":"tutorials/more_advanced/o1_style_thinking/#o1-style-thinking","title":"o1 Style Thinking\u00b6","text":"<p>In this recipe, we will show how to achieve Chain-of-Thought Reasoning. This makes LLMs to breakdown the task in multiple steps and generate a coherent output allowing to solve complex tasks in logical steps.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p>     Large Language Models (LLMs) are known to generate text that is coherent and fluent. However, they often struggle with tasks that require multi-step reasoning or logical thinking. In this recipe, we will show how to use Mirascope to guide the LLM to break down the task into multiple steps and generate a coherent output.  </p>"},{"location":"tutorials/more_advanced/o1_style_thinking/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/more_advanced/o1_style_thinking/#without-chain-of-thought-reasoning","title":"Without Chain-of-Thought Reasoning\u00b6","text":"<p>We will begin by showing how a typical LLM performs on a task that requires multi-step reasoning. In this example, we will ask the model to generate a count the number of <code>s</code>s in the word <code>Mississssippi</code> (Yes it has 7<code>s</code>'s). We will use the <code>llama-3.1-8b-instant</code> for this example.</p>"},{"location":"tutorials/more_advanced/o1_style_thinking/#with-chain-of-thought-reasoning","title":"With Chain of Thought Reasoning\u00b6","text":""},{"location":"tutorials/more_advanced/o1_style_thinking/#conclusion","title":"Conclusion\u00b6","text":"<p>Chain of Thought Reasoning is a powerful technique that allows LLMs to solve complex tasks in logical steps. However, it requires multiple calls to the model and may not always identify the correct steps to solve the task. This technique can be useful when the task requires multi-step reasoning or logical thinking.</p> <p>Care should be taken to ensure that the model is guided correctly and that the output is coherent and accurate.</p>"},{"location":"tutorials/more_advanced/pii_scrubbing/","title":"PII Scrubbing","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\nfrom openai import OpenAI\n\nPII_DEFINITION = \"\"\"\nAny representation of information that permits the identity of an individual to whom \nthe information applies to be reasonably inferred by either direct or indirect means. \nFurther, PII is defined as information: (i) that directly identifies an \nindividual (e.g., name, address, social security number or other identifying \nnumber or code, telephone number, email address, etc.) or (ii) by which an agency \nintends to identify specific individuals in conjunction with other data elements, \ni.e., indirect identification. (These data elements may include a combination of gender, \nrace, birth date, geographic indicator, and other descriptors). Additionally, \ninformation permitting the physical or online contacting of a specific individual is \nthe same as personally identifiable information. This information can be maintained \nin either paper, electronic or other media.\n\"\"\"\n\n\n@openai.call(\n    model=\"llama3.1\",\n    client=OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n    json_mode=True,\n    response_model=bool,\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are an expert at identifying personally identifiable information (PII).\n    Using the following definition of PII, \n    determine if the article contains PII with True or False?\n\n    Definition of PII: {PII_DEFINITION}\n\n    USER: {article}\n    \"\"\"\n)\ndef check_if_pii_exists(article: str) -&gt; openai.OpenAIDynamicConfig:\n    return {\"computed_fields\": {\"PII_DEFINITION\": PII_DEFINITION}}\n</pre> from mirascope.core import openai, prompt_template from openai import OpenAI  PII_DEFINITION = \"\"\" Any representation of information that permits the identity of an individual to whom  the information applies to be reasonably inferred by either direct or indirect means.  Further, PII is defined as information: (i) that directly identifies an  individual (e.g., name, address, social security number or other identifying  number or code, telephone number, email address, etc.) or (ii) by which an agency  intends to identify specific individuals in conjunction with other data elements,  i.e., indirect identification. (These data elements may include a combination of gender,  race, birth date, geographic indicator, and other descriptors). Additionally,  information permitting the physical or online contacting of a specific individual is  the same as personally identifiable information. This information can be maintained  in either paper, electronic or other media. \"\"\"   @openai.call(     model=\"llama3.1\",     client=OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),     json_mode=True,     response_model=bool, ) @prompt_template(     \"\"\"     SYSTEM:     You are an expert at identifying personally identifiable information (PII).     Using the following definition of PII,      determine if the article contains PII with True or False?      Definition of PII: {PII_DEFINITION}      USER: {article}     \"\"\" ) def check_if_pii_exists(article: str) -&gt; openai.OpenAIDynamicConfig:     return {\"computed_fields\": {\"PII_DEFINITION\": PII_DEFINITION}} <p>Using Mirascope\u2019s <code>response_model</code> we first detect if PII exists and return a <code>bool</code> , this will determine our next steps.</p> In\u00a0[2]: Copied! <pre>PII_ARTICLE = \"\"\"\nJohn Doe, born on 12/07/1985, resides at 123 Ruecker Harbor in Goodwinshire, WI. \nHis Social Security number is 325-21-4386 and he can be reached at (123) 456-7890. \n\"\"\"\n\ndoes_pii_exist = check_if_pii_exists(PII_ARTICLE)\nprint(does_pii_exist)\n</pre> PII_ARTICLE = \"\"\" John Doe, born on 12/07/1985, resides at 123 Ruecker Harbor in Goodwinshire, WI.  His Social Security number is 325-21-4386 and he can be reached at (123) 456-7890.  \"\"\"  does_pii_exist = check_if_pii_exists(PII_ARTICLE) print(does_pii_exist) <pre>True\n</pre> In\u00a0[3]: Copied! <pre>@openai.call(\n    model=\"llama3.1\",\n    client=OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are an expert at redacting personally identifiable information (PII).\n    Replace the PII in the following article with context words.\n\n    If PII exists in the article, replace it with context words. For example, if the\n    phone number is 123-456-7890, replace it with [PHONE_NUMBER].\n\n    USER: {article}\n    \"\"\"\n)\ndef scrub_pii(article: str): ...\n\n\ndef run():\n    does_pii_exist = check_if_pii_exists(PII_ARTICLE)\n    print(does_pii_exist)\n    # Output:\n    # True\n    if does_pii_exist:\n        return scrub_pii(PII_ARTICLE)\n    else:\n        return \"No PII found in the article.\"\n\n\nprint(run())\n</pre> @openai.call(     model=\"llama3.1\",     client=OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"), ) @prompt_template(     \"\"\"     SYSTEM:     You are an expert at redacting personally identifiable information (PII).     Replace the PII in the following article with context words.      If PII exists in the article, replace it with context words. For example, if the     phone number is 123-456-7890, replace it with [PHONE_NUMBER].      USER: {article}     \"\"\" ) def scrub_pii(article: str): ...   def run():     does_pii_exist = check_if_pii_exists(PII_ARTICLE)     print(does_pii_exist)     # Output:     # True     if does_pii_exist:         return scrub_pii(PII_ARTICLE)     else:         return \"No PII found in the article.\"   print(run()) <pre>True\n[NAME], born on [BIRTH_DATE], resides at [ADDRESS] in [CITY], [STATE]. His [IDENTIFICATION_NUMBER] is [SOCIAL_SECURITY_NUMBER] and he can be reached at [PHONE_NUMBER].\n</pre> <p>Additional Real-World Applications</p> <ul> <li>Medical Records: Iterate on the above recipe and scrub any PII when sharing patient data for research.</li> <li>Legal Documents: Court documents and legal filings frequently contain sensitive information that needs to be scrubbed before public release.</li> <li>Corporate Financial Reports: Companies may need to scrub proprietary financial data or trade secrets when sharing reports with external auditors or regulators.</li> <li>Social Media Content Moderation: Automatically scrub or blur out personal information like phone numbers or addresses posted in public comments.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <pre><code>- Use a larger model hosted on prem or in a private location to prevent data leaks.\n- Refine the prompts for specific types of information you want scrubbed.\n- Run the `check_if_pii_exists` call after scrubbing PII to check if all PII has been scrubbed.</code></pre>"},{"location":"tutorials/more_advanced/pii_scrubbing/#pii-scrubbing","title":"PII Scrubbing\u00b6","text":"<p>In this recipe, we go over how to detect Personal Identifiable Information, or PII and redact it from your source. Whether your source is from a database, a document, or spreadsheet, it is important prevent PII from leaving your system. We will be using Ollama for data privacy.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Model</li> </ul> <p>Background</p> <p> Prior to Natural Language Processing (NLP) and Named Entity Recognition (NER) techniques, scrubbing or redacting sensitive information was a time-consuming manual task. LLMs have improved on this by being able to understand context surrounding sensitive information. </p>"},{"location":"tutorials/more_advanced/pii_scrubbing/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/pii_scrubbing/#create-your-prompt","title":"Create your prompt\u00b6","text":"<p>The first step is to grab the definition of PII for our prompt to use. Note that in this example we will be using US Labor Laws so be sure to use your countries definition. We can access the definition here.</p>"},{"location":"tutorials/more_advanced/pii_scrubbing/#verify-the-prompt-quality","title":"Verify the prompt quality\u00b6","text":"<p>We will be using a fake document to test the accuracy of our prompt:</p>"},{"location":"tutorials/more_advanced/pii_scrubbing/#redact-pii","title":"Redact PII\u00b6","text":"<p>For articles that are flagged as containing PII, we now need to redact that information if we are still planning on sending that document. We create another prompt specific to redacting data by provide an example for the LLM to use:</p>"},{"location":"tutorials/more_advanced/query_plan/","title":"Query Plan","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[anthropic]\"\n</pre> !pip install \"mirascope[anthropic]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass Query(BaseModel):\n    id: int = Field(..., description=\"ID of the query, this is auto-incremented\")\n    question: str = Field(\n        ...,\n        description=\"The broken down question to be answered to answer the main question\",\n    )\n    dependencies: list[int] = Field(\n        description=\"List of sub questions that need to be answered before asking this question\",\n    )\n    tools: list[str] = Field(\n        description=\"List of tools that should be used to answer the question\"\n    )\n</pre> from pydantic import BaseModel, Field   class Query(BaseModel):     id: int = Field(..., description=\"ID of the query, this is auto-incremented\")     question: str = Field(         ...,         description=\"The broken down question to be answered to answer the main question\",     )     dependencies: list[int] = Field(         description=\"List of sub questions that need to be answered before asking this question\",     )     tools: list[str] = Field(         description=\"List of tools that should be used to answer the question\"     ) <p>Each query is assigned a unique ID, which can reference other queries for dependencies. We also provide necessary tools and the relevant portion of the broken-down question to each query.</p> In\u00a0[2]: Copied! <pre>import json\n\n\ndef get_weather_by_year(year: int):\n    \"\"\"Made up data to get Tokyo weather by year\"\"\"\n    if year == 2020:\n        data = {\n            \"jan\": 42,\n            \"feb\": 43,\n            \"mar\": 49,\n            \"apr\": 58,\n            \"may\": 66,\n            \"jun\": 72,\n            \"jul\": 78,\n            \"aug\": 81,\n            \"sep\": 75,\n            \"oct\": 65,\n            \"nov\": 55,\n            \"dec\": 47,\n        }\n    elif year == 2021:\n        data = {\n            \"jan\": 45,\n            \"feb\": 48,\n            \"mar\": 52,\n            \"apr\": 60,\n            \"may\": 68,\n            \"jun\": 74,\n            \"jul\": 80,\n            \"aug\": 83,\n            \"sep\": 77,\n            \"oct\": 67,\n            \"nov\": 57,\n            \"dec\": 49,\n        }\n    else:\n        data = {\n            \"jan\": 48,\n            \"feb\": 52,\n            \"mar\": 56,\n            \"apr\": 64,\n            \"may\": 72,\n            \"jun\": 78,\n            \"jul\": 84,\n            \"aug\": 87,\n            \"sep\": 81,\n            \"oct\": 71,\n            \"nov\": 61,\n            \"dec\": 53,\n        }\n    return json.dumps(data)\n</pre> import json   def get_weather_by_year(year: int):     \"\"\"Made up data to get Tokyo weather by year\"\"\"     if year == 2020:         data = {             \"jan\": 42,             \"feb\": 43,             \"mar\": 49,             \"apr\": 58,             \"may\": 66,             \"jun\": 72,             \"jul\": 78,             \"aug\": 81,             \"sep\": 75,             \"oct\": 65,             \"nov\": 55,             \"dec\": 47,         }     elif year == 2021:         data = {             \"jan\": 45,             \"feb\": 48,             \"mar\": 52,             \"apr\": 60,             \"may\": 68,             \"jun\": 74,             \"jul\": 80,             \"aug\": 83,             \"sep\": 77,             \"oct\": 67,             \"nov\": 57,             \"dec\": 49,         }     else:         data = {             \"jan\": 48,             \"feb\": 52,             \"mar\": 56,             \"apr\": 64,             \"may\": 72,             \"jun\": 78,             \"jul\": 84,             \"aug\": 87,             \"sep\": 81,             \"oct\": 71,             \"nov\": 61,             \"dec\": 53,         }     return json.dumps(data) In\u00a0[3]: Copied! <pre>from mirascope.core import anthropic, prompt_template\n\n\n@anthropic.call(\n    model=\"claude-3-5-sonnet-20240620\", response_model=list[Query], json_mode=True\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are an expert at creating a query plan for a question.\n    You are given a question and you need to create a query plan for it.\n    You need to create a list of queries that can be used to answer the question.\n\n    You have access to the following tool:\n    - get_weather_by_year\n    USER:\n    {question}\n    \"\"\"\n)\ndef create_query_plan(question: str): ...\n</pre> from mirascope.core import anthropic, prompt_template   @anthropic.call(     model=\"claude-3-5-sonnet-20240620\", response_model=list[Query], json_mode=True ) @prompt_template(     \"\"\"     SYSTEM:     You are an expert at creating a query plan for a question.     You are given a question and you need to create a query plan for it.     You need to create a list of queries that can be used to answer the question.      You have access to the following tool:     - get_weather_by_year     USER:     {question}     \"\"\" ) def create_query_plan(question: str): ... <p>We set the <code>response_model</code> to the <code>Query</code> object we just defined. We also prompt the call to add tools as necessary to the individual <code>Query</code>. Now we make a call to the LLM:</p> In\u00a0[4]: Copied! <pre>query_plan = create_query_plan(\"Compare the weather in Tokyo from 2020 to 2022\")\nprint(query_plan)\n</pre> query_plan = create_query_plan(\"Compare the weather in Tokyo from 2020 to 2022\") print(query_plan) <pre>[Query(id=1, question='What was the weather like in Tokyo in 2020?', dependencies=[], tools=['get_weather_by_year']), Query(id=2, question='What was the weather like in Tokyo in 2021?', dependencies=[], tools=['get_weather_by_year']), Query(id=3, question='What was the weather like in Tokyo in 2022?', dependencies=[], tools=['get_weather_by_year']), Query(id=4, question='Compare the weather data for Tokyo from 2020 to 2022', dependencies=[1, 2, 3], tools=[])]\n</pre> <p>We can see our <code>list[Query]</code> and their respective subquestions and tools needed to answer the main question. We can also see that the final question depends on the answers from the previous queries.</p> In\u00a0[5]: Copied! <pre>from anthropic.types import MessageParam\n\n\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\")\n@prompt_template(\n    \"\"\"\n    MESSAGES:\n    {history}\n    USER:\n    {question}\n    \"\"\"\n)\ndef run(\n    question: str, history: list[MessageParam], tools: list[str]\n) -&gt; anthropic.AnthropicDynamicConfig:\n    tools_fn = [eval(tool) for tool in tools]\n    return {\"tools\": tools_fn}\n\n\ndef execute_query_plan(query_plan: list[Query]):\n    results = {}\n    for query in query_plan:\n        history = []\n        for dependency in query.dependencies:\n            result = results[dependency]\n            history.append({\"role\": \"user\", \"content\": result[\"question\"]})\n            history.append({\"role\": \"assistant\", \"content\": result[\"content\"]})\n        result = run(query.question, history, query.tools)\n        if tool := result.tool:\n            output = tool.call()\n            results[query.id] = {\"question\": query.question, \"content\": output}\n        else:\n            return result.content\n    return results\n</pre> from anthropic.types import MessageParam   @anthropic.call(model=\"claude-3-5-sonnet-20240620\") @prompt_template(     \"\"\"     MESSAGES:     {history}     USER:     {question}     \"\"\" ) def run(     question: str, history: list[MessageParam], tools: list[str] ) -&gt; anthropic.AnthropicDynamicConfig:     tools_fn = [eval(tool) for tool in tools]     return {\"tools\": tools_fn}   def execute_query_plan(query_plan: list[Query]):     results = {}     for query in query_plan:         history = []         for dependency in query.dependencies:             result = results[dependency]             history.append({\"role\": \"user\", \"content\": result[\"question\"]})             history.append({\"role\": \"assistant\", \"content\": result[\"content\"]})         result = run(query.question, history, query.tools)         if tool := result.tool:             output = tool.call()             results[query.id] = {\"question\": query.question, \"content\": output}         else:             return result.content     return results <p>Using Mirascope\u2019s <code>DynamicConfig</code> , we can pass in the tools from the query plan into our LLM call. We also add history to the calls that have dependencies.</p> <p>Now we run <code>execute_query_plan</code>:</p> In\u00a0[6]: Copied! <pre>result = execute_query_plan(query_plan)\nprint(result)\n</pre> result = execute_query_plan(query_plan) print(result) <pre>Comparing the weather data for Tokyo from 2020 to 2022, we can observe the following trends:\n\n1. Overall warming trend:\n   - There's a consistent increase in temperatures across all months from 2020 to 2022.\n   - The average annual temperature has risen each year.\n\n2. Monthly comparisons:\n   - January: 42\u00b0F (2020) \u2192 45\u00b0F (2021) \u2192 48\u00b0F (2022)\n   - July: 78\u00b0F (2020) \u2192 80\u00b0F (2021) \u2192 84\u00b0F (2022)\n   - December: 47\u00b0F (2020) \u2192 49\u00b0F (2021) \u2192 53\u00b0F (2022)\n\n3. Seasonal patterns:\n   - Winters (Dec-Feb) have become milder each year.\n   - Summers (Jun-Aug) have become hotter each year.\n   - Spring and autumn months also show warming trends.\n\n4. Extreme temperatures:\n   - The hottest month has consistently been August, with temperatures increasing from 81\u00b0F (2020) to 87\u00b0F (2022).\n   - The coldest month has consistently been January, with temperatures rising from 42\u00b0F (2020) to 48\u00b0F (2022).\n\n5. Year-to-year changes:\n   - The temperature increase from 2020 to 2021 was generally smaller than the increase from 2021 to 2022.\n   - 2022 shows the most significant warming compared to previous years.\n\nIn summary, the data indicates a clear warming trend in Tokyo from 2020 to 2022, with each year being warmer than the last across all seasons.\n</pre> <p>Additional Real-World Examples</p> <ul> <li>Enhanced ChatBot: Provide higher quality and more accurate answers by using a query plan to answer complex questions.</li> <li>Database Administrator: Translate layperson requests into a query plan, then execute SQL commands to efficiently retrieve or manipulate data, fulfilling the user's requirements.</li> <li>Customer support: Take a user request and turn it into a query plan for easy to follow and simple instructions for troubleshooting.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <pre><code>- Agentic: Turn this example into a more flexible Agent which has access to a query plan tool.\n- Multiple providers: Use multiple LLM providers to verify whether the extracted information is accurate and not hallucination.\n- Implement Pydantic `ValidationError` and Tenacity `retry` to improve reliability and accuracy.</code></pre>"},{"location":"tutorials/more_advanced/query_plan/#query-plan","title":"Query Plan\u00b6","text":"<p>This recipe shows how to use LLMs \u2014 in this case, Anthropic\u2019s Claude 3.5 Sonnet \u2014 to create a query plan. Using a query plan is a great way to get more accurate results by breaking down a complex question into multiple smaller questions.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Tools</li> <li>Chaining</li> <li>JSON Mode</li> <li>Response Models</li> </ul>"},{"location":"tutorials/more_advanced/query_plan/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/query_plan/#create-your-query","title":"Create your Query\u00b6","text":"<p>To construct our Query Plan, we first need to define the individual queries that will comprise it using a Pydantic BaseModel:</p>"},{"location":"tutorials/more_advanced/query_plan/#create-our-tool","title":"Create our tool\u00b6","text":"<p>For the purposes of this recipe, we will define some dummy data. This tool should be replaced by web_search, a database query, or other forms of pulling data.</p>"},{"location":"tutorials/more_advanced/query_plan/#define-our-query-planner","title":"Define our Query Planner\u00b6","text":"<p>Let us prompt our LLM call to create a query plan for a particular question:</p>"},{"location":"tutorials/more_advanced/query_plan/#executing-our-query-plan","title":"Executing our Query Plan\u00b6","text":"<p>Now that we have our list of queries, we can iterate on each of the subqueries to answer our main question:</p>"},{"location":"tutorials/more_advanced/removing_semantic_duplicates/","title":"Removing Semantic Duplicates","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>movie_genres = [\n    \"sci-fi\",\n    \"romance\",\n    \"love story\",\n    \"action\",\n    \"horror\",\n    \"heist\",\n    \"crime\",\n    \"science fiction\",\n    \"fantasy\",\n    \"scary\",\n]\n</pre> movie_genres = [     \"sci-fi\",     \"romance\",     \"love story\",     \"action\",     \"horror\",     \"heist\",     \"crime\",     \"science fiction\",     \"fantasy\",     \"scary\", ] <p>To deduplicate this list, we\u2019ll extract a schema containing <code>genres</code>, the deduplicated list, and <code>duplicates</code>, a list of all duplicate items. The reason for having <code>duplicates</code> in our schema is that LLM extractions can be inconsistent, even with the most recent models - forcing it to list the duplicate items helps it reason through the call and produce a more accurate answer.</p> In\u00a0[2]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass DeduplicatedGenres(BaseModel):\n    duplicates: list[list[str]] = Field(\n        ..., description=\"A list containing lists of semantically equivalent items\"\n    )\n    genres: list[str] = Field(\n        ..., description=\"The list of genres with semantic duplicates removed\"\n    )\n</pre> from pydantic import BaseModel, Field   class DeduplicatedGenres(BaseModel):     duplicates: list[list[str]] = Field(         ..., description=\"A list containing lists of semantically equivalent items\"     )     genres: list[str] = Field(         ..., description=\"The list of genres with semantic duplicates removed\"     ) <p>We can now set this schema as our response model in a Mirascope call:</p> In\u00a0[3]: Copied! <pre>from mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=DeduplicatedGenres)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Your job is to take a list of movie genres and clean it up by removing items\n    which are semantically equivalent to one another. When coming across multiple items\n    which refer to the same genre, keep the genre name which is most commonly used.\n    For example, \"sci-fi\" and \"science fiction\" are the same genre.\n\n    USER:\n    {genres}\n    \"\"\"\n)\ndef deduplicate_genres(genres: list[str]): ...\n\n\nresponse = deduplicate_genres(movie_genres)\nassert isinstance(response, DeduplicatedGenres)\nprint(response.genres)\nprint(response.duplicates)\n</pre> from mirascope.core import openai, prompt_template   @openai.call(model=\"gpt-4o-mini\", response_model=DeduplicatedGenres) @prompt_template(     \"\"\"     SYSTEM:     Your job is to take a list of movie genres and clean it up by removing items     which are semantically equivalent to one another. When coming across multiple items     which refer to the same genre, keep the genre name which is most commonly used.     For example, \"sci-fi\" and \"science fiction\" are the same genre.      USER:     {genres}     \"\"\" ) def deduplicate_genres(genres: list[str]): ...   response = deduplicate_genres(movie_genres) assert isinstance(response, DeduplicatedGenres) print(response.genres) print(response.duplicates) <pre>['action', 'horror', 'heist', 'crime', 'fantasy', 'scary']\n[['sci-fi', 'science fiction'], ['love story', 'romance']]\n</pre> <p>Just like with a list of strings, we can create a schema of <code>DeduplicatedBooks</code> and set it as the response model, with a modified prompt to account for the different types of differences we see:</p> In\u00a0[4]: Copied! <pre>class Book(BaseModel):\n    title: str\n    author: str\n    genre: str\n\n\nduplicate_books = [\n    Book(title=\"The War of the Worlds\", author=\"H. G. Wells\", genre=\"scifi\"),\n    Book(title=\"War of the Worlds\", author=\"H.G. Wells\", genre=\"science fiction\"),\n    Book(title=\"The Sorcerer's stone\", author=\"J. K. Rowling\", genre=\"fantasy\"),\n    Book(\n        title=\"Harry Potter and The Sorcerer's stone\",\n        author=\"J. K. Rowling\",\n        genre=\"fantasy\",\n    ),\n    Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\", genre=\"fantasy\"),\n    Book(title=\"'The Name of the Wind'\", author=\"Patrick Rofuss\", genre=\"fiction\"),\n]\n\n\n@openai.call(model=\"gpt-4o\", response_model=list[Book])\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Your job is to take a database of books and clean it up by removing items which are\n    semantic duplicates. Look out for typos, formatting differences, and categorizations.\n    For example, \"Mistborn\" and \"Mistborn: The Final Empire\" are the same book \n    but \"Mistborn: Shadows of Self\" is not.\n    Then return all the unique books.\n\n    USER:\n    {books}\n    \"\"\"\n)\ndef deduplicate_books(books: list[Book]): ...\n\n\nbooks = deduplicate_books(duplicate_books)\nassert isinstance(books, list)\nfor book in books:\n    print(book)\n</pre> class Book(BaseModel):     title: str     author: str     genre: str   duplicate_books = [     Book(title=\"The War of the Worlds\", author=\"H. G. Wells\", genre=\"scifi\"),     Book(title=\"War of the Worlds\", author=\"H.G. Wells\", genre=\"science fiction\"),     Book(title=\"The Sorcerer's stone\", author=\"J. K. Rowling\", genre=\"fantasy\"),     Book(         title=\"Harry Potter and The Sorcerer's stone\",         author=\"J. K. Rowling\",         genre=\"fantasy\",     ),     Book(title=\"The Name of the Wind\", author=\"Patrick Rothfuss\", genre=\"fantasy\"),     Book(title=\"'The Name of the Wind'\", author=\"Patrick Rofuss\", genre=\"fiction\"), ]   @openai.call(model=\"gpt-4o\", response_model=list[Book]) @prompt_template(     \"\"\"     SYSTEM:     Your job is to take a database of books and clean it up by removing items which are     semantic duplicates. Look out for typos, formatting differences, and categorizations.     For example, \"Mistborn\" and \"Mistborn: The Final Empire\" are the same book      but \"Mistborn: Shadows of Self\" is not.     Then return all the unique books.      USER:     {books}     \"\"\" ) def deduplicate_books(books: list[Book]): ...   books = deduplicate_books(duplicate_books) assert isinstance(books, list) for book in books:     print(book) <pre>title='The War of the Worlds' author='H. G. Wells' genre='scifi'\ntitle='War of the Worlds' author='H.G. Wells' genre='science fiction'\n</pre> <p>Additional Real-World Examples</p> <ul> <li>Customer Relationship Management (CRM): Maintaining a single, accurate view of each customer.</li> <li>Database Management: Removing duplicate records to maintain data integrity and improve query performance</li> <li>Email: Clean up digital assets by removing duplicate attachments, emails.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Refine your prompts to provide clear instructions and examples tailored to your requirements.</li> <li>Experiment with different model providers and version to balance accuracy and speed.</li> <li>Use multiple model providers to evaluate whether all duplicates have bene removed.</li> <li>Add more information if possible to get better accuracy, e.g. some books might have similar names but are released in different years.</li> </ul>"},{"location":"tutorials/more_advanced/removing_semantic_duplicates/#removing-semantic-duplicates","title":"Removing Semantic Duplicates\u00b6","text":"<p>In this recipe, we show how to use LLMs \u2014 in this case, OpenAI's <code>gpt-4o-mini</code> \u2014 to answer remove semantic duplicates from lists and objects.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Model</li> </ul> <p>Background</p> <p> Semantic deduplication, or the removal of duplicates which are equivalent in meaning but not in data, has been a longstanding problem in NLP. LLMs which have the ability to comprehend context, semantics, and implications within that text trivializes this problem. </p>"},{"location":"tutorials/more_advanced/removing_semantic_duplicates/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/removing_semantic_duplicates/#deduplicating-a-list","title":"Deduplicating a List\u00b6","text":"<p>To start, assume we have a some entries of movie genres with semantic duplicates:</p>"},{"location":"tutorials/more_advanced/search_with_sources/","title":"Search Agent with Sources","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\" beautifulsoup4\n</pre> !pip install \"mirascope[openai]\" beautifulsoup4 In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using <p>We will need an API key for search:</p> <ul> <li>Nimble API Key or alternatively directly from Google Custom Search API.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom bs4 import BeautifulSoup\n\nNIMBLE_TOKEN = \"YOUR_NIMBLE_API_KEY\"\n\n\ndef nimble_google_search(query: str):\n    \"\"\"\n    Use Nimble to get information about the query using Google Search.\n    \"\"\"\n    url = \"https://api.webit.live/api/v1/realtime/serp\"\n    headers = {\n        \"Authorization\": f\"Basic {NIMBLE_TOKEN}\",\n        \"Content-Type\": \"application/json\",\n    }\n    search_data = {\n        \"parse\": True,\n        \"query\": query,\n        \"search_engine\": \"google_search\",\n        \"format\": \"json\",\n        \"render\": True,\n        \"country\": \"US\",\n        \"locale\": \"en\",\n    }\n    response = requests.get(url, json=search_data, headers=headers)\n    data = response.json()\n    results = data[\"parsing\"][\"entities\"][\"OrganicResult\"]\n    urls = [result.get(\"url\", \"\") for result in results]\n    search_results = {}\n    for url in urls:\n        content = get_content(url)\n        search_results[url] = content\n    return search_results\n\n\ndef get_content(url: str):\n    data = []\n    response = requests.get(url)\n    content = response.content\n    soup = BeautifulSoup(content, \"html.parser\")\n    paragraphs = soup.find_all(\"p\")\n    for paragraph in paragraphs:\n        data.append(paragraph.text)\n    return \"\\n\".join(data)\n</pre> import requests from bs4 import BeautifulSoup  NIMBLE_TOKEN = \"YOUR_NIMBLE_API_KEY\"   def nimble_google_search(query: str):     \"\"\"     Use Nimble to get information about the query using Google Search.     \"\"\"     url = \"https://api.webit.live/api/v1/realtime/serp\"     headers = {         \"Authorization\": f\"Basic {NIMBLE_TOKEN}\",         \"Content-Type\": \"application/json\",     }     search_data = {         \"parse\": True,         \"query\": query,         \"search_engine\": \"google_search\",         \"format\": \"json\",         \"render\": True,         \"country\": \"US\",         \"locale\": \"en\",     }     response = requests.get(url, json=search_data, headers=headers)     data = response.json()     results = data[\"parsing\"][\"entities\"][\"OrganicResult\"]     urls = [result.get(\"url\", \"\") for result in results]     search_results = {}     for url in urls:         content = get_content(url)         search_results[url] = content     return search_results   def get_content(url: str):     data = []     response = requests.get(url)     content = response.content     soup = BeautifulSoup(content, \"html.parser\")     paragraphs = soup.find_all(\"p\")     for paragraph in paragraphs:         data.append(paragraph.text)     return \"\\n\".join(data) <p>Now that we have created our tool, it\u2019s time to create our LLM call.</p> In\u00a0[\u00a0]: Copied! <pre>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\n    model=\"gpt-4o-mini\",\n    tools=[nimble_google_search],\n    call_params={\"tool_choice\": \"required\"},\n)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are a an expert at finding information on the web.\n    Use the `nimble_google_search` function to find information on the web.\n    Rewrite the question as needed to better find information on the web.\n\n    USER:\n    {question}\n    \"\"\"\n)\ndef search(question: str): ...\n</pre> from mirascope.core import openai, prompt_template   @openai.call(     model=\"gpt-4o-mini\",     tools=[nimble_google_search],     call_params={\"tool_choice\": \"required\"}, ) @prompt_template(     \"\"\"     SYSTEM:     You are a an expert at finding information on the web.     Use the `nimble_google_search` function to find information on the web.     Rewrite the question as needed to better find information on the web.      USER:     {question}     \"\"\" ) def search(question: str): ... <p>We ask the LLM to rewrite the question to make it more suitable for search.</p> <p>Now that we have the necessary data to answer the user query and their sources, it\u2019s time to extract all that information into a structured format using <code>response_model</code></p> In\u00a0[\u00a0]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass SearchResponse(BaseModel):\n    sources: list[str] = Field(description=\"The sources of the results\")\n    answer: str = Field(description=\"The answer to the question\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=list[SearchResponse])\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    Extract the question, results, and sources to answer the question based on the results.\n\n    Results:\n    {results}\n\n    USER:\n    {question}\n    \"\"\"\n)\ndef extract(question: str, results: str): ...\n</pre> from pydantic import BaseModel, Field   class SearchResponse(BaseModel):     sources: list[str] = Field(description=\"The sources of the results\")     answer: str = Field(description=\"The answer to the question\")   @openai.call(model=\"gpt-4o-mini\", response_model=list[SearchResponse]) @prompt_template(     \"\"\"     SYSTEM:     Extract the question, results, and sources to answer the question based on the results.      Results:     {results}      USER:     {question}     \"\"\" ) def extract(question: str, results: str): ... <p>and finally we create our <code>run</code> function to execute our chain:</p> In\u00a0[\u00a0]: Copied! <pre>def run(question: str):\n    response = search(question)\n    if tool := response.tool:\n        output = tool.call()\n        result = extract(question, output)\n        return result\n\n\nprint(run(\"What is the average price of a house in the United States?\"))\n</pre> def run(question: str):     response = search(question)     if tool := response.tool:         output = tool.call()         result = extract(question, output)         return result   print(run(\"What is the average price of a house in the United States?\")) <p>Additional Real-World Applications</p> <ul> <li>Journalism Assistant: Have the LLM do some research to quickly pull verifiable sources for blog posts and news articles.</li> <li>Education: Find and cite articles to help write academic papers.</li> <li>Technical Documentation: Generate code snippets and docs referencing official documentation.</li> </ul> <p>When adapting this recipe, consider: - Adding Tenacity <code>retry</code> for more a consistent extraction. - Use an LLM with web search tool to evaluate whether the answer produced is in the source. - Experiment with different model providers and version for quality and accuracy of results.</p>"},{"location":"tutorials/more_advanced/search_with_sources/#search-agent-with-sources","title":"Search Agent with Sources\u00b6","text":"<p>This recipe shows how to use LLMs \u2014 in this case, GPT 4o mini \u2014 to answer questions using the web. Since LLMs often time hallucinate answers, it is important to fact check and verify the accuracy of the answer.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Tools</li> <li>Chaining</li> <li>Response Model</li> </ul> <p>Background</p> <p> Users of Large Language Models (LLMs) often struggle to distinguish between factual content and potential hallucinations, leading to time-consuming fact-checking. By implementing source citation requirements, LLMs need to rely on verified information, thereby enhancing the accuracy of its responses and reducing the need for manual verification. </p>"},{"location":"tutorials/more_advanced/search_with_sources/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/more_advanced/search_with_sources/#creating-google-search-tool","title":"Creating Google Search tool\u00b6","text":"<p>We use Nimble since they provide an easy-to-use API for searching, but an alternative you can use is Google's Custom Search API. We first want to grab all the urls that are relevant to answer our question and then we take the contents of those urls, like so:</p>"},{"location":"tutorials/more_advanced/search_with_sources/#creating-the-first-call","title":"Creating the first call\u00b6","text":"<p>For this call, we force the LLM to always use its tool which we will later chain.</p>"},{"location":"tutorials/more_advanced/search_with_sources/#extracting-search-results-with-sources","title":"Extracting Search Results with Sources\u00b6","text":"<p>As mentioned earlier, it is important to fact check all answers in case of hallucination, and the first step is to ask the LLM to cite its sources:</p>"},{"location":"tutorials/more_advanced/speech_transcription/","title":"Transcribing Speech","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[gemini]\"\n</pre> !pip install \"mirascope[gemini]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[5]: Copied! <pre>import os\n\nfrom google.generativeai import configure\nfrom mirascope.core import gemini, prompt_template\n\nconfigure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n\napollo_url = \"https://storage.googleapis.com/generativeai-downloads/data/Apollo-11_Day-01-Highlights-10s.mp3\"\n\n\n@gemini.call(model=\"gemini-1.5-flash\")\n@prompt_template(\n    \"\"\"\n    Transcribe the content of this speech:\n    {url:audio}\n    \"\"\"\n)\ndef transcribe_speech_from_url(url: str): ...\n\n\nresponse = transcribe_speech_from_url(apollo_url)\n\nprint(response)\n</pre> import os  from google.generativeai import configure from mirascope.core import gemini, prompt_template  configure(api_key=os.environ[\"GOOGLE_API_KEY\"])  apollo_url = \"https://storage.googleapis.com/generativeai-downloads/data/Apollo-11_Day-01-Highlights-10s.mp3\"   @gemini.call(model=\"gemini-1.5-flash\") @prompt_template(     \"\"\"     Transcribe the content of this speech:     {url:audio}     \"\"\" ) def transcribe_speech_from_url(url: str): ...   response = transcribe_speech_from_url(apollo_url)  print(response) <pre>10 9 8 We have a goal for main engine start. We have a main engine start. \n</pre> In\u00a0[\u00a0]: Copied! <pre>from typing import Literal\n\nfrom pydantic import BaseModel, Field\n\n\nclass AudioTag(BaseModel):\n    audio_quality: Literal[\"Low\", \"Medium\", \"High\"] = Field(\n        ...,\n        description=\"\"\"The quality of the audio file.\n        Low - unlistenable due to severe static, distortion, or other imperfections\n        Medium - Audible but noticeable imperfections\n        High - crystal clear sound\"\"\",\n    )\n    imperfections: list[str] = Field(\n        ...,\n        description=\"\"\"A list of the imperfections affecting audio quality, if any.\n        Common imperfections are static, distortion, background noise, echo, but include\n        all that apply, even if not listed here\"\"\",\n    )\n    description: str = Field(\n        ..., description=\"A one sentence description of the audio content\"\n    )\n    primary_sound: str = Field(\n        ...,\n        description=\"\"\"A quick description of the main sound in the audio,\n        e.g. `Male Voice`, `Cymbals`, `Rainfall`\"\"\",\n    )\n</pre> from typing import Literal  from pydantic import BaseModel, Field   class AudioTag(BaseModel):     audio_quality: Literal[\"Low\", \"Medium\", \"High\"] = Field(         ...,         description=\"\"\"The quality of the audio file.         Low - unlistenable due to severe static, distortion, or other imperfections         Medium - Audible but noticeable imperfections         High - crystal clear sound\"\"\",     )     imperfections: list[str] = Field(         ...,         description=\"\"\"A list of the imperfections affecting audio quality, if any.         Common imperfections are static, distortion, background noise, echo, but include         all that apply, even if not listed here\"\"\",     )     description: str = Field(         ..., description=\"A one sentence description of the audio content\"     )     primary_sound: str = Field(         ...,         description=\"\"\"A quick description of the main sound in the audio,         e.g. `Male Voice`, `Cymbals`, `Rainfall`\"\"\",     ) <p>Now we make our call passing in our <code>AudioTag</code> into the <code>response_model</code> field:</p> In\u00a0[6]: Copied! <pre>@gemini.call(model=\"gemini-1.5-flash\", response_model=AudioTag, json_mode=True)\n@prompt_template(\n    \"\"\"\n    Analyze this audio file\n    {url:audio}\n\n    Give me its audio quality (low, medium, high), a list of its audio flaws (if any),\n    a quick description of the content of the audio, and the primary sound in the audio.\n    Use the tool call passed into the API call to fill it out.\n    \"\"\"\n)\ndef analyze_audio(url: str): ...\n\n\nresponse = analyze_audio(apollo_url)\nprint(response)\n</pre> @gemini.call(model=\"gemini-1.5-flash\", response_model=AudioTag, json_mode=True) @prompt_template(     \"\"\"     Analyze this audio file     {url:audio}      Give me its audio quality (low, medium, high), a list of its audio flaws (if any),     a quick description of the content of the audio, and the primary sound in the audio.     Use the tool call passed into the API call to fill it out.     \"\"\" ) def analyze_audio(url: str): ...   response = analyze_audio(apollo_url) print(response) <pre>audio_quality='Medium' imperfections=['Background noise'] description='A countdown from ten with a male voice announcing \"We have a go for main engine start\"' primary_sound='Male Voice'\n</pre> In\u00a0[\u00a0]: Copied! <pre>with open(\"YOUR_MP3_HERE\", \"rb\") as file:\n    data = file.read()\n\n    @gemini.call(model=\"gemini-1.5-flash\")\n    @prompt_template(\n        \"\"\"\n        Transcribe the content of this speech adding speaker tags \n        for example: \n            Person 1: hello \n            Person 2: good morning\n        \n        \n        {data:audio}\n        \"\"\"\n    )\n    def transcribe_speech_from_file(data: bytes): ...\n\n    response = transcribe_speech_from_file(data)\n    print(response)\n</pre> with open(\"YOUR_MP3_HERE\", \"rb\") as file:     data = file.read()      @gemini.call(model=\"gemini-1.5-flash\")     @prompt_template(         \"\"\"         Transcribe the content of this speech adding speaker tags          for example:              Person 1: hello              Person 2: good morning                           {data:audio}         \"\"\"     )     def transcribe_speech_from_file(data: bytes): ...      response = transcribe_speech_from_file(data)     print(response) <p>Additional Real-World Examples</p> <ul> <li>Subtitles and Closed Captions: Automatically generate subtitles for same and different languages for accessibility.</li> <li>Meetings: Transcribe meetings for future reference or summarization.</li> <li>Voice Assistant: Transcription is the first step to answering voice requests.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Split your audio file into multiple chunks and run the transcription in parallel.</li> <li>Compare results with traditional machine learning techniques.</li> <li>Experiment with the prompt by giving it some context before asking to transcribe the audio.</li> </ul>"},{"location":"tutorials/more_advanced/speech_transcription/#transcribing-speech","title":"Transcribing Speech\u00b6","text":"<p>In this recipe, we go over how to transcribe the speech from an audio file using Gemini 1.5 Flash\u2019s audio capabilities.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Model</li> </ul> <p>Background</p> <p> LLMs have significantly advanced speech transcription beyond traditional machine learning techniques, by improved handling of diverse accents and languages, and the ability to incorporate context for more precise transcriptions. Additionally, LLMs can leverage feedback loops to continuously improve their performance and correct errors through simple prompting. </p>"},{"location":"tutorials/more_advanced/speech_transcription/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/speech_transcription/#transcribing-speech-using-gemini","title":"Transcribing Speech using Gemini\u00b6","text":"<p>With Gemini\u2019s multimodal capabilities, audio input is treated just like text input, which means we can use it as context to ask questions. We will use an audio clip provided by Google of a countdown of the Apollo Launch. Note that if you use your own URL, Gemini currently has a byte limit of <code>20971520</code> when not using their file system.</p> <p>Since we can treat the audio like any other text context, we can create a transcription simply by inserting the audio into the prompt and asking for a transcription:</p>"},{"location":"tutorials/more_advanced/speech_transcription/#tagging-audio","title":"Tagging audio\u00b6","text":"<p>We can start by creating a Pydantic Model with the content we want to analyze:</p>"},{"location":"tutorials/more_advanced/speech_transcription/#speaker-diarization","title":"Speaker Diarization\u00b6","text":"<p>Now let's look at an audio file with multiple people talking. For the purposes of this recipe, I grabbed a snippet from Creative Commons[https://www.youtube.com/watch?v=v0l-u0ZUOSI], around 1:15 in the video and giving Gemini the audio file.</p>"},{"location":"tutorials/more_advanced/support_ticket_routing/","title":"Support Ticket Routing","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    past_purchases: list[str]\n    past_charges: list[float]\n    payment_method: str\n    password: str\n    security_question: str\n    security_answer: str\n\n\ndef get_user_by_email(email: str):\n    if email == \"johndoe@gmail.com\":\n        return User(\n            name=\"John Doe\",\n            email=\"johndoe@gmail.com\",\n            past_purchases=[\"TV\", \"Microwave\", \"Chair\"],\n            past_charges=[349.99, 349.99, 99.99, 44.99],\n            payment_method=\"AMEX 1234 1234 1234 1234\",\n            password=\"password1!\",\n            security_question=\"Childhood Pet Name\",\n            security_answer=\"Piddles\",\n        )\n    else:\n        return None\n</pre> from pydantic import BaseModel, Field   class User(BaseModel):     name: str     email: str     past_purchases: list[str]     past_charges: list[float]     payment_method: str     password: str     security_question: str     security_answer: str   def get_user_by_email(email: str):     if email == \"johndoe@gmail.com\":         return User(             name=\"John Doe\",             email=\"johndoe@gmail.com\",             past_purchases=[\"TV\", \"Microwave\", \"Chair\"],             past_charges=[349.99, 349.99, 99.99, 44.99],             payment_method=\"AMEX 1234 1234 1234 1234\",             password=\"password1!\",             security_question=\"Childhood Pet Name\",             security_answer=\"Piddles\",         )     else:         return None In\u00a0[2]: Copied! <pre>def get_sale_items():\n    return \"Sale items: we have a monitor at half off for $80!\"\n\n\ndef get_rewards(user: User):\n    if sum(user.past_charges) &gt; 300:\n        return \"Rewards: for your loyalty, you get 10% off your next purchase!\"\n    else:\n        return \"Rewards: you have no rewards available right now.\"\n\n\ndef get_billing_details(user: User):\n    return {\n        \"user_email\": user.email,\n        \"user_name\": user.name,\n        \"past_purchases\": user.past_purchases,\n        \"past_charges\": user.past_charges,\n    }\n\n\ndef get_account_details(user: User):\n    return {\n        \"user_email\": user.email,\n        \"user_name\": user.name,\n        \"password\": user.password,\n        \"security_question\": user.security_question,\n        \"security_answer\": user.security_answer,\n    }\n</pre> def get_sale_items():     return \"Sale items: we have a monitor at half off for $80!\"   def get_rewards(user: User):     if sum(user.past_charges) &gt; 300:         return \"Rewards: for your loyalty, you get 10% off your next purchase!\"     else:         return \"Rewards: you have no rewards available right now.\"   def get_billing_details(user: User):     return {         \"user_email\": user.email,         \"user_name\": user.name,         \"past_purchases\": user.past_purchases,         \"past_charges\": user.past_charges,     }   def get_account_details(user: User):     return {         \"user_email\": user.email,         \"user_name\": user.name,         \"password\": user.password,         \"security_question\": user.security_question,         \"security_answer\": user.security_answer,     } In\u00a0[3]: Copied! <pre>from typing import Literal\n\n\ndef route_to_agent(\n    agent_type: Literal[\"billing\", \"sale\", \"support\"], summary: str\n) -&gt; None:\n    \"\"\"Routes the call to an appropriate agent with a summary of the issue.\"\"\"\n    print(f\"Routed to: {agent_type}\\nSummary:\\n{summary}\")\n</pre> from typing import Literal   def route_to_agent(     agent_type: Literal[\"billing\", \"sale\", \"support\"], summary: str ) -&gt; None:     \"\"\"Routes the call to an appropriate agent with a summary of the issue.\"\"\"     print(f\"Routed to: {agent_type}\\nSummary:\\n{summary}\") In\u00a0[4]: Copied! <pre>class CallClassification(BaseModel):\n    calltype: Literal[\"billing\", \"sale\", \"support\"] = Field(\n        ...,\n        description=\"\"\"The classification of the customer's issue into one of the 3: \n        'billing' for an inquiry about charges or payment methods,\n        'sale' for making a purchase,\n        'support' for general FAQ or account-related questions\"\"\",\n    )\n    reasoning: str = Field(\n        ...,\n        description=\"\"\"A brief description of why the customer's issue fits into the\\\n              chosen category\"\"\",\n    )\n    user_email: str = Field(..., description=\"email of the user in the chat\")\n</pre> class CallClassification(BaseModel):     calltype: Literal[\"billing\", \"sale\", \"support\"] = Field(         ...,         description=\"\"\"The classification of the customer's issue into one of the 3:          'billing' for an inquiry about charges or payment methods,         'sale' for making a purchase,         'support' for general FAQ or account-related questions\"\"\",     )     reasoning: str = Field(         ...,         description=\"\"\"A brief description of why the customer's issue fits into the\\               chosen category\"\"\",     )     user_email: str = Field(..., description=\"email of the user in the chat\") <p>And we can extract information into this schema with the call <code>classify_transcript()</code>:</p> In\u00a0[5]: Copied! <pre>from mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=CallClassification)\n@prompt_template(\n    \"\"\"\n    Classify the following transcript between a customer and the service bot:\n    {transcript}\n    \"\"\"\n)\ndef classify_transcript(transcript: str): ...\n</pre> from mirascope.core import openai, prompt_template   @openai.call(model=\"gpt-4o-mini\", response_model=CallClassification) @prompt_template(     \"\"\"     Classify the following transcript between a customer and the service bot:     {transcript}     \"\"\" ) def classify_transcript(transcript: str): ... In\u00a0[6]: Copied! <pre>@openai.call(model=\"gpt-4o-mini\", tools=[route_to_agent])\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are an intermediary between a customer's interaction with a support chatbot and\n    a real life support agent. Organize the context so that the agent can best\n    facilitate the customer, but leave in details or raw data that the agent would need\n    to verify a person's identity or purchase. Then, route to the appropriate agent.\n\n    USER:\n    {context}\n    \"\"\"\n)\ndef handle_ticket(transcript: str) -&gt; openai.OpenAIDynamicConfig:\n    context = transcript\n    call_classification = classify_transcript(transcript)\n    user = get_user_by_email(call_classification.user_email)\n    if isinstance(user, User):\n        if call_classification.calltype == \"billing\":\n            context += str(get_billing_details(user))\n        elif call_classification.calltype == \"sale\":\n            context += get_sale_items()\n            context += get_rewards(user)\n        elif call_classification.calltype == \"support\":\n            context += str(get_account_details(user))\n    else:\n        context = \"This person cannot be found in our system.\"\n\n    return {\"computed_fields\": {\"context\": context}}\n</pre> @openai.call(model=\"gpt-4o-mini\", tools=[route_to_agent]) @prompt_template(     \"\"\"     SYSTEM:     You are an intermediary between a customer's interaction with a support chatbot and     a real life support agent. Organize the context so that the agent can best     facilitate the customer, but leave in details or raw data that the agent would need     to verify a person's identity or purchase. Then, route to the appropriate agent.      USER:     {context}     \"\"\" ) def handle_ticket(transcript: str) -&gt; openai.OpenAIDynamicConfig:     context = transcript     call_classification = classify_transcript(transcript)     user = get_user_by_email(call_classification.user_email)     if isinstance(user, User):         if call_classification.calltype == \"billing\":             context += str(get_billing_details(user))         elif call_classification.calltype == \"sale\":             context += get_sale_items()             context += get_rewards(user)         elif call_classification.calltype == \"support\":             context += str(get_account_details(user))     else:         context = \"This person cannot be found in our system.\"      return {\"computed_fields\": {\"context\": context}} <p>And there you have it! Let\u2019s see how <code>handle_ticket</code> deals with each of the following transcripts:</p> In\u00a0[7]: Copied! <pre>billing_transcript = \"\"\"\nBOT: Please enter your email.\nCUSTOMER: johndoe@gmail.com\nBOT: What brings you here today?\nCUSTOMER: I purchased a TV a week ago but the charge is showing up twice on my bank \\\nstatement. Can I get a refund?\n\"\"\"\n\nsale_transcript = \"\"\"\nBOT: Please enter your email.\nCUSTOMER: johndoe@gmail.com\nBOT: What brings you here today?\nCUSTOMER: I'm looking to buy a new monitor. Any discounts available?\n\"\"\"\n\nsupport_transcript = \"\"\"\nBOT: Please enter your email.\nCUSTOMER: johndoe@gmail.com\nBOT: What brings you here today?\nCUSTOMER: I forgot my site password and I'm also locked out of my email, how else can I\nverify my identity?\n\"\"\"\n\nfor transcript in [billing_transcript, sale_transcript, support_transcript]:\n    response = handle_ticket(transcript)\n    if tool := response.tool:\n        tool.call()\n</pre> billing_transcript = \"\"\" BOT: Please enter your email. CUSTOMER: johndoe@gmail.com BOT: What brings you here today? CUSTOMER: I purchased a TV a week ago but the charge is showing up twice on my bank \\ statement. Can I get a refund? \"\"\"  sale_transcript = \"\"\" BOT: Please enter your email. CUSTOMER: johndoe@gmail.com BOT: What brings you here today? CUSTOMER: I'm looking to buy a new monitor. Any discounts available? \"\"\"  support_transcript = \"\"\" BOT: Please enter your email. CUSTOMER: johndoe@gmail.com BOT: What brings you here today? CUSTOMER: I forgot my site password and I'm also locked out of my email, how else can I verify my identity? \"\"\"  for transcript in [billing_transcript, sale_transcript, support_transcript]:     response = handle_ticket(transcript)     if tool := response.tool:         tool.call() <pre>Routed to: billing\nSummary:\nCustomer John Doe (email: johndoe@gmail.com) is requesting a refund for a double charge for a TV purchase made a week ago. The customer shows two charges of $349.99 on their bank statement.\nRouted to: sale\nSummary:\nCustomer johndoe@gmail.com is interested in purchasing a new monitor and wants to know about discounts. There is a monitor available at half off for $80 and the customer is eligible for an additional 10% off for loyalty rewards.\nRouted to: support\nSummary:\nCustomer John Doe (johndoe@gmail.com) forgot their site password and is locked out of their email. They are asking for alternative ways to verify their identity. Security question: Childhood Pet Name, Answer: Piddles.\n</pre> <p>Additional Real-World Examples</p> <ul> <li>IT Help Desk: Have LLM determine whether the user request is level 1, 2, or 3 support and route accordingly</li> <li>Software-as-a-Service (SaaS) Companies: A question about how to use a specific feature might be routed to the product support team, while an issue with account access could be sent to the account management team.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <pre><code>- Update the `response_model` to more accurately reflect your use-case.\n- Implement Pydantic `ValidationError` and Tenacity `retry` to improve reliability and accuracy.\n- Evaluate the quality of extraction by using another LLM to verify classification accuracy.\n- Use a local model like Ollama to protect company or other sensitive data.</code></pre>"},{"location":"tutorials/more_advanced/support_ticket_routing/#support-ticket-routing","title":"Support Ticket Routing\u00b6","text":"<p>This recipe shows how to take an incoming support ticket/call transcript then use an LLM to summarize the issue and route it to the correct person.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Tools</li> <li>Chaining</li> <li>Response Models</li> </ul> <p>Background</p> <p> Traditional machine learning techniques like text classification were previously used to solve routing. LLMs have enhanced routing by being able to better interpret nuances of inquiries as well as using client history and knowledge of the product to make more informed decisions. </p>"},{"location":"tutorials/more_advanced/support_ticket_routing/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/support_ticket_routing/#imitating-a-companys-databasefunctionality","title":"Imitating a Company's Database/Functionality\u00b6","text":"<p>Fake Data</p> <p> For both privacy and functionality purposes, these data types and functions in no way represent how a company's API should actually look like. However, extrapolate on these gross oversimplifications to see how the LLM would interact with the company's API. </p>"},{"location":"tutorials/more_advanced/support_ticket_routing/#user","title":"User\u00b6","text":"<p>Let\u2019s create a <code>User</code> class to represent a customer as well as the function <code>get_user_by_email()</code> to imitate how one might search for the user in the database with some identifying information:</p>"},{"location":"tutorials/more_advanced/support_ticket_routing/#data-pulling-functions","title":"Data Pulling Functions\u00b6","text":"<p>Let\u2019s also define some basic functions that one might expect a company to have for specific situations. <code>get_sale_items()</code> gets the items currently on sale, <code>get_rewards()</code> gets the rewards currently available to a user, <code>get_billing_details()</code> returns user data related to billing, and <code>get_account_details()</code> returns user data related to their account.</p>"},{"location":"tutorials/more_advanced/support_ticket_routing/#routing-to-agent","title":"Routing to Agent\u00b6","text":"<p>Since we don\u2019t have an actual endpoint to route to a live agent, let\u2019s use this function <code>route_to_agent()</code> as a placeholder:</p>"},{"location":"tutorials/more_advanced/support_ticket_routing/#handling-the-ticket","title":"Handling the Ticket\u00b6","text":"<p>To handle the ticket, we will classify the issue of the ticket in one call, then use the classification to gather the corresponding context for a second call.</p>"},{"location":"tutorials/more_advanced/support_ticket_routing/#classify-the-transcript","title":"Classify the Transcript\u00b6","text":"<p>Assume we have a basic transcript from the customer\u2019s initial interactions with a support bot where they give some identifying information and their issue. We define a Pydantic <code>BaseModel</code> schema to classify the issue as well as grab the identifying information. <code>calltype</code> classifies the transcript into one of the three categories <code>billing</code>, <code>sale</code>, and <code>support</code>, and <code>user_email</code> will grab their email, assuming that\u2019s what the bot asks for. The <code>reasoning</code> field will not be used, but forcing the LLM to give a reasoning for its classification choice aids in extraction accuracy, which can be shaky:</p>"},{"location":"tutorials/more_advanced/support_ticket_routing/#provide-ticket-specific-context","title":"Provide Ticket-Specific Context\u00b6","text":"<p>Now, depending on the output of <code>classify_transcript()</code>, we would want to provide different context to the next call - namely, a <code>billing</code> ticket would necessitate the details from <code>get_billing_details()</code>, a <code>sale</code> ticket would want the output of <code>get_sale_items()</code> and <code>get_rewards()</code>, and a <code>support_ticket</code> would require <code>get_account_details</code>. We define a second call <code>handle_ticket()</code> which calls <code>classify_transcript()</code> and calls the correct functions for the scenario via dynamic configuration:</p>"},{"location":"tutorials/more_advanced/text_classification/","title":"Text Classification","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[2]: Copied! <pre>from mirascope.core import openai, prompt_template\n\n\n@openai.call(\"gpt-4o-mini\", response_model=bool)\ndef classify_spam(text: str) -&gt; str:\n    return f\"Classify the following text as spam or not spam: {text}\"\n\n\ntext = \"Would you like to buy some cheap viagra?\"\nlabel = classify_spam(text)\nassert label is True  # This text is classified as spam\n\ntext = \"Hi! It was great meeting you today. Let's stay in touch!\"\nlabel = classify_spam(text)\nassert label is False  # This text is classified as not spam\n</pre> from mirascope.core import openai, prompt_template   @openai.call(\"gpt-4o-mini\", response_model=bool) def classify_spam(text: str) -&gt; str:     return f\"Classify the following text as spam or not spam: {text}\"   text = \"Would you like to buy some cheap viagra?\" label = classify_spam(text) assert label is True  # This text is classified as spam  text = \"Hi! It was great meeting you today. Let's stay in touch!\" label = classify_spam(text) assert label is False  # This text is classified as not spam In\u00a0[2]: Copied! <pre>from enum import Enum\n\n\nclass Sentiment(Enum):\n    NEGATIVE = \"negative\"\n    NEUTRAL = \"neutral\"\n    POSITIVE = \"positive\"\n</pre> from enum import Enum   class Sentiment(Enum):     NEGATIVE = \"negative\"     NEUTRAL = \"neutral\"     POSITIVE = \"positive\" <p>Then, we set <code>response_model=Sentiment</code> to analyze the sentiment of the given text:</p> In\u00a0[3]: Copied! <pre>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o-mini\", response_model=Sentiment)\ndef classify_sentiment(text: str) -&gt; str:\n    return f\"Classify the sentiment of the following text: {text}\"\n\n\ntext = \"I hate this product. It's terrible.\"\nlabel = classify_sentiment(text)\nassert label == Sentiment.NEGATIVE\n\ntext = \"I don't feel strongly about this product.\"\nlabel = classify_sentiment(text)\nassert label == Sentiment.NEUTRAL\n\ntext = \"I love this product. It's amazing!\"\nlabel = classify_sentiment(text)\nassert label == Sentiment.POSITIVE\n</pre> from mirascope.core import openai   @openai.call(\"gpt-4o-mini\", response_model=Sentiment) def classify_sentiment(text: str) -&gt; str:     return f\"Classify the sentiment of the following text: {text}\"   text = \"I hate this product. It's terrible.\" label = classify_sentiment(text) assert label == Sentiment.NEGATIVE  text = \"I don't feel strongly about this product.\" label = classify_sentiment(text) assert label == Sentiment.NEUTRAL  text = \"I love this product. It's amazing!\" label = classify_sentiment(text) assert label == Sentiment.POSITIVE In\u00a0[4]: Copied! <pre>from enum import Enum\n\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\n\nclass Sentiment(Enum):\n    NEGATIVE = \"negative\"\n    NEUTRAL = \"neutral\"\n    POSITIVE = \"positive\"\n\n\nclass SentimentWithReasoning(BaseModel):\n    reasoning: str\n    sentiment: Sentiment\n\n\n@openai.call(\"gpt-4o-mini\", response_model=SentimentWithReasoning)\n@prompt_template(\n    \"\"\"\n    Classify the sentiment of the following text: {text}.\n    Explain your reasoning for the classified sentiment.\n    \"\"\"\n)\ndef classify_sentiment_with_reasoning(text: str): ...\n\n\ntext = \"I would recommend this product if it were cheaper...\"\nresponse = classify_sentiment_with_reasoning(text)\nprint(f\"Sentiment: {response.sentiment}\")\nprint(f\"Reasoning: {response.reasoning}\")\n</pre> from enum import Enum  from mirascope.core import openai from pydantic import BaseModel   class Sentiment(Enum):     NEGATIVE = \"negative\"     NEUTRAL = \"neutral\"     POSITIVE = \"positive\"   class SentimentWithReasoning(BaseModel):     reasoning: str     sentiment: Sentiment   @openai.call(\"gpt-4o-mini\", response_model=SentimentWithReasoning) @prompt_template(     \"\"\"     Classify the sentiment of the following text: {text}.     Explain your reasoning for the classified sentiment.     \"\"\" ) def classify_sentiment_with_reasoning(text: str): ...   text = \"I would recommend this product if it were cheaper...\" response = classify_sentiment_with_reasoning(text) print(f\"Sentiment: {response.sentiment}\") print(f\"Reasoning: {response.reasoning}\") <pre>Sentiment: Sentiment.NEUTRAL\nReasoning: The text expresses a positive sentiment towards the product because the speaker is willing to recommend it. However, the mention of 'if it were cheaper' introduces a condition that makes the overall sentiment appear somewhat negative, as it suggests dissatisfaction with the current price. Therefore, the sentiment can be classified as neutral, as it acknowledges both a positive recommendation but also a negative aspect regarding pricing.\n</pre> In\u00a0[5]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass SentimentAnalysisWithCertainty(BaseModel):\n    sentiment: Sentiment\n    certainty: float = Field(..., ge=0, le=1)\n    reasoning: str\n\n\nclass SentimentWithCertainty(BaseModel):\n    sentiment: Sentiment\n    reasoning: str\n    certainty: float\n\n\n@openai.call(\"gpt-4o-mini\", response_model=SentimentWithCertainty)\n@prompt_template(\n    \"\"\"\n    Classify the sentiment of the following text: {text}\n    Explain your reasoning for the classified sentiment.\n    Also provide a certainty score between 0 and 1, where 1 is absolute certainty.\n    \"\"\"\n)\ndef classify_sentiment_with_certainty(text: str): ...\n\n\ntext = \"This is the best product ever. And the worst.\"\nresponse = classify_sentiment_with_certainty(text)\nif response.certainty &gt; 0.8:\n    print(f\"Sentiment: {response.sentiment}\")\n    print(f\"Reasoning: {response.reasoning}\")\n    print(f\"Certainty: {response.certainty}\")\nelse:\n    print(\"The model is not certain enough about the classification.\")\n</pre> from pydantic import BaseModel, Field   class SentimentAnalysisWithCertainty(BaseModel):     sentiment: Sentiment     certainty: float = Field(..., ge=0, le=1)     reasoning: str   class SentimentWithCertainty(BaseModel):     sentiment: Sentiment     reasoning: str     certainty: float   @openai.call(\"gpt-4o-mini\", response_model=SentimentWithCertainty) @prompt_template(     \"\"\"     Classify the sentiment of the following text: {text}     Explain your reasoning for the classified sentiment.     Also provide a certainty score between 0 and 1, where 1 is absolute certainty.     \"\"\" ) def classify_sentiment_with_certainty(text: str): ...   text = \"This is the best product ever. And the worst.\" response = classify_sentiment_with_certainty(text) if response.certainty &gt; 0.8:     print(f\"Sentiment: {response.sentiment}\")     print(f\"Reasoning: {response.reasoning}\")     print(f\"Certainty: {response.certainty}\") else:     print(\"The model is not certain enough about the classification.\") <pre>The model is not certain enough about the classification.\n</pre> <p>Additional Real-World Applications</p> <ul> <li>Content ModerationClassify user-generated content as appropriate, inappropriate, or requiring manual review</li> <li>Customer Support TriageCategorize incoming support tickets by urgency or department.</li> <li>News Article CategorizationClassify news articles into topics (e.g. politics, sports, technology, etc)</li> <li>Intent RecognitionIdentify user intent in chatbot interactions (e.g. make a purchase, ask for help, etc.)</li> <li>Email ClassificationSort emails into categories like personal, work-related, promotional, or urgent</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following:</p> <ul> <li>Refine your prompts to provide clear instructions and relevant context for your specific classification task.</li> <li>Experiment with different model providers and version to balance accuracy and speed.</li> <li>Implement error handling and fallback mechanisms for cases where the model's classification is uncertain.</li> <li>Consider using a combination of classifiers for more complex categorization tasks.</li> </ul>"},{"location":"tutorials/more_advanced/text_classification/#text-classification","title":"Text Classification\u00b6","text":"<p>In this recipe we\u2019ll explore using Mirascope to implement binary classification, multi-class classification, and various other extensions of these classification techniques \u2014 specifically using Python the OpenAI API. We will also compare these solutions with more traditional machine learning and Natural Language Processing (NLP) techniques.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p>     Text classification is a fundamental classification problem and NLP task that involves categorizing text documents into predefined classes or categories. Historically this has required training text classifiers through more traditional machine learning methods. Large Language Models (LLMs) have revolutionized this field, making sophisticated classification tasks accessible through simple API calls and thoughtful prompt engineering. </p>"},{"location":"tutorials/more_advanced/text_classification/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/text_classification/#binary-classification-spam-detection","title":"Binary Classification: Spam Detection\u00b6","text":"<p>Binary classification involves categorizing text into one of two classes. We'll demonstrate this by creating a spam detector that classifies text as either spam or not spam.</p> <p>For binary classification, we can extract a boolean value by setting <code>response_model=bool</code> and prompting the model to classify the text:</p>"},{"location":"tutorials/more_advanced/text_classification/#multi-class-classification-sentiment-analysis","title":"Multi-Class Classification: Sentiment Analysis\u00b6","text":"<p>Multi-class classification extends the concept to scenarios where we need to categorize text into one of several classes. We'll demonstrate this with a sentiment analysis task.</p> <p>First, we define an <code>Enum</code> to represent our sentiment labels:</p>"},{"location":"tutorials/more_advanced/text_classification/#classification-with-reasoning","title":"Classification with Reasoning\u00b6","text":"<p>So far we've demonstrated using simple types like <code>bool</code> and <code>Enum</code> for classification, but we can extend this approach using Pydantic's <code>BaseModel</code> class to extract additional information beyond just the classification label.</p> <p>For example, we can gain insight to the LLMs reasoning for the classified label simply by including a reasoning field in our response model and updating the prompt:</p>"},{"location":"tutorials/more_advanced/text_classification/#handling-uncertainty","title":"Handling Uncertainty\u00b6","text":"<p>When dealing with LLMs for classification tasks, it's important to account for cases where the model might be uncertain about its prediction. We can modify our approach to include a certainty score and handle cases where the model's confidence is below a certain threshold.</p>"},{"location":"tutorials/more_advanced/text_classification/#comparison-with-traditional-machine-learning-models","title":"Comparison with Traditional Machine Learning Models\u00b6","text":"<p>Training text classification models requires a much more involved workflow:</p> <ul> <li>Preprocessing:<ul> <li>Read in data, clean and standardize it, and split it into training, validation, and test datasets</li> </ul> </li> <li>Feature Extraction:<ul> <li>Basic: bag of words, TF-IDF</li> <li>Advanced: word embeddings, contextual embeddings</li> </ul> </li> <li>Classification Algorithm / Machine Learning Algorithm:<ul> <li>Basic: Naive Bayes, logistic regression, linear classifiers</li> <li>Advanced: Neural networks, transformers (e.g. BERT)</li> </ul> </li> <li>Model Training:<ul> <li>Train on training data and validate on validation data, adjusting batch size and epochs.</li> <li>Things like activation layers and optimizers can greatly impact the quality of the final trained model</li> </ul> </li> <li>Model Evaluation:<ul> <li>Evaluate model quality on the test dataset using metrics such as F1-score, recall, precision, accuracy \u2014 whichever metric best suits your use-case</li> </ul> </li> </ul> <p>Many frameworks such as TensorFlow and PyTorch make implementing such workflows easier, but it is still far more involved that the approach we showed in the beginning using Mirascope.</p> <p>If you\u2019re interested in taking a deeper dive into this more traditional approach, the TensorFlow IMDB Text Classification tutorial is a great place to start.</p>"},{"location":"tutorials/more_advanced/text_summarization/","title":"Text Summarization","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\"\n</pre> !pip install \"mirascope[openai]\" In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>!curl \"https://en.wikipedia.org/wiki/Python_(programming_language)\" -o wikipedia-python.html\n</pre> !curl \"https://en.wikipedia.org/wiki/Python_(programming_language)\" -o wikipedia-python.html <pre>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100  651k  100  651k    0     0   506k      0  0:00:01  0:00:01 --:--:--  506k\r\n</pre> <p>Install beautifulsoup4 to parse the HTML file.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install beautifulsoup4\n</pre> !pip install beautifulsoup4 <p>We will be using a simple call as our baseline:</p> In\u00a0[2]: Copied! <pre>from bs4 import BeautifulSoup\nfrom mirascope.core import openai, prompt_template\n\n\ndef get_text_from_html(file_path: str) -&gt; str:\n    with open(file_path) as file:\n        html_text = file.read()\n    return BeautifulSoup(html_text, \"html.parser\").get_text()\n\n\ntext = get_text_from_html(\"wikipedia-python.html\")\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Summarize the following text:\n    {text}\n    \"\"\"\n)\ndef simple_summarize_text(text: str): ...\n\n\nprint(simple_summarize_text(text))\n</pre> from bs4 import BeautifulSoup from mirascope.core import openai, prompt_template   def get_text_from_html(file_path: str) -&gt; str:     with open(file_path) as file:         html_text = file.read()     return BeautifulSoup(html_text, \"html.parser\").get_text()   text = get_text_from_html(\"wikipedia-python.html\")   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Summarize the following text:     {text}     \"\"\" ) def simple_summarize_text(text: str): ...   print(simple_summarize_text(text)) <pre>Python is a high-level, general-purpose programming language designed for code readability and simplicity. Created by Guido van Rossum and first released in 1991, Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. Its design emphasizes dynamic typing, garbage collection, and a comprehensive standard library, often referred to as having a \"batteries included\" philosophy.\n\nKey milestones in Python's history include the release of Python 2.0 in 2000 and Python 3.0 in 2008, which introduced major changes and was not fully backwards compatible with Python 2.x. The latter version aimed to improve language simplicity and efficiency, while the support for Python 2.7 officially ended in 2020.\n\nPython's unique syntax utilizes significant whitespace for block delimiters, avoids complex punctuation, and provides an intuitive style suited for beginner and expert programmers alike. \n\nPython boasts a vast ecosystem with numerous libraries and frameworks that extend its capabilities, particularly in areas like web development, data analysis, scientific computing, and machine learning, making it a popular choice among developers globally. It is consistently ranked among the top programming languages due to its versatility and community support. The language is influenced by numerous predecessors and has significantly impacted the development of many new languages. \n\nRecent versions have focused on performance boosts, improved error reporting, and maintaining compatibility with prior code while moving forward with enhancements. Overall, Python's design philosophy, rich features, and strong community have contributed to its widespread adoption across various domains.\n</pre> <p>LLMs excel at summarizing shorter texts, but they often struggle with longer documents, failing to capture the overall structure while sometimes including minor, irrelevant details that detract from the summary's coherence and relevance.</p> <p>One simple update we can make is to improve our prompt by providing an initial outline of the text then adhere to this outline to create its summary.</p> In\u00a0[3]: Copied! <pre>@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Summarize the following text by first creating an outline with a nested structure,\n    listing all major topics in the text with subpoints for each of the major points.\n    The number of subpoints for each topic should correspond to the length and\n    importance of the major point within the text. Then create the actual summary using\n    the outline.\n    {text}\n    \"\"\"\n)\ndef summarize_text_with_outline(text: str): ...\n\n\nprint(summarize_text_with_outline(text))\n</pre> @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Summarize the following text by first creating an outline with a nested structure,     listing all major topics in the text with subpoints for each of the major points.     The number of subpoints for each topic should correspond to the length and     importance of the major point within the text. Then create the actual summary using     the outline.     {text}     \"\"\" ) def summarize_text_with_outline(text: str): ...   print(summarize_text_with_outline(text)) <pre>### Outline\n\n1. **Introduction**\n   - Overview of Python\n   - Characteristics (high-level, general-purpose, dynamic typing)\n   - Popularity and usage \n\n2. **History**\n   - Development by Guido van Rossum\n   - Key milestones (Python 0.9.0, 2.0, 3.0)\n   - Transition from Python 2 to 3\n   - Release management and versions\n\n3. **Design Philosophy and Features**\n   - Multi-paradigm programming language\n   - Emphasis on readability and simplicity\n   - The Zen of Python\n   - Extensibility and modularity\n   - Language clarity vs. functionality\n\n4. **Syntax and Semantics**\n   - Code readability features\n   - Usage of indentation for block delimitation\n   - Types of statements and control flow constructs\n   - Expressions and operator usage\n\n5. **Programming Examples**\n   - Simple example programs demonstrating Python features\n\n6. **Libraries**\n   - Overview of Python\u2019s standard library\n   - Third-party packages and Python Package Index (PyPI)\n   - Use cases in various domains (web, data analysis, etc.)\n\n7. **Development Environments**\n   - Integrated Development Environment (IDE) options\n   - Other programming shells\n\n8. **Implementations**\n   - Overview of CPython as the reference implementation\n   - Other adaptations like PyPy, MicroPython, etc.\n   - Cross-compilation to other languages\n\n9. **Development Process**\n   - Python Enhancement Proposal (PEP) process \n   - Community input and version management\n\n10. **Popularity**\n    - Rankings in programming language communities\n    - Major organizations using Python\n\n11. **Uses of Python**\n    - Application in web development, data science, machine learning, etc.\n    - Adoption in various industries and problem domains\n\n12. **Languages Influenced by Python**\n    - Overview of languages that took inspiration from Python\u2019s design \n\n### Summary\n\nPython is a high-level, multi-paradigm programming language renowned for its readability and extensive standard library. Developed by Guido van Rossum, Python was first released in 1991, evolving significantly with the introduction of versions 2.0 and 3.0, transitioning away from Python 2's legacy features. \n\nThe design philosophy of Python emphasizes clarity and simplicity, captured in the guiding principles known as the Zen of Python, which advocate for beautiful, explicit, and straightforward code while also allowing for extensibility through modules. This modularity has made Python popular for adding programmable interfaces to applications.\n\nPython\u2019s syntax is intentionally designed to enhance readability, utilizing indentation to define code blocks, a practice that differentiates it from many other languages which rely on braces or keywords. The language supports various programming constructs including statements for control flow, exception handling, and function definitions. \n\nMoreover, Python offers a robust standard library known as \"batteries included,\" and hosts a thriving ecosystem of third-party packages on PyPI, catering to diverse applications ranging from web development to data analytics and machine learning. \n\nVarious Integrated Development Environments (IDEs) and shells facilitate Python development, while CPython serves as the primary reference implementation, with alternatives like PyPy enhancing performance through just-in-time compilation.\n\nDevelopment of Python is community-driven through the Python Enhancement Proposal (PEP) process, which encourages input on new features and code standards. Python consistently ranks among the most popular programming languages and is widely adopted in major industries, influencing numerous other programming languages with its design principles.\n</pre> <p>By providing an outline, we enable the LLM to better adhere to the original article's structure, resulting in a more coherent and representative summary.</p> <p>For our next iteration, we'll explore segmenting the document by topic, requesting summaries for each section, and then composing a comprehensive summary using both the outline and these individual segment summaries.</p> In\u00a0[4]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass SegmentedSummary(BaseModel):\n    outline: str = Field(\n        ...,\n        description=\"A high level outline of major sections by topic in the text\",\n    )\n    section_summaries: list[str] = Field(\n        ..., description=\"A list of detailed summaries for each section in the outline\"\n    )\n\n\n@openai.call(model=\"gpt-4o\", response_model=SegmentedSummary)\n@prompt_template(\n    \"\"\"\n    Extract a high level outline and summary for each section of the following text:\n    {text}\n    \"\"\"\n)\ndef summarize_by_section(text): ...\n\n\n@openai.call(model=\"gpt-4o\")\n@prompt_template(\n    \"\"\"\n    The following contains a high level outline of a text along with summaries of a\n    text that has been segmented by topic. Create a composite, larger summary by putting\n    together the summaries according to the outline.\n    Outline:\n    {outline}\n\n    Summaries:\n    {summaries}\n    \"\"\"\n)\ndef summarize_text_chaining(text: str) -&gt; openai.OpenAIDynamicConfig:\n    segmented_summary = summarize_by_section(text)\n    return {\n        \"computed_fields\": {\n            \"outline\": segmented_summary.outline,\n            \"summaries\": segmented_summary.section_summaries,\n        }\n    }\n\n\nprint(summarize_text_chaining(text))\n</pre> from pydantic import BaseModel, Field   class SegmentedSummary(BaseModel):     outline: str = Field(         ...,         description=\"A high level outline of major sections by topic in the text\",     )     section_summaries: list[str] = Field(         ..., description=\"A list of detailed summaries for each section in the outline\"     )   @openai.call(model=\"gpt-4o\", response_model=SegmentedSummary) @prompt_template(     \"\"\"     Extract a high level outline and summary for each section of the following text:     {text}     \"\"\" ) def summarize_by_section(text): ...   @openai.call(model=\"gpt-4o\") @prompt_template(     \"\"\"     The following contains a high level outline of a text along with summaries of a     text that has been segmented by topic. Create a composite, larger summary by putting     together the summaries according to the outline.     Outline:     {outline}      Summaries:     {summaries}     \"\"\" ) def summarize_text_chaining(text: str) -&gt; openai.OpenAIDynamicConfig:     segmented_summary = summarize_by_section(text)     return {         \"computed_fields\": {             \"outline\": segmented_summary.outline,             \"summaries\": segmented_summary.section_summaries,         }     }   print(summarize_text_chaining(text)) <pre>Python was created in the late 1980s by Guido van Rossum as a successor to the ABC programming language. The first version, Python 0.9.0, was released in 1991. Major versions like Python 2.0 in 2000 and Python 3.0 in 2008 introduced significant changes. Python 2.7.18 was the last release of Python 2, while Python 3.x continues to evolve.\n\nPython is designed to emphasize code readability, using significant indentation. It supports multiple paradigms, including procedural, object-oriented, and functional programming. Its comprehensive standard library and dynamic typing are notable features.\n\nPython syntax uses indentation to define blocks, rather than curly braces or keywords. It includes various statements and control flows like assignment, if, for, while, try, except, and more. The language allows dynamic typing and has a robust set of built-in methods and operators for handling different data types.\n\nA Hello, World! program and a factorial calculation program demonstrate Python's straightforward syntax and readability. The language is known for its simplicity and ease of use, making it accessible for beginners and powerful for experienced programmers.\n\nThe standard library in Python is vast, offering modules for internet protocols, string operations, web services tools, and operating system interfaces. The Python Package Index (PyPI) hosts a large collection of third-party modules for various tasks.\n\nPython supports several integrated development environments (IDEs) like PyCharm and Jupyter Notebook. It also offers basic tools like IDLE for beginners. Many advanced IDEs provide additional features like auto-completion, debugging, and syntax highlighting.\n\nCPython is the reference implementation, but there are others like PyPy, Jython, and IronPython. Variants such as MicroPython are designed for microcontrollers. Some older implementations like Unladen Swallow are no longer actively maintained.\n\nPython's development is guided by a community-driven process, primarily through Python Enhancement Proposals (PEPs). The language's development is led by the Python Software Foundation and a steering council elected by core developers.\n\nPython offers tools like Sphinx and pydoc for generating API documentation. These tools help in creating comprehensive and accessible documentation for various Python projects.\n\nNamed after the British comedy group Monty Python, the language includes various playful references to the group in its documentation and culture. The term Pythonic is often used to describe idiomatic Python code.\n\nPython is highly popular, consistently ranking among the top programming languages. It is widely used by major organizations like Google, NASA, and Facebook, and has a strong presence in the scientific and data analysis communities.\n\nPython is versatile, used for web development, scientific computing, data analysis, artificial intelligence, and more. Frameworks like Django and Flask support web development, while libraries like NumPy and SciPy enable scientific research.\n\nPython has influenced many programming languages, including Julia, Swift, and Go, borrowing its design philosophy and syntax to various extents.\n</pre> <p>Additional Real-World Applications</p> <ul> <li>Meeting Notes: Convert meeting from speech-to-text then summarize the text for reference.</li> <li>Education: Create study guides or slides from textbook material using summaries.</li> <li>Productivity: Summarize email chains, slack threads, word documents for your day-to-day.</li> </ul> <p>When adapting this recipe to your specific use-case, consider the following: - Refine your prompts to provide clear instructions and relevant context for text summarization. - Experiment with different model providers and version to balance quality and speed. - Provide a feedback loop, use an LLM to evaluate the quality of the summary based on a criteria and feed that back into the prompt for refinement.</p>"},{"location":"tutorials/more_advanced/text_summarization/#text-summarization","title":"Text Summarization\u00b6","text":"<p>In this recipe, we show some techniques to improve an LLM\u2019s ability to summarize a long text from simple (e.g. <code>\"Summarize this text: {text}...\"</code>) to more complex prompting and chaining techniques. We will use OpenAI\u2019s GPT-4o-mini model (128k input token limit), but you can use any model you\u2019d like to implement these summarization techniques, as long as they have a large context window.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p>     Large Language Models (LLMs) have revolutionized text summarization by enabling more coherent and contextually aware abstractive summaries. Unlike earlier models that primarily extracted or rearranged existing sentences, LLMs can generate novel text that captures the essence of longer documents while maintaining readability and factual accuracy.  </p>"},{"location":"tutorials/more_advanced/text_summarization/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/text_summarization/#simple-call","title":"Simple Call\u00b6","text":"<p>For our examples, we\u2019ll use the Wikipedia article on python). We will be referring to this article as <code>wikipedia-python.txt</code>.</p> <p>The command below will download the article to your local machine by using the <code>curl</code> command. If you don't have <code>curl</code> installed, you can download the article manually from the link above and save it as <code>wikipedia-python.html</code>.</p>"},{"location":"tutorials/more_advanced/text_summarization/#simple-call-with-outline","title":"Simple Call with Outline\u00b6","text":"<p>This prompt engineering technique is an example of Chain of Thought (CoT), forcing the model to write out its thinking process. It also involves little work and can be done by modifying the text of the single call. With an outline, the summary is less likely to lose the general structure of the text.</p>"},{"location":"tutorials/more_advanced/text_summarization/#segment-then-summarize","title":"Segment then Summarize\u00b6","text":"<p>This more comprehensive approach not only ensures that the model adheres to the original text's structure but also naturally produces a summary whose length is proportional to the source document, as we combine summaries from each subtopic.</p> <p>To apply this technique, we create a <code>SegmentedSummary</code> Pydantic <code>BaseModel</code> to contain the outline and section summaries, and extract it in a chained call from the original summarize_text() call:</p>"},{"location":"tutorials/more_advanced/text_translation/","title":"Text Translation","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai, anthropic, gemini]\"\n</pre> !pip install \"mirascope[openai, anthropic, gemini]\"  In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_API_KEY\"\nos.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_API_KEY\" os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[1]: Copied! <pre>import asyncio\nfrom collections.abc import Callable\nfrom enum import StrEnum\n\nfrom mirascope.core import BasePrompt, openai, prompt_template\n\n\nclass Audience(StrEnum):\n    general = \"general\"\n    professional = \"professional\"\n    academic = \"academic\"\n    friendly = \"friendly\"\n    formal = \"formal\"\n\n\nclass Tone(StrEnum):\n    neutral = \"neutral\"\n    positive = \"positive\"\n    negative = \"negative\"\n    professional = \"professional\"\n    casual = \"casual\"\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are a professional translator who is a native Japanese speaker.\n    Translate the English text into natural Japanese without changing the content.\n    Please make sure that the text is easy to read for everyone by using appropriate paragraphs so that it does not become difficult to read.\n\n    Please consider the following parameters in your translation:\n\n    tone: {tone}\n    The tone can be one of the following:\n    - neutral: Maintain a balanced and impartial tone\n    - positive: Use upbeat and optimistic language\n    - negative: Convey a more critical or pessimistic view\n    - professional: Use formal and business-appropriate language\n    - casual: Use informal, conversational language\n\n    audience: {audience}\n    The audience can be one of the following:\n    - general: For the general public, use common terms and clear explanations\n    - professional: For industry professionals, use appropriate jargon and technical terms\n    - academic: For scholarly contexts, use formal language and cite sources if necessary\n    - friendly: For casual, familiar contexts, use warm and approachable language\n    - formal: For official or ceremonial contexts, use polite and respectful language\n\n    Adjust your translation style based on the specified tone and audience to ensure the message is conveyed appropriately.\n\n    USER:\n    text: {text}\n    \"\"\"\n)\nclass ParametrizedTranslatePrompt(BasePrompt):\n    tone: Tone\n    audience: Audience\n    text: str\n\n    async def translate(self, call: Callable, model: str) -&gt; str:\n        response = await self.run_async(call(model))\n        return response.content\n\n\ntext = \"\"\"\nThe old codger, a real bootstrapper, had been burning the candle at both ends trying to make his pie-in-the-sky business idea fly. He'd been spinning his wheels for months, barking up the wrong tree with his half-baked marketing schemes.\n\"\"\"\nprompt = ParametrizedTranslatePrompt(\n    text=text, tone=Tone.negative, audience=Audience.general\n)\nprompt_translation = prompt.translate(call=openai.call, model=\"gpt-4o-mini\")\n\n# In Jupyter Notebook, use the following code to run async functions:\nparametrized_translation = await prompt_translation\n\n# In Python script, use the following code:\n# parametrized_translation = asyncio.run(prompt_translation)\n\nprint(f\"Parametrized with translation: {parametrized_translation}\")\n</pre> import asyncio from collections.abc import Callable from enum import StrEnum  from mirascope.core import BasePrompt, openai, prompt_template   class Audience(StrEnum):     general = \"general\"     professional = \"professional\"     academic = \"academic\"     friendly = \"friendly\"     formal = \"formal\"   class Tone(StrEnum):     neutral = \"neutral\"     positive = \"positive\"     negative = \"negative\"     professional = \"professional\"     casual = \"casual\"   @prompt_template(     \"\"\"     SYSTEM:     You are a professional translator who is a native Japanese speaker.     Translate the English text into natural Japanese without changing the content.     Please make sure that the text is easy to read for everyone by using appropriate paragraphs so that it does not become difficult to read.      Please consider the following parameters in your translation:      tone: {tone}     The tone can be one of the following:     - neutral: Maintain a balanced and impartial tone     - positive: Use upbeat and optimistic language     - negative: Convey a more critical or pessimistic view     - professional: Use formal and business-appropriate language     - casual: Use informal, conversational language      audience: {audience}     The audience can be one of the following:     - general: For the general public, use common terms and clear explanations     - professional: For industry professionals, use appropriate jargon and technical terms     - academic: For scholarly contexts, use formal language and cite sources if necessary     - friendly: For casual, familiar contexts, use warm and approachable language     - formal: For official or ceremonial contexts, use polite and respectful language      Adjust your translation style based on the specified tone and audience to ensure the message is conveyed appropriately.      USER:     text: {text}     \"\"\" ) class ParametrizedTranslatePrompt(BasePrompt):     tone: Tone     audience: Audience     text: str      async def translate(self, call: Callable, model: str) -&gt; str:         response = await self.run_async(call(model))         return response.content   text = \"\"\" The old codger, a real bootstrapper, had been burning the candle at both ends trying to make his pie-in-the-sky business idea fly. He'd been spinning his wheels for months, barking up the wrong tree with his half-baked marketing schemes. \"\"\" prompt = ParametrizedTranslatePrompt(     text=text, tone=Tone.negative, audience=Audience.general ) prompt_translation = prompt.translate(call=openai.call, model=\"gpt-4o-mini\")  # In Jupyter Notebook, use the following code to run async functions: parametrized_translation = await prompt_translation  # In Python script, use the following code: # parametrized_translation = asyncio.run(prompt_translation)  print(f\"Parametrized with translation: {parametrized_translation}\") <pre>Parametrized with translation: \u305d\u306e\u8001\u3044\u307c\u308c\u306f\u3001\u672c\u5f53\u306b\u6839\u6027\u306e\u3042\u308b\u4eba\u3060\u3063\u305f\u304c\u3001\u81ea\u5206\u306e\u5922\u306e\u30d3\u30b8\u30cd\u30b9\u30a2\u30a4\u30c7\u30a2\u3092\u5b9f\u73fe\u3055\u305b\u3088\u3046\u3068\u3001\u7121\u7406\u3092\u3057\u3066\u9811\u5f35\u308a\u7d9a\u3051\u3066\u3044\u305f\u3002\u4f55\u30f6\u6708\u3082\u7121\u99c4\u306b\u904e\u3054\u3057\u3001\u51fa\u6765\u306e\u60aa\u3044\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u8a08\u753b\u306b\u624b\u3092\u3053\u307e\u306d\u3044\u3066\u3001\u5168\u304f\u7684\u5916\u308c\u306a\u52aa\u529b\u3092\u3057\u3066\u3044\u305f\u3002\n</pre> <p>This technique allows translators to adjust translations according to the target audience and required tone.</p> In\u00a0[6]: Copied! <pre>from collections.abc import Callable\nfrom contextlib import contextmanager\nfrom enum import StrEnum\n\nfrom mirascope.core import BasePrompt, openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Audience(StrEnum):\n    general = \"general\"\n    professional = \"professional\"\n    academic = \"academic\"\n    friendly = \"friendly\"\n    formal = \"formal\"\n\n\nclass Tone(StrEnum):\n    neutral = \"neutral\"\n    positive = \"positive\"\n    negative = \"negative\"\n    professional = \"professional\"\n    casual = \"casual\"\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are a professional translator who is a native Japanese speaker.\n    Translate the English text into natural Japanese without changing the content.\n    Please make sure that the text is easy to read for everyone by using appropriate paragraphs so that it does not become difficult to read.\n\n    Please consider the following parameters in your translation:\n\n    tone: {tone}\n    The tone can be one of the following:\n    - neutral: Maintain a balanced and impartial tone\n    - positive: Use upbeat and optimistic language\n    - negative: Convey a more critical or pessimistic view\n    - professional: Use formal and business-appropriate language\n    - casual: Use informal, conversational language\n\n    audience: {audience}\n    The audience can be one of the following:\n    - general: For the general public, use common terms and clear explanations\n    - professional: For industry professionals, use appropriate jargon and technical terms\n    - academic: For scholarly contexts, use formal language and cite sources if necessary\n    - friendly: For casual, familiar contexts, use warm and approachable language\n    - formal: For official or ceremonial contexts, use polite and respectful language\n\n    Adjust your translation style based on the specified tone and audience to ensure the message is conveyed appropriately.\n\n    USER:\n    text: {text}\n    \"\"\"\n)\nclass ParametrizedTranslatePrompt(BasePrompt):\n    tone: Tone\n    audience: Audience\n    text: str\n\n    async def translate(self, call: Callable, model: str) -&gt; str:\n        response = await self.run_async(call(model))\n        return response.content\n\n\nclass Evaluation(BaseModel):\n    clarity: float = Field(\n        ..., description=\"The clarity of the translation, ranging from 0 to 10.\"\n    )\n    naturalness: float = Field(\n        ..., description=\"The naturalness of the translation, ranging from 0 to 10.\"\n    )\n    consistency: float = Field(\n        ..., description=\"The consistency of the translation, ranging from 0 to 10.\"\n    )\n    grammatical_correctness: float = Field(\n        ...,\n        description=\"The grammatical correctness of the translation, ranging from 0 to 10.\",\n    )\n    lexical_appropriateness: float = Field(\n        ...,\n        description=\"The lexical appropriateness of the translation, ranging from 0 to 10.\",\n    )\n    subject_clarity: float = Field(\n        ...,\n        description=\"The clarity of the subject in the translation, ranging from 0 to 10.\",\n    )\n    word_order_naturalness: float = Field(\n        ...,\n        description=\"Maintenance of natural Japanese word order (0 to 10). Evaluates whether the English word order is not directly applied.\",\n    )\n    subject_handling: float = Field(\n        ...,\n        description=\"Appropriate handling of subjects (0 to 10). Evaluates whether unnecessary subjects are avoided and appropriately omitted.\",\n    )\n    modifier_placement: float = Field(\n        ...,\n        description=\"Appropriate placement of modifiers (0 to 10). Evaluates whether natural Japanese modification relationships are maintained.\",\n    )\n    sentence_length_appropriateness: float = Field(\n        ...,\n        description=\"Appropriateness of sentence length (0 to 10). Evaluates whether long sentences are properly divided when necessary.\",\n    )\n    context_dependent_expression: float = Field(\n        ...,\n        description=\"Appropriateness of context-dependent expressions (0 to 10). Evaluates whether the level of honorifics and politeness is suitable for the target audience and situation.\",\n    )\n    implicit_meaning_preservation: float = Field(\n        ...,\n        description=\"Preservation of implicit meanings (0 to 10). Evaluates whether implicit meanings and cultural nuances from the original English text are appropriately conveyed.\",\n    )\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are a professional translator who is a native Japanese speaker.\n    Please evaluate the following translation and provide feedback on how it can be improved.\n\n    USER:\n    original_text: {original_text}\n    translation_text: {translation_text}\n    \"\"\"\n)\nclass EvaluateTranslationPrompt(BasePrompt):\n    original_text: str\n    translation_text: str\n\n    async def evaluate(self, call: Callable, model: str) -&gt; Evaluation:\n        response = await self.run_async(call(model, response_model=Evaluation))\n        return response\n\n\n@prompt_template(\"\"\"\n    SYSTEM:\n    Your task is to improve the quality of a translation from English into Japanese.\n    You will be provided with the original text, the translated text, and an evaluation of the translation.\n    All evaluation criteria will be scores between 0 and 10.\n\n    The translation you are improving was intended to adhere to the desired tone and audience:\n    tone: {tone}\n    audience: {audience}\n\n    You improved translation MUST also adhere to this desired tone and audience.\n\n    Output ONLY the improved translation.\n\n    USER:\n    original text: {original_text}\n    translation: {translation_text}\n    evaluation: {evaluation}\n\"\"\")\nclass ImproveTranslationPrompt(BasePrompt):\n    tone: Tone\n    audience: Audience\n    original_text: str\n    translation_text: str\n    evaluation: Evaluation\n\n    async def improve_translation(self, call: Callable, model: str) -&gt; str:\n        response = await self.run_async(call(model))\n        return response.content\n\n\ntext = \"\"\"\nThe old codger, a real bootstrapper, had been burning the candle at both ends trying to make his pie-in-the-sky business idea fly. He'd been spinning his wheels for months, barking up the wrong tree with his half-baked marketing schemes.\n\"\"\"\n\n\n@contextmanager\ndef print_progress_message(model: str, count: int):\n    print(f\"{model=} Multi-pass translation start times: {count}\")\n    yield\n    print(f\"{model=} Multi-pass translation end times: {count}\")\n\n\nasync def multi_pass_translation(\n    original_text: str,\n    tone: Tone,\n    audience: Audience,\n    pass_count: int,\n    call: Callable,\n    model: str,\n) -&gt; str:\n    with print_progress_message(model, 1):\n        parametrized_translate_prompt = ParametrizedTranslatePrompt(\n            text=original_text, tone=tone, audience=audience\n        )\n        translation_text = await parametrized_translate_prompt.translate(call, model)\n\n    for current_count in range(2, pass_count + 1):\n        with print_progress_message(model, current_count):\n            evaluate_translation_prompt = EvaluateTranslationPrompt(\n                original_text=original_text, translation_text=translation_text\n            )\n            evaluation = await evaluate_translation_prompt.evaluate(call, model)\n            improve_translation_prompt = ImproveTranslationPrompt(\n                original_text=original_text,\n                translation_text=translation_text,\n                tone=tone,\n                audience=audience,\n                evaluation=evaluation,\n            )\n            translation_text = await improve_translation_prompt.improve_translation(\n                call, model\n            )\n    return translation_text\n\n\nmulti_pass_translation_task = multi_pass_translation(\n    text,\n    tone=Tone.casual,\n    audience=Audience.general,\n    pass_count=3,\n    call=openai.call,\n    model=\"gpt-4o-mini\",\n)\n\n# In Jupyter Notebook, use the following code to run async functions:\nmulti_pass_result = await multi_pass_translation_task\n\n# In Python script, use the following code:\n# multi_pass_result = asyncio.run(multi_pass_translation\n\nprint(f\"Multi-pass translation result: {multi_pass_result}\")\n</pre> from collections.abc import Callable from contextlib import contextmanager from enum import StrEnum  from mirascope.core import BasePrompt, openai, prompt_template from pydantic import BaseModel, Field   class Audience(StrEnum):     general = \"general\"     professional = \"professional\"     academic = \"academic\"     friendly = \"friendly\"     formal = \"formal\"   class Tone(StrEnum):     neutral = \"neutral\"     positive = \"positive\"     negative = \"negative\"     professional = \"professional\"     casual = \"casual\"   @prompt_template(     \"\"\"     SYSTEM:     You are a professional translator who is a native Japanese speaker.     Translate the English text into natural Japanese without changing the content.     Please make sure that the text is easy to read for everyone by using appropriate paragraphs so that it does not become difficult to read.      Please consider the following parameters in your translation:      tone: {tone}     The tone can be one of the following:     - neutral: Maintain a balanced and impartial tone     - positive: Use upbeat and optimistic language     - negative: Convey a more critical or pessimistic view     - professional: Use formal and business-appropriate language     - casual: Use informal, conversational language      audience: {audience}     The audience can be one of the following:     - general: For the general public, use common terms and clear explanations     - professional: For industry professionals, use appropriate jargon and technical terms     - academic: For scholarly contexts, use formal language and cite sources if necessary     - friendly: For casual, familiar contexts, use warm and approachable language     - formal: For official or ceremonial contexts, use polite and respectful language      Adjust your translation style based on the specified tone and audience to ensure the message is conveyed appropriately.      USER:     text: {text}     \"\"\" ) class ParametrizedTranslatePrompt(BasePrompt):     tone: Tone     audience: Audience     text: str      async def translate(self, call: Callable, model: str) -&gt; str:         response = await self.run_async(call(model))         return response.content   class Evaluation(BaseModel):     clarity: float = Field(         ..., description=\"The clarity of the translation, ranging from 0 to 10.\"     )     naturalness: float = Field(         ..., description=\"The naturalness of the translation, ranging from 0 to 10.\"     )     consistency: float = Field(         ..., description=\"The consistency of the translation, ranging from 0 to 10.\"     )     grammatical_correctness: float = Field(         ...,         description=\"The grammatical correctness of the translation, ranging from 0 to 10.\",     )     lexical_appropriateness: float = Field(         ...,         description=\"The lexical appropriateness of the translation, ranging from 0 to 10.\",     )     subject_clarity: float = Field(         ...,         description=\"The clarity of the subject in the translation, ranging from 0 to 10.\",     )     word_order_naturalness: float = Field(         ...,         description=\"Maintenance of natural Japanese word order (0 to 10). Evaluates whether the English word order is not directly applied.\",     )     subject_handling: float = Field(         ...,         description=\"Appropriate handling of subjects (0 to 10). Evaluates whether unnecessary subjects are avoided and appropriately omitted.\",     )     modifier_placement: float = Field(         ...,         description=\"Appropriate placement of modifiers (0 to 10). Evaluates whether natural Japanese modification relationships are maintained.\",     )     sentence_length_appropriateness: float = Field(         ...,         description=\"Appropriateness of sentence length (0 to 10). Evaluates whether long sentences are properly divided when necessary.\",     )     context_dependent_expression: float = Field(         ...,         description=\"Appropriateness of context-dependent expressions (0 to 10). Evaluates whether the level of honorifics and politeness is suitable for the target audience and situation.\",     )     implicit_meaning_preservation: float = Field(         ...,         description=\"Preservation of implicit meanings (0 to 10). Evaluates whether implicit meanings and cultural nuances from the original English text are appropriately conveyed.\",     )   @prompt_template(     \"\"\"     SYSTEM:     You are a professional translator who is a native Japanese speaker.     Please evaluate the following translation and provide feedback on how it can be improved.      USER:     original_text: {original_text}     translation_text: {translation_text}     \"\"\" ) class EvaluateTranslationPrompt(BasePrompt):     original_text: str     translation_text: str      async def evaluate(self, call: Callable, model: str) -&gt; Evaluation:         response = await self.run_async(call(model, response_model=Evaluation))         return response   @prompt_template(\"\"\"     SYSTEM:     Your task is to improve the quality of a translation from English into Japanese.     You will be provided with the original text, the translated text, and an evaluation of the translation.     All evaluation criteria will be scores between 0 and 10.      The translation you are improving was intended to adhere to the desired tone and audience:     tone: {tone}     audience: {audience}      You improved translation MUST also adhere to this desired tone and audience.      Output ONLY the improved translation.      USER:     original text: {original_text}     translation: {translation_text}     evaluation: {evaluation} \"\"\") class ImproveTranslationPrompt(BasePrompt):     tone: Tone     audience: Audience     original_text: str     translation_text: str     evaluation: Evaluation      async def improve_translation(self, call: Callable, model: str) -&gt; str:         response = await self.run_async(call(model))         return response.content   text = \"\"\" The old codger, a real bootstrapper, had been burning the candle at both ends trying to make his pie-in-the-sky business idea fly. He'd been spinning his wheels for months, barking up the wrong tree with his half-baked marketing schemes. \"\"\"   @contextmanager def print_progress_message(model: str, count: int):     print(f\"{model=} Multi-pass translation start times: {count}\")     yield     print(f\"{model=} Multi-pass translation end times: {count}\")   async def multi_pass_translation(     original_text: str,     tone: Tone,     audience: Audience,     pass_count: int,     call: Callable,     model: str, ) -&gt; str:     with print_progress_message(model, 1):         parametrized_translate_prompt = ParametrizedTranslatePrompt(             text=original_text, tone=tone, audience=audience         )         translation_text = await parametrized_translate_prompt.translate(call, model)      for current_count in range(2, pass_count + 1):         with print_progress_message(model, current_count):             evaluate_translation_prompt = EvaluateTranslationPrompt(                 original_text=original_text, translation_text=translation_text             )             evaluation = await evaluate_translation_prompt.evaluate(call, model)             improve_translation_prompt = ImproveTranslationPrompt(                 original_text=original_text,                 translation_text=translation_text,                 tone=tone,                 audience=audience,                 evaluation=evaluation,             )             translation_text = await improve_translation_prompt.improve_translation(                 call, model             )     return translation_text   multi_pass_translation_task = multi_pass_translation(     text,     tone=Tone.casual,     audience=Audience.general,     pass_count=3,     call=openai.call,     model=\"gpt-4o-mini\", )  # In Jupyter Notebook, use the following code to run async functions: multi_pass_result = await multi_pass_translation_task  # In Python script, use the following code: # multi_pass_result = asyncio.run(multi_pass_translation  print(f\"Multi-pass translation result: {multi_pass_result}\") <pre>model='gpt-4o-mini' Multi-pass translation start times: 1\nmodel='gpt-4o-mini' Multi-pass translation end times: 1\nmodel='gpt-4o-mini' Multi-pass translation start times: 2\nmodel='gpt-4o-mini' Multi-pass translation end times: 2\nmodel='gpt-4o-mini' Multi-pass translation start times: 3\nmodel='gpt-4o-mini' Multi-pass translation end times: 3\nMulti-pass translation result: \u305d\u306e\u304a\u3058\u3044\u3055\u3093\u306f\u3001\u307b\u3093\u3068\u3046\u306e\u81ea\u529b\u3067\u6210\u529f\u3092\u76ee\u6307\u3057\u3066\u3001\u663c\u3082\u591c\u3082\u9811\u5f35\u3063\u3066\u3044\u305f\u3093\u3060\u3002\u5922\u306e\u3088\u3046\u306a\u30d3\u30b8\u30cd\u30b9\u30a2\u30a4\u30c7\u30a2\u3092\u5b9f\u73fe\u3057\u3088\u3046\u3068\u596e\u95d8\u3057\u3066\u3044\u305f\u3051\u3069\u3001\u4f55\u30f6\u6708\u3082\u7121\u99c4\u306b\u6642\u9593\u3092\u4f7f\u3063\u3066\u3044\u305f\u3060\u3051\u3060\u3063\u305f\u3093\u3060\u3002\u3061\u3087\u3063\u3068\u5909\u306a\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u30d7\u30e9\u30f3\u306b\u624b\u3092\u51fa\u3057\u3061\u3083\u3063\u3066\u3001\u5b8c\u5168\u306b\u9593\u9055\u3063\u305f\u9053\u306b\u9032\u3093\u3067\u3044\u305f\u3093\u3060\u3088\u3002\n</pre> <p>This technique allows for gradual improvement in various aspects such as grammar, vocabulary, and style, resulting in more natural and accurate translations.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install ipywidgets # for Jupyter Notebook\n</pre> !pip install ipywidgets # for Jupyter Notebook In\u00a0[3]: Copied! <pre>from collections.abc import Callable\nfrom contextlib import contextmanager\nfrom enum import StrEnum\n\nfrom mirascope.core import BasePrompt, anthropic, gemini, openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Audience(StrEnum):\n    general = \"general\"\n    professional = \"professional\"\n    academic = \"academic\"\n    friendly = \"friendly\"\n    formal = \"formal\"\n\n\nclass Tone(StrEnum):\n    neutral = \"neutral\"\n    positive = \"positive\"\n    negative = \"negative\"\n    professional = \"professional\"\n    casual = \"casual\"\n\n\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You are a professional translator who is a native Japanese speaker.\n    Translate the English text into natural Japanese without changing the content.\n    Please make sure that the text is easy to read for everyone by using appropriate paragraphs so that it does not become difficult to read.\n\n    Please consider the following parameters in your translation:\n\n    tone: {tone}\n    The tone can be one of the following:\n    - neutral: Maintain a balanced and impartial tone\n    - positive: Use upbeat and optimistic language\n    - negative: Convey a more critical or pessimistic view\n    - professional: Use formal and business-appropriate language\n    - casual: Use informal, conversational language\n\n    audience: {audience}\n    The audience can be one of the following:\n    - general: For the general public, use common terms and clear explanations\n    - professional: For industry professionals, use appropriate jargon and technical terms\n    - academic: For scholarly contexts, use formal language and cite sources if necessary\n    - friendly: For casual, familiar contexts, use warm and approachable language\n    - formal: For official or ceremonial contexts, use polite and respectful language\n\n    Adjust your translation style based on the specified tone and audience to ensure the message is conveyed appropriately.\n\n    USER:\n    text: {text}\n    \"\"\"\n)\nclass ParametrizedTranslatePrompt(BasePrompt):\n    tone: Tone\n    audience: Audience\n    text: str\n\n    async def translate(self, call: Callable, model: str) -&gt; str:\n        response = await self.run_async(call(model))\n        return response.content\n\n\nclass Evaluation(BaseModel):\n    clarity: float = Field(\n        ..., description=\"The clarity of the translation, ranging from 0 to 10.\"\n    )\n    naturalness: float = Field(\n        ..., description=\"The naturalness of the translation, ranging from 0 to 10.\"\n    )\n    consistency: float = Field(\n        ..., description=\"The consistency of the translation, ranging from 0 to 10.\"\n    )\n    grammatical_correctness: float = Field(\n        ...,\n        description=\"The grammatical correctness of the translation, ranging from 0 to 10.\",\n    )\n    lexical_appropriateness: float = Field(\n        ...,\n        description=\"The lexical appropriateness of the translation, ranging from 0 to 10.\",\n    )\n    subject_clarity: float = Field(\n        ...,\n        description=\"The clarity of the subject in the translation, ranging from 0 to 10.\",\n    )\n    word_order_naturalness: float = Field(\n        ...,\n        description=\"Maintenance of natural Japanese word order (0 to 10). Evaluates whether the English word order is not directly applied.\",\n    )\n    subject_handling: float = Field(\n        ...,\n        description=\"Appropriate handling of subjects (0 to 10). Evaluates whether unnecessary subjects are avoided and appropriately omitted.\",\n    )\n    modifier_placement: float = Field(\n        ...,\n        description=\"Appropriate placement of modifiers (0 to 10). Evaluates whether natural Japanese modification relationships are maintained.\",\n    )\n    sentence_length_appropriateness: float = Field(\n        ...,\n        description=\"Appropriateness of sentence length (0 to 10). Evaluates whether long sentences are properly divided when necessary.\",\n    )\n    context_dependent_expression: float = Field(\n        ...,\n        description=\"Appropriateness of context-dependent expressions (0 to 10). Evaluates whether the level of honorifics and politeness is suitable for the target audience and situation.\",\n    )\n    implicit_meaning_preservation: float = Field(\n        ...,\n        description=\"Preservation of implicit meanings (0 to 10). Evaluates whether implicit meanings and cultural nuances from the original English text are appropriately conveyed.\",\n    )\n\n\ntext = \"\"\"\nThe old codger, a real bootstrapper, had been burning the candle at both ends trying to make his pie-in-the-sky business idea fly. He'd been spinning his wheels for months, barking up the wrong tree with his half-baked marketing schemes.\n\"\"\"\n\n\nasync def multi_pass_translation(\n    original_text: str,\n    tone: Tone,\n    audience: Audience,\n    pass_count: int,\n    call: Callable,\n    model: str,\n) -&gt; str:\n    with print_progress_message(model, 1):\n        parametrized_translate_prompt = ParametrizedTranslatePrompt(\n            text=original_text, tone=tone, audience=audience\n        )\n        translation_text = await parametrized_translate_prompt.translate(call, model)\n\n    for current_count in range(2, pass_count + 1):\n        with print_progress_message(model, current_count):\n            evaluate_translation_prompt = EvaluateTranslationPrompt(\n                original_text=original_text, translation_text=translation_text\n            )\n            evaluation = await evaluate_translation_prompt.evaluate(call, model)\n            improve_translation_prompt = ImproveTranslationPrompt(\n                original_text=original_text,\n                translation_text=translation_text,\n                tone=tone,\n                audience=audience,\n                evaluation=evaluation,\n            )\n            translation_text = await improve_translation_prompt.improve_translation(\n                call, model\n            )\n    return translation_text\n\n\nasync def multi_provider_translation(\n    original_text: str,\n    tone: Tone,\n    audience: Audience,\n    pass_count: int,\n    call_models: list[tuple[Callable, str]],\n) -&gt; None:\n    results = []\n    for call, model in call_models:\n        results.append(\n            multi_pass_translation(\n                original_text,\n                tone,\n                audience,\n                pass_count,\n                call,\n                model,\n            )\n        )\n    translations = await asyncio.gather(*results)\n    print(\"Translations:\")\n    for (_, model), translation_text in zip(call_models, translations, strict=True):\n        print(f\"Model: {model}, Translation: {translation_text}\")\n\n\nmulti_provider_translation_task = multi_provider_translation(\n    text,\n    tone=Tone.professional,\n    audience=Audience.academic,\n    pass_count=2,\n    call_models=[\n        (openai.call, \"gpt-4o-mini\"),\n        (anthropic.call, \"claude-3-5-sonnet-20240620\"),\n        (gemini.call, \"gemini-1.5-flash\"),\n    ],\n)\n\n# In Jupyter Notebook, use the following code to run async functions:\nawait multi_provider_translation_task\n\n# In Python script, use the following code:\n# asyncio.run(multi_provider_translation_task)\n</pre> from collections.abc import Callable from contextlib import contextmanager from enum import StrEnum  from mirascope.core import BasePrompt, anthropic, gemini, openai, prompt_template from pydantic import BaseModel, Field   class Audience(StrEnum):     general = \"general\"     professional = \"professional\"     academic = \"academic\"     friendly = \"friendly\"     formal = \"formal\"   class Tone(StrEnum):     neutral = \"neutral\"     positive = \"positive\"     negative = \"negative\"     professional = \"professional\"     casual = \"casual\"   @prompt_template(     \"\"\"     SYSTEM:     You are a professional translator who is a native Japanese speaker.     Translate the English text into natural Japanese without changing the content.     Please make sure that the text is easy to read for everyone by using appropriate paragraphs so that it does not become difficult to read.      Please consider the following parameters in your translation:      tone: {tone}     The tone can be one of the following:     - neutral: Maintain a balanced and impartial tone     - positive: Use upbeat and optimistic language     - negative: Convey a more critical or pessimistic view     - professional: Use formal and business-appropriate language     - casual: Use informal, conversational language      audience: {audience}     The audience can be one of the following:     - general: For the general public, use common terms and clear explanations     - professional: For industry professionals, use appropriate jargon and technical terms     - academic: For scholarly contexts, use formal language and cite sources if necessary     - friendly: For casual, familiar contexts, use warm and approachable language     - formal: For official or ceremonial contexts, use polite and respectful language      Adjust your translation style based on the specified tone and audience to ensure the message is conveyed appropriately.      USER:     text: {text}     \"\"\" ) class ParametrizedTranslatePrompt(BasePrompt):     tone: Tone     audience: Audience     text: str      async def translate(self, call: Callable, model: str) -&gt; str:         response = await self.run_async(call(model))         return response.content   class Evaluation(BaseModel):     clarity: float = Field(         ..., description=\"The clarity of the translation, ranging from 0 to 10.\"     )     naturalness: float = Field(         ..., description=\"The naturalness of the translation, ranging from 0 to 10.\"     )     consistency: float = Field(         ..., description=\"The consistency of the translation, ranging from 0 to 10.\"     )     grammatical_correctness: float = Field(         ...,         description=\"The grammatical correctness of the translation, ranging from 0 to 10.\",     )     lexical_appropriateness: float = Field(         ...,         description=\"The lexical appropriateness of the translation, ranging from 0 to 10.\",     )     subject_clarity: float = Field(         ...,         description=\"The clarity of the subject in the translation, ranging from 0 to 10.\",     )     word_order_naturalness: float = Field(         ...,         description=\"Maintenance of natural Japanese word order (0 to 10). Evaluates whether the English word order is not directly applied.\",     )     subject_handling: float = Field(         ...,         description=\"Appropriate handling of subjects (0 to 10). Evaluates whether unnecessary subjects are avoided and appropriately omitted.\",     )     modifier_placement: float = Field(         ...,         description=\"Appropriate placement of modifiers (0 to 10). Evaluates whether natural Japanese modification relationships are maintained.\",     )     sentence_length_appropriateness: float = Field(         ...,         description=\"Appropriateness of sentence length (0 to 10). Evaluates whether long sentences are properly divided when necessary.\",     )     context_dependent_expression: float = Field(         ...,         description=\"Appropriateness of context-dependent expressions (0 to 10). Evaluates whether the level of honorifics and politeness is suitable for the target audience and situation.\",     )     implicit_meaning_preservation: float = Field(         ...,         description=\"Preservation of implicit meanings (0 to 10). Evaluates whether implicit meanings and cultural nuances from the original English text are appropriately conveyed.\",     )   text = \"\"\" The old codger, a real bootstrapper, had been burning the candle at both ends trying to make his pie-in-the-sky business idea fly. He'd been spinning his wheels for months, barking up the wrong tree with his half-baked marketing schemes. \"\"\"   async def multi_pass_translation(     original_text: str,     tone: Tone,     audience: Audience,     pass_count: int,     call: Callable,     model: str, ) -&gt; str:     with print_progress_message(model, 1):         parametrized_translate_prompt = ParametrizedTranslatePrompt(             text=original_text, tone=tone, audience=audience         )         translation_text = await parametrized_translate_prompt.translate(call, model)      for current_count in range(2, pass_count + 1):         with print_progress_message(model, current_count):             evaluate_translation_prompt = EvaluateTranslationPrompt(                 original_text=original_text, translation_text=translation_text             )             evaluation = await evaluate_translation_prompt.evaluate(call, model)             improve_translation_prompt = ImproveTranslationPrompt(                 original_text=original_text,                 translation_text=translation_text,                 tone=tone,                 audience=audience,                 evaluation=evaluation,             )             translation_text = await improve_translation_prompt.improve_translation(                 call, model             )     return translation_text   async def multi_provider_translation(     original_text: str,     tone: Tone,     audience: Audience,     pass_count: int,     call_models: list[tuple[Callable, str]], ) -&gt; None:     results = []     for call, model in call_models:         results.append(             multi_pass_translation(                 original_text,                 tone,                 audience,                 pass_count,                 call,                 model,             )         )     translations = await asyncio.gather(*results)     print(\"Translations:\")     for (_, model), translation_text in zip(call_models, translations, strict=True):         print(f\"Model: {model}, Translation: {translation_text}\")   multi_provider_translation_task = multi_provider_translation(     text,     tone=Tone.professional,     audience=Audience.academic,     pass_count=2,     call_models=[         (openai.call, \"gpt-4o-mini\"),         (anthropic.call, \"claude-3-5-sonnet-20240620\"),         (gemini.call, \"gemini-1.5-flash\"),     ], )  # In Jupyter Notebook, use the following code to run async functions: await multi_provider_translation_task  # In Python script, use the following code: # asyncio.run(multi_provider_translation_task) <pre>model='gpt-4o-mini' Multi-pass translation start times: 1\nmodel='claude-3-5-sonnet-20240620' Multi-pass translation start times: 1\nmodel='gemini-1.5-flash' Multi-pass translation start times: 1\nmodel='gemini-1.5-flash' Multi-pass translation end times: 1\nmodel='gemini-1.5-flash' Multi-pass translation start times: 2\nmodel='gpt-4o-mini' Multi-pass translation end times: 1\nmodel='gpt-4o-mini' Multi-pass translation start times: 2\nmodel='gemini-1.5-flash' Multi-pass translation end times: 2\nmodel='claude-3-5-sonnet-20240620' Multi-pass translation end times: 1\nmodel='claude-3-5-sonnet-20240620' Multi-pass translation start times: 2\nmodel='gpt-4o-mini' Multi-pass translation end times: 2\nmodel='claude-3-5-sonnet-20240620' Multi-pass translation end times: 2\nTranslations:\nModel: gpt-4o-mini, Translation: \u305d\u306e\u5e74\u914d\u306e\u7537\u6027\u306f\u3001\u672c\u5f53\u306e\u81ea\u529b\u3067\u6210\u308a\u4e0a\u304c\u3063\u305f\u4eba\u7269\u3067\u3042\u308a\u3001\u5922\u306e\u30d3\u30b8\u30cd\u30b9\u30a2\u30a4\u30c7\u30a2\u3092\u5b9f\u73fe\u3059\u308b\u305f\u3081\u306b\u5fb9\u591c\u3067\u52aa\u529b\u3057\u3066\u3044\u307e\u3057\u305f\u3002\u3057\u304b\u3057\u3001\u5f7c\u306f\u4f55\u30f6\u6708\u3082\u7121\u99c4\u306b\u3057\u3001\u672a\u719f\u306a\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u6226\u7565\u3067\u8aa4\u3063\u305f\u65b9\u5411\u306b\u9032\u3093\u3067\u3044\u307e\u3057\u305f\u3002\nModel: claude-3-5-sonnet-20240620, Translation: \u9ad8\u9f62\u306e\u5b9f\u696d\u5bb6\u306f\u3001\u771f\u306e\u72ec\u7acb\u81ea\u55b6\u8005\u3068\u3057\u3066\u3001\u975e\u73fe\u5b9f\u7684\u306a\u30d3\u30b8\u30cd\u30b9\u69cb\u60f3\u306e\u5b9f\u73fe\u306b\u5411\u3051\u3066\u663c\u591c\u3092\u554f\u308f\u305a\u61f8\u547d\u306b\u52aa\u529b\u3057\u3066\u3044\u305f\u3002\u6570\u30f6\u6708\u306b\u308f\u305f\u308a\u3001\u5f7c\u306f\u52b9\u679c\u7684\u306a\u9032\u5c55\u3092\u898b\u305b\u308b\u3053\u3068\u306a\u304f\u6642\u9593\u3092\u8cbb\u3084\u3057\u3001\u4e0d\u5341\u5206\u306a\u6e96\u5099\u72b6\u614b\u306e\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u6226\u7565\u3092\u7528\u3044\u3066\u8aa4\u3063\u305f\u65b9\u5411\u6027\u306b\u52b4\u529b\u3092\u6ce8\u3044\u3067\u3044\u305f\u3002\u3053\u306e\u72b6\u6cc1\u306f\u3001\u5f7c\u306e\u52aa\u529b\u304c\u5fc5\u305a\u3057\u3082\u9069\u5207\u306a\u65b9\u5411\u306b\u5411\u3051\u3089\u308c\u3066\u3044\u306a\u304b\u3063\u305f\u3053\u3068\u3092\u793a\u5506\u3057\u3066\u3044\u308b\u3002\nModel: gemini-1.5-flash, Translation: \u305d\u306e\u8001\u4eba\u306f\u3001\u81ea\u529b\u3067\u6210\u529f\u3057\u3088\u3046\u3068\u3001\u5b9f\u73fe\u4e0d\u53ef\u80fd\u306a\u5922\u306e\u3088\u3046\u306a\u30d3\u30b8\u30cd\u30b9\u30a2\u30a4\u30c7\u30a2\u306b\u56fa\u57f7\u3057\u3001\u591c\u9045\u304f\u307e\u3067\u50cd\u304d\u8a70\u3081\u3067\u3057\u305f\u3002\u5f7c\u306f\u3001\u884c\u304d\u5f53\u305f\u308a\u3070\u3063\u305f\u308a\u306e\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u6226\u7565\u3067\u4f55\u30f6\u6708\u3082\u7121\u99c4\u306a\u52aa\u529b\u3092\u91cd\u306d\u3066\u304d\u307e\u3057\u305f\u3002 \n\n</pre> <p>This method allows for comparison of translation results from different models, enabling the selection of the most appropriate translation.</p>"},{"location":"tutorials/more_advanced/text_translation/#text-translation","title":"Text Translation\u00b6","text":"<p>This guide introduces advanced techniques for translating from English to Japanese. Due to significant differences in their origins, grammatical structures, and cultural backgrounds, generating natural Japanese through mechanical translation is extremely challenging. The methods presented here are applicable not only to English-Japanese translation but also to translation between any structurally different languages.</p> <p>We will explore innovative approaches to improve translation quality using Large Language Models (LLMs). Specifically, we will introduce three techniques:</p> <ol> <li>Parametrized Translation</li> <li>Multi-Pass Translation</li> <li>Multi-Provider Translation</li> </ol> <p>These techniques can be applied to various LLMs, including OpenAI's GPT-4, Anthropic's Claude, and Google's Gemini.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p> Translation between English and Japanese presents several unique challenges: </p> <ol> <li>Context Dependency: Japanese is a high-context language where much information is conveyed implicitly.</li> <li>Grammatical Structure Differences: Japanese follows an SOV structure, while English follows an SVO structure.</li> <li>Subject Handling: In Japanese, it's common to omit the subject when it's clear from context.</li> <li>Honorifics and Polite Expressions: Japanese has multiple levels of honorifics that need to be appropriately chosen based on social context.</li> <li>Idiomatic Expressions and Cultural Nuances: Both languages have unique idioms and culturally rooted expressions.</li> </ol>"},{"location":"tutorials/more_advanced/text_translation/#setup","title":"Setup\u00b6","text":"<p>Let's start by installing Mirascope and its dependencies:</p>"},{"location":"tutorials/more_advanced/text_translation/#translation-techniques-leveraging-llms","title":"Translation Techniques Leveraging LLMs\u00b6","text":""},{"location":"tutorials/more_advanced/text_translation/#parametrized-translation","title":"Parametrized Translation\u00b6","text":"<p>Parametrized translation introduces parameters such as tone and target audience into the translation process to generate more appropriate and context-aware translations.</p>"},{"location":"tutorials/more_advanced/text_translation/#multi-pass-translation","title":"Multi-Pass Translation\u00b6","text":"<p>Multi-Pass translation involves repeating the same translation process multiple times, evaluating and improving the translation at each pass.</p>"},{"location":"tutorials/more_advanced/text_translation/#multi-provider-translation","title":"Multi-Provider Translation\u00b6","text":"<p>Multi-Provider translation involves using multiple LLM providers in parallel and comparing their results.</p> <p>Install the required other provider's dependencies using the following command:</p>"},{"location":"tutorials/more_advanced/text_translation/#conclusion","title":"Conclusion\u00b6","text":"<p>The techniques demonstrated in this recipe can help to significantly improve the quality and efficiency of English-Japanese translation through parametrization, multi-pass, and multi-provider techniques. By combining these methods, it effectively handles complex translation tasks and flexibly addresses diverse translation needs.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/chain_of_verification/","title":"Chain of Verification: Enhancing LLM Accuracy through Self-Verification","text":"In\u00a0[3]: Copied! <pre>import asyncio\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\n@openai.call(\"gpt-4o-mini\")\ndef call(query: str) -&gt; str:\n    return query\n\n\nclass VerificationQuestions(BaseModel):\n    questions: list[str] = Field(\n        ...,\n        description=\"\"\"A list of questions that verifies if the response\n        answers the original query correctly.\"\"\",\n    )\n\n\n@openai.call(\"gpt-4o-mini\", response_model=VerificationQuestions)\n@prompt_template(\n    \"\"\"\n    SYSTEM:\n    You will be given a query and a response to the query.\n    Take the relevant statements in the response and rephrase them into questions so\n    that they can be used to verify that they satisfy the original query.\n    USER:\n    Query:\n    {query}\n\n    Response:\n    {response}\n    \"\"\"\n)\ndef get_verification_questions(query: str, response: str): ...\n\n\n@openai.call(\"gpt-4o-mini\")\nasync def answer(query: str) -&gt; str:\n    return f\"Concisely answer the following question: {query}\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Here is the original query:\n    {query}\n\n    Here is an initial response to the query:\n    {response}\n\n    Here is some fact checking on the response:\n    {verification_q_and_a:list}\n\n    Using the knowledge you learned from verification, re-answer the original query.\n    \"\"\"\n)\nasync def cov_call(query: str) -&gt; openai.OpenAIDynamicConfig:\n    response = call(query).content\n    verification_questions = get_verification_questions(query, response).questions\n    tasks = [answer(question) for question in verification_questions]\n    responses = await asyncio.gather(*tasks)\n    verification_answers = [response.content for response in responses]\n    verification_q_and_a = [\n        [f\"Q:{q}\", f\"A:{a}\"]\n        for q, a in zip(verification_questions, verification_answers)\n    ]\n    return {\n        \"computed_fields\": {\n            \"response\": response,\n            \"verification_q_and_a\": verification_q_and_a,\n        }\n    }\n\n\nasync def chain_of_verification(query: str):\n    response = await cov_call(query=query)\n    # Uncomment to see intermediate responses\n    # print(response.user_message_param[\"content\"])\n    return response\n\n\nquery = \"Name 5 politicians born in New York.\"\n\nprint(await chain_of_verification(query=query))\n</pre> import asyncio  from mirascope.core import openai, prompt_template from pydantic import BaseModel, Field   @openai.call(\"gpt-4o-mini\") def call(query: str) -&gt; str:     return query   class VerificationQuestions(BaseModel):     questions: list[str] = Field(         ...,         description=\"\"\"A list of questions that verifies if the response         answers the original query correctly.\"\"\",     )   @openai.call(\"gpt-4o-mini\", response_model=VerificationQuestions) @prompt_template(     \"\"\"     SYSTEM:     You will be given a query and a response to the query.     Take the relevant statements in the response and rephrase them into questions so     that they can be used to verify that they satisfy the original query.     USER:     Query:     {query}      Response:     {response}     \"\"\" ) def get_verification_questions(query: str, response: str): ...   @openai.call(\"gpt-4o-mini\") async def answer(query: str) -&gt; str:     return f\"Concisely answer the following question: {query}\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Here is the original query:     {query}      Here is an initial response to the query:     {response}      Here is some fact checking on the response:     {verification_q_and_a:list}      Using the knowledge you learned from verification, re-answer the original query.     \"\"\" ) async def cov_call(query: str) -&gt; openai.OpenAIDynamicConfig:     response = call(query).content     verification_questions = get_verification_questions(query, response).questions     tasks = [answer(question) for question in verification_questions]     responses = await asyncio.gather(*tasks)     verification_answers = [response.content for response in responses]     verification_q_and_a = [         [f\"Q:{q}\", f\"A:{a}\"]         for q, a in zip(verification_questions, verification_answers)     ]     return {         \"computed_fields\": {             \"response\": response,             \"verification_q_and_a\": verification_q_and_a,         }     }   async def chain_of_verification(query: str):     response = await cov_call(query=query)     # Uncomment to see intermediate responses     # print(response.user_message_param[\"content\"])     return response   query = \"Name 5 politicians born in New York.\"  print(await chain_of_verification(query=query)) <pre>Here are five politicians who were born in New York:\n\n1. **Theodore Roosevelt** - 26th President of the United States, born in New York City, New York.\n2. **Al Smith** - Governor of New York and Democratic presidential candidate, born in New York City, New York.\n3. **Andrew Cuomo** - Former Governor of New York, born in Queens, New York City.\n4. **Franklin D. Roosevelt** - 32nd President of the United States, born in Hyde Park, New York.\n5. **Donald Trump** - 45th President of the United States, born in Queens, New York City.\n\nThese individuals have all made significant contributions to American politics and governance. Note that Hillary Clinton, while a prominent politician, was actually born in Chicago, Illinois.\n</pre> <p>As we can see, the Chain of Verification process has successfully identified and corrected an error in the initial response (Hillary Clinton's birthplace), demonstrating its effectiveness in improving accuracy.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/chain_of_verification/#chain-of-verification-enhancing-llm-accuracy-through-self-verification","title":"Chain of Verification: Enhancing LLM Accuracy through Self-Verification\u00b6","text":"<p>This recipe demonstrates how to implement the Chain of Verification technique using Large Language Models (LLMs) with Mirascope. Chain of Verification is a prompt engineering method that enhances an LLM's accuracy by generating and answering verification questions based on its initial response.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> <li>Response Models</li> </ul> <p>Background</p> <p> Chain of Verification is a prompt engineering technique where one takes a prompt and its initial LLM response then generates a checklist of questions that can be used to verify the initial answer. Each of these questions are then answered individually with separate LLM calls, and the results of each verification question are used to edit the final answer. LLMs are often more truthful when asked to verify a particular fact rather than use it in their own answer, so this technique is effective in reducing hallucinations. </p>"},{"location":"tutorials/prompt_engineering/chaining_based/chain_of_verification/#implementation","title":"Implementation\u00b6","text":"<p>Let's implement the Chain of Verification technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/chain_of_verification/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Chain of Verification implementation offers several advantages:</p> <ol> <li>Improved accuracy by systematically verifying initial responses.</li> <li>Reduction of hallucinations and factual errors in LLM outputs.</li> <li>Transparent fact-checking process that can be easily audited.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing the number of verification questions with response time and computational cost.</li> <li>Tailoring the verification question generation process to your specific use case.</li> <li>Implementing error handling for cases where verification reveals significant discrepancies.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Customizing the verification question generation process for your domain.</li> <li>Implementing a feedback loop to continuously improve the verification process based on user feedback or expert review.</li> <li>Combining Chain of Verification with other techniques like Chain of Thought for even more robust reasoning capabilities.</li> <li>Experimenting with different LLM models for various stages of the verification process to optimize for accuracy and efficiency.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator, response models, and dynamic configuration, you can easily implement and customize the Chain of Verification technique to enhance your LLM's accuracy across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/decomposed_prompting/","title":"Decomposed Prompting: Enhancing LLM Problem-Solving with Tool-Based Subproblems","text":"In\u00a0[1]: Copied! <pre>import json\n\nfrom mirascope.core import openai, prompt_template\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom pydantic import BaseModel, Field\n\n\nclass Problem(BaseModel):\n    subproblems: list[str] = Field(\n        ..., description=\"The subproblems that the original problem breaks down into\"\n    )\n\n\n@openai.call(model=\"gpt-4o\", response_model=Problem)\n@prompt_template(\n    \"\"\"\n    Your job is to break a problem into subproblems so that it may be solved\n    step by step, using at most one function call at each step.\n\n    You have access to the following functions which you can use to solve a\n    problem:\n    split: split a string into individual words\n    substring: get the ith character of a single string.\n    concat: concatenate some number of strings.\n\n    Here is an example of how it would be done for the problem: Get the first two\n    letters of the phrase 'Round Robin' with a period and space in between them.\n    Steps:\n    split 'Round Robin' into individual words\n    substring the 0th char of 'Round'\n    substring the 0th char of 'Robin'\n    concat ['R', '.', ' ', 'R']\n\n    Now, turn this problem into subtasks:\n    {query}\n    \"\"\"\n)\ndef break_into_subproblems(query: str): ...\n\n\ndef split_string_to_words(string: str) -&gt; str:\n    \"\"\"Splits a string into words.\"\"\"\n    return json.dumps(string.split())\n\n\ndef substring(index: int, string: str) -&gt; str:\n    \"\"\"Gets the character at the index of a string.\"\"\"\n    return string[index]\n\n\ndef concat(strings: list[str]) -&gt; str:\n    \"\"\"Concatenates some number of strings.\"\"\"\n    return \"\".join(strings)\n\n\n@openai.call(model=\"gpt-4o-mini\", tools=[split_string_to_words, substring, concat])\n@prompt_template(\n    \"\"\"\n    SYSTEM: You are being fed subproblems to solve the actual problem: {query}\n    MESSAGES: {history}\n    \"\"\"\n)\ndef solve_next_step(history: list[ChatCompletionMessageParam], query: str): ...\n\n\ndef decomposed_prompting(query: str):\n    problem = break_into_subproblems(query=query)\n    response = None\n    history: list[ChatCompletionMessageParam] = []\n    for subproblem in problem.subproblems:\n        history.append({\"role\": \"user\", \"content\": subproblem})\n        response = solve_next_step(history, query)\n        history.append(response.message_param)\n        if tool := response.tool:\n            output = tool.call()\n            history += response.tool_message_params([(tool, output)])\n            response = solve_next_step(history, query)\n\n            # This should never return another tool call in DECOMP so don't recurse\n            history.append(response.message_param)\n    return response\n\n\nquery = \"\"\"Take the last letters of the words in \"Augusta Ada King\" and concatenate them using a space.\"\"\"\n\n\nprint(decomposed_prompting(query))\n</pre> import json  from mirascope.core import openai, prompt_template from openai.types.chat import ChatCompletionMessageParam from pydantic import BaseModel, Field   class Problem(BaseModel):     subproblems: list[str] = Field(         ..., description=\"The subproblems that the original problem breaks down into\"     )   @openai.call(model=\"gpt-4o\", response_model=Problem) @prompt_template(     \"\"\"     Your job is to break a problem into subproblems so that it may be solved     step by step, using at most one function call at each step.      You have access to the following functions which you can use to solve a     problem:     split: split a string into individual words     substring: get the ith character of a single string.     concat: concatenate some number of strings.      Here is an example of how it would be done for the problem: Get the first two     letters of the phrase 'Round Robin' with a period and space in between them.     Steps:     split 'Round Robin' into individual words     substring the 0th char of 'Round'     substring the 0th char of 'Robin'     concat ['R', '.', ' ', 'R']      Now, turn this problem into subtasks:     {query}     \"\"\" ) def break_into_subproblems(query: str): ...   def split_string_to_words(string: str) -&gt; str:     \"\"\"Splits a string into words.\"\"\"     return json.dumps(string.split())   def substring(index: int, string: str) -&gt; str:     \"\"\"Gets the character at the index of a string.\"\"\"     return string[index]   def concat(strings: list[str]) -&gt; str:     \"\"\"Concatenates some number of strings.\"\"\"     return \"\".join(strings)   @openai.call(model=\"gpt-4o-mini\", tools=[split_string_to_words, substring, concat]) @prompt_template(     \"\"\"     SYSTEM: You are being fed subproblems to solve the actual problem: {query}     MESSAGES: {history}     \"\"\" ) def solve_next_step(history: list[ChatCompletionMessageParam], query: str): ...   def decomposed_prompting(query: str):     problem = break_into_subproblems(query=query)     response = None     history: list[ChatCompletionMessageParam] = []     for subproblem in problem.subproblems:         history.append({\"role\": \"user\", \"content\": subproblem})         response = solve_next_step(history, query)         history.append(response.message_param)         if tool := response.tool:             output = tool.call()             history += response.tool_message_params([(tool, output)])             response = solve_next_step(history, query)              # This should never return another tool call in DECOMP so don't recurse             history.append(response.message_param)     return response   query = \"\"\"Take the last letters of the words in \"Augusta Ada King\" and concatenate them using a space.\"\"\"   print(decomposed_prompting(query)) <pre>The concatenated characters are \"aag\".\n</pre> <p>This implementation consists of several key components:</p> <ol> <li><code>Problem</code> class: Defines the structure for breaking down a problem into subproblems.</li> <li><code>break_into_subproblems</code>: Uses GPT-4o-mini to break the main problem into subproblems.</li> <li>Tool functions: <code>split</code>, <code>substring</code>, and <code>concat</code> for manipulating strings.</li> <li><code>solve_next_step</code>: Uses GPT-3.5-turbo to solve each subproblem, utilizing the available tools.</li> <li><code>decomposed_prompting</code>: Orchestrates the entire process, solving subproblems sequentially and maintaining conversation history.</li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/decomposed_prompting/#decomposed-prompting-enhancing-llm-problem-solving-with-tool-based-subproblems","title":"Decomposed Prompting: Enhancing LLM Problem-Solving with Tool-Based Subproblems\u00b6","text":"<p>This recipe demonstrates how to implement the Decomposed Prompting (DECOMP) technique using Large Language Models (LLMs) with Mirascope. DECOMP is a prompt engineering method that enhances an LLM's problem-solving capabilities by breaking down complex problems into subproblems and utilizing tools to solve each step.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Tools</li> <li>Response Models</li> </ul> <p>Background</p> <p> Decomposed Prompting (DECOMP) is an extension of least-to-most whereby tools are used to execute each subproblem in the problem solving process. A pre-trained call (in our case, a one shot prompt) demonstrates how to break a problem down into subproblems within the context of its given tool calls, and the output of each tool call is added to the chat's history until the problem is solved. Just like least-to-most, DECOMP shows improvements on mathematical reasoning and symbolic manipulation tasks, with better results than least-to-most. </p>"},{"location":"tutorials/prompt_engineering/chaining_based/decomposed_prompting/#implementation","title":"Implementation\u00b6","text":"<p>Let's implement the Decomposed Prompting technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/decomposed_prompting/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Decomposed Prompting implementation offers several advantages:</p> <ol> <li>Improved problem-solving capabilities for complex tasks.</li> <li>Better handling of multi-step problems that require different operations.</li> <li>Increased transparency in the problem-solving process.</li> <li>Potential for solving problems that are beyond the scope of a single LLM call.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Carefully designing the set of available tools to cover a wide range of problem-solving needs.</li> <li>Balancing the complexity of subproblems with the capabilities of the chosen LLM.</li> <li>Implementing error handling and recovery mechanisms for cases where a subproblem solution fails.</li> <li>Optimizing the prompt for breaking down problems to ensure effective decomposition.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Code Generation: Break down complex programming tasks into smaller, manageable steps.</li> <li>Data Analysis: Decompose complex data analysis queries into a series of data manipulation and calculation steps.</li> <li>Natural Language Processing: Break down complex NLP tasks like sentiment analysis or named entity recognition into subtasks.</li> <li>Automated Reasoning: Solve complex logical or mathematical problems by breaking them into simpler, solvable steps.</li> <li>Task Planning: Create detailed, step-by-step plans for complex projects or processes.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Tailoring the available tools to your domain for better performance.</li> <li>Implementing a feedback loop to refine the problem decomposition process based on solution accuracy.</li> <li>Combining Decomposed Prompting with other techniques like Chain of Thought for even more powerful problem-solving capabilities.</li> <li>Developing a mechanism to handle interdependencies between subproblems.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator, response models, and dynamic configuration, you can easily implement and customize the Decomposed Prompting technique to enhance your LLM's problem-solving capabilities across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/demonstration_ensembling/","title":"Demonstration Ensembling: Enhancing LLM Responses with Aggregated Examples","text":"In\u00a0[3]: Copied! <pre>import asyncio\nimport random\nfrom typing import TypedDict\n\nfrom mirascope.core import openai, prompt_template\n\n\nclass QA(TypedDict):\n    question: str\n    answer: str\n\n\nqa_examples: list[QA] = [\n    QA(\n        question=\"What are your company's core values?\",\n        answer=\"Our company's core values are integrity, innovation, customer-centricity, and teamwork. We believe that maintaining these values is crucial to achieving our mission and vision.\",\n    ),\n    QA(\n        question=\"How do you handle conflicts in the workplace?\",\n        answer=\"We handle conflicts by promoting open communication, understanding different perspectives, and seeking mutually beneficial solutions. We have clear policies and trained mediators to assist in resolving conflicts effectively.\",\n    ),\n    QA(\n        question=\"Can you describe a time when you exceeded a client's expectations?\",\n        answer=\"Certainly. Recently, we completed a project ahead of schedule and under budget. We also provided additional insights and recommendations that significantly benefited the client, earning their gratitude and loyalty.\",\n    ),\n    QA(\n        question=\"How do you ensure continuous improvement within the team?\",\n        answer=\"We ensure continuous improvement by encouraging regular training, fostering a culture of feedback, and implementing agile methodologies. We also review our processes regularly to identify areas for enhancement.\",\n    ),\n    QA(\n        question=\"What strategies do you use to stay ahead of industry trends?\",\n        answer=\"We stay ahead of industry trends by investing in research and development, attending industry conferences, and maintaining strong relationships with thought leaders. We also encourage our team to engage in continuous learning and innovation.\",\n    ),\n    QA(\n        question=\"How do you measure the success of a project?\",\n        answer=\"We measure the success of a project by evaluating key performance indicators such as client satisfaction, budget adherence, timeline compliance, and the quality of the deliverables. Post-project reviews help us to identify successes and areas for improvement.\",\n    ),\n    QA(\n        question=\"What is your approach to diversity and inclusion?\",\n        answer=\"Our approach to diversity and inclusion involves creating a welcoming environment for all employees, offering diversity training, and implementing policies that promote equality. We value diverse perspectives as they drive innovation and growth.\",\n    ),\n    QA(\n        question=\"How do you manage remote teams effectively?\",\n        answer=\"We manage remote teams effectively by leveraging technology for communication and collaboration, setting clear goals, and maintaining regular check-ins. We also ensure that remote employees feel included and supported.\",\n    ),\n    QA(\n        question=\"What are your short-term and long-term goals for the company?\",\n        answer=\"In the short term, our goals include expanding our market reach and enhancing our product offerings. In the long term, we aim to become industry leaders by driving innovation and achieving sustainable growth.\",\n    ),\n    QA(\n        question=\"How do you handle feedback from clients or customers?\",\n        answer=\"We handle feedback by listening actively, responding promptly, and taking necessary actions to address concerns. We view feedback as an opportunity for improvement and strive to exceed our clients' expectations continuously.\",\n    ),\n]\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Here are some examples that demonstrate the voice to use in a corporate setting.\n    {examples:lists}\n\n    With these examples, answer the following question:\n    {query}\n    \"\"\"\n)\nasync def answer(query: str) -&gt; openai.OpenAIDynamicConfig:\n    random_indices = random.sample(range(len(qa_examples)), 3)\n    examples = [\n        [\n            f\"Question: {qa_examples[i]['question']}\",\n            f\"Answer: {qa_examples[i]['answer']}\",\n        ]\n        for i in random_indices\n    ]\n    return {\"computed_fields\": {\"examples\": examples}}\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Take the following responses from an LLM and aggregate/average them into\n    one answer.\n    {responses}\n    \"\"\"\n)\nasync def aggregate_answers(\n    query: str, num_responses: int\n) -&gt; openai.OpenAIDynamicConfig:\n    tasks = [answer(query) for _ in range(num_responses)]\n    responses = await asyncio.gather(*tasks)\n    return {\"computed_fields\": {\"responses\": responses}}\n\n\nasync def demonstration_ensembling(query, ensemble_size):\n    response = await aggregate_answers(query=query, num_responses=ensemble_size)\n    print(response.content)\n\n\nquery = \"\"\"Help me write a notice that highlights the importance of attending \\\nthe company's social events. Give me just the notice, no further explanation.\"\"\"\n\nawait demonstration_ensembling(query=query, ensemble_size=5)\n</pre> import asyncio import random from typing import TypedDict  from mirascope.core import openai, prompt_template   class QA(TypedDict):     question: str     answer: str   qa_examples: list[QA] = [     QA(         question=\"What are your company's core values?\",         answer=\"Our company's core values are integrity, innovation, customer-centricity, and teamwork. We believe that maintaining these values is crucial to achieving our mission and vision.\",     ),     QA(         question=\"How do you handle conflicts in the workplace?\",         answer=\"We handle conflicts by promoting open communication, understanding different perspectives, and seeking mutually beneficial solutions. We have clear policies and trained mediators to assist in resolving conflicts effectively.\",     ),     QA(         question=\"Can you describe a time when you exceeded a client's expectations?\",         answer=\"Certainly. Recently, we completed a project ahead of schedule and under budget. We also provided additional insights and recommendations that significantly benefited the client, earning their gratitude and loyalty.\",     ),     QA(         question=\"How do you ensure continuous improvement within the team?\",         answer=\"We ensure continuous improvement by encouraging regular training, fostering a culture of feedback, and implementing agile methodologies. We also review our processes regularly to identify areas for enhancement.\",     ),     QA(         question=\"What strategies do you use to stay ahead of industry trends?\",         answer=\"We stay ahead of industry trends by investing in research and development, attending industry conferences, and maintaining strong relationships with thought leaders. We also encourage our team to engage in continuous learning and innovation.\",     ),     QA(         question=\"How do you measure the success of a project?\",         answer=\"We measure the success of a project by evaluating key performance indicators such as client satisfaction, budget adherence, timeline compliance, and the quality of the deliverables. Post-project reviews help us to identify successes and areas for improvement.\",     ),     QA(         question=\"What is your approach to diversity and inclusion?\",         answer=\"Our approach to diversity and inclusion involves creating a welcoming environment for all employees, offering diversity training, and implementing policies that promote equality. We value diverse perspectives as they drive innovation and growth.\",     ),     QA(         question=\"How do you manage remote teams effectively?\",         answer=\"We manage remote teams effectively by leveraging technology for communication and collaboration, setting clear goals, and maintaining regular check-ins. We also ensure that remote employees feel included and supported.\",     ),     QA(         question=\"What are your short-term and long-term goals for the company?\",         answer=\"In the short term, our goals include expanding our market reach and enhancing our product offerings. In the long term, we aim to become industry leaders by driving innovation and achieving sustainable growth.\",     ),     QA(         question=\"How do you handle feedback from clients or customers?\",         answer=\"We handle feedback by listening actively, responding promptly, and taking necessary actions to address concerns. We view feedback as an opportunity for improvement and strive to exceed our clients' expectations continuously.\",     ), ]   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Here are some examples that demonstrate the voice to use in a corporate setting.     {examples:lists}      With these examples, answer the following question:     {query}     \"\"\" ) async def answer(query: str) -&gt; openai.OpenAIDynamicConfig:     random_indices = random.sample(range(len(qa_examples)), 3)     examples = [         [             f\"Question: {qa_examples[i]['question']}\",             f\"Answer: {qa_examples[i]['answer']}\",         ]         for i in random_indices     ]     return {\"computed_fields\": {\"examples\": examples}}   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Take the following responses from an LLM and aggregate/average them into     one answer.     {responses}     \"\"\" ) async def aggregate_answers(     query: str, num_responses: int ) -&gt; openai.OpenAIDynamicConfig:     tasks = [answer(query) for _ in range(num_responses)]     responses = await asyncio.gather(*tasks)     return {\"computed_fields\": {\"responses\": responses}}   async def demonstration_ensembling(query, ensemble_size):     response = await aggregate_answers(query=query, num_responses=ensemble_size)     print(response.content)   query = \"\"\"Help me write a notice that highlights the importance of attending \\ the company's social events. Give me just the notice, no further explanation.\"\"\"  await demonstration_ensembling(query=query, ensemble_size=5) <pre>Here is an aggregated notice highlighting the importance of attending the company's social events:\n\n---\n\n**Notice: Importance of Attending Company Social Events**\n\nDear Team,\n\nWe would like to emphasize the significance of participating in our upcoming company social events. These gatherings provide a valuable opportunity to connect with colleagues, foster teamwork, and build a positive workplace culture. Engaging in social activities enhances collaboration and strengthens relationships across departments.\n\nWe encourage everyone to attend and contribute to our vibrant community. Your presence is vital to making these events successful and enjoyable for all, and it enriches both your experience and that of your coworkers.\n\nThank you for your continued commitment to making our workplace more connected and enjoyable.\n\nBest regards,  \n[Your Name]  \n[Your Position]  \n\n--- \n\nThis notice combines essential elements from all responses to create a cohesive message.\n</pre> <p>This implementation consists of three main functions:</p> <ol> <li><code>answer</code>: This function takes a query and returns a response based on a random subset of examples.</li> <li><code>aggregate_answers</code>: This function generates multiple responses using <code>answer</code> and then aggregates them.</li> <li><code>demonstration_ensembling</code>: This function orchestrates the entire process, calling <code>aggregate_answers</code> with the specified ensemble size.</li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/demonstration_ensembling/#demonstration-ensembling-enhancing-llm-responses-with-aggregated-examples","title":"Demonstration Ensembling: Enhancing LLM Responses with Aggregated Examples\u00b6","text":"<p>Demonstration Ensembling is a prompt engineering technique which comprises of taking an aggregate of multiple responses, each of which have been trained on a random subset of examples from the example pool. This recipe demonstrates how to implement Demonstration Ensembling using Large Language Models (LLMs) with Mirascope.</p> <p>Additional Real-World Applications</p> <ul> <li>Code Generation: Break down complex programming tasks into smaller, manageable steps.</li> <li>Data Analysis: Decompose complex data analysis queries into a series of data manipulation and calculation steps.</li> <li>Natural Language Processing: Break down complex NLP tasks like sentiment analysis or named entity recognition into subtasks.</li> <li>Automated Reasoning: Solve complex logical or mathematical problems by breaking them into simpler, solvable steps.</li> <li>Task Planning: Create detailed, step-by-step plans for complex projects or processes.</li> </ul>"},{"location":"tutorials/prompt_engineering/chaining_based/demonstration_ensembling/#implementation","title":"Implementation\u00b6","text":"<p>Let's implement the Demonstration Ensembling technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/demonstration_ensembling/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Demonstration Ensembling technique offers several advantages:</p> <ol> <li>Improved consistency and quality of responses by leveraging multiple examples.</li> <li>Reduced impact of potential biases in individual examples.</li> <li>Enhanced ability to generate responses that capture diverse perspectives.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing the ensemble size with computational cost and time constraints.</li> <li>Carefully curating the example pool to ensure diversity and relevance.</li> <li>Experimenting with different aggregation methods for the final response.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Content Generation: Use Demonstration Ensembling to create more diverse and comprehensive marketing materials.</li> <li>Customer Support: Generate more robust and consistent responses to customer inquiries.</li> <li>Data Analysis: Produce more reliable insights by aggregating multiple interpretations of data.</li> <li>Educational Content: Create well-rounded explanations of complex topics by combining multiple teaching approaches.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Tailoring the example pool to your domain for better performance.</li> <li>Implementing different methods of selecting examples (e.g., weighted sampling based on relevance).</li> <li>Combining Demonstration Ensembling with other techniques like Chain of Thought for even more nuanced responses.</li> <li>Developing a feedback mechanism to continuously improve the quality of the example pool.</li> </ul>"},{"location":"tutorials/prompt_engineering/chaining_based/diverse/","title":"DiVeRSe: Enhancing LLM Reasoning with Prompt Variations","text":"In\u00a0[2]: Copied! <pre>import asyncio\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass PromptVariations(BaseModel):\n    variations: list[str] = Field(..., description=\"Variations of the original prompt\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=PromptVariations)\n@prompt_template(\n    \"\"\"\n    Return the {num_prompts} alternate variations of the prompt which retain the\n    full meaning but uses different phrasing.\n    Prompt: {prompt}\n    \"\"\"\n)\ndef get_prompt_variations(prompt: str, num_prompts: int): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Answer the following question going step by step:\n    {query}\n    \"\"\"\n)\nasync def zero_shot_cot(query: str): ...\n\n\nclass ResponseDetails(BaseModel):\n    solution_number: int = Field(\n        ..., description=\"The actual number given as the answer in a solution.\"\n    )\n    correctness_probability: float = Field(\n        ...,\n        ge=0,\n        le=1,\n        description=\"An estimated probability that the given solution is correct from 0.0 to 1.0\",\n    )\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=ResponseDetails)\n@prompt_template(\n    \"\"\"\n    Here is a query and a response which attempts to answer the query.\n    Prompt: {query}\n    Response: {response}\n\n    Extract the raw numerical value of the answer given by the response, and also\n    give an estimate between 0.0 and 1.0 of the probability that this solution\n    is correct.\n    \"\"\"\n)\nasync def evaluate_response(query: str, response: str): ...\n\n\nasync def diverse(query: str, num_variations: int) -&gt; str:\n    # Gather the variations of the prompt\n    alternate_variations = get_prompt_variations(query, num_variations - 1)\n    all_variations = alternate_variations.variations + [query]\n\n    # Generate a unique reasoning chain for each prompt variation with CoT\n    cot_tasks = [zero_shot_cot(prompt) for prompt in all_variations]\n    cot_responses = [response.content for response in await asyncio.gather(*cot_tasks)]\n\n    # Evaluate each reasoning chain\n    eval_tasks = [\n        evaluate_response(query, cot_response) for cot_response in cot_responses\n    ]\n    eval_responses = await asyncio.gather(*eval_tasks)\n\n    response_scores = {}\n    for eval_response in eval_responses:\n        if eval_response.solution_number not in response_scores:\n            response_scores[eval_response.solution_number] = 0\n        response_scores[eval_response.solution_number] += (\n            eval_response.correctness_probability\n        )\n    best_response = max(response_scores.keys(), key=lambda k: response_scores[k])\n    return best_response\n\n\nasync def run_diverse(prompt, num_variations=3) -&gt; str:\n    response = await diverse(prompt, num_variations)\n    return response\n\n\nquery = \"\"\"\nA committee of 3 people must be formed from a pool of 6 people, but Amy and Bob do not\nget along and should not be on the committee at the same time. How many viable\ncombinations are there?\n\"\"\"\n\nprint(await run_diverse(query))\n</pre> import asyncio  from mirascope.core import openai, prompt_template from pydantic import BaseModel, Field   class PromptVariations(BaseModel):     variations: list[str] = Field(..., description=\"Variations of the original prompt\")   @openai.call(model=\"gpt-4o-mini\", response_model=PromptVariations) @prompt_template(     \"\"\"     Return the {num_prompts} alternate variations of the prompt which retain the     full meaning but uses different phrasing.     Prompt: {prompt}     \"\"\" ) def get_prompt_variations(prompt: str, num_prompts: int): ...   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Answer the following question going step by step:     {query}     \"\"\" ) async def zero_shot_cot(query: str): ...   class ResponseDetails(BaseModel):     solution_number: int = Field(         ..., description=\"The actual number given as the answer in a solution.\"     )     correctness_probability: float = Field(         ...,         ge=0,         le=1,         description=\"An estimated probability that the given solution is correct from 0.0 to 1.0\",     )   @openai.call(model=\"gpt-4o-mini\", response_model=ResponseDetails) @prompt_template(     \"\"\"     Here is a query and a response which attempts to answer the query.     Prompt: {query}     Response: {response}      Extract the raw numerical value of the answer given by the response, and also     give an estimate between 0.0 and 1.0 of the probability that this solution     is correct.     \"\"\" ) async def evaluate_response(query: str, response: str): ...   async def diverse(query: str, num_variations: int) -&gt; str:     # Gather the variations of the prompt     alternate_variations = get_prompt_variations(query, num_variations - 1)     all_variations = alternate_variations.variations + [query]      # Generate a unique reasoning chain for each prompt variation with CoT     cot_tasks = [zero_shot_cot(prompt) for prompt in all_variations]     cot_responses = [response.content for response in await asyncio.gather(*cot_tasks)]      # Evaluate each reasoning chain     eval_tasks = [         evaluate_response(query, cot_response) for cot_response in cot_responses     ]     eval_responses = await asyncio.gather(*eval_tasks)      response_scores = {}     for eval_response in eval_responses:         if eval_response.solution_number not in response_scores:             response_scores[eval_response.solution_number] = 0         response_scores[eval_response.solution_number] += (             eval_response.correctness_probability         )     best_response = max(response_scores.keys(), key=lambda k: response_scores[k])     return best_response   async def run_diverse(prompt, num_variations=3) -&gt; str:     response = await diverse(prompt, num_variations)     return response   query = \"\"\" A committee of 3 people must be formed from a pool of 6 people, but Amy and Bob do not get along and should not be on the committee at the same time. How many viable combinations are there? \"\"\"  print(await run_diverse(query)) <pre>16\n</pre> <p>This implementation consists of several key components:</p> <ol> <li><code>get_prompt_variations</code>: Generates variations of the original prompt.</li> <li><code>zero_shot_cot</code>: Implements zero-shot chain-of-thought reasoning for each prompt variation.</li> <li><code>evaluate_response</code>: Evaluates each reasoning chain and assigns a probability of correctness.</li> <li><code>diverse</code>: Orchestrates the DiVeRSe technique by generating prompt variations, creating reasoning chains, and selecting the best response.</li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/diverse/#diverse-enhancing-llm-reasoning-with-prompt-variations","title":"DiVeRSe: Enhancing LLM Reasoning with Prompt Variations\u00b6","text":"<p>This recipe demonstrates how to implement the DiVeRSe (Diverse Verifier on Reasoning Steps) technique using Large Language Models (LLMs) with Mirascope. DiVeRSe is a prompt engineering method that enhances an LLM's reasoning capabilities by generating multiple reasoning chains from variations of the original prompt.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> <li>Dynamic Configuration</li> </ul> <p>Background</p> <p> DiVeRSe is a variant of the self-consistency prompt engineering technique. Instead of generating multiple chains from the same prompt, DiVeRSe creates variations of the original prompt to generate different reasoning chains. This approach can significantly improve the LLM's ability to reason about complex problems by considering multiple perspectives and phrasings of the same question. </p>"},{"location":"tutorials/prompt_engineering/chaining_based/diverse/#implementation","title":"Implementation\u00b6","text":"<p>Let's implement the DiVeRSe technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/diverse/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The DiVeRSe implementation offers several advantages:</p> <ol> <li>Improved reasoning by considering multiple phrasings of the same problem.</li> <li>Enhanced robustness against potential misinterpretations of the original prompt.</li> <li>Potential for more accurate responses in complex reasoning tasks.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing the number of prompt variations with computational cost and time constraints.</li> <li>Adjusting the evaluation criteria for different types of problems (e.g., numerical vs. categorical answers).</li> <li>Fine-tuning the prompt variation generation to ensure meaningful diversity while maintaining the original question's intent.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Complex Problem Solving: Use DiVeRSe for multi-step problems in fields like mathematics, physics, or engineering.</li> <li>Legal Document Analysis: Apply the technique to interpret complex legal scenarios from multiple perspectives.</li> <li>Market Research: Generate diverse interpretations of consumer feedback or survey responses.</li> <li>Educational Assessment: Create and evaluate multiple versions of exam questions to ensure fairness and comprehension.</li> <li>Scientific Hypothesis Generation: Use DiVeRSe to approach research questions from various angles, potentially uncovering novel insights.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Tailoring the prompt variation generation to your domain for better performance.</li> <li>Experimenting with different evaluation methods for the reasoning chains.</li> <li>Implementing a feedback loop to refine the prompt variation process based on the accuracy of final answers.</li> <li>Combining DiVeRSe with other techniques like Self-Ask or Sim to M for even more nuanced reasoning capabilities.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator, response models, and dynamic configuration, you can easily implement and customize the DiVeRSe technique to enhance your LLM's ability to reason about complex problems across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/least_to_most/","title":"Least to Most: Enhancing LLM Reasoning with Subproblem Decomposition","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\nfrom mirascope.core.openai import OpenAICallResponse\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom pydantic import BaseModel, Field\n\nfew_shot_examples = [\n    {\n        \"question\": \"The median age in the city was 22.1 years. 10.1% of residents were under the age of 18; 56.2% were between the ages of 18 and 24; 16.1% were from 25 to 44; 10.5% were from 45 to 64; and 7% were 65 years of age or older. Which age group is larger: under the age of 18 or 18 and 24?\",\n        \"answer\": 'To answer the question \"Which age group is larger: under the age of 18 or 18 and 24?\", we need to know: \"How many percent were under the age of 18?\", \"How many percent were between the ages of 18 and 24?\".',\n    },\n    {\n        \"question\": \"Old age pensions were raised by 300 francs per month to 1,700 francs for a single person and to 3,700 francs for a couple, while health insurance benefits were made more widely available to unemployed persons and part-time employees. How many francs were the old age pensions for a single person before they were raised?\",\n        \"answer\": 'To answer the question \"How many francs were the old age pensions for a single person before they were raised?\", we need to know: \"How many francs were the old age pensions for a single person?\", \"How many francs were old age pensions raised for a single person?\".',\n    },\n    {\n        \"question\": \"In April 2011, the ECB raised interest rates for the first time since 2008 from 1% to 1.25%, with a further increase to 1.50% in July 2011. However, in 2012-2013 the ECB lowered interest rates to encourage economic growth, reaching the historically low 0.25% in November 2013. Soon after the rates were cut to 0.15%, then on 4 September 2014 the central bank reduced the rates from 0.15% to 0.05%, the lowest rates on record. How many percentage points did interest rates drop between April 2011 and September 2014?\",\n        \"answer\": 'To answer the question \"How many percentage points did interest rates drop between April 2011 and September 2014?\", we need to know: \"What was the interest rate in April 2011?\", \"What was the interest rate in September 2014?\".',\n    },\n    {\n        \"question\": \"Non-nationals make up more than half of the population of Bahrain. According to government statistics dated between 2005-2009 roughly 290,000 Indians, 125,000 Bangladeshis, 45,000 Pakistanis, 45,000 Filipinos, and 8,000 Indonesians. How many Pakistanis and Indonesians are in Bahrain?\",\n        \"answer\": 'To answer the question \"How many Pakistanis and Indonesians are in Bahrain?\", we need to know: \"How many Pakistanis are in Bahrain?\", \"How many Indonesians are in Bahrain?\".',\n    },\n]\n\n\nclass Problem(BaseModel):\n    subproblems: list[str] = Field(\n        ..., description=\"The subproblems that the original problem breaks down into\"\n    )\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Problem)\n@prompt_template(\n    \"\"\"\n    Examples to guide your answer:\n    {examples:lists}\n    Break the following query into subproblems:\n    {query}\n    \"\"\"\n)\ndef break_into_subproblems(\n    query: str, few_shot_examples: list[dict[str, str]]\n) -&gt; openai.OpenAIDynamicConfig:\n    examples = [\n        [f\"Q:{example['question']}\", f\"A:{example['answer']}\"]\n        for example in few_shot_examples\n    ]\n    return {\"computed_fields\": {\"examples\": examples}}\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef call(history: list[ChatCompletionMessageParam]) -&gt; str:\n    return f\"MESSAGES: {history}\"\n\n\ndef least_to_most(query_context: str, query_question: str) -&gt; OpenAICallResponse:\n    problem = break_into_subproblems(\n        query=query_context + query_question, few_shot_examples=few_shot_examples\n    )\n    history: list[ChatCompletionMessageParam] = [\n        {\"role\": \"user\", \"content\": query_context + problem.subproblems[0]}\n    ]\n    response = call(history=history)\n    history.append(response.message_param)\n    if len(problem.subproblems) == 1:\n        return response\n    else:\n        for i in range(1, len(problem.subproblems)):\n            history.append({\"role\": \"user\", \"content\": problem.subproblems[i]})\n            response = call(history=history)\n            history.append(response.message_param)\n        return response\n\n\nquery_context = \"\"\"The Census Bureaus 2006-2010 American Community Survey showed that \\\n(in 2010 inflation adjustment dollars) median household income was $52,056 and the \\\nmedian family income was $58,942.\"\"\"\n\nquery_question = \"How many years did the Census Bureaus American Community Survey last?\"\n\nprint(least_to_most(query_context=query_context, query_question=query_question))\n</pre> from mirascope.core import openai, prompt_template from mirascope.core.openai import OpenAICallResponse from openai.types.chat import ChatCompletionMessageParam from pydantic import BaseModel, Field  few_shot_examples = [     {         \"question\": \"The median age in the city was 22.1 years. 10.1% of residents were under the age of 18; 56.2% were between the ages of 18 and 24; 16.1% were from 25 to 44; 10.5% were from 45 to 64; and 7% were 65 years of age or older. Which age group is larger: under the age of 18 or 18 and 24?\",         \"answer\": 'To answer the question \"Which age group is larger: under the age of 18 or 18 and 24?\", we need to know: \"How many percent were under the age of 18?\", \"How many percent were between the ages of 18 and 24?\".',     },     {         \"question\": \"Old age pensions were raised by 300 francs per month to 1,700 francs for a single person and to 3,700 francs for a couple, while health insurance benefits were made more widely available to unemployed persons and part-time employees. How many francs were the old age pensions for a single person before they were raised?\",         \"answer\": 'To answer the question \"How many francs were the old age pensions for a single person before they were raised?\", we need to know: \"How many francs were the old age pensions for a single person?\", \"How many francs were old age pensions raised for a single person?\".',     },     {         \"question\": \"In April 2011, the ECB raised interest rates for the first time since 2008 from 1% to 1.25%, with a further increase to 1.50% in July 2011. However, in 2012-2013 the ECB lowered interest rates to encourage economic growth, reaching the historically low 0.25% in November 2013. Soon after the rates were cut to 0.15%, then on 4 September 2014 the central bank reduced the rates from 0.15% to 0.05%, the lowest rates on record. How many percentage points did interest rates drop between April 2011 and September 2014?\",         \"answer\": 'To answer the question \"How many percentage points did interest rates drop between April 2011 and September 2014?\", we need to know: \"What was the interest rate in April 2011?\", \"What was the interest rate in September 2014?\".',     },     {         \"question\": \"Non-nationals make up more than half of the population of Bahrain. According to government statistics dated between 2005-2009 roughly 290,000 Indians, 125,000 Bangladeshis, 45,000 Pakistanis, 45,000 Filipinos, and 8,000 Indonesians. How many Pakistanis and Indonesians are in Bahrain?\",         \"answer\": 'To answer the question \"How many Pakistanis and Indonesians are in Bahrain?\", we need to know: \"How many Pakistanis are in Bahrain?\", \"How many Indonesians are in Bahrain?\".',     }, ]   class Problem(BaseModel):     subproblems: list[str] = Field(         ..., description=\"The subproblems that the original problem breaks down into\"     )   @openai.call(model=\"gpt-4o-mini\", response_model=Problem) @prompt_template(     \"\"\"     Examples to guide your answer:     {examples:lists}     Break the following query into subproblems:     {query}     \"\"\" ) def break_into_subproblems(     query: str, few_shot_examples: list[dict[str, str]] ) -&gt; openai.OpenAIDynamicConfig:     examples = [         [f\"Q:{example['question']}\", f\"A:{example['answer']}\"]         for example in few_shot_examples     ]     return {\"computed_fields\": {\"examples\": examples}}   @openai.call(model=\"gpt-4o-mini\") def call(history: list[ChatCompletionMessageParam]) -&gt; str:     return f\"MESSAGES: {history}\"   def least_to_most(query_context: str, query_question: str) -&gt; OpenAICallResponse:     problem = break_into_subproblems(         query=query_context + query_question, few_shot_examples=few_shot_examples     )     history: list[ChatCompletionMessageParam] = [         {\"role\": \"user\", \"content\": query_context + problem.subproblems[0]}     ]     response = call(history=history)     history.append(response.message_param)     if len(problem.subproblems) == 1:         return response     else:         for i in range(1, len(problem.subproblems)):             history.append({\"role\": \"user\", \"content\": problem.subproblems[i]})             response = call(history=history)             history.append(response.message_param)         return response   query_context = \"\"\"The Census Bureaus 2006-2010 American Community Survey showed that \\ (in 2010 inflation adjustment dollars) median household income was $52,056 and the \\ median family income was $58,942.\"\"\"  query_question = \"How many years did the Census Bureaus American Community Survey last?\"  print(least_to_most(query_context=query_context, query_question=query_question)) <pre>To calculate the duration between two years, you subtract the earlier year from the later year. The formula is:\n\n\\[ \\text{Duration} = \\text{Later Year} - \\text{Earlier Year} \\]\n\nFor example, if you want to calculate the duration between 2005 and 2010:\n\n\\[ \\text{Duration} = 2010 - 2005 = 5 \\]\n\nSo, the duration between 2005 and 2010 is 5 years.\n</pre> <p>This implementation consists of three main components:</p> <ol> <li><code>break_into_subproblems</code>: This function takes a query and breaks it down into subproblems using few-shot examples.</li> <li><code>call</code>: A simple function that makes a call to the LLM with the given message history.</li> <li><code>least_to_most</code>: This function orchestrates the Least to Most technique. It first breaks the problem into subproblems, then solves each subproblem sequentially, appending the results to the message history.</li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/least_to_most/#least-to-most-enhancing-llm-reasoning-with-subproblem-decomposition","title":"Least to Most: Enhancing LLM Reasoning with Subproblem Decomposition\u00b6","text":"<p>This recipe demonstrates how to implement the Least to Most technique using Large Language Models (LLMs) with Mirascope. Least to Most is a prompt engineering method that enhances an LLM's reasoning capabilities by breaking down complex problems into smaller, more manageable subproblems.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> <li>Response Models</li> </ul> <p>Background</p> <p> Least to Most is a more extensive version of Chain of Thought, where separate calls are used to break down the original problem into subproblems as well as solve each individual step/subproblem. After solving each subproblem, the result is appended to the chat's history until the original problem is solved. Least to Most is an effective technique for symbolic and arithmetic reasoning tasks. </p>"},{"location":"tutorials/prompt_engineering/chaining_based/least_to_most/#implementing-least-to-most","title":"Implementing Least to Most\u00b6","text":"<p>Let's implement the Least to Most technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/least_to_most/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Least to Most implementation offers several advantages:</p> <ol> <li>Improved reasoning for complex problems by breaking them down into manageable steps.</li> <li>Enhanced ability to handle multi-step arithmetic and symbolic reasoning tasks.</li> <li>Potential for more accurate responses by solving subproblems individually.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Carefully crafting few-shot examples to guide the problem decomposition process.</li> <li>Balancing the number of subproblems to avoid oversimplification or overcomplexity.</li> <li>Ensuring that the query context and question are clear and contain all necessary information.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Complex Mathematical Problem Solving: Use Least to Most for multi-step mathematical proofs or calculations.</li> <li>Project Planning: Break down large projects into smaller, manageable tasks.</li> <li>Algorithmic Design: Decompose complex algorithms into simpler steps for implementation.</li> <li>Legal Case Analysis: Break down complex legal cases into individual points of law to be addressed.</li> <li>Medical Diagnosis: Analyze symptoms and test results step-by-step to reach a diagnosis.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Tailoring the few-shot examples to your domain for better problem decomposition.</li> <li>Implementing a mechanism to handle interdependent subproblems.</li> <li>Combining Least to Most with other techniques like Self-Consistency for even more robust reasoning.</li> <li>Developing a feedback loop to refine the problem decomposition process based on the accuracy of final answers.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator, response models, and dynamic configuration, you can easily implement and customize the Least to Most technique to enhance your LLM's ability to reason about complex problems across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/mixture_of_reasoning/","title":"Mixture of Reasoning: Enhancing LLM Performance with Multiple Techniques","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Answer this question, thinking step by step.\n    {query}\n    \"\"\"\n)\ndef cot_call(query: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    {query}\n    It's very important to my career.\n    \"\"\"\n)\ndef emotion_prompting_call(query: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    {query}\n    Rephrase and expand the question, and respond.\n    \"\"\"\n)\ndef rar_call(query: str): ...\n\n\nclass BestResponse(BaseModel):\n    best_response: str = Field(\n        ..., description=\"The best response of the options given, verbatim\"\n    )\n    reasoning: str = Field(\n        ...,\n        description=\"\"\"A short description of why this is the best response to\n        the query, along with reasons why the other answers were worse.\"\"\",\n    )\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=BestResponse)\n@prompt_template(\n    \"\"\"\n    Here is a query: {query}\n\n    Evaluate the following responses from LLMs and decide which one\n    is the best based on correctness, fulfillment of the query, and clarity:\n\n    Response 1:\n    {cot_response}\n\n    Response 2:\n    {emotion_prompting_response}\n\n    Response 3:\n    {rar_response}\n    \"\"\"\n)\ndef mixture_of_reasoning(query: str) -&gt; openai.OpenAIDynamicConfig:\n    cot_response = cot_call(query=query)\n    emotion_prompting_response = emotion_prompting_call(query=query)\n    rar_response = rar_call(query=query)\n\n    return {\n        \"computed_fields\": {\n            \"cot_response\": cot_response,\n            \"emotion_prompting_response\": emotion_prompting_response,\n            \"rar_response\": rar_response,\n        }\n    }\n\n\nprompt = \"What are the side lengths of a rectangle with area 8 and perimeter 12?\"\n\nbest_response = mixture_of_reasoning(prompt)\nprint(best_response.best_response)\nprint(best_response.reasoning)\n</pre> from mirascope.core import openai, prompt_template from pydantic import BaseModel, Field   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Answer this question, thinking step by step.     {query}     \"\"\" ) def cot_call(query: str): ...   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     {query}     It's very important to my career.     \"\"\" ) def emotion_prompting_call(query: str): ...   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     {query}     Rephrase and expand the question, and respond.     \"\"\" ) def rar_call(query: str): ...   class BestResponse(BaseModel):     best_response: str = Field(         ..., description=\"The best response of the options given, verbatim\"     )     reasoning: str = Field(         ...,         description=\"\"\"A short description of why this is the best response to         the query, along with reasons why the other answers were worse.\"\"\",     )   @openai.call(model=\"gpt-4o-mini\", response_model=BestResponse) @prompt_template(     \"\"\"     Here is a query: {query}      Evaluate the following responses from LLMs and decide which one     is the best based on correctness, fulfillment of the query, and clarity:      Response 1:     {cot_response}      Response 2:     {emotion_prompting_response}      Response 3:     {rar_response}     \"\"\" ) def mixture_of_reasoning(query: str) -&gt; openai.OpenAIDynamicConfig:     cot_response = cot_call(query=query)     emotion_prompting_response = emotion_prompting_call(query=query)     rar_response = rar_call(query=query)      return {         \"computed_fields\": {             \"cot_response\": cot_response,             \"emotion_prompting_response\": emotion_prompting_response,             \"rar_response\": rar_response,         }     }   prompt = \"What are the side lengths of a rectangle with area 8 and perimeter 12?\"  best_response = mixture_of_reasoning(prompt) print(best_response.best_response) print(best_response.reasoning) <pre>Response 1\nResponse 1 provides a clear and systematic approach to solving the problem by setting up the equations based on the given area and perimeter. It correctly derives the quadratic equation, factors it, and finds both possible lengths and widths, concluding that the side lengths are indeed 2 and 4. Response 2 has a similar structure and is also correct but slightly less clear in its explanation. Response 3 is overly verbose and somewhat redundant in conveying the same information, which might confuse rather than clarify.\n</pre> <p>This implementation consists of several key components:</p> <ol> <li><p>Three different prompt engineering techniques:</p> <ul> <li><code>cot_call</code>: Chain of Thought reasoning</li> <li><code>emotion_prompting_call</code>: Emotion prompting</li> <li><code>rar_call</code>: Rephrase and Respond</li> </ul> </li> <li><p>A <code>BestResponse</code> model to structure the output of the final evaluation.</p> </li> <li><p>The <code>mixture_of_reasoning</code> function, which:</p> <ul> <li>Calls each of the three prompt engineering techniques</li> <li>Uses dynamic configuration to pass the responses to the final evaluation</li> <li>Returns the best response and reasoning using the <code>BestResponse</code> model</li> </ul> </li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/mixture_of_reasoning/#mixture-of-reasoning-enhancing-llm-performance-with-multiple-techniques","title":"Mixture of Reasoning: Enhancing LLM Performance with Multiple Techniques\u00b6","text":"<p>Mixture of Reasoning is a prompt engineering technique where you set up multiple calls, each utilizing a different prompt engineering technique. This approach is best when you want to be able to handle a wide variety of responses, or have a variety of techniques that you have found to be successful for responding to a type of prompt.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> <li>Response Models</li> </ul> <p>Background</p> <p> In the original paper, a trained classifier is used to determine the best answer, but since we do not have access to that, we will use an LLM to evaluate the best answer instead. To get a clean separation between the reasoning and the actual output, we'll use <code>response_model</code> in our final evaluation call. </p>"},{"location":"tutorials/prompt_engineering/chaining_based/mixture_of_reasoning/#implementation","title":"Implementation\u00b6","text":"<p>Let's implement the Mixture of Reasoning technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/mixture_of_reasoning/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Mixture of Reasoning implementation offers several advantages:</p> <ol> <li>Improved ability to handle a wide variety of queries by leveraging multiple prompt engineering techniques.</li> <li>Enhanced performance by selecting the best response from multiple approaches.</li> <li>Flexibility to add or modify prompt engineering techniques as needed.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Carefully selecting the prompt engineering techniques to include based on your specific use case.</li> <li>Balancing the number of techniques with computational cost and time constraints.</li> <li>Fine-tuning the evaluation criteria in the final step to best suit your needs.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Complex Problem Solving: Use Mixture of Reasoning for multi-step problems in fields like mathematics, physics, or engineering.</li> <li>Customer Support: Implement different response strategies to handle various types of customer queries effectively.</li> <li>Content Generation: Generate diverse content ideas by applying multiple creative thinking techniques.</li> <li>Decision Making: Analyze complex scenarios from different perspectives to make more informed decisions.</li> <li>Educational Tutoring: Provide explanations using various teaching methods to cater to different learning styles.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Experimenting with different combinations of prompt engineering techniques.</li> <li>Implementing a feedback loop to continuously improve the selection of the best response.</li> <li>Tailoring the evaluation criteria to your specific domain or task requirements.</li> <li>Combining Mixture of Reasoning with other techniques like Self-Consistency for even more robust reasoning capabilities.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator, response models, and dynamic configuration, you can easily implement and customize the Mixture of Reasoning technique to enhance your LLM's performance across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/prompt_paraphrasing/","title":"Prompt Paraphrasing: Generating Diverse Prompts for LLM Testing and Evaluation","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Translations(BaseModel):\n    translations: list[str] = Field(\n        ..., description=\"The list of translations into the requested language\"\n    )\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Translations)\n@prompt_template(\n    \"\"\"\n    For this phrase: {phrase}\n\n\n    Give {num_translations} translations in {language}\n    \"\"\"\n)\ndef translate(phrase: str, language: str, num_translations: int): ...\n\n\ndef prompt_paraphrasing(query: str, num_translations: int = 3) -&gt; set[str]:\n    spanish_translations = translate(\n        phrase=query,\n        language=\"Spanish\",\n        num_translations=num_translations,\n    )\n    # Avoid Duplicates\n    prompt_variations = set()\n    for spanish_phrase in spanish_translations.translations:\n        back_translations = translate(\n            spanish_phrase, language=\"English\", num_translations=3\n        )\n        prompt_variations.update(back_translations.translations)\n    return prompt_variations\n\n\nprint(\n    prompt_paraphrasing(\n        query=\"What are some manageable ways to improve my focus and productivity?\"\n    )\n)\n</pre> from mirascope.core import openai, prompt_template from pydantic import BaseModel, Field   class Translations(BaseModel):     translations: list[str] = Field(         ..., description=\"The list of translations into the requested language\"     )   @openai.call(model=\"gpt-4o-mini\", response_model=Translations) @prompt_template(     \"\"\"     For this phrase: {phrase}       Give {num_translations} translations in {language}     \"\"\" ) def translate(phrase: str, language: str, num_translations: int): ...   def prompt_paraphrasing(query: str, num_translations: int = 3) -&gt; set[str]:     spanish_translations = translate(         phrase=query,         language=\"Spanish\",         num_translations=num_translations,     )     # Avoid Duplicates     prompt_variations = set()     for spanish_phrase in spanish_translations.translations:         back_translations = translate(             spanish_phrase, language=\"English\", num_translations=3         )         prompt_variations.update(back_translations.translations)     return prompt_variations   print(     prompt_paraphrasing(         query=\"What are some manageable ways to improve my focus and productivity?\"     ) ) <pre>{'What are some ways to boost my focus and achieve greater productivity in a manageable fashion?', 'How can I improve my focus and productivity?', 'What methods are effective for enhancing my concentration and productivity?', 'What are some practical strategies to boost my focus and productivity?', 'What are some feasible methods to enhance my concentration and productivity?', 'What are some manageable ways to improve my focus and productivity?', 'What are useful ways to increase my concentration and productivity?', 'How can I improve my focus and be more productive in a manageable way?', 'How can I enhance my concentration and increase my productivity in a sustainable manner?'}\n</pre> <p>This implementation consists of two main functions:</p> <ol> <li><code>translate</code>: This function takes a phrase, target language, and number of translations as input, and returns multiple translations of the phrase in the specified language.</li> <li><code>prompt_paraphrasing</code>: This function orchestrates the Prompt Paraphrasing technique. It first translates the input query into Spanish, then back-translates each Spanish translation into English, creating a set of diverse prompt variations.</li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/prompt_paraphrasing/#prompt-paraphrasing-generating-diverse-prompts-for-llm-testing-and-evaluation","title":"Prompt Paraphrasing: Generating Diverse Prompts for LLM Testing and Evaluation\u00b6","text":"<p>Prompt Paraphrasing is not a prompt engineering technique, but rather a prompt generation technique used to create ensembles of prompts for testing or other prompt engineering techniques. In this example, we cover a specific method of generating prompts mentioned in the paper whereby a prompt is translated into $B$ versions in another language, then backtranslated into $B^2$ versions to English.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul>"},{"location":"tutorials/prompt_engineering/chaining_based/prompt_paraphrasing/#implementation","title":"Implementation\u00b6","text":"<p>Let's implement the Prompt Paraphrasing technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/prompt_paraphrasing/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Prompt Paraphrasing implementation offers several advantages:</p> <ol> <li>Generation of diverse prompt variations for more robust LLM testing and evaluation.</li> <li>Potential discovery of more effective phrasings for specific tasks or queries.</li> <li>Improved understanding of LLM behavior across different linguistic formulations.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing the number of translations and languages with computational cost and time constraints.</li> <li>Selecting appropriate languages for translation based on your specific use case or target audience.</li> <li>Implementing a filtering mechanism to remove nonsensical or overly divergent paraphrases.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Robustness Testing: Use prompt paraphrasing to test LLM performance across various phrasings of the same query.</li> <li>Data Augmentation: Generate additional training data by paraphrasing existing prompts or questions.</li> <li>Chatbot Improvement: Enhance chatbot understanding by training on paraphrased versions of common queries.</li> <li>Cross-lingual Information Retrieval: Improve search results by querying with multiple paraphrased versions of the search term.</li> <li>Writing Assistance: Offer users alternative phrasings for their writing to improve clarity or style.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Experimenting with different source and target languages for translation.</li> <li>Implementing a scoring mechanism to rank paraphrases based on relevance or quality.</li> <li>Combining Prompt Paraphrasing with other techniques like Chain of Thought or Self-Consistency for more comprehensive LLM evaluation.</li> <li>Developing a feedback loop to refine the paraphrasing process based on LLM performance on different prompt variations.</li> </ul> <p>By leveraging Mirascope calls and response models, you can easily implement and customize the Prompt Paraphrasing technique to generate diverse prompts for LLM testing, evaluation, and improvement across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/reverse_chain_of_thought/","title":"Reverse Chain of Thought: Enhancing LLM Reasoning with Self-Reflection","text":"In\u00a0[1]: Copied! <pre>import asyncio\n\nfrom mirascope.core import openai, prompt_template\nfrom mirascope.core.openai import OpenAICallResponse\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom pydantic import BaseModel, Field\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef zero_shot_cot(query: str) -&gt; str:\n    return f\"{query} Let's think step by step.\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    USER:\n    Give the concrete prompt (problem) that can generate this answer.\n    The problem should contain all basic and necessary information and correspond to the answer.\n    The problem can only ask for one result.\n\n    {response}\n    \"\"\"\n)\ndef reconstruct_query(response: str): ...\n\n\nclass Decomposition(BaseModel):\n    conditions: list[str] = Field(\n        ..., description=\"A list of conditions of the problem.\"\n    )\n\n\n@openai.call(\n    model=\"gpt-4o-mini\",\n    response_model=Decomposition,\n    call_params={\"tool_choice\": \"required\"},\n)\n@prompt_template(\n    \"\"\"\n    Please list the conditions of the problem. There may be multiple conditions.\n    Do not list conditions not related to calculations, but list all necessary conditions.\n    The format should be a list of conditions with one condition per item.\n\n    {query}\n    \"\"\"\n)\nasync def decompose_query(query: str): ...\n\n\nclass Comparison(BaseModel):\n    condition: str = Field(\n        ..., description=\"The original condition the comparison was made with, verbatim\"\n    )\n    deducible: bool = Field(\n        ...,\n        description=\"Whether the condition is deducible from the list of other conditions.\",\n    )\n    illustration: str = Field(\n        ...,\n        description=\"A quick illustration of the reason the condition is/isn't deducible from the list of other conditions.\",\n    )\n\n\n@openai.call(\n    model=\"gpt-4o-mini\",\n    response_model=Comparison,\n    call_params={\"tool_choice\": \"required\"},\n)\n@prompt_template(\n    \"\"\"\n    Given a candidate condition: '{condition}'\n\n    Here is a condition list: '{condition_list}'\n\n    From a mathematical point of view, can this candidate condition be deduced from the condition list?\n    Please illustrate your reason and answer True or False.\n    \"\"\"\n)\nasync def compare_conditions(condition: str, condition_list: list[str]): ...\n\n\n@openai.call(\n    model=\"gpt-4o-mini\", response_model=bool, call_params={\"tool_choice\": \"required\"}\n)\n@prompt_template(\n    \"\"\"\n    Q1: {original_problem}\n    Q2: {reconstructed_problem}\n    \n    From a mathematical point of view, are these two problems asking the same thing at the end?\n    \"\"\"\n)\ndef compare_questions(original_problem: str, reconstructed_problem: str): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    MESSAGES: {history}\n    USER:\n    {mistakes_prompt}\n    {overlooked_prompt}\n    {hallucination_prompt}\n    {misinterpretation_prompt}\n    \"\"\"\n)\nasync def fine_grained_comparison(\n    history: list[ChatCompletionMessageParam], query: str, reconstructed_query: str\n) -&gt; openai.OpenAIDynamicConfig:\n    # Decompose both queries into conditions\n    original_conditions, reconstructed_conditions = [\n        response.conditions\n        for response in await asyncio.gather(\n            decompose_query(query), decompose_query(reconstructed_query)\n        )\n    ]\n\n    # Identify overlooked/hallucinated conditions and misinterpretation of question\n    overlooking_tasks = [\n        compare_conditions(original_condition, reconstructed_conditions)\n        for original_condition in original_conditions\n    ]\n    hallucination_tasks = [\n        compare_conditions(reconstructed_condition, original_conditions)\n        for reconstructed_condition in reconstructed_conditions\n    ]\n    full_comparison = await asyncio.gather(*(overlooking_tasks + hallucination_tasks))\n\n    question_misinterpretation = compare_questions(query, reconstructed_query)\n\n    overlooked_comparisons = [\n        comparison\n        for comparison in full_comparison[: len(original_conditions)]\n        if not comparison.deducible\n    ]\n    hallucination_comparisons = [\n        comparison\n        for comparison in full_comparison[len(original_conditions) :]\n        if not comparison.deducible\n    ]\n\n    # Fill out prompt depending on the comparisons\n    if (\n        not question_misinterpretation\n        and not overlooked_comparisons\n        and not hallucination_comparisons\n    ):\n        mistakes_prompt = \"\"\"There are no mistakes in your interpretation of the prompt.\n        Repeat your original solution verbatim.\"\"\"\n        overlooked_prompt = \"\"\n        hallucination_prompt = \"\"\n        misinterpretation_prompt = \"\"\n    else:\n        mistakes_prompt = (\n            \"Here are the mistakes and reasons in your answer to the problem.\\n\"\n        )\n\n        if overlooked_comparisons:\n            conditions = [comparison.condition for comparison in overlooked_comparisons]\n            illustrations = [\n                comparison.illustration for comparison in overlooked_comparisons\n            ]\n            overlooked_prompt = f\"\"\"\n            Overlooked Conditions:\n            You have ignored some real conditions:\n            {conditions}\n            The real problem has the conditions:\n            {original_conditions}\n            You should consider all real conditions in the problem.\n            Here are the detailed reasons:\n            {illustrations}\"\"\"\n        else:\n            overlooked_prompt = \"\"\n\n        if hallucination_comparisons:\n            conditions = [\n                comparison.condition for comparison in hallucination_comparisons\n            ]\n            illustrations = [\n                comparison.illustration for comparison in overlooked_comparisons\n            ]\n            hallucination_prompt = f\"\"\"\n            Hallucinated Conditions\n            You use some wrong candidate conditions:\n            {conditions}\n            They all can not be deduced from the true condition list.\n            The real problem has the conditions:\n            {original_conditions}\n            You should consider all real conditions in the problem.\n            Here are the detailed reasons:\n            {illustrations}\"\"\"\n        else:\n            hallucination_prompt = \"\"\n\n        if question_misinterpretation:\n            misinterpretation_prompt = f\"\"\"\n            You misunderstood the question.\n            You think the question is: {reconstructed_query}.\n            But the real question is: {query}\n            They are different. You should consider the original question.\"\"\"\n        else:\n            misinterpretation_prompt = \"\"\n    return {\n        \"computed_fields\": {\n            \"mistakes_prompt\": mistakes_prompt,\n            \"overlooked_prompt\": overlooked_prompt,\n            \"hallucination_prompt\": hallucination_prompt,\n            \"misinterpretation_prompt\": misinterpretation_prompt,\n        }\n    }\n\n\nasync def reverse_cot(query: str) -&gt; OpenAICallResponse:\n    cot_response = zero_shot_cot(query=query)\n    reconstructed_query_response = reconstruct_query(cot_response.content)\n    history = cot_response.messages + reconstructed_query_response.messages\n    response = await fine_grained_comparison(\n        history=history,\n        query=query,\n        reconstructed_query=reconstructed_query_response.content,\n    )\n    return response\n\n\nquery = \"\"\"At the trip to the county level scavenger hunt competition 90 people \\\nwere required to split into groups for the competition to begin. To break \\\npeople up into smaller groups with different leaders 9-person groups were \\\nformed. If 3/5 of the number of groups each had members bring back 2 seashells each \\\nhow many seashells did they bring?\"\"\"\n\nprint(await reverse_cot(query=query))\n</pre> import asyncio  from mirascope.core import openai, prompt_template from mirascope.core.openai import OpenAICallResponse from openai.types.chat import ChatCompletionMessageParam from pydantic import BaseModel, Field   @openai.call(model=\"gpt-4o-mini\") def zero_shot_cot(query: str) -&gt; str:     return f\"{query} Let's think step by step.\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     USER:     Give the concrete prompt (problem) that can generate this answer.     The problem should contain all basic and necessary information and correspond to the answer.     The problem can only ask for one result.      {response}     \"\"\" ) def reconstruct_query(response: str): ...   class Decomposition(BaseModel):     conditions: list[str] = Field(         ..., description=\"A list of conditions of the problem.\"     )   @openai.call(     model=\"gpt-4o-mini\",     response_model=Decomposition,     call_params={\"tool_choice\": \"required\"}, ) @prompt_template(     \"\"\"     Please list the conditions of the problem. There may be multiple conditions.     Do not list conditions not related to calculations, but list all necessary conditions.     The format should be a list of conditions with one condition per item.      {query}     \"\"\" ) async def decompose_query(query: str): ...   class Comparison(BaseModel):     condition: str = Field(         ..., description=\"The original condition the comparison was made with, verbatim\"     )     deducible: bool = Field(         ...,         description=\"Whether the condition is deducible from the list of other conditions.\",     )     illustration: str = Field(         ...,         description=\"A quick illustration of the reason the condition is/isn't deducible from the list of other conditions.\",     )   @openai.call(     model=\"gpt-4o-mini\",     response_model=Comparison,     call_params={\"tool_choice\": \"required\"}, ) @prompt_template(     \"\"\"     Given a candidate condition: '{condition}'      Here is a condition list: '{condition_list}'      From a mathematical point of view, can this candidate condition be deduced from the condition list?     Please illustrate your reason and answer True or False.     \"\"\" ) async def compare_conditions(condition: str, condition_list: list[str]): ...   @openai.call(     model=\"gpt-4o-mini\", response_model=bool, call_params={\"tool_choice\": \"required\"} ) @prompt_template(     \"\"\"     Q1: {original_problem}     Q2: {reconstructed_problem}          From a mathematical point of view, are these two problems asking the same thing at the end?     \"\"\" ) def compare_questions(original_problem: str, reconstructed_problem: str): ...   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     MESSAGES: {history}     USER:     {mistakes_prompt}     {overlooked_prompt}     {hallucination_prompt}     {misinterpretation_prompt}     \"\"\" ) async def fine_grained_comparison(     history: list[ChatCompletionMessageParam], query: str, reconstructed_query: str ) -&gt; openai.OpenAIDynamicConfig:     # Decompose both queries into conditions     original_conditions, reconstructed_conditions = [         response.conditions         for response in await asyncio.gather(             decompose_query(query), decompose_query(reconstructed_query)         )     ]      # Identify overlooked/hallucinated conditions and misinterpretation of question     overlooking_tasks = [         compare_conditions(original_condition, reconstructed_conditions)         for original_condition in original_conditions     ]     hallucination_tasks = [         compare_conditions(reconstructed_condition, original_conditions)         for reconstructed_condition in reconstructed_conditions     ]     full_comparison = await asyncio.gather(*(overlooking_tasks + hallucination_tasks))      question_misinterpretation = compare_questions(query, reconstructed_query)      overlooked_comparisons = [         comparison         for comparison in full_comparison[: len(original_conditions)]         if not comparison.deducible     ]     hallucination_comparisons = [         comparison         for comparison in full_comparison[len(original_conditions) :]         if not comparison.deducible     ]      # Fill out prompt depending on the comparisons     if (         not question_misinterpretation         and not overlooked_comparisons         and not hallucination_comparisons     ):         mistakes_prompt = \"\"\"There are no mistakes in your interpretation of the prompt.         Repeat your original solution verbatim.\"\"\"         overlooked_prompt = \"\"         hallucination_prompt = \"\"         misinterpretation_prompt = \"\"     else:         mistakes_prompt = (             \"Here are the mistakes and reasons in your answer to the problem.\\n\"         )          if overlooked_comparisons:             conditions = [comparison.condition for comparison in overlooked_comparisons]             illustrations = [                 comparison.illustration for comparison in overlooked_comparisons             ]             overlooked_prompt = f\"\"\"             Overlooked Conditions:             You have ignored some real conditions:             {conditions}             The real problem has the conditions:             {original_conditions}             You should consider all real conditions in the problem.             Here are the detailed reasons:             {illustrations}\"\"\"         else:             overlooked_prompt = \"\"          if hallucination_comparisons:             conditions = [                 comparison.condition for comparison in hallucination_comparisons             ]             illustrations = [                 comparison.illustration for comparison in overlooked_comparisons             ]             hallucination_prompt = f\"\"\"             Hallucinated Conditions             You use some wrong candidate conditions:             {conditions}             They all can not be deduced from the true condition list.             The real problem has the conditions:             {original_conditions}             You should consider all real conditions in the problem.             Here are the detailed reasons:             {illustrations}\"\"\"         else:             hallucination_prompt = \"\"          if question_misinterpretation:             misinterpretation_prompt = f\"\"\"             You misunderstood the question.             You think the question is: {reconstructed_query}.             But the real question is: {query}             They are different. You should consider the original question.\"\"\"         else:             misinterpretation_prompt = \"\"     return {         \"computed_fields\": {             \"mistakes_prompt\": mistakes_prompt,             \"overlooked_prompt\": overlooked_prompt,             \"hallucination_prompt\": hallucination_prompt,             \"misinterpretation_prompt\": misinterpretation_prompt,         }     }   async def reverse_cot(query: str) -&gt; OpenAICallResponse:     cot_response = zero_shot_cot(query=query)     reconstructed_query_response = reconstruct_query(cot_response.content)     history = cot_response.messages + reconstructed_query_response.messages     response = await fine_grained_comparison(         history=history,         query=query,         reconstructed_query=reconstructed_query_response.content,     )     return response   query = \"\"\"At the trip to the county level scavenger hunt competition 90 people \\ were required to split into groups for the competition to begin. To break \\ people up into smaller groups with different leaders 9-person groups were \\ formed. If 3/5 of the number of groups each had members bring back 2 seashells each \\ how many seashells did they bring?\"\"\"  print(await reverse_cot(query=query)) <pre>Thank you for your feedback! Based on your clarifications, here\u2019s a revised problem prompt that corresponds to your original question while considering all the necessary conditions:\n\n---\n\n**Problem Prompt:**\n\nAt the trip to the county-level scavenger hunt competition, there are 90 participants who need to be divided into groups for the event to start. Each group consists of 9 people. If \\( \\frac{3}{5} \\) of the total number of groups formed contributed by bringing back seashells, and each member of those groups brought back 2 seashells each, how many seashells were brought back in total?\n\n---\n\nThis prompt encapsulates all essential components of the problem. It clarifies the number of participants, how they are grouped, the fraction of groups contributing, and the number of seashells collected by each member, leading to the final calculation. Let's go through the reasoning for clarity:\n\n1. **Total Participants**: 90 people.\n2. **Groups Formed**: Each group has 9 members, so:\n   \\[\n   \\text{Number of groups} = \\frac{90}{9} = 10\n   \\]\n3. **Groups Contributing Shells**: \\( \\frac{3}{5} \\) of the total groups contributed:\n   \\[\n   \\text{Groups contributing} = 10 \\times \\frac{3}{5} = 6\n   \\]\n4. **Total Members in Contributing Groups**: Each contributing group has 9 members:\n   \\[\n   \\text{Total members contributing} = 6 \\times 9 = 54\n   \\]\n5. **Total Seashells Collected**: Each member brought back 2 seashells:\n   \\[\n   \\text{Total seashells} = 54 \\times 2 = 108\n   \\]\n\nThus, the total number of seashells brought back is:\n\n\\[\n\\boxed{108}\n\\] \n\nThis approach correctly follows the sequence of logical deductions based on the problem's premise. Thank you for your guidance!\n</pre> <p>This implementation consists of several key functions:</p> <ol> <li><code>zero_shot_cot</code>: Generates an initial chain of thought response.</li> <li><code>reconstruct_query</code>: Attempts to reconstruct the original query from the chain of thought response.</li> <li><code>decompose_query</code>: Breaks down a query into its individual conditions.</li> <li><code>compare_conditions</code>: Compares individual conditions to determine if they are deducible from a list of other conditions.</li> <li><code>compare_questions</code>: Checks if two questions are asking the same thing.</li> <li><code>fine_grained_comparison</code>: Performs a detailed comparison between the original and reconstructed queries, identifying overlooked conditions, hallucinations, and misinterpretations.</li> <li><code>reverse_cot</code>: Orchestrates the entire Reverse Chain of Thought process.</li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/reverse_chain_of_thought/#reverse-chain-of-thought-enhancing-llm-reasoning-with-self-reflection","title":"Reverse Chain of Thought: Enhancing LLM Reasoning with Self-Reflection\u00b6","text":"<p>This recipe demonstrates how to implement the Reverse Chain of Thought technique using Large Language Models (LLMs) with Mirascope. Reverse Chain of Thought is a prompt engineering method that enhances an LLM's reasoning capabilities by encouraging it to reflect on and correct its own thought process.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> <li>Response Models</li> </ul> <p>Background</p> <p> Reverse chain of thought is a prompt engineering technique where a chain of thought call is made for a query, then we attempt to reconstruct the query from the attempted solution. Both the original and reconstructed query are broken down into their individual conditions, and each condition is cross-referenced with the totality of conditions for the other query to determine the existence of overlooked facts or hallucinations. The questions themselves are also compared to ensure that the two queries not only share the context but also ask the same question. This fine-grained comparison is used in a final prompt. </p> <p> In the original paper, no prompt was given for the case when mistakes do not exist, so we took the liberty of asking the model to repeat a solution without mistakes.  </p> <p> Reverse chain of thought is a technique that works for any prompt which has shown signs of susceptibility to hallucinations or misinterpretations in its initial attempts to answer the question. </p>"},{"location":"tutorials/prompt_engineering/chaining_based/reverse_chain_of_thought/#implementation","title":"Implementation\u00b6","text":"<p>Let's implement the Reverse Chain of Thought technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/reverse_chain_of_thought/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Reverse Chain of Thought implementation offers several advantages:</p> <ol> <li>Improved accuracy by identifying and correcting overlooked conditions, hallucinations, and misinterpretations.</li> <li>Enhanced reasoning capabilities through self-reflection and error correction.</li> <li>More robust problem-solving, especially for complex or ambiguous queries.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing the computational cost of multiple LLM calls with the improved accuracy.</li> <li>Fine-tuning the prompts for each step to optimize the reflection and correction process.</li> <li>Adapting the technique for different types of problems or domains.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Complex Problem Solving: Use Reverse Chain of Thought for multi-step problems in fields like physics or engineering.</li> <li>Legal Analysis: Apply the technique to enhance the accuracy of legal interpretations and argumentation.</li> <li>Medical Diagnosis: Implement Reverse Chain of Thought to improve the reliability of symptom analysis and potential diagnoses.</li> <li>Financial Modeling: Enhance the accuracy of financial predictions and risk assessments by identifying overlooked factors.</li> <li>Educational Assessment: Use the technique to generate and validate complex exam questions and their solutions.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Tailoring the decomposition and comparison steps to your domain for better performance.</li> <li>Implementing a feedback loop to continuously improve the quality of the Reverse Chain of Thought responses.</li> <li>Combining Reverse Chain of Thought with other techniques like Self-Ask or Self-Consistency for even more powerful reasoning capabilities.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator, response models, and dynamic configuration, you can easily implement and customize the Reverse Chain of Thought technique to enhance your LLM's reasoning capabilities across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/self_consistency/","title":"Self-Consistency: Enhancing LLM Reasoning with Multiple Outputs","text":"In\u00a0[2]: Copied! <pre>import asyncio\nfrom collections import Counter\n\nfrom mirascope.core import openai, prompt_template\n\nfew_shot_examples = [\n    {\n        \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n        \"answer\": \"We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6.\",\n    },\n    {\n        \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n        \"answer\": \"There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\",\n    },\n    {\n        \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n        \"answer\": \"Leah had 32 chocolates and Leah\u2019s sister had 42. That means there were originally 32 + 42 = 74 chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\",\n    },\n    {\n        \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n        \"answer\": \"Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\",\n    },\n    {\n        \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n        \"answer\": \"He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he has 7 + 2 = 9 toys. The answer is 9.\",\n    },\n    {\n        \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n        \"answer\": \"There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29.\",\n    },\n    {\n        \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n        \"answer\": \"Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\",\n    },\n]\n\n\n@openai.call(model=\"gpt-4o-mini\", call_params={\"temperature\": 0.5})\n@prompt_template(\n    \"\"\"\n    Some examples on how to think step by step:\n    {examples:lists}\n\n    Answer the following question, thinking step by step:\n    {query}\n    \"\"\"\n)\nasync def chain_of_thought(\n    query: str, few_shot_examples: list[dict[str, str]]\n) -&gt; openai.OpenAIDynamicConfig:\n    examples = [\n        [f\"Q:{example['question']}\", f\"A:{example['answer']}\"]\n        for example in few_shot_examples\n    ]\n    return {\"computed_fields\": {\"examples\": examples}}\n\n\ndef most_frequent(lst):\n    \"\"\"Returns the most frequent element in a list.\"\"\"\n    counter = Counter(lst)\n    most_common = counter.most_common(1)\n    return most_common[0][0] if most_common else None\n\n\nasync def self_consistency(\n    query: str, num_samples: int, few_shot_examples: list[dict[str, str]]\n):\n    cot_tasks = [chain_of_thought(query, few_shot_examples) for _ in range(num_samples)]\n    cot_responses = [response.content for response in await asyncio.gather(*cot_tasks)]\n    # Extract final answers manually (simplified for this example)\n    final_answers = [\n        response.split(\"The answer is \")[-1].strip(\".\") for response in cot_responses\n    ]\n    return most_frequent(final_answers)\n\n\nquery = \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\nresult = await self_consistency(\n    query=query, num_samples=5, few_shot_examples=few_shot_examples\n)\nprint(f\"The most consistent answer is: {result}\")\n</pre> import asyncio from collections import Counter  from mirascope.core import openai, prompt_template  few_shot_examples = [     {         \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",         \"answer\": \"We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6.\",     },     {         \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",         \"answer\": \"There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\",     },     {         \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",         \"answer\": \"Leah had 32 chocolates and Leah\u2019s sister had 42. That means there were originally 32 + 42 = 74 chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\",     },     {         \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",         \"answer\": \"Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\",     },     {         \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",         \"answer\": \"He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he has 7 + 2 = 9 toys. The answer is 9.\",     },     {         \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",         \"answer\": \"There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29.\",     },     {         \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",         \"answer\": \"Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\",     }, ]   @openai.call(model=\"gpt-4o-mini\", call_params={\"temperature\": 0.5}) @prompt_template(     \"\"\"     Some examples on how to think step by step:     {examples:lists}      Answer the following question, thinking step by step:     {query}     \"\"\" ) async def chain_of_thought(     query: str, few_shot_examples: list[dict[str, str]] ) -&gt; openai.OpenAIDynamicConfig:     examples = [         [f\"Q:{example['question']}\", f\"A:{example['answer']}\"]         for example in few_shot_examples     ]     return {\"computed_fields\": {\"examples\": examples}}   def most_frequent(lst):     \"\"\"Returns the most frequent element in a list.\"\"\"     counter = Counter(lst)     most_common = counter.most_common(1)     return most_common[0][0] if most_common else None   async def self_consistency(     query: str, num_samples: int, few_shot_examples: list[dict[str, str]] ):     cot_tasks = [chain_of_thought(query, few_shot_examples) for _ in range(num_samples)]     cot_responses = [response.content for response in await asyncio.gather(*cot_tasks)]     # Extract final answers manually (simplified for this example)     final_answers = [         response.split(\"The answer is \")[-1].strip(\".\") for response in cot_responses     ]     return most_frequent(final_answers)   query = \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\" result = await self_consistency(     query=query, num_samples=5, few_shot_examples=few_shot_examples ) print(f\"The most consistent answer is: {result}\") <pre>The most consistent answer is: $8\n</pre> <p>This basic implementation demonstrates how to use Self-Consistency with Chain of Thought reasoning. The <code>self_consistency</code> function generates multiple CoT responses and selects the most frequent final answer.</p> In\u00a0[4]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass Solution(BaseModel):\n    solution_value: int = Field(\n        ..., description=\"The actual number of a solution to a math problem.\"\n    )\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=Solution)\n@prompt_template(\n    \"\"\"\n    Extract just the number of a solution to a math problem.\n    For example, for the solution:\n    Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has\n    58 - 23 = 35 balls. On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls.\n    The answer is 33.\n    \n    You would extract 33.\n\n    Solution to extract from:\n    {response}\n    \"\"\"\n)\nasync def extract_number(response: str): ...\n\n\nasync def enhanced_self_consistency(\n    query: str, num_samples: int, few_shot_examples: list[dict[str, str]]\n) -&gt; int:\n    cot_tasks = [chain_of_thought(query, few_shot_examples) for _ in range(num_samples)]\n    cot_responses = [response.content for response in await asyncio.gather(*cot_tasks)]\n    extract_number_tasks = [extract_number(response) for response in cot_responses]\n    response_numbers = [\n        response.solution_value\n        for response in await asyncio.gather(*extract_number_tasks)\n    ]\n    return most_frequent(response_numbers)\n\n\nresult = await enhanced_self_consistency(\n    query=query, num_samples=5, few_shot_examples=few_shot_examples\n)\nprint(f\"The most consistent answer is: {result}\")\n</pre> from pydantic import BaseModel, Field   class Solution(BaseModel):     solution_value: int = Field(         ..., description=\"The actual number of a solution to a math problem.\"     )   @openai.call(model=\"gpt-4o-mini\", response_model=Solution) @prompt_template(     \"\"\"     Extract just the number of a solution to a math problem.     For example, for the solution:     Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has     58 - 23 = 35 balls. On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls.     The answer is 33.          You would extract 33.      Solution to extract from:     {response}     \"\"\" ) async def extract_number(response: str): ...   async def enhanced_self_consistency(     query: str, num_samples: int, few_shot_examples: list[dict[str, str]] ) -&gt; int:     cot_tasks = [chain_of_thought(query, few_shot_examples) for _ in range(num_samples)]     cot_responses = [response.content for response in await asyncio.gather(*cot_tasks)]     extract_number_tasks = [extract_number(response) for response in cot_responses]     response_numbers = [         response.solution_value         for response in await asyncio.gather(*extract_number_tasks)     ]     return most_frequent(response_numbers)   result = await enhanced_self_consistency(     query=query, num_samples=5, few_shot_examples=few_shot_examples ) print(f\"The most consistent answer is: {result}\") <pre>The most consistent answer is: 8\n</pre> <p>This enhanced version introduces the <code>extract_number</code> function, which uses a response model to automatically extract the numerical answer from each CoT response. The <code>enhanced_self_consistency</code> function then uses this extracted number to determine the most consistent answer.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/self_consistency/#self-consistency-enhancing-llm-reasoning-with-multiple-outputs","title":"Self-Consistency: Enhancing LLM Reasoning with Multiple Outputs\u00b6","text":"<p>This recipe demonstrates how to implement the Self-Consistency technique using Large Language Models (LLMs) with Mirascope. Self-Consistency is a prompt engineering method that enhances an LLM's reasoning capabilities by generating multiple Chain of Thought (CoT) responses and selecting the most common answer. We'll explore both a basic implementation and an enhanced version with automated answer extraction.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> <li>Response Models</li> </ul> <p>Background</p> <p> Self-consistency is a prompt engineering technique where multiple calls are made with Chain of Thought prompting, resulting in various answers, and the most common answer is selected. Self-consistency has shown to be highly effective on mathematical and symbolic reasoning, and has also been shown to help in niche scenarios where CoT actually reduces the quality of LLM output. </p> <p> In the original paper, users manually pick the most frequent response, but we have integrated response models to automate that process once all responses have been generated. </p>"},{"location":"tutorials/prompt_engineering/chaining_based/self_consistency/#basic-self-consistency-implementation","title":"Basic Self-Consistency Implementation\u00b6","text":"<p>Let's start with a basic implementation of Self-Consistency using Chain of Thought reasoning:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/self_consistency/#enhanced-self-consistency-with-automated-answer-extraction","title":"Enhanced Self-Consistency with Automated Answer Extraction\u00b6","text":"<p>Now, let's improve our implementation by adding automated answer extraction:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/self_consistency/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Self-Consistency implementation offers several advantages:</p> <ol> <li>Improved accuracy on mathematical and symbolic reasoning tasks.</li> <li>Mitigation of occasional errors or inconsistencies in LLM outputs.</li> <li>Potential for better performance in scenarios where standard CoT might struggle.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing the number of samples with computational cost and time constraints.</li> <li>Adjusting the temperature parameter to control the diversity of generated responses.</li> <li>Fine-tuning the answer extraction process for different types of problems (e.g., numerical vs. categorical answers).</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Complex Problem Solving: Use Self-Consistency for multi-step problems in fields like physics or engineering.</li> <li>Medical Diagnosis: Apply Self-Consistency to improve the accuracy of symptom analysis and potential diagnoses.</li> <li>Financial Modeling: Implement Self-Consistency for more reliable financial predictions and risk assessments.</li> <li>Natural Language Understanding: Enhance text classification or sentiment analysis tasks with Self-Consistency.</li> <li>Educational Assessment: Use Self-Consistency to generate and validate multiple-choice questions and answers.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Tailoring the few-shot examples to your domain for better performance.</li> <li>Experimenting with different prompt formats and Chain of Thought structures.</li> <li>Implementing a feedback loop to continuously improve the quality of the Self-Consistency responses.</li> <li>Combining Self-Consistency with other techniques like Self-Ask for even more powerful reasoning capabilities.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator, response models, and dynamic configuration, you can easily implement and customize the Self-Consistency technique to enhance your LLM's reasoning capabilities across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/self_refine/","title":"Self-Refine: Enhancing LLM Outputs Through Iterative Self-Improvement","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\nfrom mirascope.core.openai import OpenAICallResponse\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef call(query: str) -&gt; str:\n    return query\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Here is a query and a response to the query. Give feedback about the answer,\n    noting what was correct and incorrect.\n    Query:\n    {query}\n    Response:\n    {response}\n    \"\"\"\n)\ndef evaluate_response(query: str, response: OpenAICallResponse): ...\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    For this query:\n    {query}\n    The following response was given:\n    {response}\n    Here is some feedback about the response:\n    {feedback}\n\n    Consider the feedback to generate a new response to the query.\n    \"\"\"\n)\ndef generate_new_response(\n    query: str, response: OpenAICallResponse\n) -&gt; openai.OpenAIDynamicConfig:\n    feedback = evaluate_response(query, response)\n    return {\"computed_fields\": {\"feedback\": feedback}}\n\n\ndef self_refine(query: str, depth: int) -&gt; str:\n    response = call(query)\n    for _ in range(depth):\n        response = generate_new_response(query, response)\n    return response.content\n\n\nquery = \"\"\"Olivia has $23. She bought five bagels for $3 each.\nHow much money does she have left?\"\"\"\nprint(self_refine(query, 1))\n</pre> from mirascope.core import openai, prompt_template from mirascope.core.openai import OpenAICallResponse   @openai.call(model=\"gpt-4o-mini\") def call(query: str) -&gt; str:     return query   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Here is a query and a response to the query. Give feedback about the answer,     noting what was correct and incorrect.     Query:     {query}     Response:     {response}     \"\"\" ) def evaluate_response(query: str, response: OpenAICallResponse): ...   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     For this query:     {query}     The following response was given:     {response}     Here is some feedback about the response:     {feedback}      Consider the feedback to generate a new response to the query.     \"\"\" ) def generate_new_response(     query: str, response: OpenAICallResponse ) -&gt; openai.OpenAIDynamicConfig:     feedback = evaluate_response(query, response)     return {\"computed_fields\": {\"feedback\": feedback}}   def self_refine(query: str, depth: int) -&gt; str:     response = call(query)     for _ in range(depth):         response = generate_new_response(query, response)     return response.content   query = \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\" print(self_refine(query, 1)) <pre>To determine how much money Olivia has left after her purchase, let's break it down step by step:\n\n1. **Starting Amount**: Olivia has $23 initially.\n2. **Cost of Bagels**: She bought 5 bagels at $3 each. The total spent on bagels is calculated as:\n   \\[\n   5 \\times 3 = 15 \\text{ dollars}\n   \\]\n3. **Amount Left**: Now, we subtract the total amount spent on the bagels from Olivia's starting amount:\n   \\[\n   23 - 15 = 8 \\text{ dollars}\n   \\]\n\nTherefore, after buying the bagels, Olivia has **$8 remaining**.\n</pre> In\u00a0[2]: Copied! <pre>from pydantic import BaseModel, Field\n\n\nclass MathSolution(BaseModel):\n    steps: list[str] = Field(..., description=\"The steps taken to solve the problem\")\n    final_answer: float = Field(..., description=\"The final numerical answer\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=MathSolution)\n@prompt_template(\n    \"\"\"\n    For this query:\n    {query}\n    The following response was given:\n    {response}\n    Here is some feedback about the response:\n    {feedback}\n\n    Consider the feedback to generate a new response to the query.\n    Provide the solution steps and the final numerical answer.\n    \"\"\"\n)\ndef enhanced_generate_new_response(\n    query: str, response: OpenAICallResponse\n) -&gt; openai.OpenAIDynamicConfig:\n    feedback = evaluate_response(query, response)\n    return {\"computed_fields\": {\"feedback\": feedback}}\n\n\ndef enhanced_self_refine(query: str, depth: int) -&gt; MathSolution:\n    response = call(query)\n    for _ in range(depth):\n        solution = enhanced_generate_new_response(query, response)\n        response = f\"Steps: {solution.steps}\\nFinal Answer: {solution.final_answer}\"\n    return solution\n\n\n# Example usage\nresult = enhanced_self_refine(query, 1)\nprint(result)\n</pre> from pydantic import BaseModel, Field   class MathSolution(BaseModel):     steps: list[str] = Field(..., description=\"The steps taken to solve the problem\")     final_answer: float = Field(..., description=\"The final numerical answer\")   @openai.call(model=\"gpt-4o-mini\", response_model=MathSolution) @prompt_template(     \"\"\"     For this query:     {query}     The following response was given:     {response}     Here is some feedback about the response:     {feedback}      Consider the feedback to generate a new response to the query.     Provide the solution steps and the final numerical answer.     \"\"\" ) def enhanced_generate_new_response(     query: str, response: OpenAICallResponse ) -&gt; openai.OpenAIDynamicConfig:     feedback = evaluate_response(query, response)     return {\"computed_fields\": {\"feedback\": feedback}}   def enhanced_self_refine(query: str, depth: int) -&gt; MathSolution:     response = call(query)     for _ in range(depth):         solution = enhanced_generate_new_response(query, response)         response = f\"Steps: {solution.steps}\\nFinal Answer: {solution.final_answer}\"     return solution   # Example usage result = enhanced_self_refine(query, 1) print(result) <pre>steps=['Olivia has $23.', 'She bought five bagels for $3 each.', 'Calculate the total cost for the bagels: 5 bagels * $3/bagel = $15.', 'Subtract the total cost of the bagels from the amount of money she had: $23 - $15 = $8.'] final_answer=8.0\n</pre> <p>This enhanced version introduces a MathSolution response model to structure the output, providing a clearer separation between solution steps and the final answer.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/self_refine/#self-refine-enhancing-llm-outputs-through-iterative-self-improvement","title":"Self-Refine: Enhancing LLM Outputs Through Iterative Self-Improvement\u00b6","text":"<p>This recipe demonstrates how to implement the Self-Refine technique using Large Language Models (LLMs) with Mirascope. Self-Refine is a prompt engineering method that enhances an LLM's output by iteratively generating feedback and improving its responses.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul> <p>Background</p> <p> Self-refine is a prompt engineering technique where a model gives feedback about its answer and uses the feedback to generate a new answer. This self refinement can take place multiple times to generate the final answer. Self-refine is helpful for reasoning, coding, and generation tasks. </p>"},{"location":"tutorials/prompt_engineering/chaining_based/self_refine/#basic-self-refine-implementation","title":"Basic Self-Refine Implementation\u00b6","text":"<p>Let's start with a basic implementation of Self-Refine using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/self_refine/#enhanced-self-refine-with-response-model","title":"Enhanced Self-Refine with Response Model\u00b6","text":"<p>Now, let's improve our implementation by adding a response model to structure the output:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/self_refine/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Self-Refine implementation offers several advantages:</p> <ol> <li>Improved accuracy through iterative refinement of responses.</li> <li>Enhanced reasoning capabilities, especially for complex problems.</li> <li>Potential for generating more detailed and step-by-step solutions.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing the number of refinement iterations with computational cost and time constraints.</li> <li>Tailoring the feedback prompts to focus on specific aspects of improvement relevant to your use case.</li> <li>Experimenting with different model parameters (e.g., temperature) for initial responses vs. refinement steps.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Essay Writing: Use Self-Refine to iteratively improve essay drafts, focusing on structure, argument coherence, and style.</li> <li>Code Generation: Apply the technique to generate, evaluate, and refine code snippets or entire functions.</li> <li>Data Analysis Reports: Enhance the quality and depth of data analysis reports through iterative self-improvement.</li> <li>Product Descriptions: Refine product descriptions to be more engaging, accurate, and tailored to target audiences.</li> <li>Legal Document Drafting: Improve the precision and comprehensiveness of legal documents through self-refinement.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Customizing the feedback prompts to focus on domain-specific criteria.</li> <li>Implementing different types of response models for various tasks (e.g., text generation, problem-solving).</li> <li>Combining Self-Refine with other techniques like Chain of Thought for more complex reasoning tasks.</li> <li>Developing a mechanism to halt refinement when improvements become marginal.</li> </ul> <p>By leveraging Mirascope's call decorator, response models, and dynamic configuration, you can easily implement and customize the Self-Refine technique to enhance your LLM's output quality across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/sim_to_m/","title":"Sim to M: Enhancing LLM Reasoning with Perspective-Taking","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai\nfrom mirascope.core.base.prompt import prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    The following is a sequence of events:\n    {story}\n    What events does {name} know about?\n    \"\"\"\n)\ndef get_one_perspective(story: str, name: str):\n    \"\"\"Gets one person's perspective of a story.\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    {story_from_perspective}\n    Based on the above information, answer the following question:\n    {query}\n    \"\"\"\n)\ndef sim_to_m(story: str, query: str, name: str) -&gt; openai.OpenAIDynamicConfig:\n    \"\"\"Executes the flow of the Sim to M technique.\"\"\"\n    story_from_perspective = get_one_perspective(story=story, name=name)\n    return {\"computed_fields\": {\"story_from_perspective\": story_from_perspective}}\n\n\nstory = \"\"\"Jim put the ball in the box. While Jim wasn't looking, Avi moved the \\\nball to the basket.\"\"\"\nquery = \"Where does Jim think the ball is?\"\n\nprint(sim_to_m(story=story, query=query, name=\"Jim\"))\n</pre> from mirascope.core import openai from mirascope.core.base.prompt import prompt_template   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     The following is a sequence of events:     {story}     What events does {name} know about?     \"\"\" ) def get_one_perspective(story: str, name: str):     \"\"\"Gets one person's perspective of a story.\"\"\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     {story_from_perspective}     Based on the above information, answer the following question:     {query}     \"\"\" ) def sim_to_m(story: str, query: str, name: str) -&gt; openai.OpenAIDynamicConfig:     \"\"\"Executes the flow of the Sim to M technique.\"\"\"     story_from_perspective = get_one_perspective(story=story, name=name)     return {\"computed_fields\": {\"story_from_perspective\": story_from_perspective}}   story = \"\"\"Jim put the ball in the box. While Jim wasn't looking, Avi moved the \\ ball to the basket.\"\"\" query = \"Where does Jim think the ball is?\"  print(sim_to_m(story=story, query=query, name=\"Jim\")) <pre>Based on the information provided, Jim believes the ball is in the box, as he is only aware of his own action of putting the ball there. He is unaware of Avi's action of moving the ball to the basket. Therefore, Jim thinks the ball is still in the box.\n</pre> <p>This implementation consists of two main functions:</p> <ol> <li><code>get_one_perspective</code>: This function takes a story and a person's name as input, and returns the events known to that person.</li> <li><code>sim_to_m</code>: This function orchestrates the Sim to M technique. It first calls <code>get_one_perspective</code> to establish the facts from one person's viewpoint, then uses this perspective to answer the given query.</li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/sim_to_m/#sim-to-m-enhancing-llm-reasoning-with-perspective-taking","title":"Sim to M: Enhancing LLM Reasoning with Perspective-Taking\u00b6","text":"<p>This recipe demonstrates how to implement the Sim to M (Simulation Theory of Mind) technique using Large Language Models (LLMs) with Mirascope. Sim to M is a prompt engineering method that enhances an LLM's ability to reason about complex situations involving multiple perspectives.</p> Mirascope Concepts Used <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul> <p>Background</p> <p>Sim to M is a prompt engineering technique for dealing with complex situations which involve multiple perspectives. First ask the LLM to establish the facts from one person's perspective, then answer the question based only on that perspective. This approach can significantly improve the LLM's ability to reason about situations involving different viewpoints or limited information.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/sim_to_m/#implementation","title":"Implementation\u00b6","text":"<p>Let's implement the Sim to M technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/sim_to_m/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Sim to M implementation offers several advantages:</p> <ol> <li>Improved reasoning about situations involving multiple perspectives or limited information.</li> <li>Enhanced ability to model and simulate different viewpoints in complex scenarios.</li> <li>Potential for more accurate responses in tasks involving theory of mind or perspective-taking.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Carefully crafting the initial story to include relevant information about different perspectives.</li> <li>Ensuring that the query is specific to a particular perspective or viewpoint.</li> <li>Experimenting with different prompts for the <code>get_one_perspective</code> function to optimize perspective extraction.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Character Analysis in Literature: Use Sim to M to analyze characters' motivations and beliefs in complex narratives.</li> <li>Conflict Resolution: Apply the technique to understand different stakeholders' viewpoints in disputes.</li> <li>User Experience Design: Simulate how different user groups might perceive and interact with a product or service.</li> <li>Historical Analysis: Model historical figures' decision-making based on their known information at the time.</li> <li>Psychological Assessments: Enhance AI-assisted psychological evaluations by better modeling individual perspectives.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Tailoring the story and query formats to your domain for better performance.</li> <li>Implementing a mechanism to handle multiple perspectives in more complex scenarios.</li> <li>Combining Sim to M with other techniques like Chain of Thought for even more nuanced reasoning.</li> <li>Developing a feedback loop to refine the perspective extraction process based on the accuracy of final answers.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator and dynamic configuration, you can easily implement and customize the Sim to M technique to enhance your LLM's ability to reason about complex, multi-perspective situations across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/skeleton_of_thought/","title":"Skeleton of Thought: Enhancing LLM Response Speed","text":"In\u00a0[1]: Copied! <pre>import asyncio\n\nfrom mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass Skeleton(BaseModel):\n    subpoints: list[str] = Field(\n        ...,\n        description=\"\"\"The subpoints of the skeleton of the original query.\n        Each is 3-5 words and starts with its point index, e.g. \n        1. Some subpoint...\"\"\",\n    )\n\n\n@openai.call(model=\"gpt-3.5-turbo\", response_model=Skeleton)\n@prompt_template(\n    \"\"\"\n    You're an organizer responsible for only giving the skeleton (not the full content) for answering the question.\n    Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question. \n    Instead of writing a full sentence, each skeleton point should be very short with only 3\u223c5 words.\n    Generally, the skeleton should have 3\u223c10 points.\n    Now, please provide the skeleton for the following question.\n    {query}\n    Skeleton:\n    \"\"\"\n)\ndef break_into_subpoints(query: str): ...\n\n\n@openai.call(model=\"gpt-3.5-turbo\")\n@prompt_template(\n    \"\"\"\n    You're responsible for continuing the writing of one and only one point in the overall answer to the following question:\n\n    {query}\n\n    The skeleton of the answer is:\n\n    {skeleton}\n\n    Continue and only continue the writing of point {point_index}. Write it very shortly in 1-2 sentences and do not continue with other points!\n    \"\"\"\n)\nasync def expand_subpoint(query: str, skeleton: list[str], point_index: int): ...\n\n\nquery = \"How can I improve my focus?\"\n\n\nasync def skeleton_of_thought(query):\n    skeleton = break_into_subpoints(query)\n    tasks = [\n        expand_subpoint(query, skeleton.subpoints, i + 1)\n        for i, subpoint in enumerate(skeleton.subpoints)\n    ]\n    results = await asyncio.gather(*tasks)\n    return \"\\n\".join([result.content for result in results])\n\n\nprint(await skeleton_of_thought(query))\n</pre> import asyncio  from mirascope.core import openai, prompt_template from pydantic import BaseModel, Field   class Skeleton(BaseModel):     subpoints: list[str] = Field(         ...,         description=\"\"\"The subpoints of the skeleton of the original query.         Each is 3-5 words and starts with its point index, e.g.          1. Some subpoint...\"\"\",     )   @openai.call(model=\"gpt-3.5-turbo\", response_model=Skeleton) @prompt_template(     \"\"\"     You're an organizer responsible for only giving the skeleton (not the full content) for answering the question.     Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question.      Instead of writing a full sentence, each skeleton point should be very short with only 3\u223c5 words.     Generally, the skeleton should have 3\u223c10 points.     Now, please provide the skeleton for the following question.     {query}     Skeleton:     \"\"\" ) def break_into_subpoints(query: str): ...   @openai.call(model=\"gpt-3.5-turbo\") @prompt_template(     \"\"\"     You're responsible for continuing the writing of one and only one point in the overall answer to the following question:      {query}      The skeleton of the answer is:      {skeleton}      Continue and only continue the writing of point {point_index}. Write it very shortly in 1-2 sentences and do not continue with other points!     \"\"\" ) async def expand_subpoint(query: str, skeleton: list[str], point_index: int): ...   query = \"How can I improve my focus?\"   async def skeleton_of_thought(query):     skeleton = break_into_subpoints(query)     tasks = [         expand_subpoint(query, skeleton.subpoints, i + 1)         for i, subpoint in enumerate(skeleton.subpoints)     ]     results = await asyncio.gather(*tasks)     return \"\\n\".join([result.content for result in results])   print(await skeleton_of_thought(query)) <pre>Identify distractions by making a list of the things that tend to pull your attention away from the task at hand. Once you know what they are, you can take steps to minimize their impact on your focus.\nEstablishing a routine can help improve focus by creating structure and consistency in your daily tasks and priorities. By sticking to a set schedule, you can reduce the likelihood of getting off track and better manage your time and energy.\nSet specific goals by breaking down your tasks into smaller, manageable steps with clear deadlines. This will help you stay on track and maintain focus on what needs to be accomplished.\n4. Practice mindfulness by staying present in the moment and focusing on your breathing to help quiet the mind and improve concentration.\nTake regular breaks to give your mind time to rest and recharge, allowing you to come back to your tasks with renewed focus and energy.\n</pre> <p>This implementation demonstrates how to use Skeleton of Thought with Mirascope. The <code>break_into_subpoints</code> function creates the initial skeleton, and <code>expand_subpoint</code> expands each subpoint in parallel. The <code>skeleton_of_thought</code> function orchestrates the entire process.</p> <p>Intermediate Response:</p> In\u00a0[2]: Copied! <pre>print(break_into_subpoints(query))\n</pre> print(break_into_subpoints(query)) <pre>subpoints=['Identify distractions', 'Implement time management techniques', 'Practice mindfulness', 'Get enough sleep', 'Stay hydrated', 'Exercise regularly', 'Set clear goals', 'Take short breaks', 'Limit multitasking']\n</pre>"},{"location":"tutorials/prompt_engineering/chaining_based/skeleton_of_thought/#skeleton-of-thought-enhancing-llm-response-speed","title":"Skeleton of Thought: Enhancing LLM Response Speed\u00b6","text":"<p>This recipe demonstrates how to implement Skeleton of Thought, a speed-oriented prompt engineering technique.</p> <p>This recipe demonstrates how to implement the Skeleton of Thought technique using Large Language Models (LLMs) with Mirascope.</p> Mirascope Concepts Used <ul> <li>Prompts</li> <li>Calls</li> <li>Response Models</li> </ul> <p>Background</p> <p>Skeleton of Thought is a prompt-engineering technique that is speed-oriented as opposed to the quality of the response. To expedite the response from a model, make an initial call to create a \"skeleton\" of the problem that outlines its solution in bulletpoints (without further explanations), then make an individual call with each of the subpoints in parallel before reconstructing the answer at the end.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/skeleton_of_thought/#basic-skeleton-of-thought-implementation","title":"Basic Skeleton of Thought Implementation\u00b6","text":"<p>Let's start with a basic implementation of Skeleton of Thought:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/skeleton_of_thought/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Skeleton of Thought implementation offers several advantages:</p> <ol> <li>Improved response speed by parallelizing the expansion of subpoints.</li> <li>Enhanced structure in responses, making them easier to read and understand.</li> <li>Potential for better performance on complex queries that benefit from a structured approach.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing the number of subpoints with the desired response length and complexity.</li> <li>Adjusting the prompt for subpoint expansion based on the specific use case or domain.</li> <li>Implementing error handling and retries to ensure robustness in production environments.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Content Creation: Use Skeleton of Thought to quickly generate outlines for articles or blog posts.</li> <li>Project Planning: Rapidly break down complex projects into manageable tasks and subtasks.</li> <li>Educational Materials: Create structured lesson plans or study guides efficiently.</li> <li>Technical Documentation: Generate quick, well-structured documentation outlines for software or products.</li> <li>Problem-Solving: Break down complex problems into smaller, more manageable components for analysis.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Customizing the skeleton generation prompt to fit your domain-specific needs.</li> <li>Experimenting with different LLM models for skeleton generation and subpoint expansion to optimize for speed and quality.</li> <li>Implementing a feedback loop to refine the skeleton based on the quality of expanded subpoints.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator, response models, and dynamic configuration, you can easily implement and customize the Skeleton of Thought technique to enhance your LLM's response speed and structure across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/step_back/","title":"Step-back Prompting: Enhancing LLM Reasoning with High-Level Questions","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai\nfrom mirascope.core.base.prompt import prompt_template\n\nfew_shot_examples = [\n    {\n        \"original_question\": \"Which position did Knox Cunningham hold from May 1955 to Apr 1956?\",\n        \"stepback_question\": \"Which positions have Knox Cunningham held in his career?\",\n    },\n    {\n        \"original_question\": \"Who was the spouse of Anna Karina from 1968 to 1974?\",\n        \"stepback_question\": \"Who were the spouses of Anna Karina?\",\n    },\n    {\n        \"original_question\": \"Which team did Thierry Audel play for from 2007 to 2008?\",\n        \"stepback_question\": \"Which teams did Thierry Audel play for in his career?\",\n    },\n    {\n        \"original_question\": \"What was the operator of GCR Class 11E from 1913 to Dec 1922?\",\n        \"stepback_question\": \"What were the operators of GCR Class 11E in history?\",\n    },\n    {\n        \"original_question\": \"Which country did Sokolovsko belong to from 1392 to 1525?\",\n        \"stepback_question\": \"Which countries did Sokolovsko belong to in history?\",\n    },\n    {\n        \"original_question\": \"when was the last time a team from canada won the stanley cup as of 2002\",\n        \"stepback_question\": \"which years did a team from canada won the stanley cup as of 2002\",\n    },\n    {\n        \"original_question\": \"when did england last get to the semi final in a world cup as of 2019\",\n        \"stepback_question\": \"which years did england get to the semi final in a world cup as of 2019?\",\n    },\n    {\n        \"original_question\": \"what is the biggest hotel in las vegas nv as of November 28, 1993\",\n        \"stepback_question\": \"what is the size of the hotels in las vegas nv as of November 28, 1993\",\n    },\n    {\n        \"original_question\": \"who has scored most runs in t20 matches as of 2017\",\n        \"stepback_question\": \"What are the runs of players in t20 matches as of 2017\",\n    },\n]\n\nstepback_prompt = \"\"\"You are an expert at world knowledge. Your task is to step \\\nback and paraphrase a question to a more generic step-back question, which is \\\neasier to answer. Here are a few examples:\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    SYSTEM: {stepback_prompt_and_examples}\n    USER: {query}\n    \"\"\"\n)\ndef get_stepback_question(\n    query: str, num_examples: int = 0\n) -&gt; openai.OpenAIDynamicConfig:\n    \"\"\"Gets the generic, step-back version of a query.\"\"\"\n    if num_examples &lt; 0 or num_examples &gt; len(few_shot_examples):\n        raise ValueError(\n            \"num_examples cannot be negative or greater than number of available examples.\"\n        )\n    example_prompts = \"\"\n    for i in range(num_examples):\n        example_prompts += (\n            f\"Original Question: {few_shot_examples[i]['original_question']}\\n\"\n        )\n        example_prompts += (\n            f\"Stepback Question: {few_shot_examples[i]['stepback_question']}\\n\"\n        )\n    return {\n        \"computed_fields\": {\n            \"stepback_prompt_and_examples\": f\"{stepback_prompt}\\n{example_prompts}\"\n            if num_examples\n            else None\n        }\n    }\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef call(query: str) -&gt; str:\n    \"\"\"A standard call to OpenAI.\"\"\"\n    return query\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    You are an expert of world knowledge. I am going to ask you a question.\n    Your response should be comprehensive and not contradicted with the\n    following context if they are relevant. Otherwise, ignore them if they are\n    not relevant.\n\n    {stepback_response}\n\n    Original Question: {query}\n    Answer:\n    \"\"\"\n)\ndef stepback(query: str, num_examples: int) -&gt; openai.OpenAIDynamicConfig:\n    \"\"\"Executes the flow of the Step-Back technique.\"\"\"\n    stepback_question = get_stepback_question(\n        query=query, num_examples=num_examples\n    ).content\n    stepback_response = call(query=stepback_question).content\n    return {\"computed_fields\": {\"stepback_response\": stepback_response}}\n\n\n# Example usage\nquery = \"\"\"Who is the highest paid player in the nba this season as of 2017\"\"\"\n\nprint(stepback(query=query, num_examples=len(few_shot_examples)))\n</pre> from mirascope.core import openai from mirascope.core.base.prompt import prompt_template  few_shot_examples = [     {         \"original_question\": \"Which position did Knox Cunningham hold from May 1955 to Apr 1956?\",         \"stepback_question\": \"Which positions have Knox Cunningham held in his career?\",     },     {         \"original_question\": \"Who was the spouse of Anna Karina from 1968 to 1974?\",         \"stepback_question\": \"Who were the spouses of Anna Karina?\",     },     {         \"original_question\": \"Which team did Thierry Audel play for from 2007 to 2008?\",         \"stepback_question\": \"Which teams did Thierry Audel play for in his career?\",     },     {         \"original_question\": \"What was the operator of GCR Class 11E from 1913 to Dec 1922?\",         \"stepback_question\": \"What were the operators of GCR Class 11E in history?\",     },     {         \"original_question\": \"Which country did Sokolovsko belong to from 1392 to 1525?\",         \"stepback_question\": \"Which countries did Sokolovsko belong to in history?\",     },     {         \"original_question\": \"when was the last time a team from canada won the stanley cup as of 2002\",         \"stepback_question\": \"which years did a team from canada won the stanley cup as of 2002\",     },     {         \"original_question\": \"when did england last get to the semi final in a world cup as of 2019\",         \"stepback_question\": \"which years did england get to the semi final in a world cup as of 2019?\",     },     {         \"original_question\": \"what is the biggest hotel in las vegas nv as of November 28, 1993\",         \"stepback_question\": \"what is the size of the hotels in las vegas nv as of November 28, 1993\",     },     {         \"original_question\": \"who has scored most runs in t20 matches as of 2017\",         \"stepback_question\": \"What are the runs of players in t20 matches as of 2017\",     }, ]  stepback_prompt = \"\"\"You are an expert at world knowledge. Your task is to step \\ back and paraphrase a question to a more generic step-back question, which is \\ easier to answer. Here are a few examples:\"\"\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     SYSTEM: {stepback_prompt_and_examples}     USER: {query}     \"\"\" ) def get_stepback_question(     query: str, num_examples: int = 0 ) -&gt; openai.OpenAIDynamicConfig:     \"\"\"Gets the generic, step-back version of a query.\"\"\"     if num_examples &lt; 0 or num_examples &gt; len(few_shot_examples):         raise ValueError(             \"num_examples cannot be negative or greater than number of available examples.\"         )     example_prompts = \"\"     for i in range(num_examples):         example_prompts += (             f\"Original Question: {few_shot_examples[i]['original_question']}\\n\"         )         example_prompts += (             f\"Stepback Question: {few_shot_examples[i]['stepback_question']}\\n\"         )     return {         \"computed_fields\": {             \"stepback_prompt_and_examples\": f\"{stepback_prompt}\\n{example_prompts}\"             if num_examples             else None         }     }   @openai.call(model=\"gpt-4o-mini\") def call(query: str) -&gt; str:     \"\"\"A standard call to OpenAI.\"\"\"     return query   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     You are an expert of world knowledge. I am going to ask you a question.     Your response should be comprehensive and not contradicted with the     following context if they are relevant. Otherwise, ignore them if they are     not relevant.      {stepback_response}      Original Question: {query}     Answer:     \"\"\" ) def stepback(query: str, num_examples: int) -&gt; openai.OpenAIDynamicConfig:     \"\"\"Executes the flow of the Step-Back technique.\"\"\"     stepback_question = get_stepback_question(         query=query, num_examples=num_examples     ).content     stepback_response = call(query=stepback_question).content     return {\"computed_fields\": {\"stepback_response\": stepback_response}}   # Example usage query = \"\"\"Who is the highest paid player in the nba this season as of 2017\"\"\"  print(stepback(query=query, num_examples=len(few_shot_examples))) <pre>As of the 2017 NBA season, the highest-paid player was Stephen Curry. He signed a four-year, $215 million contract extension with the Golden State Warriors, which was the largest contract in NBA history at that time. This contract significantly boosted his earnings, making him the top earner in the league for that season. Other players like LeBron James and Kevin Durant were also among the highest-paid, but Curry's contract set a new benchmark in player salaries at that time.\n</pre> <p>This implementation consists of three main functions:</p> <ol> <li><code>get_stepback_question</code>: This function takes a query and generates a more generic, step-back version of the question.</li> <li><code>call</code>: A standard call to OpenAI that processes the step-back question.</li> <li><code>stepback</code>: This function orchestrates the Step-back prompting technique. It first calls <code>get_stepback_question</code> to generate a high-level question, then uses <code>call</code> to get a response to this question, and finally combines this information to answer the original query.</li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/step_back/#step-back-prompting-enhancing-llm-reasoning-with-high-level-questions","title":"Step-back Prompting: Enhancing LLM Reasoning with High-Level Questions\u00b6","text":"<p>This recipe demonstrates how to implement the Step-back prompting technique using Large Language Models (LLMs) with Mirascope. Step-back prompting is a method that enhances an LLM's reasoning capabilities by asking a high-level question about relevant concepts or facts before addressing the original query.</p> Mirascope Concepts Used <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul> <p>Background</p> <p>Step-back prompting is an alternative to the Chain of Thought technique, where one asks the LLM a high-level question about relevant concepts or facts before asking it the actual question. This technique is derived from the fact that humans often take step backs and use abstractions to arrive at an answer, and it can yield correct answers at times when Chain of Thought fails.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/step_back/#implementation","title":"Implementation\u00b6","text":"<p>Let's implement the Step-back prompting technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/step_back/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The Step-back prompting implementation offers several advantages:</p> <ol> <li>Improved reasoning about complex queries by considering higher-level concepts first.</li> <li>Potential for more accurate responses in tasks that benefit from broader context.</li> <li>Ability to overcome limitations of other techniques like Chain of Thought in certain scenarios.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing the generality of the step-back question with its relevance to the original query.</li> <li>Experimenting with different numbers of few-shot examples to optimize performance.</li> <li>Adjusting the prompt for generating step-back questions based on your specific use case.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Complex Problem Solving: Use Step-back prompting for multi-step problems in fields like mathematics or engineering.</li> <li>Medical Diagnosis: Apply the technique to consider general symptoms before focusing on specific conditions.</li> <li>Legal Analysis: Implement Step-back prompting to first consider broader legal principles before addressing specific cases.</li> <li>Historical Analysis: Use the method to first consider broader historical context before analyzing specific events.</li> <li>Product Development: Apply Step-back prompting to consider general market trends before focusing on specific product features.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Tailoring the few-shot examples to your domain for better performance.</li> <li>Implementing a feedback loop to continuously improve the quality of step-back questions generated.</li> <li>Combining Step-back prompting with other techniques like Self-consistency for even more robust reasoning capabilities.</li> <li>Experimenting with different LLM models to find the best balance between performance and efficiency for your use case.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator and dynamic configuration, you can easily implement and customize the Step-back prompting technique to enhance your LLM's reasoning capabilities across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/system_to_attention/","title":"System to Attention (S2A): Enhancing LLM Focus with Query Filtering","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\nfrom pydantic import BaseModel, Field\n\n\nclass RelevantContext(BaseModel):\n    context_text: str = Field(\n        description=\"Context text related to the question (includes all content except unrelated sentences)\"\n    )\n    detailed_question: str = Field(description=\"Detailed question:\")\n\n\n@openai.call(model=\"gpt-4o-mini\", response_model=RelevantContext)\n@prompt_template(\n    \"\"\"\n    Given the following text by a user, extract the part that is related and useful, so that using that text alone would be good context for providing an accurate and correct answer to the question portion of the text.\n    Please include the actual question or query that the user is asking. \n    Separate this into two categories labeled with \u201dContext text related to the question (includes all content except unrelated sentences):\u201d and \u201dDetailed question:\u201d.\n    Do not use list.\n    Text by User: {query}\n    \"\"\"\n)\ndef remove_irrelevant_info(query: str):\n    \"\"\"Reduces a query down to its relevant context and question\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Original user query (possibly biased): {query}\n    Unbiased context: {context_text}\n    Given the above unbiased context, answer the following: {detailed_question}\n    \"\"\"\n)\ndef s2a(query: str) -&gt; openai.OpenAIDynamicConfig:\n    \"\"\"Executes the flow of the System to Attention technique.\"\"\"\n    relevant_context = remove_irrelevant_info(query=query)\n    context_text = relevant_context.context_text\n    detailed_question = relevant_context.detailed_question\n    return {\n        \"computed_fields\": {\n            \"context_text\": context_text,\n            \"detailed_question\": detailed_question,\n        }\n    }\n\n\n# Example usage\nquery = \"\"\"Sunnyvale is a city in California. \\\nSunnyvale has many parks. Sunnyvale city is \\\nclose to the mountains. Many notable people \\\nare born in Sunnyvale. \\\nIn which city was San Jose's mayor Sam \\\nLiccardo born?\"\"\"\n\nprint(s2a(query=query))\n</pre> from mirascope.core import openai, prompt_template from pydantic import BaseModel, Field   class RelevantContext(BaseModel):     context_text: str = Field(         description=\"Context text related to the question (includes all content except unrelated sentences)\"     )     detailed_question: str = Field(description=\"Detailed question:\")   @openai.call(model=\"gpt-4o-mini\", response_model=RelevantContext) @prompt_template(     \"\"\"     Given the following text by a user, extract the part that is related and useful, so that using that text alone would be good context for providing an accurate and correct answer to the question portion of the text.     Please include the actual question or query that the user is asking.      Separate this into two categories labeled with \u201dContext text related to the question (includes all content except unrelated sentences):\u201d and \u201dDetailed question:\u201d.     Do not use list.     Text by User: {query}     \"\"\" ) def remove_irrelevant_info(query: str):     \"\"\"Reduces a query down to its relevant context and question\"\"\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Original user query (possibly biased): {query}     Unbiased context: {context_text}     Given the above unbiased context, answer the following: {detailed_question}     \"\"\" ) def s2a(query: str) -&gt; openai.OpenAIDynamicConfig:     \"\"\"Executes the flow of the System to Attention technique.\"\"\"     relevant_context = remove_irrelevant_info(query=query)     context_text = relevant_context.context_text     detailed_question = relevant_context.detailed_question     return {         \"computed_fields\": {             \"context_text\": context_text,             \"detailed_question\": detailed_question,         }     }   # Example usage query = \"\"\"Sunnyvale is a city in California. \\ Sunnyvale has many parks. Sunnyvale city is \\ close to the mountains. Many notable people \\ are born in Sunnyvale. \\ In which city was San Jose's mayor Sam \\ Liccardo born?\"\"\"  print(s2a(query=query)) <pre>Sam Liccardo, the mayor of San Jose, was born in San Jose, California.\n</pre> <p>This implementation consists of two main functions:</p> <ol> <li><p><code>remove_irrelevant_info</code>: This function takes the original query and extracts the relevant context and the detailed question. It uses a <code>RelevantContext</code> response model to structure the output.</p> </li> <li><p><code>s2a</code>: This is the main function that orchestrates the S2A technique. It first calls <code>remove_irrelevant_info</code> to filter the query, then uses the filtered information to generate a response.</p> </li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/system_to_attention/#system-to-attention-s2a-enhancing-llm-focus-with-query-filtering","title":"System to Attention (S2A): Enhancing LLM Focus with Query Filtering\u00b6","text":"<p>This recipe demonstrates how to implement the System to Attention (S2A) technique using Large Language Models (LLMs) with Mirascope. S2A is a prompt engineering method that enhances an LLM's ability to focus on relevant information by filtering out irrelevant context from the initial query.</p> Mirascope Concepts Used <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> <li>Response Models</li> </ul> <p>Background</p> <p>System to Attention (S2A) is a prompt engineering technique whereby the prompt is first filtered to remove all irrelevant information from the query. This approach helps LLMs focus on the most pertinent information, potentially improving the accuracy and relevance of their responses, especially for queries containing extraneous or potentially biasing information.</p>"},{"location":"tutorials/prompt_engineering/chaining_based/system_to_attention/#implementation","title":"Implementation\u00b6","text":"<p>Let's implement the S2A technique using Mirascope:</p>"},{"location":"tutorials/prompt_engineering/chaining_based/system_to_attention/#how-it-works","title":"How It Works\u00b6","text":"<ol> <li><p>Query Filtering: The <code>remove_irrelevant_info</code> function analyzes the input query and separates it into relevant context and the actual question. This step helps remove any irrelevant or potentially biasing information.</p> </li> <li><p>Context Separation: The filtered information is structured into two parts: the context text and the detailed question. This separation allows for more focused processing in the next step.</p> </li> <li><p>Unbiased Response Generation: The <code>s2a</code> function uses the filtered context and question to generate a response. By providing the original query alongside the filtered information, it allows the model to be aware of potential biases while focusing on the relevant information.</p> </li> </ol>"},{"location":"tutorials/prompt_engineering/chaining_based/system_to_attention/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The S2A technique offers several advantages:</p> <ol> <li>Improved focus on relevant information, potentially leading to more accurate responses.</li> <li>Reduction of bias from irrelevant context in the original query.</li> <li>Clear separation of context and question, allowing for more structured reasoning.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing between removing irrelevant information and retaining important context.</li> <li>Adjusting the filtering prompt based on the specific domain or type of queries you're dealing with.</li> <li>Monitoring the performance to ensure that important information isn't being filtered out unintentionally.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Customer Support: Filter out emotional language or irrelevant details from customer queries to focus on the core issue.</li> <li>Legal Document Analysis: Extract relevant facts and questions from lengthy legal documents for more efficient processing.</li> <li>Medical Diagnosis Assistance: Focus on key symptoms and patient history while filtering out irrelevant personal information.</li> <li>Educational Q&amp;A Systems: Improve the relevance of answers by focusing on the core educational content of student questions.</li> <li>Research Query Processing: Enhance literature review processes by focusing on the most relevant aspects of research questions.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Fine-tuning the filtering process for your specific domain or types of queries.</li> <li>Experimenting with different prompt formats for both the filtering and answering stages.</li> <li>Implementing a feedback loop to continuously improve the quality of the filtering process.</li> <li>Combining S2A with other techniques like Chain of Thought or Self-Consistency for even more robust reasoning capabilities.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator, response models, and dynamic configuration, you can easily implement and customize the System to Attention technique to enhance your LLM's ability to focus on relevant information and provide more accurate responses across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/text_based/chain_of_thought/","title":"Chain of Thought","text":"In\u00a0[4]: Copied! <pre>from mirascope.core import openai, prompt_template\n\ncot_augment = \"\\nLet's think step by step.\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"{query} {cot_augment}\")\ndef call(query: str, cot_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"cot_augment\": cot_augment if cot_prompt else \"\",\n        }\n    }\n\n\nprompt = \"\"\"Olivia has $23. She bought five bagels for $3 each.\nHow much money does she have left?\"\"\"\n\nprint(call(query=prompt, cot_prompt=True))\n</pre> from mirascope.core import openai, prompt_template  cot_augment = \"\\nLet's think step by step.\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(\"{query} {cot_augment}\") def call(query: str, cot_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:     return {         \"computed_fields\": {             \"cot_augment\": cot_augment if cot_prompt else \"\",         }     }   prompt = \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\"  print(call(query=prompt, cot_prompt=True)) <pre>First, let's determine how much money Olivia spent on the bagels.\n\n1. **Calculate the total cost of the bagels:**\n   - Price of one bagel = $3\n   - Number of bagels = 5\n   - Total cost = Price of one bagel \u00d7 Number of bagels = $3 \u00d7 5 = $15\n\n2. **Subtract the total cost from Olivia's initial amount:**\n   - Initial amount = $23\n   - Amount spent = $15\n   - Amount left = Initial amount - Amount spent = $23 - $15 = $8\n\nSo, Olivia has $8 left after buying the bagels.\n</pre> In\u00a0[6]: Copied! <pre>from mirascope.core import openai\nfrom openai.types.chat import ChatCompletionMessageParam\n\nfew_shot_examples = [\n    {\n        \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n        \"answer\": \"\"\"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\"\"\",\n    },\n    {\n        \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n        \"answer\": \"\"\"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\"\"\",\n    },\n    {\n        \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n        \"answer\": \"\"\"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\"\"\",\n    },\n    {\n        \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n        \"answer\": \"\"\"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\"\"\",\n    },\n    {\n        \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n        \"answer\": \"\"\"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\"\"\",\n    },\n    {\n        \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n        \"answer\": \"\"\"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\"\"\",\n    },\n    {\n        \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n        \"answer\": \"\"\"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\"\"\",\n    },\n]\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    MESSAGES: {example_prompts}\n    USER: {query}\n    \"\"\"\n)\ndef call(query: str, num_examples: int = 0) -&gt; openai.OpenAIDynamicConfig:\n    if num_examples &lt; 0 or num_examples &gt; len(few_shot_examples):\n        raise ValueError(\n            \"num_examples cannot be negative or greater than number of available examples.\"\n        )\n    example_prompts: list[ChatCompletionMessageParam] = []\n    for i in range(num_examples):\n        example_prompts.append(\n            {\"role\": \"user\", \"content\": few_shot_examples[i][\"question\"]}\n        )\n        example_prompts.append(\n            {\"role\": \"assistant\", \"content\": few_shot_examples[i][\"answer\"]}\n        )\n    return {\"computed_fields\": {\"example_prompts\": example_prompts}}\n\n\nprompt = \"\"\"Olivia has $23. She bought five bagels for $3 each.\nHow much money does she have left?\"\"\"\n\nprint(call(query=prompt, num_examples=len(few_shot_examples)))\n</pre> from mirascope.core import openai from openai.types.chat import ChatCompletionMessageParam  few_shot_examples = [     {         \"question\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",         \"answer\": \"\"\"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\"\"\",     },     {         \"question\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",         \"answer\": \"\"\"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\"\"\",     },     {         \"question\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",         \"answer\": \"\"\"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\"\"\",     },     {         \"question\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",         \"answer\": \"\"\"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\"\"\",     },     {         \"question\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",         \"answer\": \"\"\"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\"\"\",     },     {         \"question\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",         \"answer\": \"\"\"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\"\"\",     },     {         \"question\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",         \"answer\": \"\"\"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\"\"\",     }, ]   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     MESSAGES: {example_prompts}     USER: {query}     \"\"\" ) def call(query: str, num_examples: int = 0) -&gt; openai.OpenAIDynamicConfig:     if num_examples &lt; 0 or num_examples &gt; len(few_shot_examples):         raise ValueError(             \"num_examples cannot be negative or greater than number of available examples.\"         )     example_prompts: list[ChatCompletionMessageParam] = []     for i in range(num_examples):         example_prompts.append(             {\"role\": \"user\", \"content\": few_shot_examples[i][\"question\"]}         )         example_prompts.append(             {\"role\": \"assistant\", \"content\": few_shot_examples[i][\"answer\"]}         )     return {\"computed_fields\": {\"example_prompts\": example_prompts}}   prompt = \"\"\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\"\"\"  print(call(query=prompt, num_examples=len(few_shot_examples))) <pre>Olivia bought 5 bagels for $3 each, which costs her a total of \\(5 \\times 3 = 15\\) dollars. \n\nShe started with $23, so after the purchase, she has \\(23 - 15 = 8\\) dollars left. \n\nThe answer is $8.\n</pre> <p>Effective Chain of Thought Usage</p> <ul> <li>Encourage Step-by-Step Thinking: Explicitly instruct the LLM to break down the problem into small steps.</li> <li>Provide Relevant Examples: In few-shot learning, use examples that are similar to the problem you want to solve.</li> <li>Ask for Clear Explanations: Prompt the LLM to explain its reasoning clearly at each step.</li> <li>Apply to Complex Problems: Chain of Thought is particularly effective for problems that require multiple steps or complex reasoning.</li> <li>Validate Results: Review the LLM's reasoning process and verify that each step is logical.</li> </ul> <p>By leveraging the Chain of Thought technique, you can make the LLM's reasoning process more transparent and obtain more accurate and explainable answers to complex problems. This technique is particularly useful for mathematical problems and tasks that require multi-step reasoning.</p>"},{"location":"tutorials/prompt_engineering/text_based/chain_of_thought/#chain-of-thought","title":"Chain of Thought\u00b6","text":"<p>Chain of Thought (CoT) is a common prompt engineering technique which asks the LLM to step through its reasoning and thinking process to answer a question. In its simplest form, it can be implemented by asking asking the LLM to step through a problem step by step, but is more effective when you leverage examples and patterns of reasoning similar to your query in a few shot prompt. Chain of Thought is most effective for mathematical and reasoning tasks.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul>"},{"location":"tutorials/prompt_engineering/text_based/chain_of_thought/#zero-shot-cot","title":"Zero Shot CoT\u00b6","text":"<p>Note</p> <p>Recent models will automatically explain their reasoning (to a degree) for most reasoning tasks, but explicitly asking for a step by step solution can sometimes produce better solutions and explanations.</p>"},{"location":"tutorials/prompt_engineering/text_based/chain_of_thought/#few-shot-cot","title":"Few Shot CoT\u00b6","text":""},{"location":"tutorials/prompt_engineering/text_based/common_phrases/","title":"Common Phrases (Prompt Mining)","text":"In\u00a0[2]: Copied! <pre>from mirascope.core import openai\n\n\n@openai.call(model=\"gpt-4o-mini\")\ndef call(query: str):\n    return query\n\n\ngeneric_response = call(\n    \"\"\"Does the roy in smash bros ultimate have a reliable way to knock out an\\\n        opponent that starts with the A button?\"\"\"\n)\nengineered_response = call(\n    \"\"\"In smash bros ultimate, what moves comprise Roy's kill confirm combo that\\\n        starts with jab?\"\"\"\n)\n\nprint(generic_response)\nprint(engineered_response)\n</pre> from mirascope.core import openai   @openai.call(model=\"gpt-4o-mini\") def call(query: str):     return query   generic_response = call(     \"\"\"Does the roy in smash bros ultimate have a reliable way to knock out an\\         opponent that starts with the A button?\"\"\" ) engineered_response = call(     \"\"\"In smash bros ultimate, what moves comprise Roy's kill confirm combo that\\         starts with jab?\"\"\" )  print(generic_response) print(engineered_response) <pre>Yes, in Super Smash Bros. Ultimate, Roy has a reliable way to knock out an opponent using an attack that starts with the A button. His forward tilt (dtilt), also known as the \"F tilt,\" is a strong move that can lead to knockouts if used correctly, especially at higher percentages. Additionally, his neutral attack (AAA combo) can set up for follow-up attacks or build damage, although it's not typically a kill move on its own.\n\nIf you are looking specifically for a move that can knock out and starts with the A button, Roy's forward aerial (aerial attack) is also a strong option, particularly towards the edge of the stage when opponents are at higher percentages.\n\nKeep in mind that spacing and timing are key to landing these moves effectively!\nIn Super Smash Bros. Ultimate, Roy's jab kill confirm combo typically involves starting with his jab, specifically the rapid jabs. After hitting the opponent with the jab, players can follow up with:\n\n1. **Jab (rapid jabs)** - Connects with the first few hits.\n2. **F-tilt (Forward Tilt)** - After the jab, quickly input F-tilt to catch the opponent off-guard. The jab pushes the opponent slightly away, and if done correctly, the F-tilt can connect reliably.\n\nThe timing and spacing are crucial for this combo to work effectively, and it often relies on the opponent being at a higher percentage for the F-tilt to secure the KO. Additionally, if performed correctly, this can work as an effective kill confirm at kill percentages.\n</pre> <p>As you can see, using common phrases and jargon specific to the topic (in this case, Super Smash Bros. Ultimate) can lead to more accurate and detailed responses from the LLM. This technique can be particularly useful when dealing with specialized or technical subjects.</p>"},{"location":"tutorials/prompt_engineering/text_based/common_phrases/#common-phrases-prompt-mining","title":"Common Phrases (Prompt Mining)\u00b6","text":"<p>Sometimes, an LLM can appear to know more or less about a topic depending on the phrasing you use because a specific bit of information shows up far more frequently in its training data with a specific phrasing. Prompt Mining, explained in this paper, involves various methods of searching for the phrase which gives the best response regarding a topic.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> </ul> <p>Prompt Mining itself is an extensive endeavor, but the takeaway is that LLMs are likely to give better answers when the question uses phrases that the LLM has been trained on. Here is an example where using common jargon regarding a video game produces a real answer, and a generic prompt doesn\u2019t:</p>"},{"location":"tutorials/prompt_engineering/text_based/common_phrases/#tips-for-using-common-phrases","title":"Tips for Using Common Phrases\u00b6","text":"<ol> <li><p>Research the topic: Familiarize yourself with the common terminology and phrases used in the field you're querying about.</p> </li> <li><p>Use specific jargon: Incorporate field-specific terms that are likely to appear in the LLM's training data.</p> </li> <li><p>Experiment with different phrasings: Try multiple versions of your query using different common phrases to see which yields the best results.</p> </li> <li><p>Be aware of potential biases: Remember that using common phrases might reinforce existing biases in the training data.</p> </li> <li><p>Combine with other techniques: Use this approach in conjunction with other prompt engineering techniques for even better results.</p> </li> </ol> <p>By leveraging common phrases and domain-specific language, you can often elicit more accurate and detailed responses from LLMs, especially when dealing with specialized topics or technical subjects.</p>"},{"location":"tutorials/prompt_engineering/text_based/contrastive_chain_of_thought/","title":"Contrastive Chain of Thought","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\n\nexample = \"\"\"\nExample Question: If you roll two 6 sided dice (1~6) and a 12 sided die (1~12),\nhow many possible outcomes are there?\n\nCorrect Reasoning: The smallest possible sum is 3 and the largest possible sum is 24.\nWe know two six sided die can roll anywhere from 2 to 12 from their standalone sums,\nso it stands to reason that by adding a value from (1~12) to one of those possible\nsums from 2~12, we can hit any number from 3~24 without any gaps in coverage.\nSo, there are (24-3)+1 = 22 possible outcomes.\n\nIncorrect Reasoning: 6x6x12 = 2592 outcomes\n\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    {example}\n    {query}\n    \"\"\"\n)\ndef call(query: str, ccot_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:\n    return {\"computed_fields\": {\"example\": example if ccot_prompt else \"\"}}\n\n\nprompt = \"\"\"\nIf you roll two 8 sided dice (1~8) and a 10 sided die (1~10), how many possible\noutcomes are there?\n\"\"\"\n\nprint(call(query=prompt, ccot_prompt=True))\n</pre> from mirascope.core import openai, prompt_template  example = \"\"\" Example Question: If you roll two 6 sided dice (1~6) and a 12 sided die (1~12), how many possible outcomes are there?  Correct Reasoning: The smallest possible sum is 3 and the largest possible sum is 24. We know two six sided die can roll anywhere from 2 to 12 from their standalone sums, so it stands to reason that by adding a value from (1~12) to one of those possible sums from 2~12, we can hit any number from 3~24 without any gaps in coverage. So, there are (24-3)+1 = 22 possible outcomes.  Incorrect Reasoning: 6x6x12 = 2592 outcomes \"\"\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     {example}     {query}     \"\"\" ) def call(query: str, ccot_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:     return {\"computed_fields\": {\"example\": example if ccot_prompt else \"\"}}   prompt = \"\"\" If you roll two 8 sided dice (1~8) and a 10 sided die (1~10), how many possible outcomes are there? \"\"\"  print(call(query=prompt, ccot_prompt=True)) <pre>To find the total number of possible outcomes when rolling two 8-sided dice and one 10-sided die, we can use the counting principle. \n\n1. **Two 8-sided dice:** Each die has 8 possible outcomes. Therefore, the number of outcomes for two dice is:\n   \\[\n   8 \\times 8 = 64\n   \\]\n\n2. **One 10-sided die:** This die has 10 possible outcomes.\n\n3. **Total Outcomes:** Since the rolls are independent, the total number of outcomes when rolling two 8-sided dice and one 10-sided die is the product of the outcomes from each die:\n   \\[\n   64 \\times 10 = 640\n   \\]\n\nThus, the total number of possible outcomes when rolling two 8-sided dice and one 10-sided die is **640**.\n</pre> <p>Effective Contrastive Chain of Thought Usage</p> <ul> <li>Provide Clear Examples: Include both correct and incorrect reasoning examples to guide the LLM's thought process.</li> <li>Highlight Common Mistakes: Use incorrect examples that demonstrate typical errors or misconceptions related to the problem.</li> <li>Explain the Contrast: Clearly explain why the correct reasoning is right and why the incorrect reasoning is wrong.</li> <li>Apply to Complex Problems: Use Contrastive Chain of Thought for problems where there are multiple potential approaches, some of which may lead to incorrect conclusions.</li> <li>Customize Examples: Tailor the examples to be relevant to the specific type of problem or domain you're working with.</li> </ul> <p>Contrastive Chain of Thought enhances the standard Chain of Thought approach by explicitly showing both correct and incorrect reasoning paths. This technique can be particularly effective in helping the LLM avoid common pitfalls and misconceptions, leading to more accurate and robust problem-solving across a variety of tasks, especially those prone to subtle errors or misunderstandings.</p>"},{"location":"tutorials/prompt_engineering/text_based/contrastive_chain_of_thought/#contrastive-chain-of-thought","title":"Contrastive Chain of Thought\u00b6","text":"<p>Contrastive Chain of Thought is an extension of Chain of Thought which involves adding both correct and incorrect examples to help the LLM reason. Contrastive Chain of Thought is applicable anywhere CoT is, such as mathematical and reasoning tasks, but is additionally helpful for scenarios where LLM might be prone to common errors or misunderstandings.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul>"},{"location":"tutorials/prompt_engineering/text_based/emotion_prompting/","title":"Emotion Prompting","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\n\nemotion_augment = \"This is very important to my career.\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"{query} {emotion_augment}\")\ndef call(query: str, emotion_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"emotion_augment\": emotion_augment if emotion_prompt else \"\",\n        }\n    }\n\n\nprompt = \"\"\"Write me an email I can send to my boss about how I need to\ntake a day off for mental health reasons.\"\"\"\n\nprint(call(query=prompt, emotion_prompt=True))\n</pre> from mirascope.core import openai, prompt_template  emotion_augment = \"This is very important to my career.\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(\"{query} {emotion_augment}\") def call(query: str, emotion_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:     return {         \"computed_fields\": {             \"emotion_augment\": emotion_augment if emotion_prompt else \"\",         }     }   prompt = \"\"\"Write me an email I can send to my boss about how I need to take a day off for mental health reasons.\"\"\"  print(call(query=prompt, emotion_prompt=True)) <pre>Subject: Request for a Day Off\n\nDear [Boss's Name],\n\nI hope this message finds you well. I am writing to formally request a day off for mental health reasons on [specific date]. I believe taking this time will allow me to recharge and return to work with renewed focus and energy.\n\nI understand the importance of maintaining productivity and teamwork, and I will ensure that any pressing tasks are managed before my absence. I will also make sure to communicate with the team so that there are no disruptions.\n\nThank you for your understanding and support regarding my request. I\u2019m committed to maintaining my well-being, which ultimately contributes to my overall performance and our team's success.\n\nBest regards,\n\n[Your Name]  \n[Your Position]  \n[Your Contact Information]  \n</pre> <p>This example demonstrates how to implement emotion prompting using Mirascope. The <code>emotion_augment</code> variable contains the emotional phrase that will be added to the end of the prompt when <code>emotion_prompt</code> is set to <code>True</code>.</p>"},{"location":"tutorials/prompt_engineering/text_based/emotion_prompting/#emotion-prompting","title":"Emotion Prompting\u00b6","text":"<p>Emotion Prompting is a prompt engineering technique where you end your original prompt with a phrase of psychological importance. It is most helpful for open ended tasks, but can still improve some analytical prompts:</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul>"},{"location":"tutorials/prompt_engineering/text_based/emotion_prompting/#benefits-of-emotion-prompting","title":"Benefits of Emotion Prompting\u00b6","text":"<ol> <li>Increased Engagement: Adding emotional context can make the LLM's responses more empathetic and engaging.</li> <li>Improved Relevance: Emotional prompts can help guide the LLM to provide responses that are more relevant to the user's emotional state or needs.</li> <li>Enhanced Creativity: For open-ended tasks, emotion prompting can lead to more creative and nuanced responses.</li> <li>Better Problem Solving: In some cases, emotion prompting can help the LLM focus on more critical aspects of a problem or question.</li> </ol> <p>Effective Emotion Prompting</p> <ul> <li>Choose Appropriate Emotions: Select emotional phrases that are relevant to the context of your query.</li> <li>Be Authentic: Use emotional prompts that genuinely reflect the importance or emotional weight of the task.</li> <li>Experiment: Try different emotional phrases to see which produces the best results for your specific use case.</li> <li>Balance: Be careful not to overuse emotional prompting, as it may not be appropriate for all types of queries.</li> <li>Combine with Other Techniques: Emotion prompting can be used in conjunction with other prompt engineering techniques for even better results.</li> </ul> <p>By leveraging emotion prompting, you can guide the LLM to provide responses that are more emotionally attuned and potentially more helpful for tasks that benefit from emotional context.</p>"},{"location":"tutorials/prompt_engineering/text_based/plan_and_solve/","title":"Plan and Solve","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\n\npas_augment = \"\"\"Let's first understand the problem and devise a plan to solve it.\nThen, let's carry out the plan and solve the problem step by step.\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"{modifiable_query}\")\ndef call(query: str, pas_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:\n    if pas_prompt:\n        modifiable_query = f\"Q: {query}\\nA: {pas_augment}\"\n    else:\n        modifiable_query = query\n    return {\"computed_fields\": {\"modifiable_query\": modifiable_query}}\n\n\nprompt = \"\"\"The school cafeteria ordered 42 red apples and 7 green apples for\nstudents lunches. But, if only 9 students wanted fruit, how many extra did the\ncafeteria end up with?\"\"\"\n\nprint(call(query=prompt, pas_prompt=True))\n</pre> from mirascope.core import openai, prompt_template  pas_augment = \"\"\"Let's first understand the problem and devise a plan to solve it. Then, let's carry out the plan and solve the problem step by step.\"\"\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(\"{modifiable_query}\") def call(query: str, pas_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:     if pas_prompt:         modifiable_query = f\"Q: {query}\\nA: {pas_augment}\"     else:         modifiable_query = query     return {\"computed_fields\": {\"modifiable_query\": modifiable_query}}   prompt = \"\"\"The school cafeteria ordered 42 red apples and 7 green apples for students lunches. But, if only 9 students wanted fruit, how many extra did the cafeteria end up with?\"\"\"  print(call(query=prompt, pas_prompt=True)) <pre>To find out how many extra apples the cafeteria ended up with, we can follow these steps:\n\n1. **Calculate the total number of apples ordered:**\n   - Red apples: 42\n   - Green apples: 7\n   - Total apples = Red apples + Green apples = 42 + 7 = 49 apples\n\n2. **Identify how many apples were taken by the students:**\n   - Number of students who wanted fruit = 9 apples (since each student is presumably taking one apple)\n\n3. **Calculate the number of extra apples:**\n   - Extra apples = Total apples - Apples taken by students\n   - Extra apples = 49 - 9 = 40\n\nTherefore, the cafeteria ended up with **40 extra apples** after the students took their fruit.\n</pre> <p>Effective Plan and Solve Usage</p> <ul> <li>Encourage Structured Thinking: The Plan and Solve approach promotes a more organized problem-solving process, starting with understanding and planning before execution.</li> <li>Break Down Complex Problems: Use this technique for problems that benefit from being broken down into smaller, manageable steps.</li> <li>Improve Problem Comprehension: By asking the LLM to first understand the problem, it can lead to better overall comprehension and more accurate solutions.</li> <li>Enhance Step-by-Step Reasoning: The explicit instruction to solve the problem step by step can result in clearer, more detailed explanations.</li> <li>Apply to Various Domains: While particularly effective for mathematical and reasoning tasks, Plan and Solve can be adapted for a wide range of problem types.</li> </ul> <p>Plan and Solve enhances the standard Chain of Thought approach by explicitly structuring the problem-solving process into distinct phases: understanding, planning, and execution. This structured approach can lead to more comprehensive and accurate solutions, especially for complex problems that benefit from careful planning before execution. By encouraging the LLM to first grasp the problem and outline a strategy, Plan and Solve can result in more thoughtful and well-organized responses across various types of reasoning and mathematical tasks.</p>"},{"location":"tutorials/prompt_engineering/text_based/plan_and_solve/#plan-and-solve","title":"Plan and Solve\u00b6","text":"<p>Plan and Solve is another variation of zero-shot Chain of Thought whereby the LLM is asked to reason with the improved prompt \"Q: {prompt} A: Let's first understand the problem and devise a plan to solve it. Then, let's carry out the plan and solve the problem step by step\". Plan-and-solve has shown improvements compared to standard CoT in reasoning and mathematical tasks.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul>"},{"location":"tutorials/prompt_engineering/text_based/rephrase_and_respond/","title":"Rephrase and Respond","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\n\nrar_augment = \"\\nRephrase and expand the question, and respond.\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"{query} {rar_augment}\")\ndef call(query: str, rar_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"rar_augment\": rar_augment if rar_prompt else \"\",\n        }\n    }\n\n\nprompt = \"\"\"A coin is heads up. aluino flips the coin. arthor flips the coin.\nIs the coin still heads up? Flip means reverse.\"\"\"\n\nprint(call(query=prompt, rar_prompt=True))\n</pre> from mirascope.core import openai, prompt_template  rar_augment = \"\\nRephrase and expand the question, and respond.\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(\"{query} {rar_augment}\") def call(query: str, rar_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:     return {         \"computed_fields\": {             \"rar_augment\": rar_augment if rar_prompt else \"\",         }     }   prompt = \"\"\"A coin is heads up. aluino flips the coin. arthor flips the coin. Is the coin still heads up? Flip means reverse.\"\"\"  print(call(query=prompt, rar_prompt=True)) <pre>### Rephrased and Expanded Question:\n\nA coin starts with the heads side facing up. If Aluino flips the coin, it will land with the tails side facing up. Then Arthur flips the coin again. After these two sequences of flips, can we say that the coin is still heads up? \n\n### Response:\n\nTo analyze the scenario, let's break down the actions step by step:\n\n1. **Initial State**: The coin starts with the heads side facing up.\n   \n2. **Aluino Flips the Coin**: When Aluino flips the coin, it reverses its position. Since the coin initially was heads up, after Aluino's flip, the coin will now be tails up.\n\n3. **Arthur Flips the Coin**: Next, Arthur takes his turn to flip the coin. Given that the current state of the coin is tails up, flipping it will reverse it again, resulting in the coin now being heads up.\n\nAt the end of these actions, after both Aluino and Arthur have flipped the coin, the final state of the coin is heads up once more. Thus, the answer to the question is: \n\n**No, after Aluino flips it, the coin is tails up; however, after Arthur flips it again, the coin is heads up once more.**\n</pre> <p>This example demonstrates how to implement the Rephrase and Respond technique using Mirascope. The <code>rar_augment</code> variable contains the instruction for the LLM to rephrase and expand the question before responding. This instruction is added to the end of the prompt when <code>rar_prompt</code> is set to <code>True</code>.</p>"},{"location":"tutorials/prompt_engineering/text_based/rephrase_and_respond/#rephrase-and-respond","title":"Rephrase and Respond\u00b6","text":"<p>Rephrase and respond (RaR) is a prompt engineering technique which involves asking the LLM to rephrase and expand upon the question before responding. RaR has shown improvements across all types of prompts, but we have personally found that RaR is most effective for shorter and vaguer prompts.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul>"},{"location":"tutorials/prompt_engineering/text_based/rephrase_and_respond/#benefits-of-rephrase-and-respond","title":"Benefits of Rephrase and Respond\u00b6","text":"<ol> <li>Improved Understanding: By rephrasing the question, the LLM demonstrates and often improves its understanding of the query.</li> <li>Clarity: The rephrasing can help clarify ambiguous or vague queries.</li> <li>Context Expansion: The expansion part of RaR allows the LLM to consider additional relevant context.</li> <li>Better Responses: The combination of rephrasing and expanding often leads to more comprehensive and accurate responses.</li> </ol> <p>Effective Rephrase and Respond</p> <ul> <li>Use with Shorter Prompts: RaR is particularly effective with shorter or vaguer prompts that benefit from expansion.</li> <li>Allow for Flexibility: The rephrasing may interpret the question slightly differently, which can lead to new insights.</li> <li>Review the Rephrasing: Pay attention to how the LLM rephrases the question, as it can provide insights into the model's understanding.</li> <li>Iterative Refinement: If the rephrasing misses key points, consider refining your original prompt.</li> <li>Combine with Other Techniques: RaR can be used in conjunction with other prompt engineering techniques for even better results.</li> </ul> <p>By leveraging the Rephrase and Respond technique, you can often obtain more thorough and accurate responses from the LLM, especially for queries that benefit from additional context or clarification.</p>"},{"location":"tutorials/prompt_engineering/text_based/rereading/","title":"Rereading","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"{query} {reread}\")\ndef call(query: str, reread_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"reread\": f\"Read the question again: {query}\" if reread_prompt else \"\",\n        }\n    }\n\n\nprompt = \"\"\"A coin is heads up. aluino flips the coin. arthor flips the coin.\nIs the coin still heads up? Flip means reverse.\"\"\"\n\nprint(call(query=prompt, reread_prompt=True))\n</pre> from mirascope.core import openai, prompt_template   @openai.call(model=\"gpt-4o-mini\") @prompt_template(\"{query} {reread}\") def call(query: str, reread_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:     return {         \"computed_fields\": {             \"reread\": f\"Read the question again: {query}\" if reread_prompt else \"\",         }     }   prompt = \"\"\"A coin is heads up. aluino flips the coin. arthor flips the coin. Is the coin still heads up? Flip means reverse.\"\"\"  print(call(query=prompt, reread_prompt=True)) <pre>To analyze the situation:\n\n1. The coin starts heads up.\n2. Aluino flips the coin, which reverses it to tails up.\n3. Arthor then flips the coin again, which reverses it back to heads up.\n\nSo, after both flips, the coin is heads up again. The final answer is yes, the coin is still heads up.\n</pre> <p>This example demonstrates how to implement the Rereading technique using Mirascope. The <code>reread</code> computed field is added to the prompt when <code>reread_prompt</code> is set to <code>True</code>, instructing the LLM to read the question again.</p>"},{"location":"tutorials/prompt_engineering/text_based/rereading/#rereading","title":"Rereading\u00b6","text":"<p>Note</p> <p>Our experiences indicate that re-reading is not as effective for newer, more powerful models such as Anthropic's 3.5 Sonnet or OpenAI's GPT-4o, although it remains effective in older models.</p> <p>Rereading is a prompt engineering technique that simply asks the LLM to reread a question and repeats it. When working with older, less capable LLM models, rereading has shown improvements for all types of reasoning tasks (arithmetic, symbolic, commonsense).</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul>"},{"location":"tutorials/prompt_engineering/text_based/rereading/#benefits-of-rereading","title":"Benefits of Rereading\u00b6","text":"<ol> <li>Improved Comprehension: Rereading can help the LLM better understand complex or nuanced questions.</li> <li>Enhanced Accuracy: For older models, rereading has shown to improve accuracy across various reasoning tasks.</li> <li>Reinforcement: Repeating the question can reinforce key details that might be overlooked in a single pass.</li> <li>Reduced Errors: Rereading can help minimize errors that might occur due to misreading or misinterpreting the initial question.</li> </ol> <p>Effective Rereading</p> <ul> <li>Use with Older Models: Rereading is most effective with older, less capable LLM models.</li> <li>Apply to Complex Questions: Consider using rereading for questions that involve multiple steps or complex reasoning.</li> <li>Combine with Other Techniques: Rereading can be used in conjunction with other prompt engineering techniques for potentially better results.</li> <li>Monitor Performance: Keep track of how rereading affects your model's performance, as its effectiveness may vary depending on the specific task and model used.</li> <li>Consider Model Capabilities: For newer, more advanced models, rereading might not provide significant benefits and could potentially be redundant.</li> </ul> <p>By leveraging the Rereading technique, particularly with older LLM models, you may be able to improve the model's understanding and accuracy across various types of reasoning tasks. However, always consider the capabilities of your specific model when deciding whether to apply this technique.</p>"},{"location":"tutorials/prompt_engineering/text_based/role_prompting/","title":"Role Prompting","text":"In\u00a0[2]: Copied! <pre>from mirascope.core import openai, prompt_template\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\"\"\"\n    SYSTEM: {llm_role} {audience}\n    USER: {query}\n    \"\"\")\ndef call(\n    query: str, llm_role: str | None = None, audience: str | None = None\n) -&gt; openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"llm_role\": f\"You are {llm_role}.\" if llm_role else \"\",\n            \"audience\": f\"You are talking to {audience}.\" if audience else \"\",\n        }\n    }\n\n\nresponse = call(\n    query=\"What's the square root of x^2 + 2x + 1?\",\n    llm_role=\"a math teacher\",\n    audience=\"your student\",\n)\nprint(response.content)\n</pre> from mirascope.core import openai, prompt_template   @openai.call(model=\"gpt-4o-mini\") @prompt_template(\"\"\"     SYSTEM: {llm_role} {audience}     USER: {query}     \"\"\") def call(     query: str, llm_role: str | None = None, audience: str | None = None ) -&gt; openai.OpenAIDynamicConfig:     return {         \"computed_fields\": {             \"llm_role\": f\"You are {llm_role}.\" if llm_role else \"\",             \"audience\": f\"You are talking to {audience}.\" if audience else \"\",         }     }   response = call(     query=\"What's the square root of x^2 + 2x + 1?\",     llm_role=\"a math teacher\",     audience=\"your student\", ) print(response.content) <pre>To find the square root of the expression \\( x^2 + 2x + 1 \\), we can first recognize that this expression can be factored.\n\nThe expression \\( x^2 + 2x + 1 \\) is a perfect square trinomial, and it can be factored as:\n\n\\[\n(x + 1)^2\n\\]\n\nNow, we can take the square root of this expression:\n\n\\[\n\\sqrt{x^2 + 2x + 1} = \\sqrt{(x + 1)^2}\n\\]\n\nTaking the square root of a square gives us the absolute value:\n\n\\[\n\\sqrt{(x + 1)^2} = |x + 1|\n\\]\n\nSo, the final result is:\n\n\\[\n\\sqrt{x^2 + 2x + 1} = |x + 1|\n\\]\n</pre> <p>In this example, we're using role prompting to set the LLM's role as a math teacher and the audience as a student. This context can help the LLM tailor its response to be more educational and easier to understand, as a teacher would explain to a student.</p>"},{"location":"tutorials/prompt_engineering/text_based/role_prompting/#role-prompting","title":"Role Prompting\u00b6","text":"<p>Role prompting is a commonly used prompt engineering technique where responses can be improved by setting the roles of the LLM or the audience within the conversation. The paper linked above showcases some analytics for which roles perform best for specific tasks. Role prompting can improve response quality in both accuracy based and open ended tasks.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul>"},{"location":"tutorials/prompt_engineering/text_based/role_prompting/#benefits-of-role-prompting","title":"Benefits of Role Prompting\u00b6","text":"<ol> <li>Contextual Responses: By setting roles, the LLM can provide responses that are more appropriate for the given context.</li> <li>Improved Accuracy: For certain tasks, setting the right role can lead to more accurate or relevant information.</li> <li>Tailored Language: The LLM can adjust its language and explanation style based on the roles, making responses more suitable for the intended audience.</li> <li>Enhanced Creativity: For open-ended tasks, role prompting can lead to more diverse and creative responses.</li> </ol> <p>Effective Role Prompting</p> <ul> <li>Choose Relevant Roles: Select roles that are appropriate for the task or query at hand.</li> <li>Be Specific: The more specific you are about the roles, the better the LLM can tailor its response.</li> <li>Experiment: Try different role combinations to see which produces the best results for your specific use case.</li> <li>Consider the Audience: Setting an audience role can be just as important as setting the LLM's role.</li> <li>Combine with Other Techniques: Role prompting can be used in conjunction with other prompt engineering techniques for even better results.</li> </ul> <p>By leveraging role prompting, you can guide the LLM to provide responses that are more aligned with your specific needs and context.</p>"},{"location":"tutorials/prompt_engineering/text_based/self_ask/","title":"Self-Ask","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install \"mirascope[openai]\" numpy scikit-learn\n</pre> !pip install \"mirascope[openai]\" numpy scikit-learn In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n# Set the appropriate API key for the provider you're using\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Set the appropriate API key for the provider you're using In\u00a0[2]: Copied! <pre>import inspect\n\nfrom mirascope.core import openai, prompt_template\nfrom typing_extensions import TypedDict\n\n\nclass FewShotExample(TypedDict):\n    question: str\n    answer: str\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Examples:\n    {examples:lists}\n\n    Query: {query}\n    \"\"\"\n)\ndef self_ask(query: str, examples: list[FewShotExample]) -&gt; openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"examples\": [\n                [example[\"question\"], example[\"answer\"]] for example in examples\n            ]\n        }\n    }\n\n\nfew_shot_examples = [\n    FewShotExample(\n        question=\"When does monsoon season end in the state the area code 575 is located?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which state is the area code 575 located in?\n            Intermediate answer: The area code 575 is located in New Mexico.\n            Follow up: When does monsoon season end in New Mexico?\n            Intermediate answer: Monsoon season in New Mexico typically ends in mid-September.\n            So the final answer is: mid-September.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What is the current official currency in the country where Ineabelle Diaz is a citizen?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which country is Ineabelle Diaz a citizen of?\n            Intermediate answer: Ineabelle Diaz is from Peurto Rico, which is in the United States of America.\n            Follow up: What is the current official currency in the United States of America?\n            Intermediate answer: The current official currency in the United States is the United States dollar.\n            So the final answer is: United States dollar.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"Where was the person who founded the American Institute of Public Opinion in 1935 born?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Who founded the American Institute of Public Opinion in 1935?\n            Intermediate answer: George Gallup.\n            Follow up: Where was George Gallup born?\n            Intermediate answer: George Gallup was born in Jefferson, Iowa.\n            So the final answer is: Jefferson.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What language is used by the director of Tiffany Memorandum?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Who directed the movie called Tiffany Memorandum?\n            Intermediate answer: Sergio Grieco.\n            Follow up: What language is used by Sergio Grieco?\n            Intermediate answer: Sergio Grieco speaks Italian.\n            So the final answer is: Italian.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What is the sports team the person played for who scored the first touchdown in Superbowl 1?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which player scored the first touchdown in Superbowl 1?\n            Intermediate answer: Max McGee.\n            Follow up: Which sports team did Max McGee play for?\n            Intermediate answer: Max McGee played for the Green Bay Packers.\n            So the final answer is: Green Bay Packers.\n            \"\"\"\n        ),\n    ),\n]\n\nquery = \"The birth country of Jayantha Ketagoda left the British Empire when?\"\nresponse = self_ask(query=query, examples=few_shot_examples)\nprint(response.content)\n\nresponse = self_ask(query=query, examples=[])\nprint(response.content)\n</pre> import inspect  from mirascope.core import openai, prompt_template from typing_extensions import TypedDict   class FewShotExample(TypedDict):     question: str     answer: str   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Examples:     {examples:lists}      Query: {query}     \"\"\" ) def self_ask(query: str, examples: list[FewShotExample]) -&gt; openai.OpenAIDynamicConfig:     return {         \"computed_fields\": {             \"examples\": [                 [example[\"question\"], example[\"answer\"]] for example in examples             ]         }     }   few_shot_examples = [     FewShotExample(         question=\"When does monsoon season end in the state the area code 575 is located?\",         answer=inspect.cleandoc(             \"\"\"             Are follow up questions needed here: Yes.             Follow up: Which state is the area code 575 located in?             Intermediate answer: The area code 575 is located in New Mexico.             Follow up: When does monsoon season end in New Mexico?             Intermediate answer: Monsoon season in New Mexico typically ends in mid-September.             So the final answer is: mid-September.             \"\"\"         ),     ),     FewShotExample(         question=\"What is the current official currency in the country where Ineabelle Diaz is a citizen?\",         answer=inspect.cleandoc(             \"\"\"             Are follow up questions needed here: Yes.             Follow up: Which country is Ineabelle Diaz a citizen of?             Intermediate answer: Ineabelle Diaz is from Peurto Rico, which is in the United States of America.             Follow up: What is the current official currency in the United States of America?             Intermediate answer: The current official currency in the United States is the United States dollar.             So the final answer is: United States dollar.             \"\"\"         ),     ),     FewShotExample(         question=\"Where was the person who founded the American Institute of Public Opinion in 1935 born?\",         answer=inspect.cleandoc(             \"\"\"             Are follow up questions needed here: Yes.             Follow up: Who founded the American Institute of Public Opinion in 1935?             Intermediate answer: George Gallup.             Follow up: Where was George Gallup born?             Intermediate answer: George Gallup was born in Jefferson, Iowa.             So the final answer is: Jefferson.             \"\"\"         ),     ),     FewShotExample(         question=\"What language is used by the director of Tiffany Memorandum?\",         answer=inspect.cleandoc(             \"\"\"             Are follow up questions needed here: Yes.             Follow up: Who directed the movie called Tiffany Memorandum?             Intermediate answer: Sergio Grieco.             Follow up: What language is used by Sergio Grieco?             Intermediate answer: Sergio Grieco speaks Italian.             So the final answer is: Italian.             \"\"\"         ),     ),     FewShotExample(         question=\"What is the sports team the person played for who scored the first touchdown in Superbowl 1?\",         answer=inspect.cleandoc(             \"\"\"             Are follow up questions needed here: Yes.             Follow up: Which player scored the first touchdown in Superbowl 1?             Intermediate answer: Max McGee.             Follow up: Which sports team did Max McGee play for?             Intermediate answer: Max McGee played for the Green Bay Packers.             So the final answer is: Green Bay Packers.             \"\"\"         ),     ), ]  query = \"The birth country of Jayantha Ketagoda left the British Empire when?\" response = self_ask(query=query, examples=few_shot_examples) print(response.content)  response = self_ask(query=query, examples=[]) print(response.content) <pre>Are follow up questions needed here: Yes.  \nFollow up: Which country is Jayantha Ketagoda from?  \nIntermediate answer: Jayantha Ketagoda is from Sri Lanka.  \nFollow up: When did Sri Lanka leave the British Empire?  \nIntermediate answer: Sri Lanka gained independence from the British Empire on February 4, 1948.  \nSo the final answer is: February 4, 1948.\nJayantha Ketagoda was born in Sri Lanka, which was formerly known as Ceylon. Sri Lanka gained independence from the British Empire on February 4, 1948.\n</pre> <p>This basic implementation demonstrates how to use few-shot learning with Self-Ask. The <code>self_ask</code> function takes a query and a list of examples, then uses Mirascope's <code>OpenAIDynamicConfig</code> to inject the examples into the prompt.</p> In\u00a0[3]: Copied! <pre>import inspect\n\nimport numpy as np\nfrom mirascope.core import openai, prompt_template\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom typing_extensions import TypedDict\n\n\nclass FewShotExample(TypedDict):\n    question: str\n    answer: str\n\n\ndef select_relevant_examples(\n    query: str, examples: list[FewShotExample], n: int = 3\n) -&gt; list[FewShotExample]:\n    \"\"\"Select the most relevant examples based on cosine similarity.\"\"\"\n    vectorizer = TfidfVectorizer().fit([ex[\"question\"] for ex in examples] + [query])\n    example_vectors = vectorizer.transform([ex[\"question\"] for ex in examples])\n    query_vector = vectorizer.transform([query])\n\n    similarities = cosine_similarity(query_vector, example_vectors)[0]\n    most_similar_indices = np.argsort(similarities)[-n:][::-1]\n\n    return [examples[i] for i in most_similar_indices]\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    Examples:\n    {examples:lists}\n\n    Query: {query}\n    \"\"\"\n)\ndef dynamic_self_ask(\n    query: str, examples: list[FewShotExample], n: int = 3\n) -&gt; openai.OpenAIDynamicConfig:\n    relevant_examples = select_relevant_examples(query, examples, n)\n    return {\n        \"computed_fields\": {\n            \"examples\": [\n                [example[\"question\"], example[\"answer\"]]\n                for example in relevant_examples\n            ]\n        }\n    }\n\n\nfew_shot_examples = [\n    FewShotExample(\n        question=\"When does monsoon season end in the state the area code 575 is located?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which state is the area code 575 located in?\n            Intermediate answer: The area code 575 is located in New Mexico.\n            Follow up: When does monsoon season end in New Mexico?\n            Intermediate answer: Monsoon season in New Mexico typically ends in mid-September.\n            So the final answer is: mid-September.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What is the current official currency in the country where Ineabelle Diaz is a citizen?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which country is Ineabelle Diaz a citizen of?\n            Intermediate answer: Ineabelle Diaz is from Peurto Rico, which is in the United States of America.\n            Follow up: What is the current official currency in the United States of America?\n            Intermediate answer: The current official currency in the United States is the United States dollar.\n            So the final answer is: United States dollar.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"Where was the person who founded the American Institute of Public Opinion in 1935 born?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Who founded the American Institute of Public Opinion in 1935?\n            Intermediate answer: George Gallup.\n            Follow up: Where was George Gallup born?\n            Intermediate answer: George Gallup was born in Jefferson, Iowa.\n            So the final answer is: Jefferson.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What language is used by the director of Tiffany Memorandum?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Who directed the movie called Tiffany Memorandum?\n            Intermediate answer: Sergio Grieco.\n            Follow up: What language is used by Sergio Grieco?\n            Intermediate answer: Sergio Grieco speaks Italian.\n            So the final answer is: Italian.\n            \"\"\"\n        ),\n    ),\n    FewShotExample(\n        question=\"What is the sports team the person played for who scored the first touchdown in Superbowl 1?\",\n        answer=inspect.cleandoc(\n            \"\"\"\n            Are follow up questions needed here: Yes.\n            Follow up: Which player scored the first touchdown in Superbowl 1?\n            Intermediate answer: Max McGee.\n            Follow up: Which sports team did Max McGee play for?\n            Intermediate answer: Max McGee played for the Green Bay Packers.\n            So the final answer is: Green Bay Packers.\n            \"\"\"\n        ),\n    ),\n]\n\n\nquery = \"What was the primary language spoken by the inventor of the phonograph?\"\nresponse = dynamic_self_ask(query=query, examples=few_shot_examples, n=2)\nprint(response.content)\n</pre> import inspect  import numpy as np from mirascope.core import openai, prompt_template from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import cosine_similarity from typing_extensions import TypedDict   class FewShotExample(TypedDict):     question: str     answer: str   def select_relevant_examples(     query: str, examples: list[FewShotExample], n: int = 3 ) -&gt; list[FewShotExample]:     \"\"\"Select the most relevant examples based on cosine similarity.\"\"\"     vectorizer = TfidfVectorizer().fit([ex[\"question\"] for ex in examples] + [query])     example_vectors = vectorizer.transform([ex[\"question\"] for ex in examples])     query_vector = vectorizer.transform([query])      similarities = cosine_similarity(query_vector, example_vectors)[0]     most_similar_indices = np.argsort(similarities)[-n:][::-1]      return [examples[i] for i in most_similar_indices]   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     Examples:     {examples:lists}      Query: {query}     \"\"\" ) def dynamic_self_ask(     query: str, examples: list[FewShotExample], n: int = 3 ) -&gt; openai.OpenAIDynamicConfig:     relevant_examples = select_relevant_examples(query, examples, n)     return {         \"computed_fields\": {             \"examples\": [                 [example[\"question\"], example[\"answer\"]]                 for example in relevant_examples             ]         }     }   few_shot_examples = [     FewShotExample(         question=\"When does monsoon season end in the state the area code 575 is located?\",         answer=inspect.cleandoc(             \"\"\"             Are follow up questions needed here: Yes.             Follow up: Which state is the area code 575 located in?             Intermediate answer: The area code 575 is located in New Mexico.             Follow up: When does monsoon season end in New Mexico?             Intermediate answer: Monsoon season in New Mexico typically ends in mid-September.             So the final answer is: mid-September.             \"\"\"         ),     ),     FewShotExample(         question=\"What is the current official currency in the country where Ineabelle Diaz is a citizen?\",         answer=inspect.cleandoc(             \"\"\"             Are follow up questions needed here: Yes.             Follow up: Which country is Ineabelle Diaz a citizen of?             Intermediate answer: Ineabelle Diaz is from Peurto Rico, which is in the United States of America.             Follow up: What is the current official currency in the United States of America?             Intermediate answer: The current official currency in the United States is the United States dollar.             So the final answer is: United States dollar.             \"\"\"         ),     ),     FewShotExample(         question=\"Where was the person who founded the American Institute of Public Opinion in 1935 born?\",         answer=inspect.cleandoc(             \"\"\"             Are follow up questions needed here: Yes.             Follow up: Who founded the American Institute of Public Opinion in 1935?             Intermediate answer: George Gallup.             Follow up: Where was George Gallup born?             Intermediate answer: George Gallup was born in Jefferson, Iowa.             So the final answer is: Jefferson.             \"\"\"         ),     ),     FewShotExample(         question=\"What language is used by the director of Tiffany Memorandum?\",         answer=inspect.cleandoc(             \"\"\"             Are follow up questions needed here: Yes.             Follow up: Who directed the movie called Tiffany Memorandum?             Intermediate answer: Sergio Grieco.             Follow up: What language is used by Sergio Grieco?             Intermediate answer: Sergio Grieco speaks Italian.             So the final answer is: Italian.             \"\"\"         ),     ),     FewShotExample(         question=\"What is the sports team the person played for who scored the first touchdown in Superbowl 1?\",         answer=inspect.cleandoc(             \"\"\"             Are follow up questions needed here: Yes.             Follow up: Which player scored the first touchdown in Superbowl 1?             Intermediate answer: Max McGee.             Follow up: Which sports team did Max McGee play for?             Intermediate answer: Max McGee played for the Green Bay Packers.             So the final answer is: Green Bay Packers.             \"\"\"         ),     ), ]   query = \"What was the primary language spoken by the inventor of the phonograph?\" response = dynamic_self_ask(query=query, examples=few_shot_examples, n=2) print(response.content) <pre>Are follow up questions needed here: Yes.  \nFollow up: Who invented the phonograph?  \nIntermediate answer: Thomas Edison.  \nFollow up: What language did Thomas Edison primarily speak?  \nIntermediate answer: Thomas Edison primarily spoke English.  \nSo the final answer is: English.\n</pre> <p>This enhanced version introduces the <code>select_relevant_examples</code> function, which uses TF-IDF vectorization and cosine similarity to find the most relevant examples for a given query. The <code>dynamic_self_ask</code> function then selects these relevant examples before including them in the prompt.</p>"},{"location":"tutorials/prompt_engineering/text_based/self_ask/#self-ask","title":"Self-Ask\u00b6","text":"<p>This recipe demonstrates how to implement the Self-Ask technique using Large Language Models (LLMs) with Mirascope. Self-Ask is a prompt engineering method that enhances an LLM's reasoning capabilities by encouraging it to ask and answer follow-up questions before providing a final answer. We'll explore both a basic implementation and an enhanced version with dynamic example selection.</p> <p>Additional Real-World Applications</p> <ul> <li>Automated Code Generation: Generating boilerplate or units tests for more productivity.</li> <li>Code Completion: Give LLM access to web to grab latest docs and generate code autocomplete suggestions.</li> <li>Documentation Maintenance: Make sure all documentation code snippets are runnable with proper syntax.</li> <li>Prototyping: Generating proof-of-concept applications rather than UI mocks.</li> </ul>"},{"location":"tutorials/prompt_engineering/text_based/self_ask/#setup","title":"Setup\u00b6","text":"<p>To set up our environment, first let's install all of the packages we will use:</p>"},{"location":"tutorials/prompt_engineering/text_based/self_ask/#basic-self-ask-implementation","title":"Basic Self-Ask Implementation\u00b6","text":"<p>Let's start with a basic implementation of Self-Ask using few-shot learning examples:</p>"},{"location":"tutorials/prompt_engineering/text_based/self_ask/#enhanced-self-ask-with-dynamic-example-selection","title":"Enhanced Self-Ask with Dynamic Example Selection\u00b6","text":"<p>Now, let's improve our implementation by adding dynamic example selection:</p>"},{"location":"tutorials/prompt_engineering/text_based/self_ask/#benefits-and-considerations","title":"Benefits and Considerations\u00b6","text":"<p>The enhanced Self-Ask implementation offers several advantages:</p> <ol> <li>Reduced prompt size by including only the most relevant examples.</li> <li>Potentially improved response quality by focusing on the most applicable few-shot examples.</li> <li>Ability to maintain a larger pool of examples without always including all of them in every query.</li> </ol> <p>When implementing this technique, consider:</p> <ul> <li>Balancing the number of selected examples with the desired prompt length and model context window.</li> <li>Experimenting with different similarity metrics or embedding techniques for example selection.</li> <li>Regularly updating your example pool to cover a wide range of query types and topics.</li> </ul> <p>Additional Real-World Applications</p> <ul> <li>Complex Problem Solving: Use Self-Ask for multi-step problems in fields like mathematics or engineering.</li> <li>Research Assistance: Implement Self-Ask to help researchers explore complex topics and formulate hypotheses.</li> <li>Legal Analysis: Apply Self-Ask to break down complex legal questions and explore relevant precedents.</li> <li>Medical Diagnosis: Use Self-Ask to guide through differential diagnosis processes.</li> <li>Customer Support: Implement Self-Ask to handle complex customer queries that require multiple pieces of information.</li> </ul> <p>When adapting this recipe to your specific use-case, consider:</p> <ul> <li>Tailoring the few-shot examples to your domain for better performance.</li> <li>Experimenting with different prompts and example formats to optimize the Self-Ask process.</li> <li>Implementing a feedback loop to continuously improve the quality of the Self-Ask responses.</li> <li>Combining Self-Ask with other techniques like chain-of-thought for even more powerful reasoning capabilities.</li> </ul> <p>By leveraging Mirascope's <code>call</code> decorator and <code>prompt_template</code>, you can easily implement and customize the Self-Ask technique to enhance your LLM's reasoning capabilities across a wide range of applications.</p>"},{"location":"tutorials/prompt_engineering/text_based/tabular_chain_of_thought/","title":"Tabular Chain of Thought","text":"In\u00a0[2]: Copied! <pre>from mirascope.core import openai, prompt_template\n\ntab_cot_augment = \"|step|subquestion|process|result|\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    {query}\n    {tab_cot_augment}\n    \"\"\"\n)\ndef call(query: str, tab_cot_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:\n    return {\n        \"computed_fields\": {\n            \"tab_cot_augment\": tab_cot_augment if tab_cot_prompt else \"\",\n        }\n    }\n\n\nprompt = \"\"\"A pet store had 102 puppies. In one day they sold 21 of them and put\nthe rest into cages with 9 in each cage. How many cages did they use?\"\"\"\n\nprint(call(query=prompt, tab_cot_prompt=True))\n</pre> from mirascope.core import openai, prompt_template  tab_cot_augment = \"|step|subquestion|process|result|\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     {query}     {tab_cot_augment}     \"\"\" ) def call(query: str, tab_cot_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:     return {         \"computed_fields\": {             \"tab_cot_augment\": tab_cot_augment if tab_cot_prompt else \"\",         }     }   prompt = \"\"\"A pet store had 102 puppies. In one day they sold 21 of them and put the rest into cages with 9 in each cage. How many cages did they use?\"\"\"  print(call(query=prompt, tab_cot_prompt=True)) <pre>| Step | Subquestion                     | Process                                                                                            | Result     |\n|------|---------------------------------|-----------------------------------------------------------------------------------------------------|------------|\n| 1    | How many puppies are left?     | Start with 102 puppies and subtract the 21 sold: 102 - 21 = 81                                   | 81 puppies |\n| 2    | How many cages are needed?     | Divide the remaining puppies by the number in each cage: 81 \u00f7 9 = 9                             | 9 cages    |\n| 3    | Check for remainder?           | Calculate the remainder when dividing 81 by 9: 81 mod 9 = 0 (no remainder, so no extra cage needed)| N/A        |\n\nFinal Result: The pet store used **9 cages**.\n</pre> <p>Tabular Chain of Thought is an extension of Chain of Thought, with the caveat that the LLM is asked to put each step of its reasoning process in a row of a Markdown table. The added structure can structure the LLM's reasoning and make it likelier to give a correct answer.</p> In\u00a0[3]: Copied! <pre>from mirascope.core import openai\n\n\n@openai.call(model=\"gpt-3.5-turbo\")\ndef call(query: str) -&gt; str:\n    return query\n\n\nprompt = \"\"\"\nA circle with radius 1 circumscribes (perfectly surrounds) an equilateral triangle.\nWhat's the area of the triangle?\n\"\"\"\ngeneric_response = call(prompt)\nengineered_response = call(f\"\"\"{prompt}. Explain your reasoning step by step,\nwith each step in a row in a markdown table.\"\"\")\n\nprint(generic_response)\nprint(\"\\n\\n\\n\")\nprint(engineered_response)\n</pre> from mirascope.core import openai   @openai.call(model=\"gpt-3.5-turbo\") def call(query: str) -&gt; str:     return query   prompt = \"\"\" A circle with radius 1 circumscribes (perfectly surrounds) an equilateral triangle. What's the area of the triangle? \"\"\" generic_response = call(prompt) engineered_response = call(f\"\"\"{prompt}. Explain your reasoning step by step, with each step in a row in a markdown table.\"\"\")  print(generic_response) print(\"\\n\\n\\n\") print(engineered_response) <pre>To find the area of the equilateral triangle that is circumscribed by the circle with radius 1, first we need to find the side length of the triangle.\n\nIn an equilateral triangle, all sides are equal. Let's label the side length as \"s\". \n\nThe radius of the circle will be the distance from the center of the circle to the midpoint of a side of the equilateral triangle. This forms a right triangle with the side of the equilateral triangle and half of the side of the equilateral triangle. Using the Pythagorean theorem, we have:\n\ns^2 = (s/2)^2 + 1^2\ns^2 = s^2/4 + 1\n3s^2/4 = 1\ns^2 = 4/3\ns = sqrt(4/3)\ns = 2/sqrt(3)\n\nNow that we have the side length of the equilateral triangle, we can find the area of the triangle using the formula:\n\nArea = (sqrt(3)/4)(s^2)\nArea = (sqrt(3)/4)(4/3)\nArea = sqrt(3)/3\n\nTherefore, the area of the equilateral triangle is sqrt(3)/3 units squared.\n\n\n\n\n| Step | Calculation                                      | Reasoning                                                                                  |\n|------|--------------------------------------------------|--------------------------------------------------------------------------------------------|\n| 1    | Equilateral triangle has all equal sides        | By definition, all sides of an equilateral triangle are equal                               |\n| 2    | The radius of the circle is 1                   | Given in the problem statement                                                            |\n| 3    | The radius of the circle is the distance         | From the center of the circle to any vertex of the triangle, which is also the altitude    |\n| 4    | The altitude of an equilateral triangle         | Is also the perpendicular bisector of any side of the triangle                            |\n| 5    | Dividing the angle at a vertex of the triangle  | into two equal parts gives two 30-60-90 right triangles                                  |\n| 6    | The side opposite the 60 degree angle in         | a 30-60-90 triangle is $\\sqrt{3}$ times the side opposite the 30 degree angle             |\n| 7    | The side opposite the 30 degree angle in a       | 30-60-90 triangle is 1 times the radius of the circle                                      |\n| 8    | Therefore, the side opposite the 60 degree       | angle in our equilateral triangle is $\\sqrt{3}$ times the radius of the circle, which is 1 |\n| 9    | Using the formula for the area of an equilateral | triangle with side length \"s\": $Area = \\frac{\\sqrt{3}}{4} * s^2$                           |\n| 10   | Substituting $s = \\sqrt{3}$                      | into the formula gives $Area = \\frac{\\sqrt{3}}{4} * (\\sqrt{3})^2 = \\frac{3\\sqrt{3}}{4}$     |\n| 11   | Thus, the area of the equilateral triangle       | circumscribed by a circle with radius 1 is $\\frac{3\\sqrt{3}}{4}$ units squared              |\n</pre> <p>For reference, <code>engineered_response</code> answer is correct.</p> <p>Effective Tabular Chain of Thought Usage</p> <ul> <li>Structured Reasoning: Use Tab-CoT to encourage the LLM to break down complex problems into clear, discrete steps.</li> <li>Improved Accuracy: The tabular format can lead to improved accuracy, especially in arithmetic and multi-step reasoning tasks.</li> <li>Easy Verification: The step-by-step tabular format makes it easier to verify the LLM's reasoning process.</li> <li>Consistency: Tab-CoT can help maintain consistency in the problem-solving approach across different queries.</li> <li>Visual Clarity: The table format provides a clear visual representation of the problem-solving process, which can be beneficial for understanding and presentation.</li> </ul> <p>Tabular Chain of Thought provides a structured approach to problem-solving that can enhance the LLM's reasoning capabilities. By organizing thoughts into a table format, it allows for clearer step-by-step analysis, which can lead to more accurate results, especially in complex arithmetic or logical reasoning tasks.</p>"},{"location":"tutorials/prompt_engineering/text_based/tabular_chain_of_thought/#tabular-chain-of-thought","title":"Tabular Chain of Thought\u00b6","text":"<p>Tabular Chain of Thought (Tab-CoT) is an extension of zero-shot Chain of Thought, with the caveat that the LLM is given a Markdown heading to structure each step of its response in an individual row of a Markdown table. The added structure can help the LLM's reasoning process and improves accuracy in arithmetic and reasoning tasks.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul>"},{"location":"tutorials/prompt_engineering/text_based/thread_of_thought/","title":"Thread of Thought","text":"In\u00a0[1]: Copied! <pre>from mirascope.core import openai, prompt_template\n\nrag_output = [\n    \"\"\"Apple Inc. was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and\nRonald Wayne. The company started in the garage of Jobs' childhood home in \nLos Altos, California.\"\"\",\n    \"\"\"Steve Jobs was a visionary entrepreneur and the co-founder of Apple Inc.\nHe played a key role in the development of the Macintosh, iPod, iPhone, and iPad.\"\"\",\n    \"\"\"Apple's headquarters, known as Apple Park, is located in Cupertino, California.\nThe campus, designed by Norman Foster, opened to employees in April 2017.\"\"\",\n    \"\"\"In 1977, Apple Computer, Inc. was incorporated. The Apple II, one of the first\nhighly successful mass-produced microcomputer products, was introduced that year.\"\"\",\n    \"\"\"Apple's first product, the Apple I, was sold as a fully assembled circuit board.\nThe idea for the company came from Steve Wozniak's interest in building a computer\nkit.\"\"\",\n    \"\"\"Steve Wozniak and Steve Jobs were high school friends before they founded Apple\ntogether. They were both members of the Homebrew Computer Club, where they exchanged\nideas with other computer enthusiasts.\"\"\",\n    \"\"\"The first Apple Store opened in Tysons Corner, Virginia, in May 2001.\nApple Stores have since become iconic retail spaces around the world.\"\"\",\n    \"\"\"Apple has a strong commitment to environmental sustainability. The company\naims to have its entire supply chain carbon neutral by 2030.\"\"\",\n    \"\"\"Ronald Wayne, the lesser-known third co-founder of Apple, sold his shares\nin the company just 12 days after it was founded. He believed the venture was too\nrisky and wanted to avoid potential financial loss.\"\"\",\n    \"\"\"In 1984, Apple launched the Macintosh, the first personal computer to feature\na graphical user interface and a mouse. This product revolutionized the computer\nindustry and set new standards for user-friendly design.\"\"\",\n]\n\n\ndef retrieve_passages(query: str):\n    \"\"\"Simulates RAG retrieval.\"\"\"\n    return rag_output\n\n\nthot_augment = \"\"\"Walk me through this context in manageable parts step by step,\nsummarizing and analyzing as we go\"\"\"\n\n\n@openai.call(model=\"gpt-4o-mini\")\n@prompt_template(\n    \"\"\"\n    As a content reviewer, I provide multiple retrieved passages about\n    this question; you need to answer the question.\n\n    {context}\n\n    {query} {thot_augment}\n    \"\"\"\n)\ndef call(query: str, thot_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:\n    passages = retrieve_passages(query)\n    context = [\n        f\"retrieved passage {i+1} is: {passage}\" for i, passage in enumerate(passages)\n    ]\n    return {\n        \"computed_fields\": {\n            \"context\": context,\n            \"thot_augment\": thot_augment if thot_prompt else \"\",\n        }\n    }\n\n\nprompt = \"Where was Apple founded?\"\n\nprint(call(query=prompt, thot_prompt=True))\n</pre> from mirascope.core import openai, prompt_template  rag_output = [     \"\"\"Apple Inc. was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne. The company started in the garage of Jobs' childhood home in  Los Altos, California.\"\"\",     \"\"\"Steve Jobs was a visionary entrepreneur and the co-founder of Apple Inc. He played a key role in the development of the Macintosh, iPod, iPhone, and iPad.\"\"\",     \"\"\"Apple's headquarters, known as Apple Park, is located in Cupertino, California. The campus, designed by Norman Foster, opened to employees in April 2017.\"\"\",     \"\"\"In 1977, Apple Computer, Inc. was incorporated. The Apple II, one of the first highly successful mass-produced microcomputer products, was introduced that year.\"\"\",     \"\"\"Apple's first product, the Apple I, was sold as a fully assembled circuit board. The idea for the company came from Steve Wozniak's interest in building a computer kit.\"\"\",     \"\"\"Steve Wozniak and Steve Jobs were high school friends before they founded Apple together. They were both members of the Homebrew Computer Club, where they exchanged ideas with other computer enthusiasts.\"\"\",     \"\"\"The first Apple Store opened in Tysons Corner, Virginia, in May 2001. Apple Stores have since become iconic retail spaces around the world.\"\"\",     \"\"\"Apple has a strong commitment to environmental sustainability. The company aims to have its entire supply chain carbon neutral by 2030.\"\"\",     \"\"\"Ronald Wayne, the lesser-known third co-founder of Apple, sold his shares in the company just 12 days after it was founded. He believed the venture was too risky and wanted to avoid potential financial loss.\"\"\",     \"\"\"In 1984, Apple launched the Macintosh, the first personal computer to feature a graphical user interface and a mouse. This product revolutionized the computer industry and set new standards for user-friendly design.\"\"\", ]   def retrieve_passages(query: str):     \"\"\"Simulates RAG retrieval.\"\"\"     return rag_output   thot_augment = \"\"\"Walk me through this context in manageable parts step by step, summarizing and analyzing as we go\"\"\"   @openai.call(model=\"gpt-4o-mini\") @prompt_template(     \"\"\"     As a content reviewer, I provide multiple retrieved passages about     this question; you need to answer the question.      {context}      {query} {thot_augment}     \"\"\" ) def call(query: str, thot_prompt: bool = False) -&gt; openai.OpenAIDynamicConfig:     passages = retrieve_passages(query)     context = [         f\"retrieved passage {i+1} is: {passage}\" for i, passage in enumerate(passages)     ]     return {         \"computed_fields\": {             \"context\": context,             \"thot_augment\": thot_augment if thot_prompt else \"\",         }     }   prompt = \"Where was Apple founded?\"  print(call(query=prompt, thot_prompt=True)) <pre>To answer the question \"Where was Apple founded?\" let's break down the information available in the retrieved passages step by step.\n\n### Step 1: Identify Founding Information\n\nFrom **retrieved passage 1**, we learn the following:\n- Apple Inc. was founded on April 1, 1976.\n- The founders are Steve Jobs, Steve Wozniak, and Ronald Wayne.\n- The company started in the garage of Jobs' childhood home.\n\n### Step 2: Analyze the Location\n\nThe precise location mentioned in **retrieved passage 1** is:\n- **Los Altos, California.**\nThis indicates that Apple was founded in a residential setting, specifically in a garage, which is a common story for many tech startups, illustrating humble beginnings.\n\n### Step 3: Confirming with Additional Context\n\nWhile the remaining passages provide various pieces of information about Apple, such as the development of its products and its incorporation, they do not provide an alternative founding location. Therefore, the core location remains unchanged by the additional context.\n\n### Step 4: Summarizing\n\nIn summary, Apple Inc. was founded in **Los Altos, California**, in the garage of Steve Jobs' childhood home. This information highlights the origins of a now-massive corporation, emphasizing that great companies can start in modest environments.\n\nThus, the answer to the question \"Where was Apple founded?\" is **Los Altos, California**.\n</pre> <p>Effective Thread of Thought Usage</p> <ul> <li>Use with Large Context: THoT is particularly effective when dealing with large amounts of retrieved information or context.</li> <li>Encourage Step-by-Step Analysis: The key phrase \"Walk me through this context in manageable parts step by step, summarizing and analyzing as we go\" prompts the LLM to break down and analyze information incrementally.</li> <li>Apply to Q&amp;A Tasks: THoT is especially useful for question-answering tasks that require processing and synthesizing information from multiple sources.</li> <li>Combine with Retrieval: THoT works well in combination with retrieval augmented generation (RAG) techniques.</li> <li>Review Intermediate Steps: Examine the LLM's step-by-step analysis to ensure it's properly interpreting and synthesizing the context.</li> </ul> <p>Thread of Thought enhances the zero-shot Chain of Thought approach by providing a more structured way for the LLM to process and analyze large amounts of context. This technique is particularly valuable for tasks that involve information retrieval and synthesis, allowing for more thorough and transparent reasoning in complex question-answering scenarios.</p>"},{"location":"tutorials/prompt_engineering/text_based/thread_of_thought/#thread-of-thought","title":"Thread of Thought\u00b6","text":"<p>Thread of Thought (THoT) is an extension of zero-shot Chain of Thought where the request to walk through the reasoning steps is improved. The paper tests the results of various phrases, but finds the best to be \"Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.\" It is applicable to reasoning and mathematical tasks just like CoT, but is most useful for tasks with retrieval / large amounts of context and Q and A on this context.</p> <p>Mirascope Concepts Used</p> <ul> <li>Prompts</li> <li>Calls</li> <li>Dynamic Configuration</li> </ul>"}]}